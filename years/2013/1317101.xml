<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NSF East Asia and Pacific Summer Institute (EAPSI) for FY 2013 in Japan</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2013</AwardEffectiveDate>
<AwardExpirationDate>05/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>5070.00</AwardTotalIntnAmount>
<AwardAmount>5070</AwardAmount>
<AwardInstrument>
<Value>Fellowship Award</Value>
</AwardInstrument>
<Organization>
<Code>01090000</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OISE</Abbreviation>
<LongName>Office Of Internatl Science &amp;Engineering</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anne Emig</SignBlockName>
<PO_EMAI>aemig@nsf.gov</PO_EMAI>
<PO_PHON>7032927241</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This action funds Jonathan Root of Boston University to conduct a research project in the Mathematical and Physical Sciences area during the summer of 2013 at the Institute of Statistical Mathematics in Tachikawa, Tokyo.  The project title is High Dimensional Pattern Analysis Algorithms Which Can Operate on General Types of Data. The host scientist is Dr. Kenji Fukumizu.&lt;br/&gt;&lt;br/&gt;The problem of understanding intelligence, both biological and artificial, is one of the most important scientific questions today. A current approach to this problem is through the topic of learning, or learning from data. The Fellow's research is in kernel methods with applications to learning theory. More specifically the work investigates a large dimensional theory for kernel methods, which&lt;br/&gt;has had little theoretical attention. In studying high dimensional kernel methods, it is reasonable to consider a series of domains of diverging dimensionality, for which a series of kernels is introduced. Thus this project considers the behavior of the kernel in the limit as both the dimensionality of X and the sample size diverge. In doing so, the spectrum associated to a kernel is studied.&lt;br/&gt;&lt;br/&gt;Broader impacts of an EAPSI fellowship include providing the Fellow a first-hand research experience outside the U.S.; an introduction to the science, science policy, and scientific infrastructure of the respective location; and an orientation to the society, culture and language.   These activities meet the NSF goal to educate for international collaborations early in the career of its scientists, engineers, and educators, thus ensuring a globally aware U.S. scientific workforce. In addition, the Fellow plans to publish the results of the reseach in peer-reviewed journals. Moreover, the Fellow will speak on the research at seminars at Boston University, and other universities upon invitation.</AbstractNarration>
<MinAmdLetterDate>05/16/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/16/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.079</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1317101</AwardID>
<Investigator>
<FirstName>Jonathan</FirstName>
<LastName>Root</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jonathan C Root</PI_FULL_NAME>
<EmailAddress/>
<PI_PHON>8022997683</PI_PHON>
<NSF_ID>000636204</NSF_ID>
<StartDate>05/16/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Root                    Jonathan       C</Name>
<CityName>Boston</CityName>
<ZipCode>024465321</ZipCode>
<PhoneNumber/>
<StreetAddress/>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM/>
<ORG_LGL_BUS_NAME/>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Root                    Jonathan       C]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>024465321</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramElement>
<ProgramReference>
<Code>5921</Code>
<Text>JAPAN</Text>
</ProgramReference>
<ProgramReference>
<Code>5978</Code>
<Text>EAST ASIA AND PACIFIC PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~5070</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Many modern applications of machine learning, ranging from computer vision to computational biology, require the analysis of large volumes of high-dimensional data. Complex statistical features are commonplace, including multi-modality, skewness, and rich dependency structures. A relatively new modeling framework to take into account these diverse statistical features is based on kernel embeddings of probability distributions. This project considers a high-dimensional view point of this framework in which the number of variables in the dataset of interest is of the same order of magnitude as the number of observations.</p> <p>In particular, the two sample problem addresses the question of whether two independent samples were drawn from the same distribution. The decision criterion is based on the value of a test statistic measuring the distance between the samples. We consider a test statistic based on the distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. Fixing the number of observations in the data set, it is not hard to show that the empirical estimate of this statistic has optimal estimation accuracy. We&nbsp;investigate the estimation accuracy and the asymptotic distribution of this empirical estimate under the assumption that the number of observations grows as the number of variables in the dataset diverges.</p><br> <p>            Last Modified: 12/01/2013<br>      Modified by: Jonathan&nbsp;C&nbsp;Root</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Many modern applications of machine learning, ranging from computer vision to computational biology, require the analysis of large volumes of high-dimensional data. Complex statistical features are commonplace, including multi-modality, skewness, and rich dependency structures. A relatively new modeling framework to take into account these diverse statistical features is based on kernel embeddings of probability distributions. This project considers a high-dimensional view point of this framework in which the number of variables in the dataset of interest is of the same order of magnitude as the number of observations.  In particular, the two sample problem addresses the question of whether two independent samples were drawn from the same distribution. The decision criterion is based on the value of a test statistic measuring the distance between the samples. We consider a test statistic based on the distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. Fixing the number of observations in the data set, it is not hard to show that the empirical estimate of this statistic has optimal estimation accuracy. We investigate the estimation accuracy and the asymptotic distribution of this empirical estimate under the assumption that the number of observations grows as the number of variables in the dataset diverges.       Last Modified: 12/01/2013       Submitted by: Jonathan C Root]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
