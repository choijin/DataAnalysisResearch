<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CC-NIE Networking Infrastructure: High Speed Science DMZ @ Johns Hopkins for Data Intensive Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>12/01/2013</AwardEffectiveDate>
<AwardExpirationDate>11/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>494874.00</AwardTotalIntnAmount>
<AwardAmount>494874</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This cyberinfrastucture project is focused on creating an advanced dedicated research network that will facilitate the transfer of large scientific datasets while eliminating the barriers of traditional packet based inspection technologies. The goal of this project is threefold: 1) to establish a robust research oriented Science DMZ that simplifies the network perimeter, 2) to expand high-speed connectivity to internal research groups throughout the institution, and 3) to connect to a newly constructed High Performance Research Computing Facility (HPRCF) opening in 2014.&lt;br/&gt;&lt;br/&gt;The ability to quickly transfer large scientific data sets from origination to destination is of paramount importance. The principal goal of this project will be the creation of a new network perimeter to significantly increase the transfer rates of large scale datasets. With network throughput speeds reaching 100G, traditional firewall technologies relying on packet based inspection introduce too much latency and need to be eliminated. The Science DMZ will embrace newer network protocols, such as Virtual Device Contexts and multi-packet layer switching to enable faster convergence.&lt;br/&gt;&lt;br/&gt;JHU is at the center of several data-intensive activities that are transforming whole communities across the country. The archive of the Sloan Digital Sky Survey, the world's most used astronomy facility, is hosted at JHU, and our 100TB public turbulence database is largely responsible for a new access paradigm to numerical simulations. The new HPRCF will create new collaborations between JHU and University of Maryland scientists. This project will transform the JHU backbone and bring high-speed connectivity to a much broader community.</AbstractNarration>
<MinAmdLetterDate>09/12/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/12/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1340754</AwardID>
<Investigator>
<FirstName>Jonathan</FirstName>
<LastName>Bagger</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jonathan A Bagger</PI_FULL_NAME>
<EmailAddress>bagger@aps.org</EmailAddress>
<PI_PHON>3012093200</PI_PHON>
<NSF_ID>000194302</NSF_ID>
<StartDate>09/12/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dean</FirstName>
<LastName>Zarriello</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dean Zarriello</PI_FULL_NAME>
<EmailAddress>deanz@jhu.edu</EmailAddress>
<PI_PHON>4105168668</PI_PHON>
<NSF_ID>000590877</NSF_ID>
<StartDate>09/12/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Stephanie</FirstName>
<LastName>Reel</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stephanie Reel</PI_FULL_NAME>
<EmailAddress>sreel@jhu.edu</EmailAddress>
<PI_PHON>4105168668</PI_PHON>
<NSF_ID>000644239</NSF_ID>
<StartDate>09/12/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Scott</FirstName>
<LastName>Zeger</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Scott Zeger</PI_FULL_NAME>
<EmailAddress>sz@jhu.edu</EmailAddress>
<PI_PHON>4105168668</PI_PHON>
<NSF_ID>000644322</NSF_ID>
<StartDate>09/12/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<StreetAddress2><![CDATA[Suite B001]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001910777</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>JOHNS HOPKINS UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001910777</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Johns Hopkins University]]></Name>
<CityName/>
<StateCode>MD</StateCode>
<ZipCode>212182685</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~494874</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to develop and execute a comprehensive infrastructure plan to create a dedicated research network for transferring massive scientific data sets both internally and externally. The ability to transfer large data sets throughout the institution and the world is essential in sharing information and promoting collaboration.</p> <p>&nbsp;To achieve the desired outcome, the PI and Co-PI&rsquo;s of the grant established three goals.&nbsp; (1) Create a high-speed dedicated and private research network that could transfer large data sets between research labs and three major campuses.&nbsp; (2) Establish a &ldquo;Science DMZ&rdquo; to facilitate high-speed data transfers to/from the outside world and (3) incorporate the high-speed research network and &ldquo;Science DMZ&rdquo; into the newly constructed High Performance Computing Cluster now known as MARCC (Maryland Advanced Research Computing Center).</p> <p>&nbsp;The principal success of the grant was the creation of a high-speed network backbone dedicated to research activities and the integration into the MARCC environment. As a result, the newly created research backbone is now known throughout the campuses as HorNET or Hopkins Research Network.&nbsp; The grant enabled (3) 100G points-of-presence on each of the major research campuses coupled with a mixture of 10G, 40G, and 100G endpoints strategically placed in research labs throughout Johns Hopkins. The outcome of this initiative resulted in the establishment of 15 new high-speed research lab connections. HorNET is also being used by researchers to carry out real time processes like visualization of results while data is being analyzed, as well as, post-processing of large amounts of data by applying visualization techniques. These processes would be extremely slow and, in some cases, not possible without the fast connectivity provided by HorNET. Finally, HorNet is also being used to move large amounts of data for backup and disaster recovery purposes. The Bloomberg School of Public Health is using HorNET to backup around 200 TB of existing data and copying daily snapshots with new files.</p> <p>&nbsp;A secondary goal of the grant was to connect a new jointly run High Performance Computing Facility by the University of Maryland and Johns Hopkins to HorNET. The effective functionality of Maryland Advanced Research Computing Center (MARCC) depends on three integral components: the infrastructure (building), the network, and the High Performance Computing hardware. In particular, the network is a fundamental component in data intensive research, as large amounts of data need to be transferred to MARCC from off-campus and more importantly from several research facilities distributed among the many Johns Hopkins&rsquo; campuses. The Hopkins Research Network (HorNet) allows optimized flow of data from the Homewood and East Baltimore campuses to MARCC so that researchers at the medical school can transfer several terabytes of data, for example, genome sequences or F-MRI data, in just a few hours and then take advantage of the computational power at MARCC to perform data analytics. Many researchers are moving large amounts of data to MARCC, so fast and sufficient bandwidth is critical to ensure success for all data transfer processes.</p> <p>&nbsp;The last goal of the proposal included the creation of a &ldquo;Science DMZ&rdquo;, which was established at the MARCC facility. The &ldquo;Science DMZ&rdquo; consists of high-speed interfaces coupled with the functionality of GLOBUS connect.&nbsp; The idea of creating Globus connect servers at distributed labs throughout the enterprise is gaining in popularity.</p> <p>&nbsp;In summary, this award had a profound impact on advancing and promoting the need for high-speed networking in the scientific community. Future plans include additional funding and growth for HorNET as well as continued collaboration...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to develop and execute a comprehensive infrastructure plan to create a dedicated research network for transferring massive scientific data sets both internally and externally. The ability to transfer large data sets throughout the institution and the world is essential in sharing information and promoting collaboration.   To achieve the desired outcome, the PI and Co-PIÆs of the grant established three goals.  (1) Create a high-speed dedicated and private research network that could transfer large data sets between research labs and three major campuses.  (2) Establish a "Science DMZ" to facilitate high-speed data transfers to/from the outside world and (3) incorporate the high-speed research network and "Science DMZ" into the newly constructed High Performance Computing Cluster now known as MARCC (Maryland Advanced Research Computing Center).   The principal success of the grant was the creation of a high-speed network backbone dedicated to research activities and the integration into the MARCC environment. As a result, the newly created research backbone is now known throughout the campuses as HorNET or Hopkins Research Network.  The grant enabled (3) 100G points-of-presence on each of the major research campuses coupled with a mixture of 10G, 40G, and 100G endpoints strategically placed in research labs throughout Johns Hopkins. The outcome of this initiative resulted in the establishment of 15 new high-speed research lab connections. HorNET is also being used by researchers to carry out real time processes like visualization of results while data is being analyzed, as well as, post-processing of large amounts of data by applying visualization techniques. These processes would be extremely slow and, in some cases, not possible without the fast connectivity provided by HorNET. Finally, HorNet is also being used to move large amounts of data for backup and disaster recovery purposes. The Bloomberg School of Public Health is using HorNET to backup around 200 TB of existing data and copying daily snapshots with new files.   A secondary goal of the grant was to connect a new jointly run High Performance Computing Facility by the University of Maryland and Johns Hopkins to HorNET. The effective functionality of Maryland Advanced Research Computing Center (MARCC) depends on three integral components: the infrastructure (building), the network, and the High Performance Computing hardware. In particular, the network is a fundamental component in data intensive research, as large amounts of data need to be transferred to MARCC from off-campus and more importantly from several research facilities distributed among the many Johns HopkinsÆ campuses. The Hopkins Research Network (HorNet) allows optimized flow of data from the Homewood and East Baltimore campuses to MARCC so that researchers at the medical school can transfer several terabytes of data, for example, genome sequences or F-MRI data, in just a few hours and then take advantage of the computational power at MARCC to perform data analytics. Many researchers are moving large amounts of data to MARCC, so fast and sufficient bandwidth is critical to ensure success for all data transfer processes.   The last goal of the proposal included the creation of a "Science DMZ", which was established at the MARCC facility. The "Science DMZ" consists of high-speed interfaces coupled with the functionality of GLOBUS connect.  The idea of creating Globus connect servers at distributed labs throughout the enterprise is gaining in popularity.   In summary, this award had a profound impact on advancing and promoting the need for high-speed networking in the scientific community. Future plans include additional funding and growth for HorNET as well as continued collaboration concerning the evolution of next generation "Science DMZ" solutions.       Last Modified: 12/21/2015       Submitted by: Dean Zarriello]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
