<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Small:Analog-to-Analog Compression: Fundamental Limits and Constructive Schemes</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>474978.00</AwardTotalIntnAmount>
<AwardAmount>474978</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Cozzens</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>For analog signals of high dimension, compression methods relying on the acquisition and quantization of the entire signal may not always be possible. It is hence of interest to develop methods for compressing information directly from the analog to the analog (A2A) domain. Two philosophies have proven successful in addressing the general problem of conveying analog information through an analog medium: the Shannon theoretic approach and the compressed sensing approach. This research investigates a third approach, which blends the generality of the Shannon theoretic approach with the practicality of the compressed sensing approach. The fundamental limits of A2A compression are investigated in single and multi-signal settings, and constructive schemes are proposed to achieve these limits.  &lt;br/&gt;&lt;br/&gt;The challenge of A2A compression is to achieve the maximal dimensionality reduction, i.e., a bandwidth reduction factor of the signal dimension per measurement, by  exploiting prior knowledge about the signal, which may include, but it is not limited to sparsity.  Wu-Verd√∫ have shown that the Renyi Information dimension (RID) is the fundamental limit for i.i.d. signals with known distributions. The RID is a very coarse measure of information, as many different signals lead to the same RID. This research investigates other signal features that influence the A2A compression performance beyond RID, particularly in the non-asymptotic regime. It investigates the A2A compression of signals which do not have an i.i.d. known distribution, and of multi-signal settings, where correlations among signals can be exploited. Finally, it proposes constructive schemes to achieve the fundamental limits using polar codes.</AbstractNarration>
<MinAmdLetterDate>06/12/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/12/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1319299</AwardID>
<Investigator>
<FirstName>Sergio</FirstName>
<LastName>Verdu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sergio Verdu</PI_FULL_NAME>
<EmailAddress>verdu@princeton.edu</EmailAddress>
<PI_PHON>6092585315</PI_PHON>
<NSF_ID>000313701</NSF_ID>
<StartDate>06/12/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Emmanuel</FirstName>
<LastName>Abbe</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Emmanuel A Abbe</PI_FULL_NAME>
<EmailAddress>emmanuelabbe@gmail.com</EmailAddress>
<PI_PHON>6092583090</PI_PHON>
<NSF_ID>000630884</NSF_ID>
<StartDate>06/12/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[4 New South]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~474978</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Renyi entropy and Renyi divergence evidence a long track record of usefulness in information theory and its applications. Alfred Renyi never got around to generalizing mutual information in a similar way. In fact, &nbsp;in the literature there are several possible ways to accomplish such generalization, most notably those suggested by Suguru Arimoto and Imre Csiszar. In contrast, we have introduced the non-commutative $\alpha$-mutual information measure which has strong connections with Sibson's information radius and with Gallager's error exponent function. We have shown several properties of the $\alpha$-mutual information such as its connection to Renyi entropy in the case of identical random variables, single-letterization, its explicit expression in several interesting discrete and analog channels, convexity/concavity, the data processing theorem, and the generalization of Fano's inequality. Furthermore, we have shown that Shannon's zero-error capacity is equal to the maximal zero-order mutual information. The usual converse and achievability results in the error exponent analysis of noisy communication channels admit particularly insightful expressions in terms of the $\alpha$-mutual information.&nbsp;<br />The project outcomes provide an information theoretic framework for analog-to-analog (A2A) compression beyond the first order R&eacute;nyi information fundamental limit, and proposes constructive schemes based on polar coding.<br />Our results extend the notion of R&eacute;nyi Information Dimension (RID) as an information measure from scalar random variables to a family of vector random variables. This allows us to define the joint, the conditional and the mutual RIDs between random variables. We have shown that the Hadamard matrices act as an extractor over the reals of the Re ?nyi information dimension (RID), in an analogous way to how they act as an extractor of the discrete entropy over finite fields. More precisely, we prove that the RID of an i.i.d. sequence of mixture random variables polarizes to the extremal values of 0 and 1 (corresponding to discrete and continuous distributions) when transformed by a Hadamard matrix. &nbsp; Further, we prove that the polarization pattern of the RID admits a closed form expression and follows exactly the Binary Erasure Channel (BEC) polarization pattern in the discrete setting.This gives a natural extension of the polarization phenomenon for the entropy over the finite fields to the RID over the reals. The connection is further developed by studying a universal notion of 'probabilistic girth' that is invariant to the field of the signal, connecting the problem of coding over finite fields to the problem of compressing analog signals with sparse probabilistic components. &nbsp;&nbsp;<br />We exploit this RID polarization to design deterministic partial Hadamard matrices for Compressed Sensing of i.i.d. sparse signals. The performance of resulting matrices is compared with that of traditional matrices in Compressed Sensing such as random Gaussian and random Hadamard matrices. Simulations provide evidence that the constructed matrices together with recovery algorithms such as l1-norm minimization provide comparable recovery performances while being explicitly constructed.&nbsp;</p><br> <p>            Last Modified: 09/29/2017<br>      Modified by: Sergio&nbsp;Verdu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Renyi entropy and Renyi divergence evidence a long track record of usefulness in information theory and its applications. Alfred Renyi never got around to generalizing mutual information in a similar way. In fact,  in the literature there are several possible ways to accomplish such generalization, most notably those suggested by Suguru Arimoto and Imre Csiszar. In contrast, we have introduced the non-commutative $\alpha$-mutual information measure which has strong connections with Sibson's information radius and with Gallager's error exponent function. We have shown several properties of the $\alpha$-mutual information such as its connection to Renyi entropy in the case of identical random variables, single-letterization, its explicit expression in several interesting discrete and analog channels, convexity/concavity, the data processing theorem, and the generalization of Fano's inequality. Furthermore, we have shown that Shannon's zero-error capacity is equal to the maximal zero-order mutual information. The usual converse and achievability results in the error exponent analysis of noisy communication channels admit particularly insightful expressions in terms of the $\alpha$-mutual information.  The project outcomes provide an information theoretic framework for analog-to-analog (A2A) compression beyond the first order R&eacute;nyi information fundamental limit, and proposes constructive schemes based on polar coding. Our results extend the notion of R&eacute;nyi Information Dimension (RID) as an information measure from scalar random variables to a family of vector random variables. This allows us to define the joint, the conditional and the mutual RIDs between random variables. We have shown that the Hadamard matrices act as an extractor over the reals of the Re ?nyi information dimension (RID), in an analogous way to how they act as an extractor of the discrete entropy over finite fields. More precisely, we prove that the RID of an i.i.d. sequence of mixture random variables polarizes to the extremal values of 0 and 1 (corresponding to discrete and continuous distributions) when transformed by a Hadamard matrix.   Further, we prove that the polarization pattern of the RID admits a closed form expression and follows exactly the Binary Erasure Channel (BEC) polarization pattern in the discrete setting.This gives a natural extension of the polarization phenomenon for the entropy over the finite fields to the RID over the reals. The connection is further developed by studying a universal notion of 'probabilistic girth' that is invariant to the field of the signal, connecting the problem of coding over finite fields to the problem of compressing analog signals with sparse probabilistic components.    We exploit this RID polarization to design deterministic partial Hadamard matrices for Compressed Sensing of i.i.d. sparse signals. The performance of resulting matrices is compared with that of traditional matrices in Compressed Sensing such as random Gaussian and random Hadamard matrices. Simulations provide evidence that the constructed matrices together with recovery algorithms such as l1-norm minimization provide comparable recovery performances while being explicitly constructed.        Last Modified: 09/29/2017       Submitted by: Sergio Verdu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
