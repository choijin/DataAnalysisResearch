<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:  Impact of the Summer Institutes on Faculty Teaching and Student Achievement</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>1655953.00</AwardTotalIntnAmount>
<AwardAmount>1655953</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11040200</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DUE</Abbreviation>
<LongName>Division Of Undergraduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Connie Della-Piana</SignBlockName>
<PO_EMAI>cdellapi@nsf.gov</PO_EMAI>
<PO_PHON>7032925309</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Over the last decade, the National Academies Summer Institutes (SIs) have trained almost 1,000 faculty and instructional staff in "scientific teaching", an approach to STEM instruction based on evidence about how people learn (1,2).  The SIs are designed to teach effective use of active learning and assessment with attention to fostering learning by diverse students.  The curriculum features an iterative approach to examining the learning benefits associated with these newly introduced teaching practices.  At present, only a few studies systematically examine the outcomes of faculty development efforts.  This project is an assessment of the effects of SI training on participants' teaching practices and their students' outcomes.  This project is developing an evaluation system involving a generalizable protocol for conceptualizing and measuring outcomes of faculty development efforts, using the Summer Institute populations to test this system and its protocol.  This effort to determine the outcomes of a long standing faculty development effort is a collaboration among Yale University, the University of Colorado - Boulder, Cornell University, and the University of Connecticut. &lt;br/&gt;&lt;br/&gt;Intellectual Merit:  This project is an important large-scale assessment of scientific teaching practices and their impact on student outcomes. The project is unique because it employs a systems approach to determine effects of the program on adoption of teaching practices and on student achievement. This work is designed to create knowledge about educational practices, instructor behavior change, and student outcomes, while developing broadly useful evaluation tools that can be widely disseminated to enable others to evaluate their STEM education initiatives.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  The results from the study of the institutes are designed to inform future faculty development efforts.  The resulting evaluation system provides tools for designing and conducting evaluations of complex projects.  In addition, the project is designed to develop important expertise for key participating researchers in the evaluation of faculty development for STEM education.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;1.        J. Handelsman, S. Miller, C. Pfund, Scientific Teaching.  (WH Freeman &amp; Co, 2007).&lt;br/&gt;2.        C. Pfund et al., Professional development. Summer institute to improve university science teaching. Science (New York, N.Y.) 324, 470 (2009).</AbstractNarration>
<MinAmdLetterDate>12/18/2013</MinAmdLetterDate>
<MaxAmdLetterDate>01/07/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1323258</AwardID>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Graham</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark J Graham</PI_FULL_NAME>
<EmailAddress>mark.graham@yale.edu</EmailAddress>
<PI_PHON>2034325550</PI_PHON>
<NSF_ID>000610828</NSF_ID>
<StartDate>08/01/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Graham</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark J Graham</PI_FULL_NAME>
<EmailAddress>mark.graham@yale.edu</EmailAddress>
<PI_PHON>2034325550</PI_PHON>
<NSF_ID>000610828</NSF_ID>
<StartDate>12/18/2013</StartDate>
<EndDate>08/01/2017</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Scott</FirstName>
<LastName>Strobel</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Scott A Strobel</PI_FULL_NAME>
<EmailAddress>scott.strobel@yale.edu</EmailAddress>
<PI_PHON>2034329772</PI_PHON>
<NSF_ID>000411390</NSF_ID>
<StartDate>10/12/2014</StartDate>
<EndDate>08/01/2017</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jo</FirstName>
<LastName>Handelsman</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jo E Handelsman</PI_FULL_NAME>
<EmailAddress>jo.handelsman@wisc.edu</EmailAddress>
<PI_PHON>6083164555</PI_PHON>
<NSF_ID>000158505</NSF_ID>
<StartDate>12/18/2013</StartDate>
<EndDate>10/12/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Yale University</Name>
<CityName>New Haven</CityName>
<ZipCode>065208327</ZipCode>
<PhoneNumber>2037854689</PhoneNumber>
<StreetAddress>Office of Sponsored Projects</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 208327]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<StateCode>CT</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CT03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043207562</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>YALE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>043207562</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Yale, Dept MCDB]]></Name>
<CityName>New Haven</CityName>
<StateCode>CT</StateCode>
<ZipCode>065208013</ZipCode>
<StreetAddress><![CDATA[219 Prospect St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CT03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7512</Code>
<Text>TUES-Type 3 Project</Text>
</ProgramElement>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~1655953</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In 2013, there were many questions about scientific teaching practices, instructor behavior change, and the impact of quality scientific teaching on student outcomes which remained unanswered.&nbsp;<em>What factors influence faculty adoption of scientific teaching? How might we promote student buy-in to active and engaged learning? What tools exist or need to be invented to facilitate faculty adoption of scientific teaching? How can practitioners persuade administrators to increase scientific teaching at the institutional level?&nbsp;</em>These questions, among others, motivated us to undertake a large-scale assessment of scientific teaching practices and their impact on student outcomes. The natural venue of our research became the Summer Institutes (SI).</p> <p>The Summer Institutes on Scientific Teaching, now called the National Institutes on Scientific Teaching, are multi day intensive workshops that expose college STEM instructors to the principles of evidence-based teaching. The goal of the Summer Institutes is to reduce the amount of passive lecturing in college STEM courses and increase the levels of more active and engaged learning. The long-term goal being to improve student outcomes such as increased performance and persistence in the sciences.&nbsp;</p> <p>Our project involved conducting a large-scale assessment of instructors? scientific teaching practices and their impact on student outcomes, and was unique because of its employment of a&nbsp;<em>systems approach&nbsp;</em>to large scale program evaluation. By taking&nbsp;<em>systems approach</em>, we mean that we considered the Summer Institutes to be a complex program with short-, mid-, and long-term outcomes. This acknowledgement of the ?system? in which program resides offered new perspective and allowed us to see: 1) which components of the Summer Institutes functioned well and led to positive faculty/student outcomes; and, 2) which components of the Summer Institutes were not well-developed and did not adequately lead to faculty or student outcomes.&nbsp;</p> <p>The results from studying the Summer Institutes were designed to: 1) inform the faculty development efforts of the Summer Institutes and how these impacted important student outcomes like engagement and persistence in science; 2) inform the evaluation design for other complex projects aimed at undergraduate STEM education reform; and, 3) develop future expertise in participating researchers ? like graduate students and postdocs ? to effectively evaluate faculty development efforts for STEM education.&nbsp;&nbsp;We accomplished these goals in the following ways.</p> <p>First, the results of our project informed the<em>&nbsp;faculty development efforts&nbsp;</em>of the Summer Institutes. Our efforts led us to clearly define Scientific Teaching (Couch et al., 2017), develop a framework and a measure of college student persistence in the sciences (Graham et al., 2013; Hanauer et al., 2017), measure student buy-in and student trust in their instructor in classes taught by SI-trained faculty (Cavanagh et al., 2018; Cavanagh et al., 2016), train TAs to employ scientific teaching (Chen et al., 2013), and more (see project publication list for full detail). Through our study of the Summer Institutes, certain themes such as buy-in, persistence, growth mindset, and trust emerged as major contributors to the success of the Summer Institutes.&nbsp;</p> <p>Second, the knowledge we created studying the faculty development efforts of the Summer Institutes can be translated to the<em>&nbsp;evaluation design of other complex projects</em>. Our work assessing and developing measures of buy-in, persistence, and trust in the undergraduate science context should be widely deployed by evaluators of other complex education programs. Additionally, our novel uses of the systems approach and pathway model development for complex programs have been shown to be effective in: 1) communicating complex programs to diverse stakeholders (Corwin et al., 2015; Hanauer et al., 2017; Reeves et al., 2020); and, 2) reflecting upon the design of undergraduate STEM courses, especially those taught by co-instructors (Chen et al., 2021).&nbsp;</p> <p>Third, the results of our project developed expertise for participating researchers in the evaluation of faculty development for STEM education. PI Mark Graham mentored several postdoctoral researchers ? Oriana Aragon, Megan Bathgate, Andrew Cavanagh, Brian Couch, and Cong Wang ? in Summer Institutes-related projects. Their significant involvement in this grant allowed them to develop skills in evaluation of faculty development for STEM education, an area of research that requires expertise developed through projects such as this one. Additionally, three undergraduates at Yale University worked on the project. One undergraduate, Amrutha Dorai, completed her undergraduate Cognitive Science honors thesis with the data collected from this project and with the PI Mark Graham as her thesis advisor. Several undergraduate students at Yale worked on a SI dataset to learn more about qualitative and quantitative data analysis and presentation. The&nbsp;fields of science education research, educational psychology, and large-scale program evaluation have all benefited from our collective work because the project has given us the opportunity to mentor several undergraduate and postdoctoral researchers who have gone on to work in these fields.</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/14/2021<br>      Modified by: Mark&nbsp;J&nbsp;Graham</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In 2013, there were many questions about scientific teaching practices, instructor behavior change, and the impact of quality scientific teaching on student outcomes which remained unanswered. What factors influence faculty adoption of scientific teaching? How might we promote student buy-in to active and engaged learning? What tools exist or need to be invented to facilitate faculty adoption of scientific teaching? How can practitioners persuade administrators to increase scientific teaching at the institutional level? These questions, among others, motivated us to undertake a large-scale assessment of scientific teaching practices and their impact on student outcomes. The natural venue of our research became the Summer Institutes (SI).  The Summer Institutes on Scientific Teaching, now called the National Institutes on Scientific Teaching, are multi day intensive workshops that expose college STEM instructors to the principles of evidence-based teaching. The goal of the Summer Institutes is to reduce the amount of passive lecturing in college STEM courses and increase the levels of more active and engaged learning. The long-term goal being to improve student outcomes such as increased performance and persistence in the sciences.   Our project involved conducting a large-scale assessment of instructors? scientific teaching practices and their impact on student outcomes, and was unique because of its employment of a systems approach to large scale program evaluation. By taking systems approach, we mean that we considered the Summer Institutes to be a complex program with short-, mid-, and long-term outcomes. This acknowledgement of the ?system? in which program resides offered new perspective and allowed us to see: 1) which components of the Summer Institutes functioned well and led to positive faculty/student outcomes; and, 2) which components of the Summer Institutes were not well-developed and did not adequately lead to faculty or student outcomes.   The results from studying the Summer Institutes were designed to: 1) inform the faculty development efforts of the Summer Institutes and how these impacted important student outcomes like engagement and persistence in science; 2) inform the evaluation design for other complex projects aimed at undergraduate STEM education reform; and, 3) develop future expertise in participating researchers ? like graduate students and postdocs ? to effectively evaluate faculty development efforts for STEM education.  We accomplished these goals in the following ways.  First, the results of our project informed the faculty development efforts of the Summer Institutes. Our efforts led us to clearly define Scientific Teaching (Couch et al., 2017), develop a framework and a measure of college student persistence in the sciences (Graham et al., 2013; Hanauer et al., 2017), measure student buy-in and student trust in their instructor in classes taught by SI-trained faculty (Cavanagh et al., 2018; Cavanagh et al., 2016), train TAs to employ scientific teaching (Chen et al., 2013), and more (see project publication list for full detail). Through our study of the Summer Institutes, certain themes such as buy-in, persistence, growth mindset, and trust emerged as major contributors to the success of the Summer Institutes.   Second, the knowledge we created studying the faculty development efforts of the Summer Institutes can be translated to the evaluation design of other complex projects. Our work assessing and developing measures of buy-in, persistence, and trust in the undergraduate science context should be widely deployed by evaluators of other complex education programs. Additionally, our novel uses of the systems approach and pathway model development for complex programs have been shown to be effective in: 1) communicating complex programs to diverse stakeholders (Corwin et al., 2015; Hanauer et al., 2017; Reeves et al., 2020); and, 2) reflecting upon the design of undergraduate STEM courses, especially those taught by co-instructors (Chen et al., 2021).   Third, the results of our project developed expertise for participating researchers in the evaluation of faculty development for STEM education. PI Mark Graham mentored several postdoctoral researchers ? Oriana Aragon, Megan Bathgate, Andrew Cavanagh, Brian Couch, and Cong Wang ? in Summer Institutes-related projects. Their significant involvement in this grant allowed them to develop skills in evaluation of faculty development for STEM education, an area of research that requires expertise developed through projects such as this one. Additionally, three undergraduates at Yale University worked on the project. One undergraduate, Amrutha Dorai, completed her undergraduate Cognitive Science honors thesis with the data collected from this project and with the PI Mark Graham as her thesis advisor. Several undergraduate students at Yale worked on a SI dataset to learn more about qualitative and quantitative data analysis and presentation. The fields of science education research, educational psychology, and large-scale program evaluation have all benefited from our collective work because the project has given us the opportunity to mentor several undergraduate and postdoctoral researchers who have gone on to work in these fields.          Last Modified: 07/14/2021       Submitted by: Mark J Graham]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
