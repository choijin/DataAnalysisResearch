<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:  Visual Cortex on Silicon</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1450000.00</AwardTotalIntnAmount>
<AwardAmount>1450000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The human vision system understands and interprets complex scenes for a wide range of visual tasks in real-time while consuming less than 20 Watts of power. This Expeditions-in-Computing project explores holistic design of machine vision systems that have the potential to approach and eventually exceed the capabilities of human vision systems. This will enable the next generation of machine vision systems to not only record images but also understand visual content. Such smart machine vision systems will have a multi-faceted impact on society, including visual aids for visually impaired persons, driver assistance for reducing automotive accidents, and augmented reality for enhanced shopping, travel, and safety. The transformative nature of the research will inspire and train a new generation of students in inter-disciplinary work that spans neuroscience, computing and engineering discipline.&lt;br/&gt;&lt;br/&gt;While several machine vision systems today can each successfully perform one or a few human tasks ? such as detecting human faces in point-and-shoot cameras ? they are still limited in their ability to perform a wide range of visual tasks, to operate in complex, cluttered environments, and to provide reasoning for their decisions.  In contrast, the mammalian visual cortex excels in a broad variety of goal-oriented cognitive tasks, and is at least three orders of magnitude more energy efficient than customized state-of-the-art machine vision systems. The proposed research envisions a holistic design of a machine vision system that will approach the cognitive abilities of the human cortex, by developing a comprehensive solution consisting of vision algorithms, hardware design, human-machine interfaces, and information storage. The project aims to understand the fundamental mechanisms used in the visual cortex to enable the design of new vision algorithms and hardware fabrics that can improve power, speed, flexibility, and recognition accuracies relative to existing machine vision systems. Towards this goal, the project proposes an ambitious inter-disciplinary research agenda that will (i) understand goal-directed visual attention mechanisms in the brain to design task-driven vision algorithms; (ii) develop vision theory and algorithms that scale in performance with increasing complexity of a scene; (iii) integrate complementary approaches in biological and machine vision techniques; (iv) develop a new-genre of computing architectures inspired by advances in both the understanding of the visual cortex and the emergence of electronic devices; and (v) design human-computer interfaces that will effectively assist end-users while preserving privacy and maximizing utility. These advances will allow us to replace current-day cameras with cognitive visual systems that more intelligently analyze and understand complex scenes, and dynamically interact with users.&lt;br/&gt;&lt;br/&gt;Machine vision systems that understand and interact with their environment in ways similar to humans will enable new transformative applications. The project will develop experimental platforms to: (1) assist visually impaired people; (2) enhance driver attention; and (3) augment reality to provide enhanced experience for retail shopping or a vacation visit, and enhanced safety for critical public infrastructure. This project will result in education and research artifacts that will be disseminated widely through a web portal and via online lecture delivery. The resulting artifacts and prototypes will enhance successful ongoing outreach programs to under-represented minorities and the general public, such as museum exhibits, science fairs, and a summer camp aimed at K-12 students. It will also spur similar new outreach efforts at other partner locations. The project will help identify and develop course material and projects directed at instilling interest in computing fields for students in four-year colleges. Partnerships with two Hispanic serving institutes, industry, national labs and international projects are also planned.</AbstractNarration>
<MinAmdLetterDate>09/17/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/03/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1317433</AwardID>
<Investigator>
<FirstName>Laurent</FirstName>
<LastName>Itti</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Laurent Itti</PI_FULL_NAME>
<EmailAddress>itti@pollux.usc.edu</EmailAddress>
<PI_PHON>2137403527</PI_PHON>
<NSF_ID>000487896</NSF_ID>
<StartDate>09/17/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890001</ZipCode>
<StreetAddress><![CDATA[3720 S. Flower St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7723</Code>
<Text>Expeditions in Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7723</Code>
<Text>EXPERIMENTAL EXPEDITIONS</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~580000</FUND_OBLG>
<FUND_OBLG>2014~290000</FUND_OBLG>
<FUND_OBLG>2015~580000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this project, we worked as a team to develop new collaborations which have yielded many new results. In collaboration with neuroscientists, we have shown for the first time a direct neural correlate of visual salience in the monkey brain, as predicted 20 years ago by our bottom-up visual saliency model. This project has been many years in the working and we are particularly pleased that it was completed (published in Nature Communications, with a follow-up in PNAS). As proposed, we have developed several new theories of top-down visual attention, especially as pertains to predicting eye movement behavior of humans and in integrating a new format theory of goal relevance into our models. Some of these models have been applied to machine vision and robotics. Others have been applied to the development of new screening tools for neurological disorders: through detailed comparison between saliency model predictions and recorded human gaze patterns while participants watch 5 minutes of TV, we have shown that one can reliably discover whether the participant has ADHD (attention deficit hyperactivity disorder) or FASD (fetal alcohol spectrum disorder) in children, or Parkinson&rsquo;s disease in the elderly.</p> <p>Broader impacts have been a particularly exciting component of this expedition. Each year, we have hosted a Robotics Open House, drawing 1,500 &ndash; 2,000 visitors each time, from elementary school students to adult professionals and tinkerers to college students. In addition, we have developed a new hardware and software framework to enable younger generations to use machine vision, including our neuromorophic algorithms, in their projects. We created JeVois, a tiny USB webcam that integrates a quad-core processor running Linux, OpenCV, TensorFlow, and many other software previously reserved to researchers, in the camera itself. JeVois has been developed and launched as a product through kickstarter, with about 8,000 units sold to date. The range of people interested in this project is the broadest ever reached through our research efforts, from middle-school students in FIRST Robotics competitions, to many startup companies, to tech giants like Intel, Google, Dyson, and Amazon.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/20/2019<br>      Modified by: Laurent&nbsp;Itti</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this project, we worked as a team to develop new collaborations which have yielded many new results. In collaboration with neuroscientists, we have shown for the first time a direct neural correlate of visual salience in the monkey brain, as predicted 20 years ago by our bottom-up visual saliency model. This project has been many years in the working and we are particularly pleased that it was completed (published in Nature Communications, with a follow-up in PNAS). As proposed, we have developed several new theories of top-down visual attention, especially as pertains to predicting eye movement behavior of humans and in integrating a new format theory of goal relevance into our models. Some of these models have been applied to machine vision and robotics. Others have been applied to the development of new screening tools for neurological disorders: through detailed comparison between saliency model predictions and recorded human gaze patterns while participants watch 5 minutes of TV, we have shown that one can reliably discover whether the participant has ADHD (attention deficit hyperactivity disorder) or FASD (fetal alcohol spectrum disorder) in children, or Parkinson’s disease in the elderly.  Broader impacts have been a particularly exciting component of this expedition. Each year, we have hosted a Robotics Open House, drawing 1,500 &ndash; 2,000 visitors each time, from elementary school students to adult professionals and tinkerers to college students. In addition, we have developed a new hardware and software framework to enable younger generations to use machine vision, including our neuromorophic algorithms, in their projects. We created JeVois, a tiny USB webcam that integrates a quad-core processor running Linux, OpenCV, TensorFlow, and many other software previously reserved to researchers, in the camera itself. JeVois has been developed and launched as a product through kickstarter, with about 8,000 units sold to date. The range of people interested in this project is the broadest ever reached through our research efforts, from middle-school students in FIRST Robotics competitions, to many startup companies, to tech giants like Intel, Google, Dyson, and Amazon.          Last Modified: 12/20/2019       Submitted by: Laurent Itti]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
