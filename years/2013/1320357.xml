<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Cumulon: Easy and Efficient Statistical Big-Data Analysis in the Cloud</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>499992.00</AwardTotalIntnAmount>
<AwardAmount>499992</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Aidong Zhang</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>"Big data" have been growing in volume and diversity at an explosive rate, bringing enormous potential for transforming science and society. Driven by the desire to convert data into insights, analysis has become increasingly statistical, and there are more people than ever interested in analyzing big data.  The rise of cloud computing in recent years offers a promising possibility for supporting big data analytics.  However, it remains frustratingly difficult for many scientists and statisticians to use the cloud for any non-trivial statistical analysis of big data.  &lt;br/&gt;&lt;br/&gt;The first challenge is development---users need to code and think in low-level, platform-specific ways, and, in many cases, resort to extensive manual tuning to achieve acceptable performance.  The second challenge is deployment---users are faced with a maddening array of choices, ranging from hardware provisioning (e.g., type and number of machines to request), software configuration (e.g., number of parallel execution slots per machine), to execution parameters and implementation alternatives.&lt;br/&gt;  &lt;br/&gt;This project aims to build Cumulon, an end-to-end solution for making statistical computing over big data easier and more efficient in the cloud.  For development, users can think and code in a declarative fashion, without worrying about how to map data and computation onto specific hardware and software.  For deployment, Cumulon presents users with best "plans" meeting their requirements, along with completion time and monetary cost to help them make decisions.  A plan encodes choices of not only implementation alternatives and execution parameters, but also cluster resource and configuration parameters. This project develops effective cost modeling and efficient optimization techniques for the vast search space of possible plans. Cumulon addresses the challenges of uncertainty and extensibility (in terms of not only functionality but also optimizability). Cumulon also features a performance trace repository, which collects data from past deployments and uses them to improve cost modeling and optimization.&lt;br/&gt;&lt;br/&gt;Cumulon aims to make statistical computing over big data easier and more cost-effective for a wide range of users including scientists and statisticians.  Besides leveraging the cloud to provide on-demand, pay-as-you-go access to computing resources, Cumulon further simplifies development and deployment, reduces reliance on programming and tuning support, and accelerates data-driven discoveries.  More than a one-shot solution, Cumulon is designed as a basis for an evolvable, open-source ecosystem that keeps up with advances in big-data analytics. Its repository of performance traces benefits the community in independent ways.  &lt;br/&gt;&lt;br/&gt;With the growing importance of quantitative, data-driven methods, Cumulon can impact many domains. The interdisciplinary team of PIs---from computer science, statistics, etc. ---is applying Cumulon to concrete applications in biomedical research and computational journalism. Through collaboration, the PIs seek to attract diverse talents, motivate them to work on problems with potential societal impacts, and help  prepare them for the new challenges of big data.&lt;br/&gt;&lt;br/&gt;For further information see the web site at: http://db.cs.duke.edu/projects/cumulon</AbstractNarration>
<MinAmdLetterDate>09/09/2013</MinAmdLetterDate>
<MaxAmdLetterDate>03/10/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1320357</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Ward</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael D Ward</PI_FULL_NAME>
<EmailAddress>michael.d.ward@duke.edu</EmailAddress>
<PI_PHON>9196604373</PI_PHON>
<NSF_ID>000293470</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jun</FirstName>
<LastName>Yang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jun Yang</PI_FULL_NAME>
<EmailAddress>junyang@cs.duke.edu</EmailAddress>
<PI_PHON>9196606587</PI_PHON>
<NSF_ID>000486379</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sayan</FirstName>
<LastName>Mukherjee</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sayan Mukherjee</PI_FULL_NAME>
<EmailAddress>sayan@stat.duke.edu</EmailAddress>
<PI_PHON>9196684747</PI_PHON>
<NSF_ID>000169747</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Shivnath</FirstName>
<LastName>Babu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shivnath Babu</PI_FULL_NAME>
<EmailAddress>shivnath@cs.duke.edu</EmailAddress>
<PI_PHON>9196606579</PI_PHON>
<NSF_ID>000488390</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate>03/10/2017</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277080129</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~330438</FUND_OBLG>
<FUND_OBLG>2015~169554</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>"Big data" have been growing in volume and diversity at an explosive rate, bringing enormous potential for transforming science and society. Driven by the desire to convert messy data into insights, analysis has become increasingly statistical, and there are more people than ever interested in analyzing big data. The rise of cloud computing in recent years, exemplified by the popularity of services such as Amazon EC2, offers a promising possibility for supporting big-data analytics. Its "pay-as-you-go" business model is especially attractive: users gain on-demand access to computing resources while avoiding hardware acquisition and maintenance costs. However, it remains frustratingly difficult for many scientists and statisticians to use the cloud for any nontrivial statistical analysis of big data. First, developing efficient statistical computing programs requires a great deal of expertise and effort.</p> <p>This project has built three generations of the Cumulon system, as an end-to-end solution for making statistical computing over big data easier and more efficient in the cloud. With Cumulon, when developing data analysis programs, users can think and code in a declarative fashion, without being concerned with how to map data and computation onto specific hardware and software platforms. When deploying such programs, Cumulon automatically suggests the best strategies meeting user requirements, along with information that is actually helpful in making decisions---in terms of completion time and monetary cost. These strategies make a wide array of choices, ranging from resource provisioning (e.g., type and number of machines to request on Amazon EC2), software configuration (e.g., number of parallel execution slots per machine for Hadoop), to execution parameters and implementation alternatives. The latest generation of Cumulon is able to leverage auction-based markets (e.g., Amazon spot instances) to lower execution costs further; it bids for machines dynamically and intelligently, adapts automatically to market fluctuations and execution progress, and recovers gracefully from sudden, massive departure of machines acquired through bidding.</p> <p>This project has also developed more efficient support for new standard hardware in the cloud such as solid-state drives, and explored how to apply Cumulon to statistical inferences in new directions. The project has resulted in a number of publications in high-impact research conferences and journals, as well as new tutorial and course materials. The project has graduated three PhD students and trained two junior ones. It also supported training of two postdoctoral researchers.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/30/2017<br>      Modified by: Jun&nbsp;Yang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ "Big data" have been growing in volume and diversity at an explosive rate, bringing enormous potential for transforming science and society. Driven by the desire to convert messy data into insights, analysis has become increasingly statistical, and there are more people than ever interested in analyzing big data. The rise of cloud computing in recent years, exemplified by the popularity of services such as Amazon EC2, offers a promising possibility for supporting big-data analytics. Its "pay-as-you-go" business model is especially attractive: users gain on-demand access to computing resources while avoiding hardware acquisition and maintenance costs. However, it remains frustratingly difficult for many scientists and statisticians to use the cloud for any nontrivial statistical analysis of big data. First, developing efficient statistical computing programs requires a great deal of expertise and effort.  This project has built three generations of the Cumulon system, as an end-to-end solution for making statistical computing over big data easier and more efficient in the cloud. With Cumulon, when developing data analysis programs, users can think and code in a declarative fashion, without being concerned with how to map data and computation onto specific hardware and software platforms. When deploying such programs, Cumulon automatically suggests the best strategies meeting user requirements, along with information that is actually helpful in making decisions---in terms of completion time and monetary cost. These strategies make a wide array of choices, ranging from resource provisioning (e.g., type and number of machines to request on Amazon EC2), software configuration (e.g., number of parallel execution slots per machine for Hadoop), to execution parameters and implementation alternatives. The latest generation of Cumulon is able to leverage auction-based markets (e.g., Amazon spot instances) to lower execution costs further; it bids for machines dynamically and intelligently, adapts automatically to market fluctuations and execution progress, and recovers gracefully from sudden, massive departure of machines acquired through bidding.  This project has also developed more efficient support for new standard hardware in the cloud such as solid-state drives, and explored how to apply Cumulon to statistical inferences in new directions. The project has resulted in a number of publications in high-impact research conferences and journals, as well as new tutorial and course materials. The project has graduated three PhD students and trained two junior ones. It also supported training of two postdoctoral researchers.          Last Modified: 11/30/2017       Submitted by: Jun Yang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
