<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CPS: Synergy: Collaborative Research: High-Level Perception and Control for Autonomous Reconfigurable Modular Robots</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>416000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bruce Kramer</SignBlockName>
<PO_EMAI>bkramer@nsf.gov</PO_EMAI>
<PO_PHON>7032925348</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of the project is the development of the theory, hardware and computational infrastructure that will enable automatically transforming user-defined, high-level tasks such as inspection of hazardous environments and object retrieval, into provably-correct control for modular robots. Modular robots are composed of simple individual modules; while a single module has limited capabilities, connecting multiple modules in different configurations allows the system to perform complex actions such as climbing, manipulating objects, traveling in unstructured environments and self-reconfiguring (breaking into multiple independent robots and reassembling into larger structures).  The project includes (i) defining and populating a large library of perception and actuation building blocks both manually through educational activities and automatically through novel algorithms, (ii) creating automated tools to assign values to probabilistic metrics associated with the performance of library components, (iii) developing a grammar and automated tools for control synthesis that sequence different components of the library to accomplish higher level tasks, if possible, or provide feedback to the user if the task cannot be accomplished and (iv) designing and building a novel modular robot platform capable of rapid and robust self-reconfiguration.&lt;br/&gt; &lt;br/&gt;This research will have several outcomes. First, it will lay the foundations for making modular robots easily controlled by anyone. This will enrich the robotic industry with new types of robots with unique capabilities. Second, the research will create novel algorithms that tightly combine perception, control and hardware capabilities. Finally, this project will create an open-source infrastructure that will allow the public to contribute basic controllers to the library thus promoting general research and social interest in robotics and engineering.</AbstractNarration>
<MinAmdLetterDate>09/09/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/15/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1329620</AwardID>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Yim</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark Yim</PI_FULL_NAME>
<EmailAddress>yim@grasp.upenn.edu</EmailAddress>
<PI_PHON>2158985269</PI_PHON>
<NSF_ID>000230349</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046315</ZipCode>
<StreetAddress><![CDATA[220 S. 33rd Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7918</Code>
<Text>CPS-Cyber-Physical Systems</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7918</Code>
<Text>CYBER-PHYSICAL SYSTEMS (CPS)</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~400000</FUND_OBLG>
<FUND_OBLG>2014~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 2"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>This research develops the theory, hardware and computational infrastructure that will enable automatically transforming user &shy;defined, high-&shy;level tasks into correct, low&shy;-level perception informed control and configurations for modular robots. This is a collaborative project &nbsp;where University of Pennsylvania focuses on developing the hardware and low-level software and Cornell University focuses on developing&nbsp;&nbsp;the software and library development. </span></p> <p><span>The research approach has three technical components: designing a robust reconfigurable hardware platform (UPenn), building a basic component library of useful but simple behaviors with an infrastructure to populate this library (Cornell and UPenn), and combining entries in his library in a provably&shy;correct way to create complex behaviors and perform high&shy;level tasks (Cornell).&nbsp;</span></p> </div> </div> </div> </div> <div class="page" title="Page 2"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>We have developed three prototypes of a second generation SMORES robot. These cube-shaped modules have round docking faces that can act like wheels to drive around an environment dock with other modules to form larger articulated robots (e.g. snake-shaped or dog-shaped) automatically.</span></p> <p>These docking faces include a novel electro&shy; permanent (EP) magnet latching mechanism for the self-&shy;reconfigurable robot system. This hardware has demonstrated the functionality of locomotion, manipulation of ferrous objects (with the magnetic latches), docking and undocking with other SMORES modules. Twenty-five modules have been constructed with a substantial subset delivered to Cornell for algorithm co&shy;development.&nbsp;</p> <p><span>&nbsp;</span>In collaboration with Cornell, we have developed a system for composing configurations sensing an environment and automatically reconfiguring to achieve tasks.</p> <p>An example of the capabilities we have developed is illustrated by the demonstration task where the robot needs to find and move two objects in an indoor environment. The objects are automatically identified by unique colors. The environment is sensed using an RGB-D sensor carried by the robot as a special module. The environment is understood well enough to determine under what conditions a given robot configuration will collide with the environment and a new configuration is required. The robot self-reconfigures, grasps the objects with the magnetic EP faces and delivers them to the target location.</p> <p>This simple example demonstrates the integration of several fundamental operations: 3D sensing and building of an environment from a mobile sensor, understanding the environment relative to a robot's own shape (one that is reconfigurable), adaptation of the robot's configuration to adapt to the requirements of the environment, path planning and self-reconfiguration planning, task planning and task behaviour construction.</p> <p>All of these elements together can lead to many more important autonomous tasks that will be more generically useful using self-reconfigurable robots so that they can adapt their shape to required tasks, and programmed easily from a library of tasks.</p> </div> </div> </div> </div> <p>&nbsp;</p><br> <p>            Last Modified: 12/29/2016<br>      Modified by: Mark&nbsp;Yim</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1329620/1329620_10277178_1483070296171_smoresreconfigdemo--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1329620/1329620_10277178_1483070296171_smoresreconfigdemo--rgov-800width.jpg" title="SMORES reconfiguration"><img src="/por/images/Reports/POR/2016/1329620/1329620_10277178_1483070296171_smoresreconfigdemo--rgov-66x44.jpg" alt="SMORES reconfiguration"></a> <div class="imageCaptionContainer"> <div class="imageCaption">SMORES robot reasons that it must reconfigure the modules into an arm to reach into the space to retrieve the object.</div> <div class="imageCredit">Univ. of Pennsylvania and Cornell Univ.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Mark&nbsp;Yim</div> <div class="imageTitle">SMORES reconfiguration</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[     This research develops the theory, hardware and computational infrastructure that will enable automatically transforming user &shy;defined, high-&shy;level tasks into correct, low&shy;-level perception informed control and configurations for modular robots. This is a collaborative project  where University of Pennsylvania focuses on developing the hardware and low-level software and Cornell University focuses on developing  the software and library development.   The research approach has three technical components: designing a robust reconfigurable hardware platform (UPenn), building a basic component library of useful but simple behaviors with an infrastructure to populate this library (Cornell and UPenn), and combining entries in his library in a provably&shy;correct way to create complex behaviors and perform high&shy;level tasks (Cornell).           We have developed three prototypes of a second generation SMORES robot. These cube-shaped modules have round docking faces that can act like wheels to drive around an environment dock with other modules to form larger articulated robots (e.g. snake-shaped or dog-shaped) automatically.  These docking faces include a novel electro&shy; permanent (EP) magnet latching mechanism for the self-&shy;reconfigurable robot system. This hardware has demonstrated the functionality of locomotion, manipulation of ferrous objects (with the magnetic latches), docking and undocking with other SMORES modules. Twenty-five modules have been constructed with a substantial subset delivered to Cornell for algorithm co&shy;development.    In collaboration with Cornell, we have developed a system for composing configurations sensing an environment and automatically reconfiguring to achieve tasks.  An example of the capabilities we have developed is illustrated by the demonstration task where the robot needs to find and move two objects in an indoor environment. The objects are automatically identified by unique colors. The environment is sensed using an RGB-D sensor carried by the robot as a special module. The environment is understood well enough to determine under what conditions a given robot configuration will collide with the environment and a new configuration is required. The robot self-reconfigures, grasps the objects with the magnetic EP faces and delivers them to the target location.  This simple example demonstrates the integration of several fundamental operations: 3D sensing and building of an environment from a mobile sensor, understanding the environment relative to a robot's own shape (one that is reconfigurable), adaptation of the robot's configuration to adapt to the requirements of the environment, path planning and self-reconfiguration planning, task planning and task behaviour construction.  All of these elements together can lead to many more important autonomous tasks that will be more generically useful using self-reconfigurable robots so that they can adapt their shape to required tasks, and programmed easily from a library of tasks.              Last Modified: 12/29/2016       Submitted by: Mark Yim]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
