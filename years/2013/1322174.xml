<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Pilot Investigation of Using Gaze in a Reading Tutor</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2013</AwardEffectiveDate>
<AwardExpirationDate>12/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Janet L. Kolodner</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The big question the PIs are addressing in this project is how to unobtrusively track silent reading of novice readers so as to be able to use an intelligent tutoring system to aid reading comprehension. This EAGER project focuses on the first steps in answering that question. This pilot project builds on previous work in vision and speech technology, sensor fusion, machine learning, user modeling, intelligent tutors, and eye movements in an effort to identify the feasibility of using eye tracking techniques, along with other information collected from an intelligent reading tutor, to predict reading difficulties of novice/young readers. In particular, the work plan includes collecting gaze data in real-world conditions in the context of using the existing Reading Tutor, designing software to display those traces so that accuracy can be guauged, testing gazepoint accuracy and detecting gaze-speech discrepancies, and using that data to develop heuristics for detecting tracking errors in real time and calibrating eye tracking data to noisy school environments, a primary environment where the augmented Reading Tutor would ultimately be used. The intellectual merit of this project is in identifying and addressing challenges in relating children's gaze data to their silent reading, in making technical contributions to calibrating eye trackers so that they can be used in normal everyday applications, and in setting the stage for intelligent tutors across diverse domains to exploit gaze more broadly.&lt;br/&gt;&lt;br/&gt;The project's most important potential broader impacts is in establishing a foundation for exploiting gaze input to build intelligent computing systems that can be used to help children with reading difficulties learn to read and read to learn. If successful, the PIs will develop a larger project that will extent the successful Project Listen Reading Tutor so that it can track readers as they are reading silently and help them with their comprehension -- both comprehension of text itself and strategies for coming to deep understanding.</AbstractNarration>
<MinAmdLetterDate>03/20/2013</MinAmdLetterDate>
<MaxAmdLetterDate>03/20/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1322174</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Mostow</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David J Mostow</PI_FULL_NAME>
<EmailAddress>mostow@cs.cmu.edu</EmailAddress>
<PI_PHON>4122681330</PI_PHON>
<NSF_ID>000112424</NSF_ID>
<StartDate>03/20/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jessica</FirstName>
<LastName>Nelson-Taylor</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jessica Nelson-Taylor</PI_FULL_NAME>
<EmailAddress>jrnelson@cs.cmu.edu</EmailAddress>
<PI_PHON>4122689527</PI_PHON>
<NSF_ID>000638664</NSF_ID>
<StartDate>03/20/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8020</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>8045</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The educational focus of this work is arguably the most important cross-cutting skill in modern society: &nbsp;reading. &nbsp;Reading is a high-bandwidth source of information to the reader, yet the act of reading typically provides little or no information to an automated tutor about what the reader did, or did not, learn from the text. &nbsp;Current methods to assess comprehension of a given text are time-consuming and obtrusive. &nbsp;Unobtrusive methods to monitor reading comprehension in real-time would be invaluable for intelligent tutors and many other applications. &nbsp;This project addressed this goal by using speech and gaze input to analyze oral and silent reading. &nbsp;It focused on children in grades 2-4 at the crucial transition from learning to read, to reading to learn, but results will likely also apply to older children and adults.<br />Intellectual Merit:This project builds on a unique school-deployed platform developed with prior support from the National Science Foundation and the Department of Education. &nbsp;Project LISTEN's automated Reading Tutor listens to children read aloud, and helps them learn to read. &nbsp;It offers a compelling combination of advantages for the proposed work. &nbsp;First, children already use the Reading Tutor regularly at school. &nbsp;This usage made it possible to collect gaze data during authentic educational activities in ecologically valid settings, not just artificial tasks in short lab experiments. &nbsp;Second, the Reading Tutor already logs detailed, timestamped, multi-resolution, longitudinal data to its database, including children's speech and its own actions. &nbsp;The gaze measurements were indexed by millisecond-level timestamps to this data. &nbsp;Third, the Reading Tutor already embeds within-subject randomized trials in its tutorial interactions. &nbsp;This capability enabled controlled experiments to test the effects of different text features on children's gaze and speech patterns.&nbsp; &nbsp;This work built on previous work in vision and speech technology, sensor fusion, machine learning, user modeling, intelligent tutors, cognitive psychology, and decades of research on eye movements in reading. &nbsp;For example, eye-voice span -- the distance from the word being read aloud to the word the reader is looking at -- has been shown to be a sensitive indicator of reading proficiency, comprehension of the text, and word-to-word fluctuation in cognitive load.&nbsp; &nbsp;Analysis drew on existing tools to mine data logged by the Reading Tutor. &nbsp;The project used statistical and machine learning methods to train models on noisy speech and gaze data to monitor engagement, decoding, fluency, vocabulary, and comprehension.&nbsp; &nbsp;Expected contributions of this work include advances in relating gaze data to children's oral and silent reading, thereby establishing a foundation for exploiting gaze input to help children learn to read and read to learn, as well as technical contributions such as methods to maintain eye tracker calibration by exploiting normal application input.<br />&nbsp; &nbsp;Broader impacts: &nbsp;This research incorporated direct participation of students, extended the Reading Tutor into an even more powerful research platform to study intelligent tutoring, guided the improvement of its reading instruction, contributed to the literacy of children who use the Reading Tutor, and paved the way for exploiting gaze input in intelligent tutors across diverse domains.&nbsp;</p> <p>&nbsp;</p> <p>By tracking a student's gaze while reading text on a computer screen, we were able to darken the computer screen when a child's gaze left the screen either from looking away or fidgiting. This "Rumblestrip" feature developed during this project proved to be a useful tool for determining when children's gaze drifted from the screen and reminded the student to sit still or return their atten...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The educational focus of this work is arguably the most important cross-cutting skill in modern society:  reading.  Reading is a high-bandwidth source of information to the reader, yet the act of reading typically provides little or no information to an automated tutor about what the reader did, or did not, learn from the text.  Current methods to assess comprehension of a given text are time-consuming and obtrusive.  Unobtrusive methods to monitor reading comprehension in real-time would be invaluable for intelligent tutors and many other applications.  This project addressed this goal by using speech and gaze input to analyze oral and silent reading.  It focused on children in grades 2-4 at the crucial transition from learning to read, to reading to learn, but results will likely also apply to older children and adults. Intellectual Merit:This project builds on a unique school-deployed platform developed with prior support from the National Science Foundation and the Department of Education.  Project LISTEN's automated Reading Tutor listens to children read aloud, and helps them learn to read.  It offers a compelling combination of advantages for the proposed work.  First, children already use the Reading Tutor regularly at school.  This usage made it possible to collect gaze data during authentic educational activities in ecologically valid settings, not just artificial tasks in short lab experiments.  Second, the Reading Tutor already logs detailed, timestamped, multi-resolution, longitudinal data to its database, including children's speech and its own actions.  The gaze measurements were indexed by millisecond-level timestamps to this data.  Third, the Reading Tutor already embeds within-subject randomized trials in its tutorial interactions.  This capability enabled controlled experiments to test the effects of different text features on children's gaze and speech patterns.   This work built on previous work in vision and speech technology, sensor fusion, machine learning, user modeling, intelligent tutors, cognitive psychology, and decades of research on eye movements in reading.  For example, eye-voice span -- the distance from the word being read aloud to the word the reader is looking at -- has been shown to be a sensitive indicator of reading proficiency, comprehension of the text, and word-to-word fluctuation in cognitive load.   Analysis drew on existing tools to mine data logged by the Reading Tutor.  The project used statistical and machine learning methods to train models on noisy speech and gaze data to monitor engagement, decoding, fluency, vocabulary, and comprehension.   Expected contributions of this work include advances in relating gaze data to children's oral and silent reading, thereby establishing a foundation for exploiting gaze input to help children learn to read and read to learn, as well as technical contributions such as methods to maintain eye tracker calibration by exploiting normal application input.    Broader impacts:  This research incorporated direct participation of students, extended the Reading Tutor into an even more powerful research platform to study intelligent tutoring, guided the improvement of its reading instruction, contributed to the literacy of children who use the Reading Tutor, and paved the way for exploiting gaze input in intelligent tutors across diverse domains.      By tracking a student's gaze while reading text on a computer screen, we were able to darken the computer screen when a child's gaze left the screen either from looking away or fidgiting. This "Rumblestrip" feature developed during this project proved to be a useful tool for determining when children's gaze drifted from the screen and reminded the student to sit still or return their attention to the computer screen. This feature has a potential to be used in all areas of education or any field which makes use of computers.       Last Modified: 03/28/2014       Submitted by: David J Mostow]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
