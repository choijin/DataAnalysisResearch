<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Accomplisment Based Renewal (ABR) to the award Flight-Worthy Condor: Enabling Scientific Discovery</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>7499960.00</AwardTotalIntnAmount>
<AwardAmount>7499960</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In the foreseen future a mix of changes in technologies, user and application requirements and the business model of delivering computing capacity will continue to pose new challenges to the effectiveness of high throughput computing (HTC) technologies.  To address these challenges, this ongoing research and development effort will devise new policy-driven capabilities to increase throughput within a defined budget by effectively managing extremely large workloads of homogenous jobs running on homogenous machines provisioned by cloud services. These capabilities will be augmented with effective schedulers for servers that have multiple cores of execution, many disks, perhaps several GPUs each with different capabilities, and multiple networking interfaces. New communities will be introduced to the power of HTC through customized, easy to deploy and secure software. Novel tools for profiling the requirements and dependencies of scientific applications will expand the reach of distributed computing infrastructures that leverage advanced networks to cross institutional and national boundaries. &lt;br/&gt;&lt;br/&gt;The advances in HTC technologies will be delivered through the widely adopted HTCondor software tools. More than 150 domestic universities, a growing number of national and international science communities and a wide spectrum of commercial organizations employ HTCondor to improve the throughput of their compute and data intensive applications. This project  will sustain a software engineering process that enables translational work to occur in a transitional manner, building upon the previous generation of software while simultaneously continuing to offer and support dependable software that is suitable to handle the ever growing amounts of experimental and simulated scientific data.</AbstractNarration>
<MinAmdLetterDate>06/28/2013</MinAmdLetterDate>
<MaxAmdLetterDate>03/18/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1321762</AwardID>
<Investigator>
<FirstName>Miron</FirstName>
<LastName>Livny</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Miron Livny</PI_FULL_NAME>
<EmailAddress>miron@cs.wisc.edu</EmailAddress>
<PI_PHON>6083164336</PI_PHON>
<NSF_ID>000340383</NSF_ID>
<StartDate>06/28/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Todd</FirstName>
<LastName>Tannenbaum</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Todd Tannenbaum</PI_FULL_NAME>
<EmailAddress>tannenba@cs.wisc.edu</EmailAddress>
<PI_PHON>6082637132</PI_PHON>
<NSF_ID>000334476</NSF_ID>
<StartDate>06/28/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Wisconsin-Madison</Name>
<CityName>MADISON</CityName>
<ZipCode>537151218</ZipCode>
<PhoneNumber>6082623822</PhoneNumber>
<StreetAddress>21 North Park Street</StreetAddress>
<StreetAddress2><![CDATA[Suite 6401]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<StateCode>WI</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WI02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>161202122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WISCONSIN SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041188822</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Wisconsin-Madison]]></Name>
<CityName>Madison</CityName>
<StateCode>WI</StateCode>
<ZipCode>537061613</ZipCode>
<StreetAddress><![CDATA[1210 West Dayton Stree]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WI02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramElement>
<Code>7369</Code>
<Text>International Res Ret Connect</Text>
</ProgramElement>
<ProgramElement>
<Code>7553</Code>
<Text>PHYSICS AT THE INFO FRONTIER</Text>
</ProgramElement>
<ProgramElement>
<Code>8027</Code>
<Text>Cybersecurity Innovation</Text>
</ProgramElement>
<ProgramReference>
<Code>7245</Code>
<Text>PHYSICS GRID COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>7369</Code>
<Text>INTERNATIONAL RES NET CONNECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>7553</Code>
<Text>PHYSICS AT THE INFO FRONTIER</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~6579960</FUND_OBLG>
<FUND_OBLG>2014~230000</FUND_OBLG>
<FUND_OBLG>2015~230000</FUND_OBLG>
<FUND_OBLG>2016~230000</FUND_OBLG>
<FUND_OBLG>2017~230000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Scientific advancements today increasingly rely upon sustained large capacity computing power, or <em>high-throughput computing</em>, which organizes hardware resources into effective capacity for science.&nbsp; This can be thought as building a &ldquo;community of computers&rdquo;, allowing sharing of resources and the democratization of access to these resources. Researchers from a growing number of science domains require results from heterogeneous workloads that may include hundreds of thousands of computational tasks. Because individual institutions may not have enough local computing resources, <em>distributed high-throughput computing</em> technologies enable the sharing of computing power across institutional boundaries. Following methodologies and computing principals that were developed and refined over more than three decades, this project translated innovations in distributed computing into widely deployed software tools to advance the impact of distributed high-throughput computing on scientific discovery. The enhanced capabilities of the <em>HTCondor Software Suite</em> implemented by this project enable more researchers to use capacity to compute larger ensembles of computational tasks with less effort.&nbsp; It allows organizations to deploy and operate computing environments that federate resources.&nbsp; These resources can span from a user&rsquo;s laptop to hundreds of thousands of heterogenous servers spanning standalone servers, institutional clusters, national facilities, and commercial clouds.</p> <p>Dynamic heterogeneous computing environments that cross administrative and physical boundaries coupled with dependable task management software present a powerful capability with broad impact. The HTCondor Software Suite developed by this project has seen wide-spread adoption and is now deployed in production at hundreds of universities, government labs, and commercial organizations. HTCondor has been used to manage the computational workloads ranging from homework assignments for graduate students to the computational work of young investigators to international Nobel-prize winning collaborations. Commercial organizations across the country have also benefitted from the technology advancements of this project; the HTCondor Software Suite is deployed in the private sector to produce animated feature films, perform risk assessment of insurance policies and investment portfolios, and manage continuous software build and assurance frameworks for government agencies. Engineers have been using HTCondor to design planes, develop missile systems, and simulate wind turbines.</p> <p>Sharing, mutual trust and autonomy are the principals that underpin the HTCondor Software Suite. The outreach, training and education activities of the project have presented these principles in the context of a widely used operational software. As part of its efforts to help democratize research computing, the project strived to ensure the technologies and the underpinning vision meet the needs of universities that are starting to formulate and build their computational research capabilities.&nbsp; The broad impact of the software suite developed and maintained by this project helped to communicate the value and applicability of these principles to students, educators, and the public. The Open Science Grid consortium and fabric of distributed high-throughput computing services it provides are a clear display for the transformative power such a combination of principles and software tools can have on research and education at the national level.</p><br> <p>            Last Modified: 02/19/2021<br>      Modified by: Miron&nbsp;Livny</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Scientific advancements today increasingly rely upon sustained large capacity computing power, or high-throughput computing, which organizes hardware resources into effective capacity for science.  This can be thought as building a "community of computers", allowing sharing of resources and the democratization of access to these resources. Researchers from a growing number of science domains require results from heterogeneous workloads that may include hundreds of thousands of computational tasks. Because individual institutions may not have enough local computing resources, distributed high-throughput computing technologies enable the sharing of computing power across institutional boundaries. Following methodologies and computing principals that were developed and refined over more than three decades, this project translated innovations in distributed computing into widely deployed software tools to advance the impact of distributed high-throughput computing on scientific discovery. The enhanced capabilities of the HTCondor Software Suite implemented by this project enable more researchers to use capacity to compute larger ensembles of computational tasks with less effort.  It allows organizations to deploy and operate computing environments that federate resources.  These resources can span from a user’s laptop to hundreds of thousands of heterogenous servers spanning standalone servers, institutional clusters, national facilities, and commercial clouds.  Dynamic heterogeneous computing environments that cross administrative and physical boundaries coupled with dependable task management software present a powerful capability with broad impact. The HTCondor Software Suite developed by this project has seen wide-spread adoption and is now deployed in production at hundreds of universities, government labs, and commercial organizations. HTCondor has been used to manage the computational workloads ranging from homework assignments for graduate students to the computational work of young investigators to international Nobel-prize winning collaborations. Commercial organizations across the country have also benefitted from the technology advancements of this project; the HTCondor Software Suite is deployed in the private sector to produce animated feature films, perform risk assessment of insurance policies and investment portfolios, and manage continuous software build and assurance frameworks for government agencies. Engineers have been using HTCondor to design planes, develop missile systems, and simulate wind turbines.  Sharing, mutual trust and autonomy are the principals that underpin the HTCondor Software Suite. The outreach, training and education activities of the project have presented these principles in the context of a widely used operational software. As part of its efforts to help democratize research computing, the project strived to ensure the technologies and the underpinning vision meet the needs of universities that are starting to formulate and build their computational research capabilities.  The broad impact of the software suite developed and maintained by this project helped to communicate the value and applicability of these principles to students, educators, and the public. The Open Science Grid consortium and fabric of distributed high-throughput computing services it provides are a clear display for the transformative power such a combination of principles and software tools can have on research and education at the national level.       Last Modified: 02/19/2021       Submitted by: Miron Livny]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
