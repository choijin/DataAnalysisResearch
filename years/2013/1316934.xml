<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Small: Multirobot-Human Coordination for Visual Scene Understanding</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>771934.00</AwardTotalIntnAmount>
<AwardAmount>821934</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Miller</SignBlockName>
<PO_EMAI>damiller@nsf.gov</PO_EMAI>
<PO_PHON>7032924914</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The objective of this research is to enable the development of teams of robots, equipped with vision and other sensors, capable of working alongside humans in critical missions, such as search and rescue. Key requirements are situational awareness and coordinated action. The approach is to develop mathematical frameworks and algorithms to enable such a team of robots to coordinate their paths, share and analyze their sensor data, maintain communications, and interact effectively and safely with humans. The project brings together experts in computer vision, robotics, estimation theory and controls.&lt;br/&gt;&lt;br/&gt;Intellectual Merit. Realizing the above goals will require advances in several inter-related domains. Specifically, the sensing, estimation, and trajectory control tasks must seamlessly integrate visual analysis with navigation and control strategies, as well as inputs from humans. Novel distributed estimation strategies must be developed to accommodate difficult and dynamic environments. Efficient human-robot coordination necessitates methodologies for joint exploration and mapping, identifying important visual information, and robots? operation at different levels of autonomy.&lt;br/&gt;&lt;br/&gt;Broader Impact. The success of this project will be a major step towards the deployment of teams of robots to assist humans in dangerous and complex tasks like disaster response. Search-and-rescue experts will advise the team in developing a prototype system, and evaluating it in situations that mimic operational conditions. The developed software tools will be disseminated to other researchers so they can build on the results. Undergraduates from UCR's highly diverse student population will gain valuable experience working alongside graduate student researchers.</AbstractNarration>
<MinAmdLetterDate>08/24/2013</MinAmdLetterDate>
<MaxAmdLetterDate>04/22/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1316934</AwardID>
<Investigator>
<FirstName>Jay</FirstName>
<LastName>Farrell</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jay A Farrell</PI_FULL_NAME>
<EmailAddress>farrell@ece.ucr.edu</EmailAddress>
<PI_PHON>9518272159</PI_PHON>
<NSF_ID>000315041</NSF_ID>
<StartDate>08/24/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Amit</FirstName>
<LastName>Roy Chowdhury</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amit K Roy Chowdhury</PI_FULL_NAME>
<EmailAddress>amitrc@ece.ucr.edu</EmailAddress>
<PI_PHON>9518277886</PI_PHON>
<NSF_ID>000309390</NSF_ID>
<StartDate>08/24/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Anastasios</FirstName>
<LastName>Mourikis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Anastasios Mourikis</PI_FULL_NAME>
<EmailAddress>mourikis@ee.ucr.edu</EmailAddress>
<PI_PHON>9518275535</PI_PHON>
<NSF_ID>000514830</NSF_ID>
<StartDate>08/24/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Riverside</Name>
<CityName>RIVERSIDE</CityName>
<ZipCode>925210217</ZipCode>
<PhoneNumber>9518275535</PhoneNumber>
<StreetAddress>Research &amp; Economic Development</StreetAddress>
<StreetAddress2><![CDATA[245 University Office Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>44</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA44</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>627797426</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA AT RIVERSIDE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Riverside]]></Name>
<CityName>Riverside</CityName>
<StateCode>CA</StateCode>
<ZipCode>925210001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>41</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA41</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7298</Code>
<Text>International Research Collab</Text>
</ProgramElement>
<ProgramElement>
<Code>7484</Code>
<Text>IIS Special Projects</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>5920</Code>
<Text>ITALY</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~771934</FUND_OBLG>
<FUND_OBLG>2015~50000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Employing robots, instead of humans, for dangerous tasks in security and disaster-response operations is a long-standing goal. Towards this goal, this project worked on some fundamental scientific challenges facing robot teams that will operate alongside humans, to jointly accomplish a given mission. Specifically, this project addressed fundamental research problems to&nbsp; enable coordination within a team of humans and robots for wide-area situation awareness.&nbsp;</p> <p>Below is an overview of the accomplishments of the project.</p> <p>- Robots rely on sensors, often visual sensors, to understand their environment. The data from these sensors is processed by machine learning algorithms which have been trained to recognize certain objects and activities. However, the appearance of these objects are affected by environmental conditions like lighting. Thus the learned models need to be updated. Once of the questions we addressed in this proposal is how the models can be updated with minimal human interaction, which can be time-consuming and erroneous.</p> <p>- One of the important tasks for robots with visual sensors is to estimate their pose, and the pose of other objects, in the scene. This has been done using direct image features which may lack invariance to lighting conditions and camera imagin parameters. In this project, we showed how visual-inertial odometry can help with more accuracte estimates of the state of objects, and demonstrated it in a robot localization application. Our proposed method outperformed existing approaches by a large margin.&nbsp;</p> <p>- Teams of robots with visual sensors collect large amounts of data. It is very difficult, often impossible, to analyze all of this data. Thus, methods for summarizing videos from a network of cameras is needed to extract the most relevant information. We proposed novel diversity-aware multi-camera video summarization approaches and demonstrated their applicability on multiple datasets.&nbsp;</p> <p>- Robots working with each other and humans need to predict the behaviors of others in the team, as well as other objects in the scene. This is essential for making informed decisions like how to navigate in a dynamic environment. Predicting the behaviors of other objects in a very challenging problem. In this project, we considered this problem and showed how behaviors can be predicted in video, as well as provide natural language descriptions of the predicted behaviors.</p> <p>- Coordination between robots requires them to communicate with each other and make decisions by reaching consensus. While such problems have been studied for many years, most of the existing works did not consider the specific characcteristics of visual sensors, e.g., the directionality aspect of cameras. We developed novel approaches for distributed estimation in a team of visual sensors that are specifically aware of these issues, leading to novel theoretical and practical insights into distributed estimation approaches.&nbsp;</p> <p>- Accurate and reliable awareness of world interactions is a key requirement for effective commercial deployment of autonomous and connected vehicles. Awareness arises from onboard sensors and ubiquitous communication between vehicles and infrastructure. Many applications require state estimation&nbsp; while mitigating&nbsp; measurement outliers. Outliers are those measurements that are inconsistent with the model and the remainder of the set of measurements.&nbsp;For sensor-rich applications (which is the case with video sensors), our research has focused on solving the problem of optimally selecting the subset of measurements to use that minimizes risk of outlier inclusion while achieving a performance specification.&nbsp;</p> <p>The research results have been published in the very top conferences and journals in computer vision, image processing, controls, and robotics. Associated software is available online along with the papers. Some of the methods are being used in other projects, thus leading to further development.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/15/2019<br>      Modified by: Amit&nbsp;K&nbsp;Roy Chowdhury</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1316934/1316934_10270405_1565892709891_system_overview_AM2--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1316934/1316934_10270405_1565892709891_system_overview_AM2--rgov-800width.jpg" title="Human Robot Interaction Framework"><img src="/por/images/Reports/POR/2019/1316934/1316934_10270405_1565892709891_system_overview_AM2--rgov-66x44.jpg" alt="Human Robot Interaction Framework"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The figure shows the main components of a human-robot coordination framework, including cooperation within human, within robots, and between humans and robots. (QOI stands for Quality of Information)</div> <div class="imageCredit">PIs</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Amit&nbsp;K&nbsp;Roy Chowdhury</div> <div class="imageTitle">Human Robot Interaction Framework</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Employing robots, instead of humans, for dangerous tasks in security and disaster-response operations is a long-standing goal. Towards this goal, this project worked on some fundamental scientific challenges facing robot teams that will operate alongside humans, to jointly accomplish a given mission. Specifically, this project addressed fundamental research problems to  enable coordination within a team of humans and robots for wide-area situation awareness.   Below is an overview of the accomplishments of the project.  - Robots rely on sensors, often visual sensors, to understand their environment. The data from these sensors is processed by machine learning algorithms which have been trained to recognize certain objects and activities. However, the appearance of these objects are affected by environmental conditions like lighting. Thus the learned models need to be updated. Once of the questions we addressed in this proposal is how the models can be updated with minimal human interaction, which can be time-consuming and erroneous.  - One of the important tasks for robots with visual sensors is to estimate their pose, and the pose of other objects, in the scene. This has been done using direct image features which may lack invariance to lighting conditions and camera imagin parameters. In this project, we showed how visual-inertial odometry can help with more accuracte estimates of the state of objects, and demonstrated it in a robot localization application. Our proposed method outperformed existing approaches by a large margin.   - Teams of robots with visual sensors collect large amounts of data. It is very difficult, often impossible, to analyze all of this data. Thus, methods for summarizing videos from a network of cameras is needed to extract the most relevant information. We proposed novel diversity-aware multi-camera video summarization approaches and demonstrated their applicability on multiple datasets.   - Robots working with each other and humans need to predict the behaviors of others in the team, as well as other objects in the scene. This is essential for making informed decisions like how to navigate in a dynamic environment. Predicting the behaviors of other objects in a very challenging problem. In this project, we considered this problem and showed how behaviors can be predicted in video, as well as provide natural language descriptions of the predicted behaviors.  - Coordination between robots requires them to communicate with each other and make decisions by reaching consensus. While such problems have been studied for many years, most of the existing works did not consider the specific characcteristics of visual sensors, e.g., the directionality aspect of cameras. We developed novel approaches for distributed estimation in a team of visual sensors that are specifically aware of these issues, leading to novel theoretical and practical insights into distributed estimation approaches.   - Accurate and reliable awareness of world interactions is a key requirement for effective commercial deployment of autonomous and connected vehicles. Awareness arises from onboard sensors and ubiquitous communication between vehicles and infrastructure. Many applications require state estimation  while mitigating  measurement outliers. Outliers are those measurements that are inconsistent with the model and the remainder of the set of measurements. For sensor-rich applications (which is the case with video sensors), our research has focused on solving the problem of optimally selecting the subset of measurements to use that minimizes risk of outlier inclusion while achieving a performance specification.   The research results have been published in the very top conferences and journals in computer vision, image processing, controls, and robotics. Associated software is available online along with the papers. Some of the methods are being used in other projects, thus leading to further development.          Last Modified: 08/15/2019       Submitted by: Amit K Roy Chowdhury]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
