<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Collaborative Research: Towards Modeling Source Separation from Measured Cortical Responses</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>180006.00</AwardTotalIntnAmount>
<AwardAmount>180006</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will use new technologies for measuring brain activity to understand in detail how human listeners are able to separate competing, overlapping voices, and thereby to help design automatic systems capable of the same feat. Natural environments are full of overlapping sounds, and successful audio processing by both humans and machines relies on a fundamental ability to separate out sound sources of interest. This is commonly referred to as the "cocktail party effect," based on the ability of people to hear what a single person is saying despite the noisy background audio from other speakers. Despite the long history of research in hearing, this exceptional human capability for sound source separation is still poorly understood, and efforts to automatically separate overlapping voices by machine are correspondingly crude: although great advances have been made in robust processing of noisy speech by machine, separation of complex natural sounds (such as overlapping voices) remains a challenge. Advances in sensor technology now enable the modeling of this function in humans, giving an unprecedented, detailed view of sound representation processing in the brain.  This project works specifically with measurements of neuroelectric response made directly on the surface of the human cortex (currently with a 256-electrode sensor array) for patients awaiting neurosurgery. Using such measurements made for controlled mixtures of voices, the project will endeavor to both develop models of voice separation in the human cortex by reconstructing an approximation to the acoustic stimulus from the neural population response, and in the process learning the linear mapping between the neural response back to a spectrogram measure of the stimulus. To attempt to significantly improve the ability of machine algorithms to mimic human source separation capability, the project will also focus on a signal processing framework that supports experiments with different combinations of cues and strategies to optimize agreement with the recordings of neural activity. The engineering model is based on the Computational Auditory Scene Analysis (CASA) framework, a family of approaches that have shown competitive results for handling sound mixtures.</AbstractNarration>
<MinAmdLetterDate>09/08/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/08/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1320366</AwardID>
<Investigator>
<FirstName>Edward</FirstName>
<LastName>Chang</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Edward F Chang</PI_FULL_NAME>
<EmailAddress>changed@neurosurg.ucsf.edu</EmailAddress>
<PI_PHON>4154762977</PI_PHON>
<NSF_ID>000596881</NSF_ID>
<StartDate>09/08/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Francisco</Name>
<CityName>San Francisco</CityName>
<ZipCode>941034249</ZipCode>
<PhoneNumber>4154762977</PhoneNumber>
<StreetAddress>1855 Folsom St Ste 425</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>094878337</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, SAN FRANCISCO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Francisco]]></Name>
<CityName>San Francisco</CityName>
<StateCode>CA</StateCode>
<ZipCode>941432208</ZipCode>
<StreetAddress><![CDATA[675 Nelson Rising Lane]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~180006</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project is a collaborative effort between the International Computer Science Institute (ICSI) at the University of California, Berkeley led by PI Nelson Morgan, PhD and the University of California, San Francisco (UCSF) under PI Edward F. Chang, MD. Our goal is to better understand multi-speaker speech processing by measuring brain activity to determine how human listeners are able to separate and understand individual speakers when more than one person is talking at the same time. This report details the project outcomes made on the UCSF side.</p> <p>The UCSF team recorded human brain signals during single- and multi-speaker listening and then decoded these signals to extract relevant data for use in machine learning. We then analyzed these brain signals to advance our understanding of human speech perception of multi-speaker speech. In our research, we found first and foremost that prosodic aspects of speech cannot be ignored. "Prosody" refers to the rising and falling of voice pitch, the so-called melody or intonation of speech, and also to the coming and going of stressed syllables, the so-called rhythm of speech. These aspects provide critical cues during natural listening, and particularly under noise or multi-speaker conditions. Our attempts to study the segmental level (the sequence of individuals, lower-level speech sounds, corresponding roughly to each letter in a word) were only partially successful, and it was determined that the prosodic level carries far more of the variance of the human brain signal than the segmental level. That is, the human brain cares tremendously about prosody, and the representation of each individual phoneme (~letter) is at a more refined level within the neural network that we have difficulty detecting by today's methods.</p> <p>The broader impact of these results is that we have now developed advanced methods for decoding speech from brain waves, which is one of the major quests in contemporary neuroscience (so-called "mind reading", whereby we can detect what the person hears by monitoring their brain activity). Secondly, we have significantly advanced our understanding of human auditory cortex organization during this project period. Literally, we are now poised to update the human auditory neuroscience understanding with a new model that will have widespread clinical and theoretical impact, and is currently in preparation. Thirdly, we have deepened our understanding of prosody in language in general and in its neural representation in particular. Prosody is an often-neglected aspect of speech, but, as we found, it is crucial for actually understanding the brain signals from human auditory regions, and the potential broader impact of understanding prosody is major - aside from obvious clinical benefit (dyslexia may involve deficits in rhythm, amusia is tied to intonation, speech therapist require prosody methods, applications to deaf teaching and cochlear implants, etc.), an understanding of prosody is critical in a variety of applied fields in society (communication studies, marketing, politics, art, etc.).</p> <p>We&nbsp; believe there is tremendous intellectual merit in our result obtained during the project period, as already outlined above. The intellectual merit is most significantly highlighted by noting that we were not able to do what we originally intended/proposed to do (which was decode the segmental level, as described above), but instead we required a complete advance in our intellectual understanding, of auditory neurobiology and linguistic prosodic studies, in order to get towards our general goals at all. These required intense scholarly study and exploratory data analyses, and are now poised for the broader impact described above.</p> <p>There were no commercial or physical products generated from this project. A variety of software code developed during this project is now being used widely within t...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project is a collaborative effort between the International Computer Science Institute (ICSI) at the University of California, Berkeley led by PI Nelson Morgan, PhD and the University of California, San Francisco (UCSF) under PI Edward F. Chang, MD. Our goal is to better understand multi-speaker speech processing by measuring brain activity to determine how human listeners are able to separate and understand individual speakers when more than one person is talking at the same time. This report details the project outcomes made on the UCSF side.  The UCSF team recorded human brain signals during single- and multi-speaker listening and then decoded these signals to extract relevant data for use in machine learning. We then analyzed these brain signals to advance our understanding of human speech perception of multi-speaker speech. In our research, we found first and foremost that prosodic aspects of speech cannot be ignored. "Prosody" refers to the rising and falling of voice pitch, the so-called melody or intonation of speech, and also to the coming and going of stressed syllables, the so-called rhythm of speech. These aspects provide critical cues during natural listening, and particularly under noise or multi-speaker conditions. Our attempts to study the segmental level (the sequence of individuals, lower-level speech sounds, corresponding roughly to each letter in a word) were only partially successful, and it was determined that the prosodic level carries far more of the variance of the human brain signal than the segmental level. That is, the human brain cares tremendously about prosody, and the representation of each individual phoneme (~letter) is at a more refined level within the neural network that we have difficulty detecting by today's methods.  The broader impact of these results is that we have now developed advanced methods for decoding speech from brain waves, which is one of the major quests in contemporary neuroscience (so-called "mind reading", whereby we can detect what the person hears by monitoring their brain activity). Secondly, we have significantly advanced our understanding of human auditory cortex organization during this project period. Literally, we are now poised to update the human auditory neuroscience understanding with a new model that will have widespread clinical and theoretical impact, and is currently in preparation. Thirdly, we have deepened our understanding of prosody in language in general and in its neural representation in particular. Prosody is an often-neglected aspect of speech, but, as we found, it is crucial for actually understanding the brain signals from human auditory regions, and the potential broader impact of understanding prosody is major - aside from obvious clinical benefit (dyslexia may involve deficits in rhythm, amusia is tied to intonation, speech therapist require prosody methods, applications to deaf teaching and cochlear implants, etc.), an understanding of prosody is critical in a variety of applied fields in society (communication studies, marketing, politics, art, etc.).  We  believe there is tremendous intellectual merit in our result obtained during the project period, as already outlined above. The intellectual merit is most significantly highlighted by noting that we were not able to do what we originally intended/proposed to do (which was decode the segmental level, as described above), but instead we required a complete advance in our intellectual understanding, of auditory neurobiology and linguistic prosodic studies, in order to get towards our general goals at all. These required intense scholarly study and exploratory data analyses, and are now poised for the broader impact described above.  There were no commercial or physical products generated from this project. A variety of software code developed during this project is now being used widely within the lab, but there is no external products generated.       Last Modified: 12/14/2015       ...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
