<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS:CLCCA:Collaborative Research:Harnessing Highly Threaded Hardware for Server Workloads</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>341617.00</AwardTotalIntnAmount>
<AwardAmount>341617</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Data centers provide the computational and storage infrastructure required to meet today's ever increasing demand for Internet-based services. Web servers deliver a vast range of information on demand, ranging from static content such as files, images, video and audio streaming services, to dynamic content created via scripting languages (e.g,. PHP) or stand alone C/C++ applications (e.g., search results).  Server performance, scaling and energy efficiency (throughput/Watt) are crucial factors in reducing total cost of ownership (TCO) in today's server-based industries.  Unfortunately, current system designs based on commodity multicore processors may not be the most power/energy efficient for all server workloads.  &lt;br/&gt;&lt;br/&gt;Today?s massively parallel accelerators (e.g., GPUs) provide exceptional performance per Watt for certain workloads versus conventional many core CPUs. Unfortunately, these devices have not found wide-spread general purpose use outside the high-performance computing domain. This project expands the use of massively parallel accelerators to server and operating system-intensive workloads by innovating across the application, runtime, operating system, and architecture layers.&lt;br/&gt;&lt;br/&gt;This research builds on the observation that server workload execution patterns are not completely unique across multiple requests. The goal of this project is to develop computer systems (software and hardware) that exploit similarity across requests to improve server performance and power/energy efficiency by launching data parallel executions for request cohorts.  The three primary aspects of this research are 1) mapping traditional thread/task parallel workloads onto data parallel hardware, 2) developing a new accelerator-centric operating system architecture, and 3) developing new architectural mechanisms to support this new class of accelerator workloads and operating system software.</AbstractNarration>
<MinAmdLetterDate>09/11/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/09/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1333594</AwardID>
<Investigator>
<FirstName>Emmett</FirstName>
<LastName>Witchel</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Emmett Witchel</PI_FULL_NAME>
<EmailAddress>witchel@cs.utexas.edu</EmailAddress>
<PI_PHON>5122327889</PI_PHON>
<NSF_ID>000164959</NSF_ID>
<StartDate>09/11/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Silberstein</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark Silberstein</PI_FULL_NAME>
<EmailAddress>marks@cs.utexas.edu</EmailAddress>
<PI_PHON>5124716424</PI_PHON>
<NSF_ID>000611398</NSF_ID>
<StartDate>09/11/2013</StartDate>
<EndDate>05/05/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787137726</ZipCode>
<StreetAddress><![CDATA[P.O. Box 7726]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>21</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX21</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~341617</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Today's massively parallel accelerators (e.g., GPUs) provide exceptional performance per Watt for certain workloads compared with conventional many core CPUs.&nbsp; Unfortunately, these devices have not found wide-spread general purpose use outside the high-performance computing domain.&nbsp; This project has developed prototype systems to use parallel accelerators for server-type workloads by innovating across the application, runtime, operating system, and architecture layers.</p> <p>GPUs lack software abstractions to direct the flow of data within a system, leaving the developer with only low-level control over I/O. Therefore, certain classes of applications that could benefit from GPU's computational density require unacceptable development costs to realize their full performance potential.</p> <p>Major project accomplishments include the GPUnet framework for developing GPU programs that directly control the network.&nbsp; Also, the Rhythm system showed how to exploit similarity across requests to improve server performance and power/energy efficiency by enabling data parallel executions for server code.&nbsp; At UT Austin, the focus has been on supporting operating system abstractions for GPU processors: GPUfs for the file system and GPUnet for the network.</p> <p>The intellectual merit of GPUnet comes from defining a native GPU networking layer that provides a socket abstraction and high-level networking APIs to GPU programs. GPUnet enables individual threads in one GPU to communicate with threads in other GPUs or CPUs via standard and familiar socket interfaces, regardless of whether they are in the same or different machines. Native GPU networking cuts the CPU out of GPU-NIC interactions, simplifying code and increasing performance. It also unifies application compute and I/O logic within the GPU program, providing a simpler programming model. GPUnet uses advanced NIC and GPU hardware capabilities and applies sophisticated code optimizations that yield high application performance equal to or exceeding hand-tuned traditional implementations.</p> <p>We have quantitatively evaluated GPUnet in two major ways: by building an in-memory MapReduce framework and a face verification server.&nbsp; Our distributed in-GPU-memory MapReduce framework allows GPUs to fully control all of the I/O: they read and write files (via GPUfs), and communicate with other GPUs (via GPUnet).&nbsp; This architecture demonstrates the ability of GPUnet to support complex communication patterns across GPUs, and for the word count and K-means workloads, it scales to four GPUs providing speedups of 2.9-3.5x over a single GPU.</p> <p>We build a face verification server that matches images and interacts with a memory cache running on the CPU (memcached) directly from GPU code.&nbsp; The server processes 53K client requests/second on a single NVIDIA K20Xm GPU, exceeding the throughput of a 6-core Intel CPU and a CUDA-based server by 1.5x and 2.3x respectively, while maintaining 3x lower latency than the CPU and requiring half as much code than the other versions.</p> <p>The broader impacts of the project come from professional development for students including two doctoral theses, five masters theses, and three undergraduate projects. The ideas in the project have influenced corporate directions at NVIDIA.&nbsp; Finally, work from the project has been integrated into graduate and undergraduate computer systems curricula.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/19/2017<br>      Modified by: Emmett&nbsp;Witchel</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Today's massively parallel accelerators (e.g., GPUs) provide exceptional performance per Watt for certain workloads compared with conventional many core CPUs.  Unfortunately, these devices have not found wide-spread general purpose use outside the high-performance computing domain.  This project has developed prototype systems to use parallel accelerators for server-type workloads by innovating across the application, runtime, operating system, and architecture layers.  GPUs lack software abstractions to direct the flow of data within a system, leaving the developer with only low-level control over I/O. Therefore, certain classes of applications that could benefit from GPU's computational density require unacceptable development costs to realize their full performance potential.  Major project accomplishments include the GPUnet framework for developing GPU programs that directly control the network.  Also, the Rhythm system showed how to exploit similarity across requests to improve server performance and power/energy efficiency by enabling data parallel executions for server code.  At UT Austin, the focus has been on supporting operating system abstractions for GPU processors: GPUfs for the file system and GPUnet for the network.  The intellectual merit of GPUnet comes from defining a native GPU networking layer that provides a socket abstraction and high-level networking APIs to GPU programs. GPUnet enables individual threads in one GPU to communicate with threads in other GPUs or CPUs via standard and familiar socket interfaces, regardless of whether they are in the same or different machines. Native GPU networking cuts the CPU out of GPU-NIC interactions, simplifying code and increasing performance. It also unifies application compute and I/O logic within the GPU program, providing a simpler programming model. GPUnet uses advanced NIC and GPU hardware capabilities and applies sophisticated code optimizations that yield high application performance equal to or exceeding hand-tuned traditional implementations.  We have quantitatively evaluated GPUnet in two major ways: by building an in-memory MapReduce framework and a face verification server.  Our distributed in-GPU-memory MapReduce framework allows GPUs to fully control all of the I/O: they read and write files (via GPUfs), and communicate with other GPUs (via GPUnet).  This architecture demonstrates the ability of GPUnet to support complex communication patterns across GPUs, and for the word count and K-means workloads, it scales to four GPUs providing speedups of 2.9-3.5x over a single GPU.  We build a face verification server that matches images and interacts with a memory cache running on the CPU (memcached) directly from GPU code.  The server processes 53K client requests/second on a single NVIDIA K20Xm GPU, exceeding the throughput of a 6-core Intel CPU and a CUDA-based server by 1.5x and 2.3x respectively, while maintaining 3x lower latency than the CPU and requiring half as much code than the other versions.  The broader impacts of the project come from professional development for students including two doctoral theses, five masters theses, and three undergraduate projects. The ideas in the project have influenced corporate directions at NVIDIA.  Finally, work from the project has been integrated into graduate and undergraduate computer systems curricula.             Last Modified: 10/19/2017       Submitted by: Emmett Witchel]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
