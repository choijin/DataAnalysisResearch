<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Distance and Dissimilarity Information in Statistical Model Building</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>400002.00</AwardTotalIntnAmount>
<AwardAmount>400002</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The objective of this research is to greatly expand the collection of statistical tools that exploit pairwise distance and dissimilarity information in statistical model building for regression, classification, and variable/pattern selection at different scales. This includes use of information that involves non-metric pairwise dissimilarity information. In this work dissimilarity information may be subjective, noisy, incomplete, confined within a nonlinear manifold, may come from multiple sources and may be inconsistent. Previous results have shown how this information may be embedded into a Euclidean space, so that methods that operate in a Euclidean space can be used. Two recent novel and very powerful tools, distance correlation and distance components, have provided for principled testing of correlations between arbitrary groups of variables and testing of equality of distributions, based only on pairwise Euclidean distances, and requiring essentially no distributional assumptions. Thus, combining methods that embed non-metric information into a Euclidean space followed by use of distance correlation and distance components that operate on Euclidean data provide an important new approach to using "messy" pairwise data. Furthermore distance correlation and distance components are being extended to certain regression, classification and variable/pattern selection problems via parametrization, tuning and testing techniques, preceded, when appropriate by embedding techniques. A series of tasks to implement aspects of this program provides advances in the major statistical tasks of regression, classification and variable/pattern selection for non-traditional information in a principled way.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This work provides a vast extension of the set of practical tools available to the statistician/data analyst and to modelers in a wide variety of scientific fields to extract information to predict, classify, and select important variables/patterns from data sets from small to large, that include distance or dissimilarity information from a variety of structures that are becoming increasingly available and important in practice. The proposed work provides a new set of important and useful tools for improved statistical data analysis that will be widely disseminated, and impact society to the extent that they provide aid to researchers in the extraction of information in biological, medical, environmental and other data sets that contain information of public interest. The project includes high level training of a Ph.D. student in an important STEM area.</AbstractNarration>
<MinAmdLetterDate>07/24/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/23/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1308877</AwardID>
<Investigator>
<FirstName>Grace</FirstName>
<LastName>Wahba</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Grace G Wahba</PI_FULL_NAME>
<EmailAddress>wahba@stat.wisc.edu</EmailAddress>
<PI_PHON>6082622598</PI_PHON>
<NSF_ID>000201345</NSF_ID>
<StartDate>07/24/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Wisconsin-Madison</Name>
<CityName>MADISON</CityName>
<ZipCode>537151218</ZipCode>
<PhoneNumber>6082623822</PhoneNumber>
<StreetAddress>21 North Park Street</StreetAddress>
<StreetAddress2><![CDATA[Suite 6401]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<StateCode>WI</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WI02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>161202122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WISCONSIN SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041188822</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[UW Madison, Dept. of Statistics]]></Name>
<CityName>Madison</CityName>
<StateCode>WI</StateCode>
<ZipCode>537061510</ZipCode>
<StreetAddress><![CDATA[1300 University Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WI02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~91382</FUND_OBLG>
<FUND_OBLG>2014~98565</FUND_OBLG>
<FUND_OBLG>2015~102800</FUND_OBLG>
<FUND_OBLG>2016~107255</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 16.0px Courier} span.s1 {font-variant-ligatures: no-common-ligatures} --> <p class="p1"><span class="s1"><span>&nbsp;</span></span></p> <p class="p1"><span class="s1"><span>Many aspects of scientific research involve data sets collected for the purpose of understanding associations or predictive relations between multiple variables of interest. For example randomized clinical trials can be carried out to study the impact of genetic patterns and other variables on the efficacy of treatments of interest. Similarly demographic studies, based on observational data from cooperating subjects living in particular cities, practicing particular professions, or in other affinity groups may contain data on a large number of individual attributes (e. g. age, gender, body mass index, smoking, blood pressure, cholesterol, diet, exercise, education, socioeconomic status, properties of medical images, genetic variables, incidence of various diseases, observations on close relatives, mortality, to name a few). Often it is of interest to establish which among many candidate variables or clusters of interacting variables actually have relationships which may be of scientific interest. The variable selection problem particularly occurs in some genetic studies where there are multiple genetic variants to be considered for their possible predictive ability. </span></span></p> <p class="p1">The major objective of the research is to greatly expand the collection of so called &rdquo;statistical machine learning&rdquo; tools (a. k. a. models) that can be used given databases from clinical trials, demographic studies, medical records and other sources of information, to learn relationships between various interacting attributes or risk factors and various responses or outcomes of interest. These tools generally include procedures for tuning models so as not to overfit noisy data, and methods for validating the results in practical applications, typically by holding out subsets of the observations to see how well predictions work on data not involved in building the model.</p> <p class="p1">In this project we have built a number of highly useful new statistical machine learning tools for these purposes and demonstrated their practical use in the analysis of particular clinical trials and demographic studies. Four examples are:</p> <p class="p1">1) Improved methods for combining information from multicenter clinical trials when the different centers sample people from populations with different demographics (e. g. age, gender) and have different laboratory protocols. This is notoriously difficult to do correctly. H. H. Zhou, V. Singh, S. Johnson and G. Wahba gave conditions for when and how the joint analysis of raw data can be carried out with statistical rigor, providing a much more efficient use of such data than previously known. This example involved two different Alzheimers demographic studies which had subjects with age, sex, 8 different Cerebral Spinal Fluid (CSF) measurements, hippocampus&nbsp;volume (a brain component involved with memory), and diagnosis (Normal, Mild Cognitive Impairment, or Alzheimers Disease). After carrying out the proposed methodology it was demonstrated that prediction of hippocampus volume and diagnosis from age, sex and CSF measurements was much improved over the usual approach of analyzing each population separately and combining only the results.</p> <p class="p1">2) An improved method for predicting conditional lifetime expectancy (your expected lifetime assuming you have attained a particular age), involving many candidate variables, some of which may be censored (that is, only known to be smaller or larger than some cutoff). The efficacy of the method was demonstrated in a demographic study where clusters of the variables attained age, gender, smoking history, body mass index, educational attainment, socioeconomic status, disease status and other attributes combined to show greater or lesser associations with conditional lifetime expectancy. J. Kong, R. Klein, B. E. K. Klein, G. Wahba</p> <p class="p1">3) An improved method for selecting relevant gene expression profiles.In a study of four types of childhood tumors the method had a nearly perfect ability to classify tumor type correctly. In a study of ovarian cancer patients, the method was used to classify whether a patient would respond to chemotherapy. Inferences were difficult in this study. The analysis resulted in three possible patient assessments: highly likely to respond, highly likely not to respond and &rsquo;can&rsquo;t determine&rsquo;. J. Kong, S. Wang, G. Wahba</p> <p class="p1">4) New methods for assessing associations of the four categories: familial relationships (pedigrees), lifestyle factors, diseases and mortality. It is difficult to tease out answers in this nature vs nurture debate - one such example may be the effect of say, smoking vs genetic inheritance, because both smoking and genetic variants run in families. The data motivating this study included extensive pedigree information, (e. g. siblings, parents, children, etc.), and in fact demonstrated significant associations between each pair of the four categories. J. Kong, R. Klein, B. E. K. Klein, K. Lee, G. Wahba</p> <p class="p1">These research projects have all been part of the training of PhD students who worked on them, as well as other students who benefited from regular group discussions led by the PI. The graduates so far have all gone into productive jobs in industry and academia, contributing to the storehouse of scientific talent in the US</p> <p class="p1"><span class="s1"><span><br /></span></span></p><br> <p>            Last Modified: 06/27/2018<br>      Modified by: Grace&nbsp;G&nbsp;Wahba</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Many aspects of scientific research involve data sets collected for the purpose of understanding associations or predictive relations between multiple variables of interest. For example randomized clinical trials can be carried out to study the impact of genetic patterns and other variables on the efficacy of treatments of interest. Similarly demographic studies, based on observational data from cooperating subjects living in particular cities, practicing particular professions, or in other affinity groups may contain data on a large number of individual attributes (e. g. age, gender, body mass index, smoking, blood pressure, cholesterol, diet, exercise, education, socioeconomic status, properties of medical images, genetic variables, incidence of various diseases, observations on close relatives, mortality, to name a few). Often it is of interest to establish which among many candidate variables or clusters of interacting variables actually have relationships which may be of scientific interest. The variable selection problem particularly occurs in some genetic studies where there are multiple genetic variants to be considered for their possible predictive ability.  The major objective of the research is to greatly expand the collection of so called "statistical machine learning" tools (a. k. a. models) that can be used given databases from clinical trials, demographic studies, medical records and other sources of information, to learn relationships between various interacting attributes or risk factors and various responses or outcomes of interest. These tools generally include procedures for tuning models so as not to overfit noisy data, and methods for validating the results in practical applications, typically by holding out subsets of the observations to see how well predictions work on data not involved in building the model. In this project we have built a number of highly useful new statistical machine learning tools for these purposes and demonstrated their practical use in the analysis of particular clinical trials and demographic studies. Four examples are: 1) Improved methods for combining information from multicenter clinical trials when the different centers sample people from populations with different demographics (e. g. age, gender) and have different laboratory protocols. This is notoriously difficult to do correctly. H. H. Zhou, V. Singh, S. Johnson and G. Wahba gave conditions for when and how the joint analysis of raw data can be carried out with statistical rigor, providing a much more efficient use of such data than previously known. This example involved two different Alzheimers demographic studies which had subjects with age, sex, 8 different Cerebral Spinal Fluid (CSF) measurements, hippocampus volume (a brain component involved with memory), and diagnosis (Normal, Mild Cognitive Impairment, or Alzheimers Disease). After carrying out the proposed methodology it was demonstrated that prediction of hippocampus volume and diagnosis from age, sex and CSF measurements was much improved over the usual approach of analyzing each population separately and combining only the results. 2) An improved method for predicting conditional lifetime expectancy (your expected lifetime assuming you have attained a particular age), involving many candidate variables, some of which may be censored (that is, only known to be smaller or larger than some cutoff). The efficacy of the method was demonstrated in a demographic study where clusters of the variables attained age, gender, smoking history, body mass index, educational attainment, socioeconomic status, disease status and other attributes combined to show greater or lesser associations with conditional lifetime expectancy. J. Kong, R. Klein, B. E. K. Klein, G. Wahba 3) An improved method for selecting relevant gene expression profiles.In a study of four types of childhood tumors the method had a nearly perfect ability to classify tumor type correctly. In a study of ovarian cancer patients, the method was used to classify whether a patient would respond to chemotherapy. Inferences were difficult in this study. The analysis resulted in three possible patient assessments: highly likely to respond, highly likely not to respond and ?can?t determine?. J. Kong, S. Wang, G. Wahba 4) New methods for assessing associations of the four categories: familial relationships (pedigrees), lifestyle factors, diseases and mortality. It is difficult to tease out answers in this nature vs nurture debate - one such example may be the effect of say, smoking vs genetic inheritance, because both smoking and genetic variants run in families. The data motivating this study included extensive pedigree information, (e. g. siblings, parents, children, etc.), and in fact demonstrated significant associations between each pair of the four categories. J. Kong, R. Klein, B. E. K. Klein, K. Lee, G. Wahba These research projects have all been part of the training of PhD students who worked on them, as well as other students who benefited from regular group discussions led by the PI. The graduates so far have all gone into productive jobs in industry and academia, contributing to the storehouse of scientific talent in the US         Last Modified: 06/27/2018       Submitted by: Grace G Wahba]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
