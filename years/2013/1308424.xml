<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Computer Experiments with Tuning or Calibration Parameters: Modeling, Estimation and Design</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>170000.00</AwardTotalIntnAmount>
<AwardAmount>170000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In the statistical approach to computer experiments, Gaussian process models are often employed to describe the relationship between the simulation output and the input variables. There are three types of input variables: control variables, tuning parameters and calibration parameters. The tuning parameter can be the mesh density in finite element analysis. Calibration parameters are also part of the computer code but not part of the physical experiment. The combined data from computer and physical experiments are used to calibrate the computer model. These two types have received much less attention in the literature. The main goal of this proposal is to study some issues in modeling, estimation and design for tuning and calibration parameters in computer experiments. A class of nonstationary Gaussian process models is proposed, which can be used to efficiently link data from simulations with different tuning parameter values. Issues on covariance modeling and comparisons of competing models are studied. For designing computer experiments, typical use of space-filling designs is replaced by non-uniform designs that can better reflect the nonstationary nature of information in the data. For calibration parameters, the standard estimation procedure is shown to be asymptotically inconsistent. A new theoretical framework is proposed for studying the estimation properties, including modification and new estimation procedures to achieve consistency and optimal convergence rates.&lt;br/&gt;&lt;br/&gt;The last decade has seen rapid advances in realistic physical modeling and efficient numerical methods, which make it possible to use complex mathematical models to mimic physical realities. Computer simulations can be much faster or less costly than running physical experiments. Furthermore, physical experiments can be difficult or infeasible to conduct. Therefore computer simulations are now routinely used in lieu of physical experimentations. Computer modeling and experiments have become popular in scientific and engineering investigations. They have helped reap benefits ranging from reduced development cycle time, better product, to cost reduction. In view of the wide range of applications of complex system simulations, the proposed work should have broad-based impacts on a variety of problems in autos and aerospace, computational material design, geological and atmospheric studies, and green energy simulations. It will be incorporated into publicly released software like R, thus directly benefiting practitioners in industries and researchers in academe.</AbstractNarration>
<MinAmdLetterDate>06/27/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/04/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1308424</AwardID>
<Investigator>
<FirstName>C. F. Jeff</FirstName>
<LastName>Wu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>C. F. Jeff Wu</PI_FULL_NAME>
<EmailAddress>jeffwu@isye.gatech.edu</EmailAddress>
<PI_PHON>4048942300</PI_PHON>
<NSF_ID>000454426</NSF_ID>
<StartDate>06/27/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Aenue, NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~55018</FUND_OBLG>
<FUND_OBLG>2014~56384</FUND_OBLG>
<FUND_OBLG>2015~58598</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In the past 15 years there has seen a revolution in the use of computer simulators, such as finite element analysis and agent-based models, to experimentally determine the relationship among input and output variables. Compared with classical physical system experiments, the use of computer simulator experiments are the next generation experimental tool. Computer experiments can be much less expensive to conduct than physical system tests and thus allow more complete evaluation of the control variables that determine the output of the system or product such as the product quality or reproducibility of fabrication method like 3D printing.</p> <p>When both computer simulation and physical experiment data are available, a fundamental scientific challenge is how to combine the two sources of data seamlessly and in a most efficient way to gain better understanding of the input-output relationship and to optimize the output. Especially challenging is how to use the data to estimate the calibration parameters that are present in both sources of data but only available to the investigators for computational manipulation. A prevailing approach, called the Kennedy-O&rsquo;Hagan calibration modeling, has been known to have some deficiencies in the lack of parameter identifiability. This project develops new theory and method to tackle this problem. The theory shows how the lack of identifiability can be rectified. It then leads to the development of a new calibration method called L2 calibration, which is shown to be efficient (i.e., optimal) for the estimation of the calibration parameters. A related wok is the development of a new criterion called EQI for optimizing the output of simulation experiments which are conducted at different levels of fidelity such as using finer and coarser grids in finite element analysis. Another related work is to develop a sequential design strategy for conducting computer experiments by using Bayesian uncertainty quantification. The work as reported in the project should have broad-based impacts on a variety of problems like engineering design, manufacturing, geological and environmental studies, computational material design, thermal and green energy applications, etc.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/28/2017<br>      Modified by: C. F. Jeff&nbsp;Wu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In the past 15 years there has seen a revolution in the use of computer simulators, such as finite element analysis and agent-based models, to experimentally determine the relationship among input and output variables. Compared with classical physical system experiments, the use of computer simulator experiments are the next generation experimental tool. Computer experiments can be much less expensive to conduct than physical system tests and thus allow more complete evaluation of the control variables that determine the output of the system or product such as the product quality or reproducibility of fabrication method like 3D printing.  When both computer simulation and physical experiment data are available, a fundamental scientific challenge is how to combine the two sources of data seamlessly and in a most efficient way to gain better understanding of the input-output relationship and to optimize the output. Especially challenging is how to use the data to estimate the calibration parameters that are present in both sources of data but only available to the investigators for computational manipulation. A prevailing approach, called the Kennedy-O?Hagan calibration modeling, has been known to have some deficiencies in the lack of parameter identifiability. This project develops new theory and method to tackle this problem. The theory shows how the lack of identifiability can be rectified. It then leads to the development of a new calibration method called L2 calibration, which is shown to be efficient (i.e., optimal) for the estimation of the calibration parameters. A related wok is the development of a new criterion called EQI for optimizing the output of simulation experiments which are conducted at different levels of fidelity such as using finer and coarser grids in finite element analysis. Another related work is to develop a sequential design strategy for conducting computer experiments by using Bayesian uncertainty quantification. The work as reported in the project should have broad-based impacts on a variety of problems like engineering design, manufacturing, geological and environmental studies, computational material design, thermal and green energy applications, etc.          Last Modified: 09/28/2017       Submitted by: C. F. Jeff Wu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
