<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EDU: Competing to Build Secure Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Victor Piotrowski</SignBlockName>
<PO_EMAI>vpiotrow@nsf.gov</PO_EMAI>
<PO_PHON>7032925141</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Even as security has long been a tenet of good programming practice, developers continue to produce insecure software resulting in a litany of data breaches and other compromises. This project aims to improve education on secure software development and add evidence to understanding methods, tools, techniques, and other factors that best contribute to writing secure code. The project centers on a novel multiphase programming competition that combines ideas from two traditionally disparate kinds of contests: those for building code and those for finding bugs in others' code. In phase one, contestants are tasked with building secure code. In phase two, contestants perform vulnerability analyses to attempt to break the code submitted by the other contestants in the first phase. The original builders finally aim to fix exploits discovered in phase two to recover lost points. Educators, practitioners, and policymakers broadly view secure code as important, and yet there is little consensus as to how best to teach and encourage secure-programming practices. By developing a competition, this project creates a setting that is more engaging to students, improving learning outcomes, and moreover enables greater insight into both practice and pedagogy through the analysis of data on how the participants approach secure programming, what techniques they use, and what methodologies succeed or fail for different programming tasks. The educational impact is significant, as the competition scales to hundreds of participants over two years, improving the design of the contest based on each offering. The artifacts and data produced by this project are made freely available to assist secure-programming endeavors across educational institutions. Finally, the students involved in the design, implementation, and execution of the contest are trained in advanced research and pedagogical methods.</AbstractNarration>
<MinAmdLetterDate>09/10/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/10/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1319147</AwardID>
<Investigator>
<FirstName>Atif</FirstName>
<LastName>Memon</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Atif Memon</PI_FULL_NAME>
<EmailAddress>atif@cs.umd.edu</EmailAddress>
<PI_PHON>3014053071</PI_PHON>
<NSF_ID>000217317</NSF_ID>
<StartDate>09/10/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Hicks</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael W Hicks</PI_FULL_NAME>
<EmailAddress>mwh@cs.umd.edu</EmailAddress>
<PI_PHON>3014052710</PI_PHON>
<NSF_ID>000116426</NSF_ID>
<StartDate>09/10/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jandelyn</FirstName>
<LastName>Plane</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jandelyn D Plane</PI_FULL_NAME>
<EmailAddress>jplane@cs.umd.edu</EmailAddress>
<PI_PHON>3014052754</PI_PHON>
<NSF_ID>000341622</NSF_ID>
<StartDate>09/10/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Levin</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David M Levin</PI_FULL_NAME>
<EmailAddress>dml@cs.umd.edu</EmailAddress>
<PI_PHON>3014058515</PI_PHON>
<NSF_ID>000629973</NSF_ID>
<StartDate>09/10/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1668</Code>
<Text>CYBERCORPS: SCHLAR FOR SER</Text>
</ProgramElement>
<ProgramReference>
<Code>7254</Code>
<Text>CYBER SECURITY ACT PROPOSALS</Text>
</ProgramReference>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0413</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica; -webkit-text-stroke: #000000} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica; -webkit-text-stroke: #000000; min-height: 14.0px} span.s1 {font-kerning: none} --> <p class="p1"><span class="s1">Experts have long advocated that achieving security in a computer system requires careful design and implementation from the ground up. &nbsp;For decades, practitioners have written about processes and methods for designing and building more secure software, and researchers (including ourselves) and companies have, among other activities, developed software analysis tools to help developers find vulnerabilities.</span></p> <p class="p1"><span class="s1">Despite this increased attention, many developers continue to produce insecure software, which remains vulnerable despite billions spent annually on security appliances and other defenses. We believe this situation arises, at least in part, from a lack of evidence and education. For example, code reviews, penetration testing, and static code analysis are all known to improve security by finding vulnerabilities. But the relative costs and benefits of these techniques are largely unknown, which leaves educators and policymakers in a bind: they may believe that secure coding is important, but they do not know which ideas are fundamental, so they are unsure, given limited time, what to invest in and teach. Even assuming that educators do teach methods to develop more secure code, little is known about how students approach the problem of writing secure software (or breaking it), and understanding those thought processes is important for creating more effective training materials. We suspect that students may not appreciate the importance of secure development methods, since they do not directly experience the "agony of defeat." Security is not a feature, and so its absence is not easily felt, especially in a classroom setting.</span></p> <p class="p1"><span class="s1">As a remedy to this state of affairs, we have developed and run a secure coding contest called <em>Build-it Break-it Fix-it</em> (BIBIFI) that has two goals: (1) give student contestants a competitive setting to learn about secure software creation, and (2) experimentally measure the outcomes of the contest to add to the evidence of what works and what does not.</span>&nbsp;</p> <p class="p1"><span class="s1">This grant has permitted us to design, implement, and refine BIBIFI. A BIBIFI contest has three phases. The first phase, Build-it, asks small development teams to build software according to a provided specification that includes security goals. The software is scored for being correct, efficient, and feature-ful. The second phase, Break-it, asks teams to find defects in other teams&rsquo; build-it submissions. Reported defects, proved via test cases vetted by an oracle implementation, benefit a break-it team&rsquo;s score and penalize the build-it team&rsquo;s score; more points are assigned to security-relevant problems. (A team&rsquo;s break-it and build-it scores are independent, with prizes for top scorers in each category.) The final phase, Fix-it, asks builders to fix bugs and thereby get points back if the process discovers that distinct break-it test cases identify the same defect.</span></p> <p class="p1"><span class="s1">This grant has also enabled us to hold four separate contests in which 147 teams of students participated. These competitions have resulted in a unique educational opportunity for hundreds of students from dozens of countries (with more participants from the U.S. than from any other country).</span></p> <p class="p2">In addition to providing a novel educational experience, BIBIFI has provided us the opportunity to study the building and breaking process scientifically. In particular, BIBIFI contests may serve as a quasi-controlled experiment that correlates participation data with final outcome. Quantitative analysis from three contests in 2015 found that the most efficient build-it submissions used C/C++, but submissions coded in other statically-typed languages were less likely to have a security flaw; build-it teams with diverse programming-language knowledge also produced more secure code. Shorter programs correlated with better scores. Break-it teams that were also successful build-it teams were significantly better at finding security bugs.</p> <p class="p1"><span class="s1">We have distilled the lessons learned from running the competition so that they may be applied when developing effective secure-programming courses and, we hope, more generally to actual programming practice. To this end, we have broadly disseminated the results and insights gleaned from BIBIFI through conference publications, conference panels, talks at academic and industrial institutions, and through a widely read blog run by PI Hicks. Finally, we are making the software for BIBIFI&rsquo;s backend infrastructure publicly available, to facilitate others running their own competitions in and out of the classroom.</span></p><br> <p>            Last Modified: 12/02/2016<br>      Modified by: David&nbsp;M&nbsp;Levin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Experts have long advocated that achieving security in a computer system requires careful design and implementation from the ground up.  For decades, practitioners have written about processes and methods for designing and building more secure software, and researchers (including ourselves) and companies have, among other activities, developed software analysis tools to help developers find vulnerabilities. Despite this increased attention, many developers continue to produce insecure software, which remains vulnerable despite billions spent annually on security appliances and other defenses. We believe this situation arises, at least in part, from a lack of evidence and education. For example, code reviews, penetration testing, and static code analysis are all known to improve security by finding vulnerabilities. But the relative costs and benefits of these techniques are largely unknown, which leaves educators and policymakers in a bind: they may believe that secure coding is important, but they do not know which ideas are fundamental, so they are unsure, given limited time, what to invest in and teach. Even assuming that educators do teach methods to develop more secure code, little is known about how students approach the problem of writing secure software (or breaking it), and understanding those thought processes is important for creating more effective training materials. We suspect that students may not appreciate the importance of secure development methods, since they do not directly experience the "agony of defeat." Security is not a feature, and so its absence is not easily felt, especially in a classroom setting. As a remedy to this state of affairs, we have developed and run a secure coding contest called Build-it Break-it Fix-it (BIBIFI) that has two goals: (1) give student contestants a competitive setting to learn about secure software creation, and (2) experimentally measure the outcomes of the contest to add to the evidence of what works and what does not.  This grant has permitted us to design, implement, and refine BIBIFI. A BIBIFI contest has three phases. The first phase, Build-it, asks small development teams to build software according to a provided specification that includes security goals. The software is scored for being correct, efficient, and feature-ful. The second phase, Break-it, asks teams to find defects in other teams? build-it submissions. Reported defects, proved via test cases vetted by an oracle implementation, benefit a break-it team?s score and penalize the build-it team?s score; more points are assigned to security-relevant problems. (A team?s break-it and build-it scores are independent, with prizes for top scorers in each category.) The final phase, Fix-it, asks builders to fix bugs and thereby get points back if the process discovers that distinct break-it test cases identify the same defect. This grant has also enabled us to hold four separate contests in which 147 teams of students participated. These competitions have resulted in a unique educational opportunity for hundreds of students from dozens of countries (with more participants from the U.S. than from any other country). In addition to providing a novel educational experience, BIBIFI has provided us the opportunity to study the building and breaking process scientifically. In particular, BIBIFI contests may serve as a quasi-controlled experiment that correlates participation data with final outcome. Quantitative analysis from three contests in 2015 found that the most efficient build-it submissions used C/C++, but submissions coded in other statically-typed languages were less likely to have a security flaw; build-it teams with diverse programming-language knowledge also produced more secure code. Shorter programs correlated with better scores. Break-it teams that were also successful build-it teams were significantly better at finding security bugs. We have distilled the lessons learned from running the competition so that they may be applied when developing effective secure-programming courses and, we hope, more generally to actual programming practice. To this end, we have broadly disseminated the results and insights gleaned from BIBIFI through conference publications, conference panels, talks at academic and industrial institutions, and through a widely read blog run by PI Hicks. Finally, we are making the software for BIBIFI?s backend infrastructure publicly available, to facilitate others running their own competitions in and out of the classroom.       Last Modified: 12/02/2016       Submitted by: David M Levin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
