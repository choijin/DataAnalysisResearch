<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SI2-SSI: Collaborative Research: STORM: A Scalable Toolkit for an Open Community Supporting Near Realtime High Resolution Coastal Modeling</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2014</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>730000.00</AwardTotalIntnAmount>
<AwardAmount>730000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bogdan Mihaila</SignBlockName>
<PO_EMAI>bmihaila@nsf.gov</PO_EMAI>
<PO_PHON>7032928235</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The ADCIRC coastal circulation and storm surge model has a long standing track record of extremely high societal impact. It has been used to define risk (e.g., 100-yr, 500-yr coastal flood levels) for the FEMA National Flood Insurance Program in coastal states from New York to Texas, it has been used to design the multi-billion dollar Hurricane and Storm Damage Risk Reduction System around greater New Orleans and southern Louisiana by the Army Corps of Engineers, it is currently run operationally by NOAA/NWS National Center for Environmental Prediction to forecast storm surge, to name just a few of its current and recent applications. Thus there is a well-established user network in place to convert improvements in ADCIRC into significant broader impacts. The proposed research provides transformative intellectual contributions that focus on applying new parallelization schemes to enable major advances in the algorithms, implementation and utilization of the ADCIRC model. The broadening of ADCIRC to a multi-algorithmic framework and the resulting performance gains that are anticipated will help ensure ADCIRC's sustainability as a core community model for at least the next 20 years. In addition, the proposed collaboration will impact computer science by serving as a high impact use case to inform the design of new approaches to efficient scalable computing. Together, the advancements in coastal modeling and parallelization technology will make a significant contribution to the science of modeling and HPC. The results of the proposed research will be disseminated to a wider community through ongoing educational outreach activities at the participating organizations as well as through refereed conference and journal papers, and invited presentations. The involvement of graduate students and post-doctoral fellows will be crucial towards the success of this project. The PIs have a long history of training and mentoring students and post-docs in computational science and engineering, coastal engineering and marine science. The recruitment and involvement of underrepresented groups in these efforts has always been a high priority. In addition, aspects of the proposed research will be incorporated into the curricula of several courses taught by the PIs in the areas of finite element methods, scientific computation, hydrology and oceanography.&lt;br/&gt;The aim of this project is to broaden the ADCIRC coastal circulation and storm surge model from a successful, but somewhat static coastal modeling tool that is tied to a single solution algorithm and the MPI parallelization paradigm, to a dynamic computational platform that is comprised of multiple solution algorithms, that readily admits new solution algorithms and that is built on a transformational new parallelization scheme that will allow us to scale to at least 256k compute cores on modern high performance computing (HPC) systems. We will do this by creating a living, evolving coastal modeling framework that will continue to lead the community in merging physical science / engineering and high performance computing and we will make the framework available to the broader community as a sustainable long term solution for its coastal modeling needs. In addition we will utilize these advancements in the highly demanding coastal storm surge forecasting system that we presently operate to demonstrate both improved robustness and speed of the model solution. We expect this effort will shorten the time required to provide reliable forecasting results and improve our ability to provide highly resolved, accurate, and physically complete predictions on an unprecedented scale. Concurrently, it should enable the use of smaller resources for simulations of increased scale which improves the usability and widens the applicability of ADCIRC in a broader community. The development of tightly integrated web-oriented products like CERA (www.coastalemergency.org) will enable the wide and timely dissemination of forecast modeling results to reach a broad audience.</AbstractNarration>
<MinAmdLetterDate>08/26/2014</MinAmdLetterDate>
<MaxAmdLetterDate>10/20/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1339738</AwardID>
<Investigator>
<FirstName>Joannes</FirstName>
<LastName>Westerink</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joannes J Westerink</PI_FULL_NAME>
<EmailAddress>jjw@nd.edu</EmailAddress>
<PI_PHON>5746316475</PI_PHON>
<NSF_ID>000181253</NSF_ID>
<StartDate>08/26/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tim</FirstName>
<LastName>Stitt</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tim Stitt</PI_FULL_NAME>
<EmailAddress>tstitt@nd.edu</EmailAddress>
<PI_PHON>5746315287</PI_PHON>
<NSF_ID>000586813</NSF_ID>
<StartDate>08/26/2014</StartDate>
<EndDate>10/20/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Damrongsak</FirstName>
<LastName>Wirasaet</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Damrongsak Wirasaet</PI_FULL_NAME>
<EmailAddress>dwirasae@nd.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000674593</NSF_ID>
<StartDate>10/20/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Notre Dame</Name>
<CityName>NOTRE DAME</CityName>
<ZipCode>465565708</ZipCode>
<PhoneNumber>5746317432</PhoneNumber>
<StreetAddress>940 Grace Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>824910376</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NOTRE DAME DU LAC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>048994727</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Notre Dame]]></Name>
<CityName/>
<StateCode>IN</StateCode>
<ZipCode>465565602</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1610</Code>
<Text>PHYSICAL OCEANOGRAPHY</Text>
</ProgramElement>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>2878</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramElement>
<Code>8074</Code>
<Text>EarthCube</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8009</Code>
<Text>Scientifc Software Integration</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~730000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-de2bde49-7fff-9619-4a99-13fcdf8466f4"> <span id="docs-internal-guid-85e4d873-7fff-8a0f-a6f5-f7c590e9c112"> </span></span></p> <p dir="ltr">The STORM project has focused on modernizing coastal ocean circulation codes used to predict coastal flooding as well as nearshore and estuarine flow patterns and how they impact navigation, water quality, fisheries, shoreline erosion and deposition. Many fundamental features of today's operational codes are based on legacy algorithms and coding paradigms. The STORM project's goal has been to develop and/or implement state of the art numerical algorithms and coding paradigms into two leading coastal ocean circulation codes, ADCIRC and DGSWEM, in order to continue to advance their accuracy, adaptability, scalability, efficiency and flexibility. Beyond modernization, we have evolved these codes so that they continue to be platforms for future growth and development.</p> <p dir="ltr">This project's advances enable coastal ocean models to better resolve the topology of the coastal system and its hydrodynamics. The coastal ocean and adjacent inland waterbodies are highly complex topographic systems with submarine canyons, narrow inlet connections, intricate estuaries and bays, and dendritic networks of inland rivers and channels all controlling pathways for water movement and constituent transport.&nbsp; Relevant scales for circulation are even smaller than that of the geographic system scales and can range from hundreds of kilometers down to meters. To improve model accuracy, it is vital that these scales be resolved through continued growth in geometric and process resolution.</p> <p dir="ltr">This project has enabled better and more efficient representations of the system and its hydrodynamics in the intricate coastal zone by using much more efficient and accurate ways of mathematically describing these features and flows. Our codes are based on unstructured mesh finite element methods which allow for variable resolution of the physical system and its hydrodynamic response. We have progressed beyond well-established but simple linear interpolants within each finite element and are now using higher order cubic functions which allow for larger but fewer finite elements, resulting in better efficiency and total computer run-time cost reductions.</p> <p dir="ltr">In addition to advancing the underlying algorithms, we are implementing better ways of coding these algorithms to take advantage of evolving modern computer chip sets and architectures. Specifically, we are now taking full advantage of the vector capabilities of these chip sets that efficiently process entire arrays of numbers as opposed to handling each number within the array one at a time. In addition, we are implementing better ways of harnessing parallel processing methods through splitting the computation into parts and interleaving communication among computer cores to maximize the level of concurrent computations. Advancing the underlying parallel implementations has been done along two lines of development. Both are aimed at allowing the finite element meshes to evolve in extent and complexity as a simulation advances and localized regions need to be included/excluded in the computation and/or resolved more finely/coarsely. For example, when a hurricane arrives and inundates a coastal floodplain, the newly wetted regions need to be incorporated into the computational domain. This requires dynamically re-balancing the parallel computations so that all computational cores maintain balanced workloads. We have implemented dynamic domain decomposition using Sandia National Laboratories' Zoltan Message Passing Interface (MPI) toolkit for in memory data re-partitioning and data migration management. This has evolved the ADCIRC code significantly as computational times are now proportional only to the flooded elements. With the dynamic runtime wet/dry element informed parallel decomposition, the current high-resolution storm surge model for a storm event can be run at greatly reduced cost, a significant benefit for generating high-fidelity forecasts. Furthermore, dynamic load balancing will make possible ultra-high resolution modeling with resolution on the floodplain down to the street level (10m) at little additional cost. Enabling higher resolution across the complicated floodplain, where features such as raised roads and railways can inhibit flood propagation and require high resolution, will significantly enhance accuracy.</p> <p dir="ltr">In addition to the improvements in parallel communications in ADCIRC, we have focused on parallelizing the high order DGSWEM coastal circulation code using the novel and evolving High Performance ParalleX (HPX) framework. HPX is a general purpose C++ runtime system for asynchronous parallel computing using a global address space. HPX is foundationally different from the much more widely used MPI/mesh decomposition approach and strives to provide a unified programming model which transparently utilizes the available compute resources to achieve high levels of scalability. Many of the features we developed for the high order DGSWEM implementation as well as many of ADCIRC's dynamic domain decomposition paradigms are being incorporated into the HPX implementation of DGSWEM by the LSU and UT Austin teams. The HPX implementation will enable less costly and more efficient parallelization and allow flexible and evolving high resolution and therefore more accurate computations.&nbsp;</p> <p dir="ltr">All these state of the art features provide researchers in the coastal ocean hydrodynamics modeling community with more accurate results as well as making high-resolution modeling much more efficient and accessible.</p> <p dir="ltr">&nbsp;</p><br> <p>            Last Modified: 02/11/2019<br>      Modified by: Joannes&nbsp;J&nbsp;Westerink</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    The STORM project has focused on modernizing coastal ocean circulation codes used to predict coastal flooding as well as nearshore and estuarine flow patterns and how they impact navigation, water quality, fisheries, shoreline erosion and deposition. Many fundamental features of today's operational codes are based on legacy algorithms and coding paradigms. The STORM project's goal has been to develop and/or implement state of the art numerical algorithms and coding paradigms into two leading coastal ocean circulation codes, ADCIRC and DGSWEM, in order to continue to advance their accuracy, adaptability, scalability, efficiency and flexibility. Beyond modernization, we have evolved these codes so that they continue to be platforms for future growth and development. This project's advances enable coastal ocean models to better resolve the topology of the coastal system and its hydrodynamics. The coastal ocean and adjacent inland waterbodies are highly complex topographic systems with submarine canyons, narrow inlet connections, intricate estuaries and bays, and dendritic networks of inland rivers and channels all controlling pathways for water movement and constituent transport.  Relevant scales for circulation are even smaller than that of the geographic system scales and can range from hundreds of kilometers down to meters. To improve model accuracy, it is vital that these scales be resolved through continued growth in geometric and process resolution. This project has enabled better and more efficient representations of the system and its hydrodynamics in the intricate coastal zone by using much more efficient and accurate ways of mathematically describing these features and flows. Our codes are based on unstructured mesh finite element methods which allow for variable resolution of the physical system and its hydrodynamic response. We have progressed beyond well-established but simple linear interpolants within each finite element and are now using higher order cubic functions which allow for larger but fewer finite elements, resulting in better efficiency and total computer run-time cost reductions. In addition to advancing the underlying algorithms, we are implementing better ways of coding these algorithms to take advantage of evolving modern computer chip sets and architectures. Specifically, we are now taking full advantage of the vector capabilities of these chip sets that efficiently process entire arrays of numbers as opposed to handling each number within the array one at a time. In addition, we are implementing better ways of harnessing parallel processing methods through splitting the computation into parts and interleaving communication among computer cores to maximize the level of concurrent computations. Advancing the underlying parallel implementations has been done along two lines of development. Both are aimed at allowing the finite element meshes to evolve in extent and complexity as a simulation advances and localized regions need to be included/excluded in the computation and/or resolved more finely/coarsely. For example, when a hurricane arrives and inundates a coastal floodplain, the newly wetted regions need to be incorporated into the computational domain. This requires dynamically re-balancing the parallel computations so that all computational cores maintain balanced workloads. We have implemented dynamic domain decomposition using Sandia National Laboratories' Zoltan Message Passing Interface (MPI) toolkit for in memory data re-partitioning and data migration management. This has evolved the ADCIRC code significantly as computational times are now proportional only to the flooded elements. With the dynamic runtime wet/dry element informed parallel decomposition, the current high-resolution storm surge model for a storm event can be run at greatly reduced cost, a significant benefit for generating high-fidelity forecasts. Furthermore, dynamic load balancing will make possible ultra-high resolution modeling with resolution on the floodplain down to the street level (10m) at little additional cost. Enabling higher resolution across the complicated floodplain, where features such as raised roads and railways can inhibit flood propagation and require high resolution, will significantly enhance accuracy. In addition to the improvements in parallel communications in ADCIRC, we have focused on parallelizing the high order DGSWEM coastal circulation code using the novel and evolving High Performance ParalleX (HPX) framework. HPX is a general purpose C++ runtime system for asynchronous parallel computing using a global address space. HPX is foundationally different from the much more widely used MPI/mesh decomposition approach and strives to provide a unified programming model which transparently utilizes the available compute resources to achieve high levels of scalability. Many of the features we developed for the high order DGSWEM implementation as well as many of ADCIRC's dynamic domain decomposition paradigms are being incorporated into the HPX implementation of DGSWEM by the LSU and UT Austin teams. The HPX implementation will enable less costly and more efficient parallelization and allow flexible and evolving high resolution and therefore more accurate computations.  All these state of the art features provide researchers in the coastal ocean hydrodynamics modeling community with more accurate results as well as making high-resolution modeling much more efficient and accessible.         Last Modified: 02/11/2019       Submitted by: Joannes J Westerink]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
