<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Small: Virtualized Robot Test and Integration Laboratory</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>05/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>1000000.00</AwardTotalIntnAmount>
<AwardAmount>1093640</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Virtual Robot Test and Integration Laboratory (VRTIL) addresses the need to verify robot software using real data, produced by real sensors, in all its richness and imperfection. The project is embedding robot software in a synthetic environment in order to mimic its sensor interfaces. In an advance over the state of the art, these interfaces are windows into data bases that store the actual sensor data that a robot did read when it was in a particular state.&lt;br/&gt;The project goal is to reproduce the real world interface as exactly as possible using data. Two basic tools enable such ?virtualization? of a real robot in a real environment:&lt;br/&gt;&lt;br/&gt;* Virtualized reality provides a means to produce synthetic views from virtual viewpoints that are near to an actual viewpoint for which a real image is available;&lt;br/&gt;&lt;br/&gt;* High fidelity synthesized motion gathers large amounts of data to calibrate a model that is valid across all of state space.&lt;br/&gt;&lt;br/&gt;Combining these ideas will result in a  software test environment for robot software that has the realism of data logs and the responsiveness of a simulator.  This may fundamentally transform the development of robots by lowering the cost and time of testing and debugging.</AbstractNarration>
<MinAmdLetterDate>09/04/2013</MinAmdLetterDate>
<MaxAmdLetterDate>03/08/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1317803</AwardID>
<Investigator>
<FirstName>Alonzo</FirstName>
<LastName>Kelly</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alonzo Kelly</PI_FULL_NAME>
<EmailAddress>alonzo@ri.cmu.edu</EmailAddress>
<PI_PHON>4126832550</PI_PHON>
<NSF_ID>000207357</NSF_ID>
<StartDate>09/04/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName/>
<StateCode>PA</StateCode>
<ZipCode>152133890</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramElement>
<Code>P154</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~1000000</FUND_OBLG>
<FUND_OBLG>2016~93640</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <div>The scale and complexity of robotics is increasing. Efforts are underway to deploy robots as self-driving cars and in warehouses. Many applications need to be developed for robots, including even basic tasks such as perceiving and interpreting the environment. Application development is challenging in reality. It may be expensive, with delays due to robot hardware issues. It may also be impractical to exhaustively test robots in reality. Simulation is an attractive answer to these challenges, and is a timely topic that has received recent attention. It is further relevant given the widespread use of machine learning algorithms, which rely on large amounts of data.</div> <div>Our work takes steps to close the gap between simulation and reality, for Lidar simulation. Lidars are a key technology for autonomous systems. They are used to perceive the environment, and applications are routinely written to process Lidar data. The reliable operation of these applications is key to making good decisions. We adopt the perspective that the eventual purpose of a simulator is a tool for robot application development. Our framework for sensor simulation consists of three components. The first is sensor modeling, which describes how a sensor interacts with the environment. The second is scene generation, needed to construct simulated worlds corresponding to reality. The third is simulator evaluation, based on comparing real and simulated data. Our framework is broadly applicable to simulating sensors other than Lidars.</div> <div>The instantiation of our framework has produced two Lidar simulators. The first is a planar Lidar simulator for indoor environments. Our sensor models are largely data-driven. We take advantage of the large amounts of data generated by sensors, and advances in machine learning methods. Our indoor Lidar models have found application in education, where they were part of a simulator used by students in a course. The simulator helped students make progress when real robots were difficult to work with. The second simulator is a 3D Lidar simulator for off-road environments. We build and extract models of terrain from data. This data-based approach makes our simulator flexible, as the models can be composed to build new simulated worlds. This simulator finds application in off-road mobile robots, and could also benefit aerial robots.</div> <div>Our simulators span the length scale: while indoor environments are several meters in dimension, off-road ones are hundreds of meters long. Our work improves significantly on existing open-source simulators. We evaluate our simulators in complex real-world environments, while existing simulators are often evaluated only in toy worlds. We also validate our simulators holistically, for the purpose of robot application development. Existing simulators, by contrast, are only validated in pieces. In summary, we built flexible, and high-fidelity Lidar simulators. We believe simulators will play a vital role in the robot application development cycle. Our work suggests a generic approach to building useful sensor simulators.&nbsp;</div> <p>&nbsp;</p><br> <p>            Last Modified: 07/02/2018<br>      Modified by: Alonzo&nbsp;Kelly</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530541774416_indoor_comparisons--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530541774416_indoor_comparisons--rgov-800width.jpg" title="Indoor Comparisons"><img src="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530541774416_indoor_comparisons--rgov-66x44.jpg" alt="Indoor Comparisons"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Indoor Comparisons</div> <div class="imageCredit">Abhijeet Tallavajhula</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alonzo&nbsp;Kelly</div> <div class="imageTitle">Indoor Comparisons</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530541835717_motivation_offroad--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530541835717_motivation_offroad--rgov-800width.jpg" title="Motivation - Offroad"><img src="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530541835717_motivation_offroad--rgov-66x44.jpg" alt="Motivation - Offroad"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Motivation - Offroad</div> <div class="imageCredit">Abhijeet Tallavajhula</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alonzo&nbsp;Kelly</div> <div class="imageTitle">Motivation - Offroad</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530541883336_offroad_comparisons--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530541883336_offroad_comparisons--rgov-800width.jpg" title="Offroad Comparisons"><img src="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530541883336_offroad_comparisons--rgov-66x44.jpg" alt="Offroad Comparisons"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Offroad Comparisons</div> <div class="imageCredit">Abhijeet Tallavajhula</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alonzo&nbsp;Kelly</div> <div class="imageTitle">Offroad Comparisons</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530542071014_motivation_education_cropped--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530542071014_motivation_education_cropped--rgov-800width.jpg" title="Motivation - Education"><img src="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530542071014_motivation_education_cropped--rgov-66x44.jpg" alt="Motivation - Education"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Motivation - Education</div> <div class="imageCredit">Abhijeet Tallavajula</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alonzo&nbsp;Kelly</div> <div class="imageTitle">Motivation - Education</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530542135525_our_work_1--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530542135525_our_work_1--rgov-800width.jpg" title="Our Work 1"><img src="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530542135525_our_work_1--rgov-66x44.jpg" alt="Our Work 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Our Work 1</div> <div class="imageCredit">Abhijeet Tallavajula</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alonzo&nbsp;Kelly</div> <div class="imageTitle">Our Work 1</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530542198822_our_work_2--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530542198822_our_work_2--rgov-800width.jpg" title="Our Work 2"><img src="/por/images/Reports/POR/2018/1317803/1317803_10275114_1530542198822_our_work_2--rgov-66x44.jpg" alt="Our Work 2"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Our Work 2</div> <div class="imageCredit">Abhijeet Tallavajula</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alonzo&nbsp;Kelly</div> <div class="imageTitle">Our Work 2</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   The scale and complexity of robotics is increasing. Efforts are underway to deploy robots as self-driving cars and in warehouses. Many applications need to be developed for robots, including even basic tasks such as perceiving and interpreting the environment. Application development is challenging in reality. It may be expensive, with delays due to robot hardware issues. It may also be impractical to exhaustively test robots in reality. Simulation is an attractive answer to these challenges, and is a timely topic that has received recent attention. It is further relevant given the widespread use of machine learning algorithms, which rely on large amounts of data. Our work takes steps to close the gap between simulation and reality, for Lidar simulation. Lidars are a key technology for autonomous systems. They are used to perceive the environment, and applications are routinely written to process Lidar data. The reliable operation of these applications is key to making good decisions. We adopt the perspective that the eventual purpose of a simulator is a tool for robot application development. Our framework for sensor simulation consists of three components. The first is sensor modeling, which describes how a sensor interacts with the environment. The second is scene generation, needed to construct simulated worlds corresponding to reality. The third is simulator evaluation, based on comparing real and simulated data. Our framework is broadly applicable to simulating sensors other than Lidars. The instantiation of our framework has produced two Lidar simulators. The first is a planar Lidar simulator for indoor environments. Our sensor models are largely data-driven. We take advantage of the large amounts of data generated by sensors, and advances in machine learning methods. Our indoor Lidar models have found application in education, where they were part of a simulator used by students in a course. The simulator helped students make progress when real robots were difficult to work with. The second simulator is a 3D Lidar simulator for off-road environments. We build and extract models of terrain from data. This data-based approach makes our simulator flexible, as the models can be composed to build new simulated worlds. This simulator finds application in off-road mobile robots, and could also benefit aerial robots. Our simulators span the length scale: while indoor environments are several meters in dimension, off-road ones are hundreds of meters long. Our work improves significantly on existing open-source simulators. We evaluate our simulators in complex real-world environments, while existing simulators are often evaluated only in toy worlds. We also validate our simulators holistically, for the purpose of robot application development. Existing simulators, by contrast, are only validated in pieces. In summary, we built flexible, and high-fidelity Lidar simulators. We believe simulators will play a vital role in the robot application development cycle. Our work suggests a generic approach to building useful sensor simulators.           Last Modified: 07/02/2018       Submitted by: Alonzo Kelly]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
