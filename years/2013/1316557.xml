<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>The Validity of Technology-Enhanced Assessment in Geometry</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2013</AwardEffectiveDate>
<AwardExpirationDate>12/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>432812.00</AwardTotalIntnAmount>
<AwardAmount>437341</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Karen King</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Assessment developers, state departments of education, and national consortia have focused extensive efforts on including Technology-Enhanced Items (TEIs) on summative and formative assessments. TEIs have a number of potential benefits over traditional, selected-response (SR) items, including the potential to measure higher-level constructs, the reduction of the effects of test-taking skills and guessing, the capture of rich diagnostic information, the reduction of cognitive load from non-relevant constructs, and the engaging nature of their design. The first three benefits are true of constructed-response (CR) items, but TEIs have the added benefit of being automatically scored by computer. Despite the potential benefits of TEIs, and the strong push to include these types of items in assessments, there is a death of research on the validity of inferences made by TEIs and on whether TEIs provide improved measurement over traditional item types. The Validity of Technology-Enhanced Assessment in Geometry (VTAG) project contributes to the small research base by exploring the validity of TEIs in the context of elementary geometry. &lt;br/&gt;&lt;br/&gt;The project addresses three research questions: &lt;br/&gt;&lt;br/&gt;RQ1: To what extent are TEIs a valid measurement of geometry standards in the elementary grades? &lt;br/&gt;RQ2: To what extent do TEIs provide an improved measurement compared to SR items? &lt;br/&gt;RQ3: What are the general characteristics of mathematics standards that might be better measured through TEIs?&lt;br/&gt;&lt;br/&gt;To address these research questions, the researchers develop 20 items (ten SR items and 10 TEIs) for each of the seven Common Core State Standards in fourth and fifth grade geometry. The researchers collect validity evidence using expert review, cognitive labs, and classroom administration of the items. The first two research questions are addressed by evaluating the validity of the items based on a variety of sources, including test content, internal structure, the relationship to other variables, and student response processes. To address the third research question, informed by the results of the prior two, the researchers use qualitative analysis to identify common themes of the standards that were identified as being better measured through TEIs.</AbstractNarration>
<MinAmdLetterDate>08/01/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/17/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1316557</AwardID>
<Investigator>
<FirstName>Jessica</FirstName>
<LastName>Masters</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jessica Masters</PI_FULL_NAME>
<EmailAddress>jessica.masters@researchmattersllc.org</EmailAddress>
<PI_PHON>8885313347</PI_PHON>
<NSF_ID>000589901</NSF_ID>
<StartDate>08/01/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Measured Progress</Name>
<CityName>Dover</CityName>
<ZipCode>038205816</ZipCode>
<PhoneNumber>6037499102</PhoneNumber>
<StreetAddress>P.O. Box 1217</StreetAddress>
<StreetAddress2><![CDATA[100 Education Way]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<StateCode>NH</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NH01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>108183682</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MEASURED PROGRESS, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>108183682</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Measured Progress]]></Name>
<CityName>Dover</CityName>
<StateCode>NH</StateCode>
<ZipCode>038205814</ZipCode>
<StreetAddress><![CDATA[100 Education Wa]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NH01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7645</Code>
<Text>Discovery Research K-12</Text>
</ProgramElement>
<ProgramReference>
<Code>CL10</Code>
<Text>CLB-Career Life Balance</Text>
</ProgramReference>
<Appropriation>
<Code>0413</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~432812</FUND_OBLG>
<FUND_OBLG>2014~4529</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The Validity of Technology-Enhanced Assessment in Geometry (VTAG) project collected evidence about the extent to which technology-enhanced (TE) items provided a valid measure of fourth and fifth grade geometry standards and the extent to which TE items provided improved measurement, compared to traditional selected-response (SR) items. TE items were defined based on the level of constraint placed on the response, not based on the interface or interaction required. TE items required students to <em>produce</em> a response, rather than <em>select</em> a response from a limited set of options. The project collected evidence of validity based on test content, student response processes, internal structure of items, and the relationship of items to other variables. Evidence was collected through expert review, cognitive labs, and field tests (large-scale classroom administrations).</p> <p>There was ample evidence to indicate that TE items provided a valid measure of student knowledge. The evidence showed that the TE items measured the intended construct, had strong internal structure, and were significantly correlated with an independent measure of student knowledge. There was no evidence that students&rsquo; comfort with technology or attitude towards technology or math was related to performance on TE items.</p> <p>The extent to which TE items provided more or better information compared to SR items was highly dependent on the nature of the construct. The expert review noted that many TE items seemed better designed to assess some constructs. This was supported with evidence from the cognitive labs and field tests, which revealed that some TE items provided a more accurate measure of student knowledge, had stronger internal structure, and were better aligned to the independent measure. But the evidence was not consistent across all items. Further, TE items consistently required more time, which must be balanced against potential measurement improvements.</p> <p>TE items have become rapidly ubiquitous in low- and high-stakes testing, often without research-based evidence of validity. Based on the VTAG findings, the researchers recommend that item developers not consider TE items a panacea to solve all modern measurement challenges. Rather, TE item types should be treated as one of many tools in the item developer&rsquo;s toolbox. This represents a kind of <em>back to basics</em> for item writing: the selection of item type should be based on the construct being measured. TE items should not be automatically favored over SR items because it is assumed they will always provide a better measure. Whether TE items will provide improved measurement is likely to be highly dependent on the construct. Item writers should consider the action students will take to demonstrate knowledge to inform the selection of item type. For example, measuring a student&rsquo;s ability to identify or recognize might be actions that can be accurately and fully measured with an SR item because the action can be captured even when a high degree of constraint is placed upon the response options. Measuring a student&rsquo;s ability to draw, classify, represent, or interpret might be better measured with a TE item because those actions might be better demonstrated with a less constrained response space.</p> <p>The researchers further recommend that TE items should not be avoided because they are likely to require more time, are typically more expensive to develop and test, or because they might introduce construct-irrelevant factors. Well-designed TE items do have potential to provide improved measurement of some constructs. Thus, again, item developers should choose the item type based on the best way to measure a specifically targeted construct, considering the varying levels of constraint represented by SR and TE item types as the broad set of possible item types from which to choose.</p> <p>When using TE items, assessment designers must take new factors into considerations during the validation process. In addition to the traditional concerns of validity (context, language, bias, etc.), validity work must consider the complexity of items, the time required to produce a response, the item interface, the test platform interface, the hardware and software used by students, etc. These factors will influence whether TE items will provide valid measures and whether that measure will be an improvement over what could have been measured with an SR item.</p> <p>Finally, the researchers recommend that more research and consideration is given to the interfaces used in the design of individual TE items and in the design of computer-based testing platforms. Careful design of items interfaces and test platforms can mitigate potential construct-irrelevant variance that could otherwise inhibit TE items from measuring the construct. For example, students should have tutorials that describe key features of the platform (e.g., how to navigate between items) and have ample opportunities to practice using the different features and interfaces of TE items (e.g., dragging and dropping items into categories, drawing figures). For even well-designed TE items to provide accurate or improved measurement, students (including modern, tech-savvy students) need training, practice, and support.</p><br> <p>            Last Modified: 03/28/2017<br>      Modified by: Jessica&nbsp;Masters</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Validity of Technology-Enhanced Assessment in Geometry (VTAG) project collected evidence about the extent to which technology-enhanced (TE) items provided a valid measure of fourth and fifth grade geometry standards and the extent to which TE items provided improved measurement, compared to traditional selected-response (SR) items. TE items were defined based on the level of constraint placed on the response, not based on the interface or interaction required. TE items required students to produce a response, rather than select a response from a limited set of options. The project collected evidence of validity based on test content, student response processes, internal structure of items, and the relationship of items to other variables. Evidence was collected through expert review, cognitive labs, and field tests (large-scale classroom administrations).  There was ample evidence to indicate that TE items provided a valid measure of student knowledge. The evidence showed that the TE items measured the intended construct, had strong internal structure, and were significantly correlated with an independent measure of student knowledge. There was no evidence that students? comfort with technology or attitude towards technology or math was related to performance on TE items.  The extent to which TE items provided more or better information compared to SR items was highly dependent on the nature of the construct. The expert review noted that many TE items seemed better designed to assess some constructs. This was supported with evidence from the cognitive labs and field tests, which revealed that some TE items provided a more accurate measure of student knowledge, had stronger internal structure, and were better aligned to the independent measure. But the evidence was not consistent across all items. Further, TE items consistently required more time, which must be balanced against potential measurement improvements.  TE items have become rapidly ubiquitous in low- and high-stakes testing, often without research-based evidence of validity. Based on the VTAG findings, the researchers recommend that item developers not consider TE items a panacea to solve all modern measurement challenges. Rather, TE item types should be treated as one of many tools in the item developer?s toolbox. This represents a kind of back to basics for item writing: the selection of item type should be based on the construct being measured. TE items should not be automatically favored over SR items because it is assumed they will always provide a better measure. Whether TE items will provide improved measurement is likely to be highly dependent on the construct. Item writers should consider the action students will take to demonstrate knowledge to inform the selection of item type. For example, measuring a student?s ability to identify or recognize might be actions that can be accurately and fully measured with an SR item because the action can be captured even when a high degree of constraint is placed upon the response options. Measuring a student?s ability to draw, classify, represent, or interpret might be better measured with a TE item because those actions might be better demonstrated with a less constrained response space.  The researchers further recommend that TE items should not be avoided because they are likely to require more time, are typically more expensive to develop and test, or because they might introduce construct-irrelevant factors. Well-designed TE items do have potential to provide improved measurement of some constructs. Thus, again, item developers should choose the item type based on the best way to measure a specifically targeted construct, considering the varying levels of constraint represented by SR and TE item types as the broad set of possible item types from which to choose.  When using TE items, assessment designers must take new factors into considerations during the validation process. In addition to the traditional concerns of validity (context, language, bias, etc.), validity work must consider the complexity of items, the time required to produce a response, the item interface, the test platform interface, the hardware and software used by students, etc. These factors will influence whether TE items will provide valid measures and whether that measure will be an improvement over what could have been measured with an SR item.  Finally, the researchers recommend that more research and consideration is given to the interfaces used in the design of individual TE items and in the design of computer-based testing platforms. Careful design of items interfaces and test platforms can mitigate potential construct-irrelevant variance that could otherwise inhibit TE items from measuring the construct. For example, students should have tutorials that describe key features of the platform (e.g., how to navigate between items) and have ample opportunities to practice using the different features and interfaces of TE items (e.g., dragging and dropping items into categories, drawing figures). For even well-designed TE items to provide accurate or improved measurement, students (including modern, tech-savvy students) need training, practice, and support.       Last Modified: 03/28/2017       Submitted by: Jessica Masters]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
