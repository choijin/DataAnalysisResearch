<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Small: Collaborative Research: Active Sensing for Robotic Cameramen</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>292000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>With advances in camera technologies, and as cloud storage, network bandwidth and protocols become available, visual media are becoming ubiquitous. Video recording became de facto universal means of instruction for a wide range of applications such as physical exercise, technology, assembly, or cooking. This project addresses the scientific and technological challenges of video shooting in terms of coverage and optimal views planning while leaving high level aspects including creativity to the video editing and post-production stages. &lt;br/&gt;&lt;br/&gt;Camera placement and novel view selection challenges are modeled as optimization problems that minimize the uncertainty in the location of actors and objects, maximize coverage and effective appearance resolution, and optimize object detection for the sake of semantic annotation of the scene. New probabilistic models capture long range correlations when the trajectories of actors are only partially observable.  Quality of potential novel views is modeled in terms of resolution that is optimized by maximizing the coverage of a 3D orientation histogram while an active view selection process for object detection minimizes a dynamic programming objective function capturing the loss due to classification error as well as the resources spent for each view.&lt;br/&gt;&lt;br/&gt;The project advances active sensing and perception and provides the technology for further automation on video capturing. Such technology has broader impact on the production of education videos for online courses as well as in telepresence applications. Research results are integrated into robotics and digital media programs addressing K-12 students.</AbstractNarration>
<MinAmdLetterDate>09/12/2013</MinAmdLetterDate>
<MaxAmdLetterDate>04/29/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1317788</AwardID>
<Investigator>
<FirstName>Ibrahim</FirstName>
<LastName>Isler</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ibrahim V Isler</PI_FULL_NAME>
<EmailAddress>isler@cs.umn.edu</EmailAddress>
<PI_PHON>6126251067</PI_PHON>
<NSF_ID>000233463</NSF_ID>
<StartDate>09/12/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<StateCode>MN</StateCode>
<ZipCode>554550159</ZipCode>
<StreetAddress><![CDATA[200 Union Street SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~292000</FUND_OBLG>
<FUND_OBLG>2015~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Many computer vision tasks such as scene coverage and object detection rely on the acquisition of high quality images related to the task. In this project, we explored mechanisms to actively control cameras to ensure the capture of such images.&nbsp; The algorithms can be used at a variety of scales ranging from planning the trajectories of a camera mounted on an aerial vehicle inspecting a field to the case of a camera mounted on a robot arm for visual inspection.</p> <p>In terms of intellectual merit, the primary contributions of the project&nbsp; included an algorithm with theoretical performance guaranteess to visit a given set of viewing cones in the shortest amount of time. This problem arises in a number of settings including visual inspection of farms where the cones correspond to the viewing regions of areas of interest (e.g. potential infestations). We also developed a novel visual servoing platform where a camera is mounted on a robot arm and is controlled by interpreting the images acquired in an online fashion. We demonstrated the algorithm in an orchard inspection task which is challenging because the scene is not static (due to wind) and light conditions can be varying drastically: there can be bright, direct sunlight or many shadows.</p> <p>In terms of broader impact, such view planning algorithms can be useful in a variety of applications of societal importance. These include agriculture as mentioned above, industrial inspection or search and rescue. Graduate and undergraduate students (some of whom were underrepresented minorities) were trained as part of this project.</p><br> <p>            Last Modified: 11/30/2017<br>      Modified by: Ibrahim&nbsp;V&nbsp;Isler</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1317788/1317788_10278885_1512065183905_huskywitharm--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1317788/1317788_10278885_1512065183905_huskywitharm--rgov-800width.jpg" title="Active Sensing Platform"><img src="/por/images/Reports/POR/2017/1317788/1317788_10278885_1512065183905_huskywitharm--rgov-66x44.jpg" alt="Active Sensing Platform"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A camera mounted on a robot arm, which in turn is mounted on a mobile base, can be actively controlled for a variety of inspection tasks.</div> <div class="imageCredit">Volkan Isler</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ibrahim&nbsp;V&nbsp;Isler</div> <div class="imageTitle">Active Sensing Platform</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Many computer vision tasks such as scene coverage and object detection rely on the acquisition of high quality images related to the task. In this project, we explored mechanisms to actively control cameras to ensure the capture of such images.  The algorithms can be used at a variety of scales ranging from planning the trajectories of a camera mounted on an aerial vehicle inspecting a field to the case of a camera mounted on a robot arm for visual inspection.  In terms of intellectual merit, the primary contributions of the project  included an algorithm with theoretical performance guaranteess to visit a given set of viewing cones in the shortest amount of time. This problem arises in a number of settings including visual inspection of farms where the cones correspond to the viewing regions of areas of interest (e.g. potential infestations). We also developed a novel visual servoing platform where a camera is mounted on a robot arm and is controlled by interpreting the images acquired in an online fashion. We demonstrated the algorithm in an orchard inspection task which is challenging because the scene is not static (due to wind) and light conditions can be varying drastically: there can be bright, direct sunlight or many shadows.  In terms of broader impact, such view planning algorithms can be useful in a variety of applications of societal importance. These include agriculture as mentioned above, industrial inspection or search and rescue. Graduate and undergraduate students (some of whom were underrepresented minorities) were trained as part of this project.       Last Modified: 11/30/2017       Submitted by: Ibrahim V Isler]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
