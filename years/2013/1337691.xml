<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Collaborative Research: Statistical Learning of Language Universals</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>109887.00</AwardTotalIntnAmount>
<AwardAmount>109887</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As modern technology infrastructure spreads throughout the world, the quantity of electronic text, written in hundreds of different languages, continues to grow in size and diversity. Building effective information retrieval, extraction, and translation systems across this vast array of languages currently requires time-consuming and expensive linguistic annotations for each language.  Generic, fully unsupervised, methods are unlikely to provide a language independent solution to this problem.&lt;br/&gt;&lt;br/&gt;Focusing on part-of-speech prediction, this project undertakes a novel approach, combining elements of supervised and unsupervised learning without assuming any specific knowledge of the target language.  Instead of treating individual languages as closed systems, language-independent "universals" are statistically estimated from dozens of languages for which annotated corpora exist, and these learned universals are used to predict the part-of-speech categories of unannotated languages. At the heart of the project is a data-driven exploration of language-independent corpus characteristics that relate cross-lingual linguistic categories to surface statistics of text. These learned patterns are incorporated into expressive structured prediction models using novel approximate learning and inference methods developed by the Principal Investigators of the project.&lt;br/&gt;&lt;br/&gt;Of the world?s spoken languages, hundreds are at risk of immediate extinction and thousands more are likely to disappear over the coming decades. By facilitating the rapid creation of language-independent linguistic analysis tools, the technology developed under this project has the potential to revolutionize the documentation of endangered languages. In the long-term, this research direction will also help realize the full social benefits of the global technology infrastructure by creating intelligent text processing tools for hundreds of low-resource languages.</AbstractNarration>
<MinAmdLetterDate>02/27/2013</MinAmdLetterDate>
<MaxAmdLetterDate>02/05/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1337691</AwardID>
<Investigator>
<FirstName>Ben</FirstName>
<LastName>Taskar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ben Taskar</PI_FULL_NAME>
<EmailAddress>taskar@gmail.com</EmailAddress>
<PI_PHON>2065431695</PI_PHON>
<NSF_ID>000491164</NSF_ID>
<StartDate>02/27/2013</StartDate>
<EndDate>02/05/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Luke</FirstName>
<LastName>Zettlemoyer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Dr.</PI_SUFX_NAME>
<PI_FULL_NAME>Luke Zettlemoyer</PI_FULL_NAME>
<EmailAddress>lsz@cs.washington.edu</EmailAddress>
<PI_PHON>2066851227</PI_PHON>
<NSF_ID>000581613</NSF_ID>
<StartDate>02/05/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName/>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~109887</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>As modern technology infrastructure spreads throughout the world, the quantity of electronic text, written in hundreds of different languages, continues to grow in size and diversity. Building effective information retrieval, extraction, and translation systems across this vast array of languages currently requires time-consuming and expensive linguistic annotations for each language. Generic, fully unsupervised, methods are unlikely to provide a language independent solution to this problem.</span><br /><br /><span>This project undertook a novel approach, combining elements of supervised and unsupervised learning without assuming any specific knowledge of the target language. Instead of treating individual languages as closed systems, language-independent "universals" are statistically estimated from dozens of languages for which annotated corpora exist, and these learned universals are used to predict aspects of the unannotated languages. At the heart of the project is a data-driven exploration of language-independent corpus characteristics that relate cross-lingual linguistic categories to surface statistics of text. These learned patterns are incorporated into expressive structured prediction models using novel approximate learning and inference methods developed by the Principal Investigators of the project.&nbsp;</span>We considered two specific settings.</p> <p>We completed work on mult-label classification where, for example, you might aim to recover all of the keywords that an image could be tagged with. Here, the goal was&nbsp;to (1) learn an accurate classifier for each label, (2) model missing data labels, which have been shown to be common in these problems especially as the number of labels grows, and (3) encourage sparse predictions. By regularizing the posterior we areable to introduce soft, global constraints that, for the first time, jointly achieve all three goals.</p> <p>We also make significant progress on the ongoing efforts to&nbsp;generalizing these results to multi-lingual relation extraction with distant supervision. Here, the goal is to show that linguistic univerals can help find sentences that describe facts in a target database, such as Freebase. Previous work has shown that weak alignment information is helpful in English extraction, including aligning the string in the database to mentions in sentences, but have suffered from noise in the alignment process and sometimes fail to find high qulaity extactors. Our work showed that in the multi-lingual setting each language describes the facts differently, creating a redundancy that allows a new algorithm we introduced learn to improve the overall extraction results.&nbsp;</p> <p><span><br /></span></p><br> <p>            Last Modified: 01/18/2016<br>      Modified by: Luke&nbsp;Zettlemoyer</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As modern technology infrastructure spreads throughout the world, the quantity of electronic text, written in hundreds of different languages, continues to grow in size and diversity. Building effective information retrieval, extraction, and translation systems across this vast array of languages currently requires time-consuming and expensive linguistic annotations for each language. Generic, fully unsupervised, methods are unlikely to provide a language independent solution to this problem.  This project undertook a novel approach, combining elements of supervised and unsupervised learning without assuming any specific knowledge of the target language. Instead of treating individual languages as closed systems, language-independent "universals" are statistically estimated from dozens of languages for which annotated corpora exist, and these learned universals are used to predict aspects of the unannotated languages. At the heart of the project is a data-driven exploration of language-independent corpus characteristics that relate cross-lingual linguistic categories to surface statistics of text. These learned patterns are incorporated into expressive structured prediction models using novel approximate learning and inference methods developed by the Principal Investigators of the project. We considered two specific settings.  We completed work on mult-label classification where, for example, you might aim to recover all of the keywords that an image could be tagged with. Here, the goal was to (1) learn an accurate classifier for each label, (2) model missing data labels, which have been shown to be common in these problems especially as the number of labels grows, and (3) encourage sparse predictions. By regularizing the posterior we areable to introduce soft, global constraints that, for the first time, jointly achieve all three goals.  We also make significant progress on the ongoing efforts to generalizing these results to multi-lingual relation extraction with distant supervision. Here, the goal is to show that linguistic univerals can help find sentences that describe facts in a target database, such as Freebase. Previous work has shown that weak alignment information is helpful in English extraction, including aligning the string in the database to mentions in sentences, but have suffered from noise in the alignment process and sometimes fail to find high qulaity extactors. Our work showed that in the multi-lingual setting each language describes the facts differently, creating a redundancy that allows a new algorithm we introduced learn to improve the overall extraction results.           Last Modified: 01/18/2016       Submitted by: Luke Zettlemoyer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
