<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: MEDIUM: Collaborative Research: Transfer Learning in Software Engineering</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>482852.00</AwardTotalIntnAmount>
<AwardAmount>482852</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of the research is to enable software engineers to find software development best practices from past empirical data. The increasing availability of software development project data, plus new machine learning techniques, make it possible for researchers to study the generalizability of results across projects using the concept of transfer learning. Using data from real software projects, the project will determine and validate best practices in three areas: predicting software development effort; isolating software detects; effective code inspection practices. &lt;br/&gt;&lt;br/&gt;This research will deliver new data mining technologies in the form of transfer learning techniques and tools that overcome current limitations in the state-of-the-art to provide accurate learning within and across projects. It will design new empirical studies, which apply transfer learning to empirical data collected from industrial software projects. It will build an on-line model analysis service, making the techniques and tools available to other researchers who are investigating validity of principles for best practice. &lt;br/&gt;&lt;br/&gt;The broader impacts of the research will be to make empirical software engineering research results more transferable to practice, and to improve the research processes for the empirical software engineering community.  By providing a means to test principles about software development, this work stands to transform empirical software engineering research and enable software managers to rely on scientifically obtained facts and conclusions rather than anecdotal evidence and one-off studies. Given the immense importance and cost of software in commercial and critical systems, the research has long-term economic impacts.</AbstractNarration>
<MinAmdLetterDate>04/19/2013</MinAmdLetterDate>
<MaxAmdLetterDate>06/01/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1302169</AwardID>
<Investigator>
<FirstName>Forrest</FirstName>
<LastName>Shull</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Forrest Shull</PI_FULL_NAME>
<EmailAddress>fshull@fc-md.umd.edu</EmailAddress>
<PI_PHON>2404872904</PI_PHON>
<NSF_ID>000216375</NSF_ID>
<StartDate>04/19/2013</StartDate>
<EndDate>04/03/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Lucas</FirstName>
<LastName>Layman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lucas Layman</PI_FULL_NAME>
<EmailAddress>laymanl@uncw.edu</EmailAddress>
<PI_PHON>9109623672</PI_PHON>
<NSF_ID>000602659</NSF_ID>
<StartDate>04/03/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Lucas</FirstName>
<LastName>Layman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lucas Layman</PI_FULL_NAME>
<EmailAddress>laymanl@uncw.edu</EmailAddress>
<PI_PHON>9109623672</PI_PHON>
<NSF_ID>000602659</NSF_ID>
<StartDate>04/19/2013</StartDate>
<EndDate>04/03/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Fraunhofer Center for Experimental Software Engineering</Name>
<CityName>Riverdale</CityName>
<ZipCode>207371250</ZipCode>
<PhoneNumber>3013146070</PhoneNumber>
<StreetAddress>5700 Rivertech Court</StreetAddress>
<StreetAddress2><![CDATA[Suite 210]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>116420691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>FRAUNHOFER USA, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>317938488</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Fraunhofer Center for Experimental Software Engineering]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207403823</ZipCode>
<StreetAddress><![CDATA[5825 University Research Ct.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~244149</FUND_OBLG>
<FUND_OBLG>2015~118390</FUND_OBLG>
<FUND_OBLG>2016~120313</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Software development is often unpredictable and imprecise for many reasons &ndash; new technologies emerge, the needs of customers change, and we are constantly building larger and more complex software systems. Unlike traditional engineering disciplines, there are no blueprints for software. Best practices and lessons learned on how to best build software vary from person to person, organization to organization, and project to project. These &ldquo;rules&rdquo; for building better software are often based on personal experience or anecdotes and, while useful, are rarely backed by scientific evidence or apply only in a very limited number of scenarios. For decades, researchers have attempted to derive rules for controlling the cost and quality of software, but with mixed success at best.</p> <p>The NSF Transfer Learning in Software Engineering project sought to address one of the major reasons that scientific predictability of software quality and effort has eluded us: that it is difficult to identify <em>relevant</em> data from past software projects from which to draw rules for engineering the software system at hand. To tackle this problem, we engaged in a number of activities.</p> <p>We developed the <em>XTREE algorithm</em> that evaluates the impact of proposed changes to software structure (i.e., refactoring) on software quality. For example, will reducing the complexity of a particular piece of code actual reduce the number of bugs in the system? These decisions are evaluated using data from the project&rsquo;s history of changes. The XTREE algorithm enables software project managers to make decisions on how to allocate effort based on past performance, rather than guesswork and anecdote.</p> <p>We developed the <em>LACE2 privacy algorithm</em>, which enables private companies and other organizations to contribute highly-detailed software development data (for example, the size of source code files and the number of defects in them) without revealing the origin of the data. By growing the amount of data available, it is more likely we can identify a past project from which to transfer rules to the current project.</p> <p>We applied a tool from psychology, the <em>repertory grid</em>, to extract lessons learned from professional software engineers and compare them in a scientifically valid way. This technique shows promise for documenting, comparing, and quantifying lessons learned rather than relying on simple anecdotes.</p> <p>We applied <em>natural language processing techniques</em> to pool together data from multiple NASA missions on how software bugs occur in operation. These techniques show promise for generating new sources of lessons learned from unstructured data (i.e., reports, emails) that can be used to make quantitative decisions.</p> <p>Finally, we examined whether one of the most commonly-held laws of software development is, in fact, a law in the scientific sense. In a survey of several dozen software engineers, the assertion that &ldquo;the longer a problem remains in the system, the more expensive it is to fix&rdquo; was the most commonly-believed &ldquo;law&rdquo; of software engineering. Yet, in a study of several <em>hundred</em> software projects that employed the Team Software Process, we find that this assertion was not true.</p> <p>In total, this research produced five publications authored by Fraunhofer staff and supported the research effort of three undergraduate students. This work brought together software engineering data and lessons learned from commercial companies, government organizations, and open source repositories.</p> <p>This research made significant headway to addressing the problem of transferring rules for developing better software quantitatively from project to project. There is still much work to be done to tame the complexities of software development introduced by variations in individuals, organizations, and their processes.</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/30/2017<br>      Modified by: Lucas&nbsp;Layman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Software development is often unpredictable and imprecise for many reasons &ndash; new technologies emerge, the needs of customers change, and we are constantly building larger and more complex software systems. Unlike traditional engineering disciplines, there are no blueprints for software. Best practices and lessons learned on how to best build software vary from person to person, organization to organization, and project to project. These "rules" for building better software are often based on personal experience or anecdotes and, while useful, are rarely backed by scientific evidence or apply only in a very limited number of scenarios. For decades, researchers have attempted to derive rules for controlling the cost and quality of software, but with mixed success at best.  The NSF Transfer Learning in Software Engineering project sought to address one of the major reasons that scientific predictability of software quality and effort has eluded us: that it is difficult to identify relevant data from past software projects from which to draw rules for engineering the software system at hand. To tackle this problem, we engaged in a number of activities.  We developed the XTREE algorithm that evaluates the impact of proposed changes to software structure (i.e., refactoring) on software quality. For example, will reducing the complexity of a particular piece of code actual reduce the number of bugs in the system? These decisions are evaluated using data from the project?s history of changes. The XTREE algorithm enables software project managers to make decisions on how to allocate effort based on past performance, rather than guesswork and anecdote.  We developed the LACE2 privacy algorithm, which enables private companies and other organizations to contribute highly-detailed software development data (for example, the size of source code files and the number of defects in them) without revealing the origin of the data. By growing the amount of data available, it is more likely we can identify a past project from which to transfer rules to the current project.  We applied a tool from psychology, the repertory grid, to extract lessons learned from professional software engineers and compare them in a scientifically valid way. This technique shows promise for documenting, comparing, and quantifying lessons learned rather than relying on simple anecdotes.  We applied natural language processing techniques to pool together data from multiple NASA missions on how software bugs occur in operation. These techniques show promise for generating new sources of lessons learned from unstructured data (i.e., reports, emails) that can be used to make quantitative decisions.  Finally, we examined whether one of the most commonly-held laws of software development is, in fact, a law in the scientific sense. In a survey of several dozen software engineers, the assertion that "the longer a problem remains in the system, the more expensive it is to fix" was the most commonly-believed "law" of software engineering. Yet, in a study of several hundred software projects that employed the Team Software Process, we find that this assertion was not true.  In total, this research produced five publications authored by Fraunhofer staff and supported the research effort of three undergraduate students. This work brought together software engineering data and lessons learned from commercial companies, government organizations, and open source repositories.  This research made significant headway to addressing the problem of transferring rules for developing better software quantitatively from project to project. There is still much work to be done to tame the complexities of software development introduced by variations in individuals, organizations, and their processes.          Last Modified: 06/30/2017       Submitted by: Lucas Layman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
