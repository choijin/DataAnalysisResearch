<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NeTS: Small: Scalable Network Virtualization for Multi-Core Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>470315.00</AwardTotalIntnAmount>
<AwardAmount>470315</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Deepankar Medhi</SignBlockName>
<PO_EMAI>dmedhi@nsf.gov</PO_EMAI>
<PO_PHON>7032922935</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As virtualization spreads throughout the data center, the communication endpoints within the data center are becoming virtual machines (VMs), not physical servers. Therefore, the network facilities for packet processing and security must operate all the way to the VM. This last hop switching among virtual machines within the physical server has become a critically important component of the data center network.  This project will explore the design space of server/switch integration, in which software and hardware based switching are more efficiently integrated directly into the server. More specifically, it will decouple the switching operations and distribute their implementation throughout the virtualized system. This project will be comprised of the following thrusts: &lt;br/&gt;   - Efficient Software Switching via a decoupled software switching architecture using a flow-based approach that eliminates these&lt;br/&gt;     overheads and that enables efficient switching in software&lt;br/&gt;   - Judicious choice and use of hardware acceleration.  &lt;br/&gt;   - The Rack Becomes the Server via having the network control plane learning the hardware topology, monitoring the network traffic,&lt;br/&gt;     instructing the hypervisor to move communicating VMs closer to each other. &lt;br/&gt;&lt;br/&gt;As the demand for data center capacity continues to increase at an incredible rate, it is becoming critical to minimize the number of physical machines. This research will transform the way in which networking is implemented on future systems, enabling the effective use of 100s of virtual machines per physical machine. This reduction in physical machines can have significant societal and economic impacts. If U.S. data centers used virtualization technology to achieve just half of the power reductions predicted by Intel, we would save $3.5 billion of electricity and prevent the release of 322 million pounds of CO2 per year.</AbstractNarration>
<MinAmdLetterDate>08/19/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/19/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1320965</AwardID>
<Investigator>
<FirstName>Alan</FirstName>
<LastName>Cox</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alan Cox</PI_FULL_NAME>
<EmailAddress>alc@rice.edu</EmailAddress>
<PI_PHON>7133485730</PI_PHON>
<NSF_ID>000277789</NSF_ID>
<StartDate>08/19/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>William Marsh Rice University</Name>
<CityName>Houston</CityName>
<ZipCode>770051827</ZipCode>
<PhoneNumber>7133484820</PhoneNumber>
<StreetAddress>6100 MAIN ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>050299031</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WILLIAM MARSH RICE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>050299031</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[William Marsh Rice University]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>770051827</ZipCode>
<StreetAddress><![CDATA[6100 MAIN ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~470315</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As the migration of computing into the cloud continues at an<br />incredible pace, so will the demand for data center capacity.&nbsp; To meet<br />this demand at low cost, it is critical to make effective use of the<br />physical servers in the data center.&nbsp; Historically, most of these<br />servers have been underutilized, wasting not only space but also<br />energy.&nbsp; Now, machine virtualization is widely recognized as a<br />successful approach to addressing these issues.<br /><br />As the use of machine virtualization has spread throughout the data<br />center, the communication endpoints (or hosts) within the data center<br />have become virtual machines (VMs), not physical servers.&nbsp; Therefore,<br />the network facilities for packet processing and security must operate<br />all the way to the VM.&nbsp; This last hop switching among virtual machines<br />within the physical server has become a critically important component<br />of the data center network.&nbsp; The aim of this research on the data<br />center network has been to transform the way in which networking is<br />implemented on future multi-core systems, eliminating one barrier to<br />effectively running hundreds of VMs per physical server.<br /><br />In current datacenter networks, failures have become the norm.<br />However, the latency to respond to these failures is increasingly<br />problematic as software recovery struggles or even fails to meet tight<br />service-level agreements.&nbsp; On the other hand, hardware recovery can be<br />near instantaneous.&nbsp; In this project, we have developed an approach to<br />practically implement t-resilient hardware, which protects against up<br />to t simultaneous failures.&nbsp; Moreover, we show that the next<br />generation switching chips that could be embedded within a virtualized<br />physical server can implement moderate yet interesting values of t.<br />Although forwarding table size is a limiting factor, we find that low<br />levels of resilience are highly effective at preventing failures.&nbsp; For<br />example, only 0.0002% of the pairwise paths between hosts are expected<br />to fail given 4-resilience and 64 edge failures on a 2048-host network<br />topology.<br /><br />To reduce forwarding table size, we have explored different approaches to<br />achieving forwarding table compression.&nbsp; First, we have developed<br />a new forwarding table compression algorithm.&nbsp; Then, we have<br />observed that only forwarding table entries that share both the<br />same output and the same packet modification action can be<br />compressed, which implies that the achievable compression ratio is<br />limited by the number of unique (output, action) pairs in the<br />table.&nbsp; Thus, this project's next two contributions were<br />explicitly conceived as ways to increase the number of common<br />outputs and actions.&nbsp; First, we introduced the concept of<br />compression-aware routing, which increases the number of entries<br />with common forwarding table outputs.&nbsp; Second, we created<br />Plinko, a new forwarding model in which all entries in the<br />forwarding table apply the same action.<br /><br />Lastly, storage area networking is driving data center switches to support<br />lossless Ethernet (DCB).&nbsp; Unfortunately, to enable DCB for all traffic<br />on arbitrary network topologies, we must address several problems that<br />can arise in lossless networks, e.g., large buffering delays,<br />unfairness, head of line blocking, and deadlock.&nbsp; To this end, we have<br />developed TCP-Bolt, a TCP variant that not only addresses the first<br />three problems but reduces flow completion times by as much as 70%.<br />We have also developed a simple, practical deadlock-free routing<br />scheme that eliminates deadlock while achieving aggregate network<br />throughput within 15% of ECMP routing.&nbsp; This small compromise in<br />potential routing capacity is well worth the gains in flow completion<br />time.</p><br> <p>            Last Modified: 11/12/2019<br>      Modified by: Alan&nbsp;Cox</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As the migration of computing into the cloud continues at an incredible pace, so will the demand for data center capacity.  To meet this demand at low cost, it is critical to make effective use of the physical servers in the data center.  Historically, most of these servers have been underutilized, wasting not only space but also energy.  Now, machine virtualization is widely recognized as a successful approach to addressing these issues.  As the use of machine virtualization has spread throughout the data center, the communication endpoints (or hosts) within the data center have become virtual machines (VMs), not physical servers.  Therefore, the network facilities for packet processing and security must operate all the way to the VM.  This last hop switching among virtual machines within the physical server has become a critically important component of the data center network.  The aim of this research on the data center network has been to transform the way in which networking is implemented on future multi-core systems, eliminating one barrier to effectively running hundreds of VMs per physical server.  In current datacenter networks, failures have become the norm. However, the latency to respond to these failures is increasingly problematic as software recovery struggles or even fails to meet tight service-level agreements.  On the other hand, hardware recovery can be near instantaneous.  In this project, we have developed an approach to practically implement t-resilient hardware, which protects against up to t simultaneous failures.  Moreover, we show that the next generation switching chips that could be embedded within a virtualized physical server can implement moderate yet interesting values of t. Although forwarding table size is a limiting factor, we find that low levels of resilience are highly effective at preventing failures.  For example, only 0.0002% of the pairwise paths between hosts are expected to fail given 4-resilience and 64 edge failures on a 2048-host network topology.  To reduce forwarding table size, we have explored different approaches to achieving forwarding table compression.  First, we have developed a new forwarding table compression algorithm.  Then, we have observed that only forwarding table entries that share both the same output and the same packet modification action can be compressed, which implies that the achievable compression ratio is limited by the number of unique (output, action) pairs in the table.  Thus, this project's next two contributions were explicitly conceived as ways to increase the number of common outputs and actions.  First, we introduced the concept of compression-aware routing, which increases the number of entries with common forwarding table outputs.  Second, we created Plinko, a new forwarding model in which all entries in the forwarding table apply the same action.  Lastly, storage area networking is driving data center switches to support lossless Ethernet (DCB).  Unfortunately, to enable DCB for all traffic on arbitrary network topologies, we must address several problems that can arise in lossless networks, e.g., large buffering delays, unfairness, head of line blocking, and deadlock.  To this end, we have developed TCP-Bolt, a TCP variant that not only addresses the first three problems but reduces flow completion times by as much as 70%. We have also developed a simple, practical deadlock-free routing scheme that eliminates deadlock while achieving aggregate network throughput within 15% of ECMP routing.  This small compromise in potential routing capacity is well worth the gains in flow completion time.       Last Modified: 11/12/2019       Submitted by: Alan Cox]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
