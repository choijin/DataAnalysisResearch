<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Medium: Collaborative Research: New Approaches to Robustness in High-Dimensions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>695369.00</AwardTotalIntnAmount>
<AwardAmount>695369</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Phillip Regalia</SignBlockName>
<PO_EMAI>pregalia@nsf.gov</PO_EMAI>
<PO_PHON>7032922981</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Rapid development of large-scale data collection technology has&lt;br/&gt;ignited research into high-dimensional machine learning.  For&lt;br/&gt;instance, the problem of designing recommender systems, such as those&lt;br/&gt;used by Amazon, Netflix and other on-line companies, involves&lt;br/&gt;analyzing large matrices that describe users' behavior in past&lt;br/&gt;situations.  In sociology, researchers are interested in fitting&lt;br/&gt;networks to large-scale data sets, involving hundreds or thousands of&lt;br/&gt;individuals.  In medical imaging, the goal is to reconstruct&lt;br/&gt;complicated phenomena (e.g., brain images; videos of a beating heart)&lt;br/&gt;based on a minimal number of incomplete and possibly corrupted&lt;br/&gt;measurements.  Motivated by such applications, the goal of this&lt;br/&gt;research is to develop and analyze models and algorithms for&lt;br/&gt;extracting relevant structure from such high-dimensional data sets in&lt;br/&gt;a robust and scalable fashion.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The research leverages tools from convex optimization, signal&lt;br/&gt;processing, and robust statistics.  It consists of three main thrusts:&lt;br/&gt;(1) Model restrictiveness: Successful methods for high-dimensional&lt;br/&gt;data exploit low-dimensional structure; however, many real-world&lt;br/&gt;problems fall outside the scope of existing models.  This proposal&lt;br/&gt;significantly extends the basic set-up by allowing for multiple&lt;br/&gt;structures, leading to computationally efficient algorithms while&lt;br/&gt;eliminating negative effects of model mismatch.  (2) Non-ideal data:&lt;br/&gt;Missing data are prevalent in real-world problems, and can cause major&lt;br/&gt;breakdowns in standard algorithms for high-dimensional data. The&lt;br/&gt;second thrust devises relaxations and greedy approaches for these&lt;br/&gt;non-convex problems.  (3) Arbitrary Outliers: Gross errors can arise&lt;br/&gt;for various reasons, including fault-prone sensors and manipulative&lt;br/&gt;agents.  The third thrust proposes efficient and randomized algorithms&lt;br/&gt;to address arbitrary outliers.</AbstractNarration>
<MinAmdLetterDate>03/19/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1302435</AwardID>
<Investigator>
<FirstName>Constantine</FirstName>
<LastName>Caramanis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Constantine Caramanis</PI_FULL_NAME>
<EmailAddress>constantine@utexas.edu</EmailAddress>
<PI_PHON>5124719269</PI_PHON>
<NSF_ID>000102556</NSF_ID>
<StartDate>03/19/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sujay</FirstName>
<LastName>Sanghavi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sujay Sanghavi</PI_FULL_NAME>
<EmailAddress>sanghavi@mail.utexas.edu</EmailAddress>
<PI_PHON>5124759798</PI_PHON>
<NSF_ID>000535791</NSF_ID>
<StartDate>03/19/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787595316</ZipCode>
<StreetAddress><![CDATA[3925 W Braker Lane, Ste 3.340]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramElement>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~316716</FUND_OBLG>
<FUND_OBLG>2015~378653</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Increasingly, machine learning and data science are being used in security-critical applications &ndash; applications that are intrinsically linked with our financial system, or in safety critical applications such as autonomous driving, to name a few examples. Insuring the reliability of the solutions, predictions and recommendations of machine learning algorithms is of paramount importance.</p> <p>&nbsp;</p> <p>At the same time, it is well documented that machine learning-based algorithms &ndash; even those that enjoy state-of-the-art accuracy and performance on unperturbed data &ndash; are remarkably fragile to perturbations in training and testing data.</p> <p>&nbsp;</p> <p>The work funded by this grant has been focused on developing new analysis for understanding these threats, as well as computationally efficient, highly scalable algorithms, that are <em>robust</em> to such perturbations or attacks on the data. Specifically, the focus has been on modern machine learning problems in what is known as the <em>high dimensional statistical regime</em>. This is the setting where the dimensionality and complexity of the data are vast, and in fact are often have higher dimensionality &ndash; and hence degrees of freedom &ndash; than the number of data points available. This is particularly relevant as our ability to sense and also store data increases, and hence we collect and store increasingly high resolution and complex data.</p> <p>&nbsp;</p> <p>The results obtained have developed robustness for many fundamental algorithms that form the underpinning of many of the key statistical routines used in diverse areas of science and technology. This includes regression and principal component analysis. Our results include state of the art algorithms boasting the best robustness and also fastest computation times, for problems including principal component analysis with deleted data (erasures) and sparse corruption. Our algorithms have also proved ground breaking for mixture problems. Mixture problems are important as they are a key technique for building more complicated and accurate statistical models from simple building blocks.</p> <p>&nbsp;</p> <p>Finally, the funded work has also made pioneering advances in robustness for neural networks, providing simple algorithms that make neural network training robust, but at little or no additional computational expense.</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/20/2020<br>      Modified by: Constantine&nbsp;Caramanis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Increasingly, machine learning and data science are being used in security-critical applications &ndash; applications that are intrinsically linked with our financial system, or in safety critical applications such as autonomous driving, to name a few examples. Insuring the reliability of the solutions, predictions and recommendations of machine learning algorithms is of paramount importance.     At the same time, it is well documented that machine learning-based algorithms &ndash; even those that enjoy state-of-the-art accuracy and performance on unperturbed data &ndash; are remarkably fragile to perturbations in training and testing data.     The work funded by this grant has been focused on developing new analysis for understanding these threats, as well as computationally efficient, highly scalable algorithms, that are robust to such perturbations or attacks on the data. Specifically, the focus has been on modern machine learning problems in what is known as the high dimensional statistical regime. This is the setting where the dimensionality and complexity of the data are vast, and in fact are often have higher dimensionality &ndash; and hence degrees of freedom &ndash; than the number of data points available. This is particularly relevant as our ability to sense and also store data increases, and hence we collect and store increasingly high resolution and complex data.     The results obtained have developed robustness for many fundamental algorithms that form the underpinning of many of the key statistical routines used in diverse areas of science and technology. This includes regression and principal component analysis. Our results include state of the art algorithms boasting the best robustness and also fastest computation times, for problems including principal component analysis with deleted data (erasures) and sparse corruption. Our algorithms have also proved ground breaking for mixture problems. Mixture problems are important as they are a key technique for building more complicated and accurate statistical models from simple building blocks.     Finally, the funded work has also made pioneering advances in robustness for neural networks, providing simple algorithms that make neural network training robust, but at little or no additional computational expense.          Last Modified: 01/20/2020       Submitted by: Constantine Caramanis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
