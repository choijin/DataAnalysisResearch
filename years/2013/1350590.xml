<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Computational and Statistical Tradeoffs in Massive Data Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2014</AwardEffectiveDate>
<AwardExpirationDate>01/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>475000.00</AwardTotalIntnAmount>
<AwardAmount>475000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Phillip Regalia</SignBlockName>
<PO_EMAI>pregalia@nsf.gov</PO_EMAI>
<PO_PHON>7032922981</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In modern signal processing, one is frequently faced with statistical inference problems involving massive datasets. For example, the experiments at the Large Hadron Collider at CERN generate hundreds of petabytes of data each year, which must be stored and processed efficiently in order to further our understanding of particle physics. Similar challenges also arise in seismic monitoring where massive amounts of data are acquired over large areas via cellphone accelerometers. Analyzing such large datasets is usually viewed as a substantial computational challenge. However, if data are a signal processor?s main resource then access to more data should be viewed as an asset rather than as a burden, and larger datasets should lead to a reduction in the runtime of data analysis algorithms.&lt;br/&gt;&lt;br/&gt;This project blends concepts from computer science and from statistical signal processing to address the challenge with massive datasets by developing ?algorithm weakening? frameworks in which a data analysis procedure backs off to simpler methods as the data scale in size, leveraging the growing inferential strength of the data to ensure that a desired level of statistical accuracy is achieved with reduced runtime.  The approach is concretely illustrated across a range of statistical estimation tasks, with convex relaxation techniques playing a prominent role as an algorithm weakening mechanism.  In seeking a precise characterization of the computational and statistical tradeoffs obtained via convex relaxation, the investigator formalizes and studies new measures for characterizing the quality of approximation of one convex set by another.  An interesting feature of this research is that convex relaxations which provide poor performance in combinatorial optimization problems may nonetheless yield useful solutions when employed in problems with inferential objectives.</AbstractNarration>
<MinAmdLetterDate>01/22/2014</MinAmdLetterDate>
<MaxAmdLetterDate>06/17/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1350590</AwardID>
<Investigator>
<FirstName>Venkat</FirstName>
<LastName>Chandrasekaran</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Venkat Chandrasekaran</PI_FULL_NAME>
<EmailAddress>venkatc@caltech.edu</EmailAddress>
<PI_PHON>6174609090</PI_PHON>
<NSF_ID>000618011</NSF_ID>
<StartDate>01/22/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>California Institute of Technology</Name>
<CityName>PASADENA</CityName>
<ZipCode>911250600</ZipCode>
<PhoneNumber>6263956219</PhoneNumber>
<StreetAddress>1200 E California Blvd</StreetAddress>
<StreetAddress2><![CDATA[Mail Code 273-6]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>27</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA27</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009584210</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CALIFORNIA INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009584210</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[California Institute of Technology]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>911250001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>27</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA27</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~185533</FUND_OBLG>
<FUND_OBLG>2016~191460</FUND_OBLG>
<FUND_OBLG>2018~98007</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In modern data analysis, one is frequently faced with statistical inference problems involving massive datasets. Processing such large datasets is usually viewed as a substantial computational challenge.</p> <p>However, if data are a signal processor&rsquo;s main resource then access to more data should be viewed as an asset rather than as a burden, and larger datasets should lead to a reduction in the runtime of statistical estimation algorithms. This project developed new convex optimization approaches to address these challenges by blending concepts from statistical signal processing and from computer science, leading to new methods in which a data analysis procedure operates within a given computational budget and comes with statistical performance guarantees.&nbsp; Specific outcomes include:</p> <p>&nbsp;</p> <p>- Data analysis frameworks in which one backs off to simpler methods as the data scale in size, leveraging the growing inferential strength of the data to ensure that a desired level of statistical accuracy is achieved with reduced runtime.</p> <p>- Convex relaxations for identifying structured subgraphs inside larger graphs</p> <p>- Tractable methods based on relative entropy inequalities for solving nonconvex signomial optimization problems</p> <p>- Geometric approaches for assessing and controlling for false discoveries in statistical inference</p> <p>- A framework for learning convex regularizers from data that operate within a desired computational budget</p> <p>- A new statewide model for the California reservoir system, highlighting systemic conditions under which multiple large reservoirs can go dry&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/20/2020<br>      Modified by: Venkat&nbsp;Chandrasekaran</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In modern data analysis, one is frequently faced with statistical inference problems involving massive datasets. Processing such large datasets is usually viewed as a substantial computational challenge.  However, if data are a signal processorâ€™s main resource then access to more data should be viewed as an asset rather than as a burden, and larger datasets should lead to a reduction in the runtime of statistical estimation algorithms. This project developed new convex optimization approaches to address these challenges by blending concepts from statistical signal processing and from computer science, leading to new methods in which a data analysis procedure operates within a given computational budget and comes with statistical performance guarantees.  Specific outcomes include:     - Data analysis frameworks in which one backs off to simpler methods as the data scale in size, leveraging the growing inferential strength of the data to ensure that a desired level of statistical accuracy is achieved with reduced runtime.  - Convex relaxations for identifying structured subgraphs inside larger graphs  - Tractable methods based on relative entropy inequalities for solving nonconvex signomial optimization problems  - Geometric approaches for assessing and controlling for false discoveries in statistical inference  - A framework for learning convex regularizers from data that operate within a desired computational budget  - A new statewide model for the California reservoir system, highlighting systemic conditions under which multiple large reservoirs can go dry           Last Modified: 06/20/2020       Submitted by: Venkat Chandrasekaran]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
