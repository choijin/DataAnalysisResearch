<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Medium: Collaborative Research: Workload-Aware Storage Architectures for Optimal Performance and Energy Efficiency</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>306077.00</AwardTotalIntnAmount>
<AwardAmount>306077</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The most significant performance and energy bottlenecks in a computer are&lt;br/&gt;often caused by the storage system, because the gap between storage device&lt;br/&gt;and CPU speeds is greater than in any other part of the machine.  Big data&lt;br/&gt;and new storage media only make things worse, because today's systems are&lt;br/&gt;still optimized for legacy workloads and hard disks.  The team at Stony&lt;br/&gt;Brook University, Harvard University, and Harvey Mudd College has shown that&lt;br/&gt;large systems are poorly optimized, resulting in waste that increases&lt;br/&gt;computing costs, slows scientific progress, and jeopardizes the nation's&lt;br/&gt;energy independence.&lt;br/&gt;&lt;br/&gt;First, the team is examining modern workloads running on a variety of&lt;br/&gt;platforms, including individual computers, large compute farms, and a&lt;br/&gt;next-generation infrastructure, such as Stony Brook's Reality Deck, a&lt;br/&gt;massive gigapixel visualization facility.  These workloads produce combined&lt;br/&gt;performance and energy traces that are being released to the community.&lt;br/&gt;&lt;br/&gt;Second, the team is applying techniques such as statistical feature&lt;br/&gt;extraction, Hidden Markov Modeling, data-mining, and conditional likelihood&lt;br/&gt;maximization to analyze these data sets and traces.  The Reality Deck is&lt;br/&gt;used to visualize the resulting multi-dimensional performance/energy data&lt;br/&gt;sets.  The team's analyses reveal fundamental phenomena and principles that&lt;br/&gt;inform future designs.&lt;br/&gt;&lt;br/&gt;Third, the findings from the first two efforts are being combined to develop&lt;br/&gt;new storage architectures that best balance performance and energy under&lt;br/&gt;different workloads when used with modern devices, such as solid-state&lt;br/&gt;drives (SSDs), phase-change memories, etc.  The designs leverage the team's&lt;br/&gt;work on storage-optimized algorithms, multi-tier storage, and new optimized&lt;br/&gt;data structures.</AbstractNarration>
<MinAmdLetterDate>09/09/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/09/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1302334</AwardID>
<Investigator>
<FirstName>Margo</FirstName>
<LastName>Seltzer</LastName>
<PI_MID_INIT>I</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Margo I Seltzer</PI_FULL_NAME>
<EmailAddress>margo@eecs.harvard.edu</EmailAddress>
<PI_PHON>6174965663</PI_PHON>
<NSF_ID>000146422</NSF_ID>
<StartDate>09/09/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard University]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021383846</ZipCode>
<StreetAddress><![CDATA[33 Oxford Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~306077</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Monaco; color: #000000; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} --> <p class="p1"><span class="s1">Today's computers process data much more quickly than they can store that data.<span>&nbsp; </span>For example, a modern computer can process the contents of the Library of Congress American Memory collection in approximately 35 hours, while it would take 3,500 hours (146 days) to store that data onto a solid state drive (SSD), should a large enough one exist, and about 600 days to store that data on a hard drive. This project addresses the question of how one can best organize data in the presence of the enormous disparities in performance between computation and storage. This disparity is frequently referred to as the IO-gap, where IO stands for input/output and refers to the act of reading and writing data from/to persistent storage.</span></p> <p class="p1"><span class="s1">This research addressed two areas: efficient support for processing graph-structured data (e.g., a social network, a map of roads) and recent hard disk storage technologies called Shingled Magnetic Recording (SMR) drives.</span></p> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Monaco; color: #000000; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} --> <p class="p1"><span class="s1">Graph-structured data is quite different from conventional business data in that each object is relatively small and the different objects are connected in multiple, complex ways. Thus, there is no natural order to the data that allows a system to place it on persistent storage so it can be accessed efficiently. There are two different problems that must be addressed to efficiently handle graph-structured data.<span>&nbsp; </span>First, one must design efficient mechanisms to store and access data from a single machine. Second, when the data is too large to process on a single machine, it must be partitioned across multiple machines in a manner that allows each partition to be processed with little or no interaction with other partitions.</span></p> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Monaco; color: #000000; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} --> <p class="p1"><span class="s1">This research solved the first problem by introducing the Linked-node analytics using Large Multiversioned Arrays (LLAMA) library. LLAMA is a storage format that provides efficient processing of persistent graph-structured data residing on an SSD.<span>&nbsp; </span>While LLAMA is based on an existing representation, called Compressed Sparse Row (CSR), unlike prior implementations, LLAMA makes the data persistent, allows it to be modified, and can provide multiple copies of the data, representing its state at different points in time.</span></p> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Monaco; color: #000000; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} --> <p class="p1"><span class="s1">Addressing the second problem, partitioning, is challenging for two reasons.<span>&nbsp; </span>First, constructing good partitions, partitions that enable local processing with little inter-machine communication, is a slow operation. Second, since the graph is large enough to require multiple machine to process, it requires multiple machines to partition, and frequently, the algorithms used to partition the graph work well only if each machine can work independently.<span>&nbsp; </span>But this is precisely the goal of partitioning.<span>&nbsp; </span>The Scalable Host-tree Embeddings for Efficient Partitioning (SHEEP) algorithm developed in this project produces high quality partitions, rapidly and, more significantly, guarantees that the quality of the partitions is independent of how the graph is split across the multiple nodes during the partitioning. That is, the graph is randomly distributed across multiple machines, which can compute partitions in parallel and when those computations are combined, the result is guaranteed to be the same as that achieved had the entire graph been processed on a single machine.</span></p> <!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Monaco; color: #000000; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} --> <p class="p1"><span class="s1">While LLAMA leveraged modern SSDs, SMR drives represent another promising storage technology. However, while SMR drives offer high density and outstanding sequential read and write performance, they do not handle random updates to data (e.g., changing small parts of a graph). To address this challenge, SMR vendors provide an SMR Translation Layer (STL) on the drives that stores random writes and eventually applies them sequentially.<span>&nbsp; </span>This project produced alternative approaches to the vendor-supplied STL.<span>&nbsp; </span>The research investigated the trade offs in using the vendor STL versus using a file system specifically designed to make all writes sequential (a log-structured file system) versus providing a device driver in the operating system the performs the same kinds of functions as the vendor STL, but knows more about the data being read and written and has more resources (e.g., memory) available.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 12/27/2017<br>      Modified by: Margo&nbsp;I&nbsp;Seltzer</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Today's computers process data much more quickly than they can store that data.  For example, a modern computer can process the contents of the Library of Congress American Memory collection in approximately 35 hours, while it would take 3,500 hours (146 days) to store that data onto a solid state drive (SSD), should a large enough one exist, and about 600 days to store that data on a hard drive. This project addresses the question of how one can best organize data in the presence of the enormous disparities in performance between computation and storage. This disparity is frequently referred to as the IO-gap, where IO stands for input/output and refers to the act of reading and writing data from/to persistent storage. This research addressed two areas: efficient support for processing graph-structured data (e.g., a social network, a map of roads) and recent hard disk storage technologies called Shingled Magnetic Recording (SMR) drives.  Graph-structured data is quite different from conventional business data in that each object is relatively small and the different objects are connected in multiple, complex ways. Thus, there is no natural order to the data that allows a system to place it on persistent storage so it can be accessed efficiently. There are two different problems that must be addressed to efficiently handle graph-structured data.  First, one must design efficient mechanisms to store and access data from a single machine. Second, when the data is too large to process on a single machine, it must be partitioned across multiple machines in a manner that allows each partition to be processed with little or no interaction with other partitions.  This research solved the first problem by introducing the Linked-node analytics using Large Multiversioned Arrays (LLAMA) library. LLAMA is a storage format that provides efficient processing of persistent graph-structured data residing on an SSD.  While LLAMA is based on an existing representation, called Compressed Sparse Row (CSR), unlike prior implementations, LLAMA makes the data persistent, allows it to be modified, and can provide multiple copies of the data, representing its state at different points in time.  Addressing the second problem, partitioning, is challenging for two reasons.  First, constructing good partitions, partitions that enable local processing with little inter-machine communication, is a slow operation. Second, since the graph is large enough to require multiple machine to process, it requires multiple machines to partition, and frequently, the algorithms used to partition the graph work well only if each machine can work independently.  But this is precisely the goal of partitioning.  The Scalable Host-tree Embeddings for Efficient Partitioning (SHEEP) algorithm developed in this project produces high quality partitions, rapidly and, more significantly, guarantees that the quality of the partitions is independent of how the graph is split across the multiple nodes during the partitioning. That is, the graph is randomly distributed across multiple machines, which can compute partitions in parallel and when those computations are combined, the result is guaranteed to be the same as that achieved had the entire graph been processed on a single machine.  While LLAMA leveraged modern SSDs, SMR drives represent another promising storage technology. However, while SMR drives offer high density and outstanding sequential read and write performance, they do not handle random updates to data (e.g., changing small parts of a graph). To address this challenge, SMR vendors provide an SMR Translation Layer (STL) on the drives that stores random writes and eventually applies them sequentially.  This project produced alternative approaches to the vendor-supplied STL.  The research investigated the trade offs in using the vendor STL versus using a file system specifically designed to make all writes sequential (a log-structured file system) versus providing a device driver in the operating system the performs the same kinds of functions as the vendor STL, but knows more about the data being read and written and has more resources (e.g., memory) available.          Last Modified: 12/27/2017       Submitted by: Margo I Seltzer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
