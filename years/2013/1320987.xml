<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF:  Small: Realizing Chip-scale Bio-inspired Spiking Neural Networks with Monolithically Integrated Nano-scale Memristors</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2013</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sankar Basu</SignBlockName>
<PO_EMAI>sabasu@nsf.gov</PO_EMAI>
<PO_PHON>7032927843</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Computer processing power is so integrated into our daily lives that we hardly notice how it enables everything from a routine text message to solving "large data" problems that mandate immense supercomputing resources. And yet, the processing power of today's computers pales in comparison to that most advanced processor - the human brain. It is now believed that technological progress would enable us to take up the challenge of developing a new kind of computing architecture that functions more like the brain. In recent years, scientists have examined the electrical interaction between brain synapses and how biological neurons interact. They have also derived mathematical models to explain how these processes work. By employing these models in combination with a new device technology that exhibits similar electrical response to the neural synapses, this project will design entirely new computer processing chips that mimic how the brain processes information.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;We envision these chips performing pattern recognition with machine complexity an order of magnitude higher than traditional computing and digital signal processor (DSP) architectures. And even though the chip physical size and weight will match current processors, their power consumption will be orders of magnitude lower than with the von Neumann computing architecture that forms the basis for most of today's computer processors. Therefore, by mimicking the brain's billions of interconnections and pattern recognition capabilities, we may ultimately introduce a new paradigm in speed and power, and potentially enable systems that include the ability to learn, adapt, and respond to their environment.</AbstractNarration>
<MinAmdLetterDate>06/25/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1320987</AwardID>
<Investigator>
<FirstName>Elisa</FirstName>
<LastName>Barney Smith</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Elisa H Barney Smith</PI_FULL_NAME>
<EmailAddress>EBarneySmith@boisestate.edu</EmailAddress>
<PI_PHON>2084262214</PI_PHON>
<NSF_ID>000160707</NSF_ID>
<StartDate>06/25/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kristy</FirstName>
<LastName>Campbell</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kristy A Campbell</PI_FULL_NAME>
<EmailAddress>kriscampbell@boisestate.edu</EmailAddress>
<PI_PHON>2084265968</PI_PHON>
<NSF_ID>000426852</NSF_ID>
<StartDate>06/25/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Vishal</FirstName>
<LastName>Saxena</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vishal Saxena</PI_FULL_NAME>
<EmailAddress>vsaxena@udel.edu</EmailAddress>
<PI_PHON>3028314365</PI_PHON>
<NSF_ID>000601339</NSF_ID>
<StartDate>06/25/2013</StartDate>
<EndDate>09/07/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Boise State University</Name>
<CityName>Boise</CityName>
<ZipCode>837250001</ZipCode>
<PhoneNumber>2084261574</PhoneNumber>
<StreetAddress>1910 University Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Idaho</StateName>
<StateCode>ID</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>ID02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072995848</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOISE STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072995848</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Boise State University]]></Name>
<CityName>Boise</CityName>
<StateCode>ID</StateCode>
<ZipCode>837251135</ZipCode>
<StreetAddress><![CDATA[1910 University Dr]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Idaho</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>ID02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7945</Code>
<Text>DES AUTO FOR MICRO &amp; NANO SYST</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7945</Code>
<Text>DES AUTO FOR MICRO &amp; NANO SYST</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>To build the computer processors we use today, computer manufacturers have come to use a standard approach called the von Neumann computing architecture. However, as technologists push for increased processing speed, lower power consumption, and smaller processor and device sizes, this architecture is reaching its limits. As a possible alternative, researchers considered whether it was feasible to create a new architecture based on one of the most powerful computers known&mdash;the brain. During the project period, a project team sought to create artificial devices that would function like brain neurons and synapses. The neuron holds information and the synapse use electrical activity to pass that information from one cell to another. Project activity also addressed how to minimize device size, and how individual components performed as an integrated computing architecture.</p> <p><strong><em>Outcomes and Broader Impacts</em></strong>. Project outcomes have resulted in steps toward making this architecture a practical reality. In addition, as explained below, the project made an electrical component called a memristor available to the university project team, which included undergraduate researchers. Such devices are not often available at universities, and not part of the standard undergraduate curriculum. Over a dozen undergraduate students participated in this research, providing an experience not often available in basic electrical engineering education. In addition, multiple corporations have previously licensed the memristors that the team used. Completed research may suggest more potential uses for these devices, thereby encouraging more companies to license the technology and further technical developments in their use.</p> <p><strong><em>Facilitating New and Improved Computer Processing Opportunities.</em></strong> The brain has approximately 100 billion neurons. Using very little power as compared to the processors we use in our computers, neurons connect to 100 trillion synapses that transfer information. A processor built in silicon that mimics this function would substantially increase speed and reduce power consumption over current von Neuman designs. Further, we would open up opportunities for new and improved computer applications that like the brain, can mimic the way people think (cognitive data processing), more effectively recognize and classify patterns, are able to learn without being explicitly programmed to do so (machine learning), and can work on different parts of a problem at the same time (massively parallel architecture).</p> <p><strong><em>Making Artificial Neurons and Synapses. </em></strong>To begin, we designed and built a computer chip that&mdash;like the neuron&mdash;stores information. It is a 0.18&mu;m Complementary Metal-Oxide Semiconductor (CMOS) chip, see Figure 6 TCAS, and includes analog circuits that function as a leaky integrate-and-fire neuron, see Figure 2 IJCNN. Signals sent between neurons change the strength of the synapse through a process called spike-timing-dependent-plasticity (STDP). When spikes from the previous neuron and the following neuron come close in time, synapse strength changes. Therefore, to mimic the synapse and gain control of this activity, the team connected memristors between neurons. These electrical components limit or regulate the flow of electrical current in a circuit, remember the amount of charge that has previously flowed through it, and retain information in memory even when power to a device is off. We are able to alter memristor properties by applying voltage signals with a spiked shape.</p> <p>We designed the integrate-and-fire neuron circuit to generate action potentials&mdash;or spikes&mdash;that can induce weight change in the memristors governed by the spike-timing-dependent-plasticity (STDP) learning rule. Individual spikes should not change the memristor weight when applied in isolation, i.e. the synapse weight should change only when engineers apply a post- and pre-synaptic spike-pair within a specific time window. When fabricated in a common structure called a crossbar memristor array, it will enable dense on-chip integration, and ultimately allow manufacturers to produce devices in smaller sizes than currently possible.</p> <p><strong><em>Expanding Potential Design Uses.</em></strong> Next, we explored the response of the memristors to different spike signal shapes/forms, see Figure 1. Doing so may ultimately provide chip designers with a greater array of design choices so signals to improve machine learning results. Additionally, we explored the effect of pulsing several shapes at time scales over a range of 10 orders of magnitude. This means that the memristors can be used at the biological timescale, or significantly faster allowing greater computing output. The team fit measured memristor voltages and currents to a mathematical model of a memristor developed by Yakopcic (2011) to find applicable model parameters.</p> <p><strong><em>Validating Performance.</em></strong> Finally, we examined how the complete architecture performed. The team ran network simulations for two applications: Digit Recognition, and Pavlov's Dog. Digits were recognized with a 96% accuracy, see Figure 6 JETCAS. We successfully ran the Pavlov dog scenario on the circuit neuron hardware with connected memristors, See Figure 2 Senior Design. As a result, scientists now have a better understanding of how to use memristors in a neural circuit, beyond simulation. The project team has also gained an improved understanding of the response of memristors to spiking signals.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/29/2017<br>      Modified by: Elisa&nbsp;H&nbsp;Barney Smith</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506703939027_Image-Frontiers--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506703939027_Image-Frontiers--rgov-800width.jpg" title="Figure 1"><img src="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506703939027_Image-Frontiers--rgov-66x44.jpg" alt="Figure 1"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A) An example of a presynaptic spike. (B) Examples of postsynaptic spikes. They are at time offsets ?T ? {?20, ?10, 0, 10, 20} from the presynaptic spike in (A). Here they have the same shape as the presynaptic spike. (C) Examples of the net (difference spike for the five cases in (B).</div> <div class="imageCredit">Elisa Barney Smith</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Elisa&nbsp;H&nbsp;Barney Smith</div> <div class="imageTitle">Figure 1</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506704113912_Image-SrDesign--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506704113912_Image-SrDesign--rgov-800width.jpg" title="Figure 2"><img src="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506704113912_Image-SrDesign--rgov-66x44.jpg" alt="Figure 2"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Die created for Pavlov?s Dog scenario by Senior Design students.</div> <div class="imageCredit">Matthew Stevens, Sean Brasfield, Prabesh Subedi, Kris Campbell</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Elisa&nbsp;H&nbsp;Barney Smith</div> <div class="imageTitle">Figure 2</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506704345519_Image-JETCAS--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506704345519_Image-JETCAS--rgov-800width.jpg" title="Figure 6 JETCAS"><img src="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506704345519_Image-JETCAS--rgov-66x44.jpg" alt="Figure 6 JETCAS"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A spiking neural system for the pattern recognition application of optical character recognition (OCR).</div> <div class="imageCredit">X. Wu, V. Saxena, and K. Zhu, ?Homogeneous Spiking Neuromorphic System for Real-World Pattern Recognition,? IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol 5, no. 2, June 2015 (Digital Object Identifier 10.1109/JETCAS.2015.2433552).</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Elisa&nbsp;H&nbsp;Barney Smith</div> <div class="imageTitle">Figure 6 JETCAS</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506704696326_Image-IJCNN--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506704696326_Image-IJCNN--rgov-800width.jpg" title="Figure 2 IJCNN"><img src="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506704696326_Image-IJCNN--rgov-66x44.jpg" alt="Figure 2 IJCNN"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Dual-mode operation of the proposed leaky integrate-and-fire neuron.</div> <div class="imageCredit">X. Wu, V. Saxena, and K. Zhu, ?A CMOS Spiking Neuron for Dense Memristor Synapse Connectivity for Brain-Like Computing,? in International Joint Conference on Neural Networks (IJCNN), Killarney, Ireland, July 2015.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Elisa&nbsp;H&nbsp;Barney Smith</div> <div class="imageTitle">Figure 2 IJCNN</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506704875667_Image-TCAS--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506704875667_Image-TCAS--rgov-800width.jpg" title="Figure 6 TCAS"><img src="/por/images/Reports/POR/2017/1320987/1320987_10254264_1506704875667_Image-TCAS--rgov-66x44.jpg" alt="Figure 6 TCAS"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Micrograph of the test chip in 180nm CMOS.</div> <div class="imageCredit">X. Wu, V. Saxena, and K. Zhu, ?A CMOS Spiking Neuron for Brain-Inspired Neural Netowrks with Resistive Synapses and In-Situ Learning,? IEEE Transactions on Circuits and Systems (TCAS) II: Express Briefs, vol. 62, no. 11 pp. 1088-1092.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Elisa&nbsp;H&nbsp;Barney Smith</div> <div class="imageTitle">Figure 6 TCAS</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ To build the computer processors we use today, computer manufacturers have come to use a standard approach called the von Neumann computing architecture. However, as technologists push for increased processing speed, lower power consumption, and smaller processor and device sizes, this architecture is reaching its limits. As a possible alternative, researchers considered whether it was feasible to create a new architecture based on one of the most powerful computers known&mdash;the brain. During the project period, a project team sought to create artificial devices that would function like brain neurons and synapses. The neuron holds information and the synapse use electrical activity to pass that information from one cell to another. Project activity also addressed how to minimize device size, and how individual components performed as an integrated computing architecture.  Outcomes and Broader Impacts. Project outcomes have resulted in steps toward making this architecture a practical reality. In addition, as explained below, the project made an electrical component called a memristor available to the university project team, which included undergraduate researchers. Such devices are not often available at universities, and not part of the standard undergraduate curriculum. Over a dozen undergraduate students participated in this research, providing an experience not often available in basic electrical engineering education. In addition, multiple corporations have previously licensed the memristors that the team used. Completed research may suggest more potential uses for these devices, thereby encouraging more companies to license the technology and further technical developments in their use.  Facilitating New and Improved Computer Processing Opportunities. The brain has approximately 100 billion neurons. Using very little power as compared to the processors we use in our computers, neurons connect to 100 trillion synapses that transfer information. A processor built in silicon that mimics this function would substantially increase speed and reduce power consumption over current von Neuman designs. Further, we would open up opportunities for new and improved computer applications that like the brain, can mimic the way people think (cognitive data processing), more effectively recognize and classify patterns, are able to learn without being explicitly programmed to do so (machine learning), and can work on different parts of a problem at the same time (massively parallel architecture).  Making Artificial Neurons and Synapses. To begin, we designed and built a computer chip that&mdash;like the neuron&mdash;stores information. It is a 0.18&mu;m Complementary Metal-Oxide Semiconductor (CMOS) chip, see Figure 6 TCAS, and includes analog circuits that function as a leaky integrate-and-fire neuron, see Figure 2 IJCNN. Signals sent between neurons change the strength of the synapse through a process called spike-timing-dependent-plasticity (STDP). When spikes from the previous neuron and the following neuron come close in time, synapse strength changes. Therefore, to mimic the synapse and gain control of this activity, the team connected memristors between neurons. These electrical components limit or regulate the flow of electrical current in a circuit, remember the amount of charge that has previously flowed through it, and retain information in memory even when power to a device is off. We are able to alter memristor properties by applying voltage signals with a spiked shape.  We designed the integrate-and-fire neuron circuit to generate action potentials&mdash;or spikes&mdash;that can induce weight change in the memristors governed by the spike-timing-dependent-plasticity (STDP) learning rule. Individual spikes should not change the memristor weight when applied in isolation, i.e. the synapse weight should change only when engineers apply a post- and pre-synaptic spike-pair within a specific time window. When fabricated in a common structure called a crossbar memristor array, it will enable dense on-chip integration, and ultimately allow manufacturers to produce devices in smaller sizes than currently possible.  Expanding Potential Design Uses. Next, we explored the response of the memristors to different spike signal shapes/forms, see Figure 1. Doing so may ultimately provide chip designers with a greater array of design choices so signals to improve machine learning results. Additionally, we explored the effect of pulsing several shapes at time scales over a range of 10 orders of magnitude. This means that the memristors can be used at the biological timescale, or significantly faster allowing greater computing output. The team fit measured memristor voltages and currents to a mathematical model of a memristor developed by Yakopcic (2011) to find applicable model parameters.  Validating Performance. Finally, we examined how the complete architecture performed. The team ran network simulations for two applications: Digit Recognition, and Pavlov's Dog. Digits were recognized with a 96% accuracy, see Figure 6 JETCAS. We successfully ran the Pavlov dog scenario on the circuit neuron hardware with connected memristors, See Figure 2 Senior Design. As a result, scientists now have a better understanding of how to use memristors in a neural circuit, beyond simulation. The project team has also gained an improved understanding of the response of memristors to spiking signals.          Last Modified: 09/29/2017       Submitted by: Elisa H Barney Smith]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
