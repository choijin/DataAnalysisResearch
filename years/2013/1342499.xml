<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Thin Client Performance Benchmarking based Resource Adaptation in Virtual Desktop Clouds</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2013</AwardEffectiveDate>
<AwardExpirationDate>07/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>41716.00</AwardTotalIntnAmount>
<AwardAmount>49716</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Joseph Lyles</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Popular applications such as email, image/video galleries, and file storage are increasingly being supported by ?cloud? platforms in residential, academia and industry communities. The next frontier for these user communities will be to transition ?traditional desktops? that have dedicated hardware and software configurations into ?virtual desktop clouds? that are accessible via thin clients. This project aims to develop optimal resource allocation frameworks and performance benchmarking tools that can enable building and managing thin-client based virtual desktop clouds at Internet-scale. Virtual desktop cloud experiments under realistic user and system loads will be conducted by leveraging multiple kinds of GENI resources such as aggregates, user opt-in mechanisms, measurement services and experimenter workflow tools. Project outcomes will help in minimizing costly cloud resource over-commitment, and in avoiding thin client protocol configuration guesswork, while delivering optimum user experience. Further, they will positively impact computer desktop user communities, GENI-like testbeds, and equipment vendors.</AbstractNarration>
<MinAmdLetterDate>05/09/2013</MinAmdLetterDate>
<MaxAmdLetterDate>05/09/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1342499</AwardID>
<Investigator>
<FirstName>Prasad</FirstName>
<LastName>Calyam</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Prasad Calyam</PI_FULL_NAME>
<EmailAddress>calyamp@missouri.edu</EmailAddress>
<PI_PHON>6142705254</PI_PHON>
<NSF_ID>000268773</NSF_ID>
<StartDate>05/09/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Missouri-Columbia</Name>
<CityName>COLUMBIA</CityName>
<ZipCode>652110001</ZipCode>
<PhoneNumber>5738827560</PhoneNumber>
<StreetAddress>115 Business Loop 70 W</StreetAddress>
<StreetAddress2><![CDATA[Mizzou North, Room 501]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<StateCode>MO</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MO04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153890272</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MISSOURI SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006326904</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Missouri-Columbia]]></Name>
<CityName/>
<StateCode>MO</StateCode>
<ZipCode>652111230</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MO04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7363</Code>
<Text>RES IN NETWORKING TECH &amp; SYS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~41716</FUND_OBLG>
<FUND_OBLG>2012~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Common user applications such as email, photos, videos and file storage are already being supported at Internet-scale by &ldquo;cloud&rdquo; platforms (e.g., HP Cloud Assure, Google Mail, Amazon S3). Even academia is increasingly adopting cloud infrastructures and related research themes (e.g., NSF Cloud, DOE Magellan) to support scientific communities. The next frontier for these user communities will be to transition &ldquo;traditional distributed desktops&rdquo; that have dedicated hardware and software installations into &ldquo;virtual desktop clouds&rdquo; (VDCs) that are accessible via thin-clients.</p> <p>Moreover, in the not so distant future, we can envisage home users signing-up for virtual desktops (VDs) with a VD Cloud Service Provider (CSP) providing Desktop-as-a-Service (DaaS) as a utility. With such a utility service, a thin-client i.e., a set-top-box can be shipped to a residential user to access his/her personalized VD, similar to the model we have today for other common computing and communication needs such as VoIP (e.g., Vonage), and IPTV (e.g., Roku). This box can be connected to television monitors, or computer monitors, and multiple residential users can have their own unique login through this box to their VDs.</p> <p>By transitioning from &ldquo;traditional distributed desktops&rdquo; that have dedicated hardware and software installations into &ldquo;virtual desktop clouds&rdquo; (VDCs) that are accessible via thin-clients, the society will have greater user convenience, performance and mobility to access desktop applications in national priority areas of education, healthcare, and manufacturing.</p> <p class="Default">Broadly, our project goals to investigate and develop tools for VDC related schemes and technologies are as follows:</p> <ol> <li>Development and evaluation of performance benchmarking tools to collect measurements that can be leveraged for resource adaptation in virtual desktop clouds (VDCs)</li> <li>Investigation of resource management schemes (i.e., provisioning and placement) for thin-client based virtual desktop clouds at Internet-scale</li> <li>Deployment of the resource management schemes and performance benchmarking tools in the GENI facility to validate our schemes under realistic user and system loads</li> </ol> <p>We gave a live demonstration of our virtual desktop cloud experiment in GENI at the GEC10 Networking Reception. We setup 2 data centers, one at OSU VMLab and one at Utah Emulab and reserved several other nodes to act as thin-client VD connectors. On the reserved nodes, we installed hypervisor,&nbsp;OnTimeMeasure, and VMware tools. In addition, we installed several measurement automation scripts on the reserved nodes that leveraged the other installed software to control the load generation of thin-client VD connections and their corresponding performance measurements in the network and host resources. We used Matlab-based animation of a horse point cloud as the thin-client application and demonstrated that Utility-directed Resource Allocation Model (U-RAM) provides &ldquo;improved performance&rdquo; and &ldquo;increased scalability&rdquo; in comparison to today&rsquo;s Fixed Resource Allocation Model (F-RAM).</p> <p>We gave another live demonstration of our virtual desktop cloud experiment in GENI during our GEC15 Plenary Talk. We setup thin-clients at different sites in the meso-scale backbone network and used an&nbsp;OpenFlow controller application to establish VD connections and re-route them when their performance was affected by cross-traffic. We had the VMLab data center at The Ohio State University (location: Columbus, Ohio) (running VMware ESXi hypervisor for creating VD pools) hosting popular applications (e.g., Excel, Internet Explorer, Media Player) as well as advanced scientific computing applications (e.g., Matlab, Moldflow), connected to the GENI&nbsp;OpenFlow&nbsp;network. A utility-direct...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Common user applications such as email, photos, videos and file storage are already being supported at Internet-scale by "cloud" platforms (e.g., HP Cloud Assure, Google Mail, Amazon S3). Even academia is increasingly adopting cloud infrastructures and related research themes (e.g., NSF Cloud, DOE Magellan) to support scientific communities. The next frontier for these user communities will be to transition "traditional distributed desktops" that have dedicated hardware and software installations into "virtual desktop clouds" (VDCs) that are accessible via thin-clients.  Moreover, in the not so distant future, we can envisage home users signing-up for virtual desktops (VDs) with a VD Cloud Service Provider (CSP) providing Desktop-as-a-Service (DaaS) as a utility. With such a utility service, a thin-client i.e., a set-top-box can be shipped to a residential user to access his/her personalized VD, similar to the model we have today for other common computing and communication needs such as VoIP (e.g., Vonage), and IPTV (e.g., Roku). This box can be connected to television monitors, or computer monitors, and multiple residential users can have their own unique login through this box to their VDs.  By transitioning from "traditional distributed desktops" that have dedicated hardware and software installations into "virtual desktop clouds" (VDCs) that are accessible via thin-clients, the society will have greater user convenience, performance and mobility to access desktop applications in national priority areas of education, healthcare, and manufacturing. Broadly, our project goals to investigate and develop tools for VDC related schemes and technologies are as follows:  Development and evaluation of performance benchmarking tools to collect measurements that can be leveraged for resource adaptation in virtual desktop clouds (VDCs) Investigation of resource management schemes (i.e., provisioning and placement) for thin-client based virtual desktop clouds at Internet-scale Deployment of the resource management schemes and performance benchmarking tools in the GENI facility to validate our schemes under realistic user and system loads   We gave a live demonstration of our virtual desktop cloud experiment in GENI at the GEC10 Networking Reception. We setup 2 data centers, one at OSU VMLab and one at Utah Emulab and reserved several other nodes to act as thin-client VD connectors. On the reserved nodes, we installed hypervisor, OnTimeMeasure, and VMware tools. In addition, we installed several measurement automation scripts on the reserved nodes that leveraged the other installed software to control the load generation of thin-client VD connections and their corresponding performance measurements in the network and host resources. We used Matlab-based animation of a horse point cloud as the thin-client application and demonstrated that Utility-directed Resource Allocation Model (U-RAM) provides "improved performance" and "increased scalability" in comparison to todayÆs Fixed Resource Allocation Model (F-RAM).  We gave another live demonstration of our virtual desktop cloud experiment in GENI during our GEC15 Plenary Talk. We setup thin-clients at different sites in the meso-scale backbone network and used an OpenFlow controller application to establish VD connections and re-route them when their performance was affected by cross-traffic. We had the VMLab data center at The Ohio State University (location: Columbus, Ohio) (running VMware ESXi hypervisor for creating VD pools) hosting popular applications (e.g., Excel, Internet Explorer, Media Player) as well as advanced scientific computing applications (e.g., Matlab, Moldflow), connected to the GENI OpenFlow network. A utility-directed provisioning and placement scheme was used to intelligently manage the traffic flows by updating flow rules on OpenFlow switches, and the user QoE was measured/demonstrated with a HD wildlife video clip in the VD session.  Virtual desktop infrastructur...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
