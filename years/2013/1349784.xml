<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Data Debugging</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>02/29/2016</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>150000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Nina Amla</SignBlockName>
<PO_EMAI>namla@nsf.gov</PO_EMAI>
<PO_PHON>7032927991</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Today, systems rely as heavily on data as on the software that manipulates that data.  Unlike software, data cannot be easily tested or analyzed for correctness. Part of the problem is that it is difficult to decide whether data is wrong.  Typographical errors can change data items by orders of magnitude. For example, the number 1234 might be entered when the correct value is 12.34. Unfortunately, finding this kind of mistake via manual data auditing is onerous, unscalable, and error-prone.  Data errors can be costly: Errors in spreadsheet data have led to million dollar losses, and poor data quality has been estimated to cost the US economy more than $600 billion per year. &lt;br/&gt;&lt;br/&gt;Data debugging is a new approach for locating likely data errors by leveraging the interaction between data and the programs that operate on it. Since it is impossible to know a priori whether data is incorrect, data debugging aims to do the next best thing: identifying data that has an unusual impact on the computation. Intuitively, data with an inordinate impact on the result of a computation is either very important, or it is wrong. By contrast, wrong data whose presence has no particularly unusual effect on the final result does not matter.  By calling attention to data with inordinately high impact, data debugging can provide insights into both the data and the computation and reveal errors. Data debugging is especially well-suited for data-intensive programming environments like databases and spreadsheets that intertwine data and programs.  Data debugging can dramatically reduce the risks of human data entry errors or data corruption, increase the reliability of computations over data, and potentially save the US economy millions of dollars.</AbstractNarration>
<MinAmdLetterDate>08/19/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/19/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1349784</AwardID>
<Investigator>
<FirstName>Emery</FirstName>
<LastName>Berger</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Emery Berger</PI_FULL_NAME>
<EmailAddress>emery@cs.umass.edu</EmailAddress>
<PI_PHON>4135450698</PI_PHON>
<NSF_ID>000483414</NSF_ID>
<StartDate>08/19/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alexandra</FirstName>
<LastName>Meliou</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexandra Meliou</PI_FULL_NAME>
<EmailAddress>ameli@cs.umass.edu</EmailAddress>
<PI_PHON>4135450698</PI_PHON>
<NSF_ID>000637003</NSF_ID>
<StartDate>08/19/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName>Amherst</CityName>
<StateCode>MA</StateCode>
<ZipCode>010039241</ZipCode>
<StreetAddress><![CDATA[70 Butterfield Terrace]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Poor data quality is a challenging and persistent problem. Society's increasing reliance on data makes addressing this problem of paramount importance. This project developed algorithms and systems targeting the detection, prevention, and repair of errors in data, both for databases and spreadsheets. These approaches have the potential to dramatically reduce the incidence and impact of errors in data.</p> <p>Error detection in spreadsheets:&nbsp;Spreadsheets are the most widely used data management application for end users. Their wide use, notably in government, science, and industry, means that errors in spreadsheets can have a catastrophic impact. Unfortunately, because data is often entered manually, spreadsheets frequently contain serious data errors: on average, morethan 5% of data cells are wrong. While these errors are sometimes be benign, they can also dramatically change the results of a calculation. Due to the burden of auditing spreadsheets, which may contain hundreds or even thousands of data cells, these errors often go undetected. &nbsp;We designed a program-guided sensitivity analysis algorithm called "data debugging" that quickly locates the cells that have the largest effect on the results of spreadsheet formulas; these are either very important or wrong. &nbsp;We demonstrated data debugging with a point-and-click auditing tool called CheckCell, which we implemented as a plugin for Microsoft Excel. &nbsp;Our analysis using a corpus of real world spreadsheets and the influential but erroneous Reinhart-Rogoff spreadsheet shows that CheckCell quickly reveals suspect data so that users can prioritize their auditinge ffort on the data that has the largest impact. &nbsp;This work appeared at OOPSLA 2014, one of the premier venues for programming language research. &nbsp;CheckCell is available as an open-source,ready-to-install download at https://github.com/plasma-umass/DataDebug.</p> <p>Error detection in databases:&nbsp;In large-scale database applications, data is typically collected with a variety of methods and tools, and it is integrated across multiple data sources. This results in richer and more complete data, but it also introduces challenges, as data can contain conflicts. Traditional methods resolve conflicts naively, e.g., by trusting the values that are reported by a majority of sources. &nbsp;However, today it is common for data to be copied (e.g., from one website to another), so majority voting often results in erroneous data. &nbsp;We developed twoalgorithms, PrecRec and PrecRecCorr, that are more effective at detecting data errors than other state-of-the-art techniques. &nbsp;Our evaluation on three real-world datasets, as well as synthetic data showed that our algorithms can handle a variety of rich correlation scenarios. This work appeared in June 2014 at SIGMOD, one of the premier venues in data management.</p> <p>Error prevention:&nbsp;Data is almost always in a state of flux, evolving constantly as it is updated and augmented. &nbsp;Thus, even when a dataset has been cleaned of errors, it is unlikely to stay clean, as queries and other computations that operate on the data can easily introduce new errors. We developed a data error prevention tool, ConTest, which uses a novel data debugging methodology based on continuous testing. &nbsp;ConTest continuously monitors update accesses to data to identify potentially harmful operations. &nbsp;It provides a low-&shy;overhead, delay-free technique that quickly identifies likely data errors by continuously executing domain&shy;-specific test queries. A user study with 96 crowd workers showed that ConTest is extremely effective at guarding against data entry errors. With ConTest, users corrected 98.4% of their errors, as opposed to 40.2% without, even when we injected 40% false positives into ConTest's output. This work appeared at two premier software engineering conferences, FSE 2013 an...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Poor data quality is a challenging and persistent problem. Society's increasing reliance on data makes addressing this problem of paramount importance. This project developed algorithms and systems targeting the detection, prevention, and repair of errors in data, both for databases and spreadsheets. These approaches have the potential to dramatically reduce the incidence and impact of errors in data.  Error detection in spreadsheets: Spreadsheets are the most widely used data management application for end users. Their wide use, notably in government, science, and industry, means that errors in spreadsheets can have a catastrophic impact. Unfortunately, because data is often entered manually, spreadsheets frequently contain serious data errors: on average, morethan 5% of data cells are wrong. While these errors are sometimes be benign, they can also dramatically change the results of a calculation. Due to the burden of auditing spreadsheets, which may contain hundreds or even thousands of data cells, these errors often go undetected.  We designed a program-guided sensitivity analysis algorithm called "data debugging" that quickly locates the cells that have the largest effect on the results of spreadsheet formulas; these are either very important or wrong.  We demonstrated data debugging with a point-and-click auditing tool called CheckCell, which we implemented as a plugin for Microsoft Excel.  Our analysis using a corpus of real world spreadsheets and the influential but erroneous Reinhart-Rogoff spreadsheet shows that CheckCell quickly reveals suspect data so that users can prioritize their auditinge ffort on the data that has the largest impact.  This work appeared at OOPSLA 2014, one of the premier venues for programming language research.  CheckCell is available as an open-source,ready-to-install download at https://github.com/plasma-umass/DataDebug.  Error detection in databases: In large-scale database applications, data is typically collected with a variety of methods and tools, and it is integrated across multiple data sources. This results in richer and more complete data, but it also introduces challenges, as data can contain conflicts. Traditional methods resolve conflicts naively, e.g., by trusting the values that are reported by a majority of sources.  However, today it is common for data to be copied (e.g., from one website to another), so majority voting often results in erroneous data.  We developed twoalgorithms, PrecRec and PrecRecCorr, that are more effective at detecting data errors than other state-of-the-art techniques.  Our evaluation on three real-world datasets, as well as synthetic data showed that our algorithms can handle a variety of rich correlation scenarios. This work appeared in June 2014 at SIGMOD, one of the premier venues in data management.  Error prevention: Data is almost always in a state of flux, evolving constantly as it is updated and augmented.  Thus, even when a dataset has been cleaned of errors, it is unlikely to stay clean, as queries and other computations that operate on the data can easily introduce new errors. We developed a data error prevention tool, ConTest, which uses a novel data debugging methodology based on continuous testing.  ConTest continuously monitors update accesses to data to identify potentially harmful operations.  It provides a low-&shy;overhead, delay-free technique that quickly identifies likely data errors by continuously executing domain&shy;-specific test queries. A user study with 96 crowd workers showed that ConTest is extremely effective at guarding against data entry errors. With ConTest, users corrected 98.4% of their errors, as opposed to 40.2% without, even when we injected 40% false positives into ConTest's output. This work appeared at two premier software engineering conferences, FSE 2013 and ISSTA 2015.  Error repair: Traditional data cleaning techniques attempt to fix errors directly by identifying the errors and inferring a fix.  They ...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
