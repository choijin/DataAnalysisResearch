<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: SDA: Collaborative Research: A Scalable and Distributed System Framework for Compute-Intensive and Data-Parallel Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>375000.00</AwardTotalIntnAmount>
<AwardAmount>375000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tevfik Kosar</SignBlockName>
<PO_EMAI>tkosar@nsf.gov</PO_EMAI>
<PO_PHON>7032927992</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Whereas traditional high-performance computing (HPC) applications are computationally intensive, recent HPC applications require more data-intensive analysis and visualization to extract knowledge. In many cases, these applications execute the same computational algorithm as in the past (e.g., parallel search or parallel rendering) but now must do so for significantly larger data sets. For example, the life sciences, along with the cross-cutting area of scientific visualization, constitute an emerging category of HPC applications that not only perform sophisticated calculations but also ingest a sea of data. Running these new HPC data-parallel applications on today's computing platforms imposes new challenges and demands additional functionality.&lt;br/&gt;&lt;br/&gt;However, today's HPC platforms still adopt a compute-centric model and do not handle these new challenges well. Such a model often moves a large amount of data to various parallel computational processes. Consequently, long CPU wait times for I/O to complete and enormous data-movement overhead become major stumbling blocks to high performance and scalability. This project encompasses the creation of a scalable cross-layer software framework to enable both computationally intensive and data-intensive parallel HPC applications to run on distributed file systems. This framework consists of two interwoven research tasks: (1) an adaptive, data locality-aware, middleware system that dynamically schedules compute processes to access local data by monitoring physical data locations and (2) a framework that captures the computation and data I/O processing relationship from parallel applications and coordinates the scheduling of the corresponding process and I/O execution for maximum parallel efficiency. The success of this project contributes enhanced productivity and return on investment on HPC resources via the elimination of both CPU wait time and network transfer of frequently accessed data in scientific applications. An open-source, sustainable, and reusable software framework is delivered to speed-up the discovery and innovation process in areas such as bioinformatics, climate, high-energy physics, cosmology, astrophysics, and chromodynamics. The synergy in the two proposing institutions, Virginia Tech and the University of Central Florida, and their collaborating DOE national laboratories, will catalyze new and beneficial perspectives in the graduate education of students and prepare a 21st-century workforce in HPC.</AbstractNarration>
<MinAmdLetterDate>08/22/2013</MinAmdLetterDate>
<MaxAmdLetterDate>08/22/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1337131</AwardID>
<Investigator>
<FirstName>Wuchun</FirstName>
<LastName>Feng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wuchun Feng</PI_FULL_NAME>
<EmailAddress>feng@cs.vt.edu</EmailAddress>
<PI_PHON>5402311192</PI_PHON>
<NSF_ID>000066142</NSF_ID>
<StartDate>08/22/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Heshan</FirstName>
<LastName>Lin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Heshan Lin</PI_FULL_NAME>
<EmailAddress>hlin2@cs.vt.edu</EmailAddress>
<PI_PHON>5402315281</PI_PHON>
<NSF_ID>000529382</NSF_ID>
<StartDate>08/22/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Virginia Polytechnic Institute and State University</Name>
<CityName>BLACKSBURG</CityName>
<ZipCode>240610001</ZipCode>
<PhoneNumber>5402315281</PhoneNumber>
<StreetAddress>Sponsored Programs 0170</StreetAddress>
<StreetAddress2><![CDATA[300 Turner Street NW, Suite 4200]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003137015</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>VIRGINIA POLYTECHNIC INSTITUTE AND STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003137015</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Virginia Polytechnic Institute and State University]]></Name>
<CityName>Blacksburg</CityName>
<StateCode>VA</StateCode>
<ZipCode>240603580</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~375000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Today, big data applications can generate large-scale datasets at an unprecedented rate. In fact, many sources project that the volume of data will continue to grow exponentially; specifically, the size of the digital universe will double every two years. As a consequence, today's high-performance computing (HPC) systems, i.e., supercomputers, must be designed in tandem with big-data software to realize high-performance (and hopefully, real-time performance) in data analytics.</p> <p>Unfortunately, while many high-performance computing (HPC) systems provide mechanisms to efficiently partition, process, and analyze the data, and in turn, realize insights about the data, such mechanisms can be tedious to use and prone to human error. Therefore, in this research project, we rigorously profiled and analyzed these big data applications on HPC systems, and from our profiling and analysis, we created a generalized big-data prototype that semi-automates the partitioning and analysis for high-performance (and arguably, even real-time) data analytics. When compared to big data applications that have been manually enhanced and optimized by human programming experts, our (ultimate) semi-automated big data tool performs as well, and in some cases, even better than the manually optimized, big data applications. Future work includes incorporating artificial intelligence (in the form of machine learning or deep learning) to more intelligently automate the process.</p><br> <p>            Last Modified: 02/14/2018<br>      Modified by: Wuchun&nbsp;Feng</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Today, big data applications can generate large-scale datasets at an unprecedented rate. In fact, many sources project that the volume of data will continue to grow exponentially; specifically, the size of the digital universe will double every two years. As a consequence, today's high-performance computing (HPC) systems, i.e., supercomputers, must be designed in tandem with big-data software to realize high-performance (and hopefully, real-time performance) in data analytics.  Unfortunately, while many high-performance computing (HPC) systems provide mechanisms to efficiently partition, process, and analyze the data, and in turn, realize insights about the data, such mechanisms can be tedious to use and prone to human error. Therefore, in this research project, we rigorously profiled and analyzed these big data applications on HPC systems, and from our profiling and analysis, we created a generalized big-data prototype that semi-automates the partitioning and analysis for high-performance (and arguably, even real-time) data analytics. When compared to big data applications that have been manually enhanced and optimized by human programming experts, our (ultimate) semi-automated big data tool performs as well, and in some cases, even better than the manually optimized, big data applications. Future work includes incorporating artificial intelligence (in the form of machine learning or deep learning) to more intelligently automate the process.       Last Modified: 02/14/2018       Submitted by: Wuchun Feng]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
