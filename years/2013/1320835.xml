<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF:Small: Foundations of Transactional Memory Scheduling</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>365897.00</AwardTotalIntnAmount>
<AwardAmount>365897</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Multi-core computer architectures offer unprecedented performance benefits and present new challenges for the efficient synchronization of concurrent computations. Transactional memory is a prominent programming model that simplifies the synchronization of shared memory accesses, and avoids the complications of fine-grained locking mechanisms. A memory transaction represents a sequence of (read/write) shared memory operations that need to be performed atomically by a computation thread. A transaction either commits, or aborts in case of conflicts with other transactions that concurrently access the same shared resources. This project aims to design, develop, and analyze contention managers that schedule efficiently memory transactions in a variety of systems. The goal is to provide schedulers that have provable formal performance guarantees and at the same time are practically efficient; thus, bridging the gap between theory and practice that currently appears in the literature.&lt;br/&gt;&lt;br/&gt;The project considers a wide range of distributed systems, including tightly-coupled systems such as multi-core processors, and larger scale systems such as distributed networked processors. One of the main objectives is to provide scheduling algorithms which scale gracefully with the various system sizes and complexities. In order to fulfill this objective, this work proposes new analytical techniques to obtain good formal bounds with appropriate performance metrics, and also conducts experimental evaluations in real world workloads to obtain good performance in practical scenarios. The project establishes foundations for investigating the performance of transactional memory systems, and also provides analytical tools to the research community for exploring transactional memory to its full potential. The proposed research impacts the larger computing community because it affects the efficiency of distributed and parallel programs running on widely used distributed and multi-core systems.</AbstractNarration>
<MinAmdLetterDate>07/29/2013</MinAmdLetterDate>
<MaxAmdLetterDate>07/29/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1320835</AwardID>
<Investigator>
<FirstName>Konstantin</FirstName>
<LastName>Busch</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Konstantin Busch</PI_FULL_NAME>
<EmailAddress>kbusch@augusta.edu</EmailAddress>
<PI_PHON>7067210493</PI_PHON>
<NSF_ID>000109990</NSF_ID>
<StartDate>07/29/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Louisiana State University</Name>
<CityName>Baton Rouge</CityName>
<ZipCode>708032701</ZipCode>
<PhoneNumber>2255782760</PhoneNumber>
<StreetAddress>202 Himes Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Louisiana</StateName>
<StateCode>LA</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>LA06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>075050765</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LOUISIANA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>940050792</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Louisiana State University & Agricultural and Mechanical College]]></Name>
<CityName>Baton Rouge</CityName>
<StateCode>LA</StateCode>
<ZipCode>708032701</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Louisiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>LA06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramElement>
<ProgramElement>
<Code>7943</Code>
<Text>PROGRAMMING LANGUAGES</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7943</Code>
<Text>PROGRAMMING LANGUAGES</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~365897</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Transactional memory is an important concept for concurrent programming in multi-core and distributed systems. Critical sections of code are contained within transactions, similar to database transactions. The benefit of memory transactions is that the programs are lock-free and therefore they do not suffer from the typical drawbacks of lock-based programming such as thread deadlocks or starvations. Therefore, transactional memory is more resilient to programming errors. However, the performance is sensitive to the abort rate of transactions.</p> <p>The conflict manager plays a significant role in the efficient execution of transactional memory since it determines which transactions will commit or abort. Transactions conflict when they execute instructions with colliding memory accesses to the same shared memory addresses. There are several criteria for the transaction scheduler to determine which transactions to commit or abort, based to thread priorities, kinds of memory operations (read/write), length of transactions, and the number of retries.</p> <p>This&nbsp;project provided new insights into the design of conflict managers for efficient transaction scheduling.&nbsp; Various performance metrics were considered including the execution time of the transactions, the number of aborts per transaction, communication delay, and maximum load per node. The project provided novel transaction scheduling algorithms with provable performance guarantees for tightly-coupled systems and distributed systems. In many cases, the algorithms were proven to have near optimal asymptotic performance. Moreover,&nbsp; the theoretical findings were also verified through experimental evaluations.</p> <p>In addition, this project provided impossibility results in the distributed setting demonstrating that there is a limit on how well can multiple aspects of the scheduling problems be optimized. For instance, it is proven impossible to simultaneously optimize the execution time and communication cost in the general setting. Nevertheless, in several interesting special distributed network topologies, time and communication cost can be optimized simultaneously.&nbsp;</p> <p>The results of this research have significant impacts&nbsp;on the study of transactional memory and more generally in understanding scheduling problems in multicore and distributed systems. The new transactional memory algorithms can be applied to other scheduling problems as well. For example, they can be used to solve scheduling problems with task dependencies created from sharing the same system resources. The lower bounds provided in this project have also an impact on design decisions for transactional memory systems and in determining the requirements to solve task scheduling problems in the general setting.</p> <p>The findings of this project affect the development of concurrent/multithreaded programs, and also the productivity of software for multicore and distributed systems. The overall impacts are far-reaching, from simple multicore devices to large-scale distributed clusters. The discovered improvements in the performance of systems impact positively other scientific disciplines as well that depend on improved multiprocessor performance.</p> <p>The results of this research have appeared in renowned publication&nbsp;venues including the Symposium on Principles of Distributed Computing (PODC), the Symposium on Parallelism in Algorithms and Architectures (SPAA), and Journal on Distributed Computing. Several graduate students and a postdoc have been supported by this project and played a&nbsp;key role in this research.</p><br> <p>            Last Modified: 03/03/2019<br>      Modified by: Konstantin&nbsp;Busch</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Transactional memory is an important concept for concurrent programming in multi-core and distributed systems. Critical sections of code are contained within transactions, similar to database transactions. The benefit of memory transactions is that the programs are lock-free and therefore they do not suffer from the typical drawbacks of lock-based programming such as thread deadlocks or starvations. Therefore, transactional memory is more resilient to programming errors. However, the performance is sensitive to the abort rate of transactions.  The conflict manager plays a significant role in the efficient execution of transactional memory since it determines which transactions will commit or abort. Transactions conflict when they execute instructions with colliding memory accesses to the same shared memory addresses. There are several criteria for the transaction scheduler to determine which transactions to commit or abort, based to thread priorities, kinds of memory operations (read/write), length of transactions, and the number of retries.  This project provided new insights into the design of conflict managers for efficient transaction scheduling.  Various performance metrics were considered including the execution time of the transactions, the number of aborts per transaction, communication delay, and maximum load per node. The project provided novel transaction scheduling algorithms with provable performance guarantees for tightly-coupled systems and distributed systems. In many cases, the algorithms were proven to have near optimal asymptotic performance. Moreover,  the theoretical findings were also verified through experimental evaluations.  In addition, this project provided impossibility results in the distributed setting demonstrating that there is a limit on how well can multiple aspects of the scheduling problems be optimized. For instance, it is proven impossible to simultaneously optimize the execution time and communication cost in the general setting. Nevertheless, in several interesting special distributed network topologies, time and communication cost can be optimized simultaneously.   The results of this research have significant impacts on the study of transactional memory and more generally in understanding scheduling problems in multicore and distributed systems. The new transactional memory algorithms can be applied to other scheduling problems as well. For example, they can be used to solve scheduling problems with task dependencies created from sharing the same system resources. The lower bounds provided in this project have also an impact on design decisions for transactional memory systems and in determining the requirements to solve task scheduling problems in the general setting.  The findings of this project affect the development of concurrent/multithreaded programs, and also the productivity of software for multicore and distributed systems. The overall impacts are far-reaching, from simple multicore devices to large-scale distributed clusters. The discovered improvements in the performance of systems impact positively other scientific disciplines as well that depend on improved multiprocessor performance.  The results of this research have appeared in renowned publication venues including the Symposium on Principles of Distributed Computing (PODC), the Symposium on Parallelism in Algorithms and Architectures (SPAA), and Journal on Distributed Computing. Several graduate students and a postdoc have been supported by this project and played a key role in this research.       Last Modified: 03/03/2019       Submitted by: Konstantin Busch]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
