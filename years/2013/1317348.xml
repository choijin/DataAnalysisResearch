<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Visual Cortex on Silicon</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2013</AwardEffectiveDate>
<AwardExpirationDate>09/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>750000.00</AwardTotalIntnAmount>
<AwardAmount>750000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The human vision system understands and interprets complex scenes for a wide range of visual tasks in real-time while consuming less than 20 Watts of power. This Expeditions-in-Computing project explores holistic design of machine vision systems that have the potential to approach and eventually exceed the capabilities of human vision systems. This will enable the next generation of machine vision systems to not only record images but also understand visual content. Such smart machine vision systems will have a multi-faceted impact on society, including visual aids for visually impaired persons, driver assistance for reducing automotive accidents, and augmented reality for enhanced shopping, travel, and safety. The transformative nature of the research will inspire and train a new generation of students in inter-disciplinary work that spans neuroscience, computing and engineering discipline.&lt;br/&gt;&lt;br/&gt;While several machine vision systems today can each successfully perform one or a few human tasks ? such as detecting human faces in point-and-shoot cameras ? they are still limited in their ability to perform a wide range of visual tasks, to operate in complex, cluttered environments, and to provide reasoning for their decisions.  In contrast, the mammalian visual cortex excels in a broad variety of goal-oriented cognitive tasks, and is at least three orders of magnitude more energy efficient than customized state-of-the-art machine vision systems. The proposed research envisions a holistic design of a machine vision system that will approach the cognitive abilities of the human cortex, by developing a comprehensive solution consisting of vision algorithms, hardware design, human-machine interfaces, and information storage. The project aims to understand the fundamental mechanisms used in the visual cortex to enable the design of new vision algorithms and hardware fabrics that can improve power, speed, flexibility, and recognition accuracies relative to existing machine vision systems. Towards this goal, the project proposes an ambitious inter-disciplinary research agenda that will (i) understand goal-directed visual attention mechanisms in the brain to design task-driven vision algorithms; (ii) develop vision theory and algorithms that scale in performance with increasing complexity of a scene; (iii) integrate complementary approaches in biological and machine vision techniques; (iv) develop a new-genre of computing architectures inspired by advances in both the understanding of the visual cortex and the emergence of electronic devices; and (v) design human-computer interfaces that will effectively assist end-users while preserving privacy and maximizing utility. These advances will allow us to replace current-day cameras with cognitive visual systems that more intelligently analyze and understand complex scenes, and dynamically interact with users.&lt;br/&gt;&lt;br/&gt;Machine vision systems that understand and interact with their environment in ways similar to humans will enable new transformative applications. The project will develop experimental platforms to: (1) assist visually impaired people; (2) enhance driver attention; and (3) augment reality to provide enhanced experience for retail shopping or a vacation visit, and enhanced safety for critical public infrastructure. This project will result in education and research artifacts that will be disseminated widely through a web portal and via online lecture delivery. The resulting artifacts and prototypes will enhance successful ongoing outreach programs to under-represented minorities and the general public, such as museum exhibits, science fairs, and a summer camp aimed at K-12 students. It will also spur similar new outreach efforts at other partner locations. The project will help identify and develop course material and projects directed at instilling interest in computing fields for students in four-year colleges. Partnerships with two Hispanic serving institutes, industry, national labs and international projects are also planned.</AbstractNarration>
<MinAmdLetterDate>09/17/2013</MinAmdLetterDate>
<MaxAmdLetterDate>09/03/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1317348</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Desimone</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Desimone</PI_FULL_NAME>
<EmailAddress>desimone@mit.edu</EmailAddress>
<PI_PHON>6173240141</PI_PHON>
<NSF_ID>000523667</NSF_ID>
<StartDate>09/17/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 Massachusetts Ave.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7723</Code>
<Text>Expeditions in Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7723</Code>
<Text>EXPERIMENTAL EXPEDITIONS</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~300000</FUND_OBLG>
<FUND_OBLG>2014~150000</FUND_OBLG>
<FUND_OBLG>2015~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>In a cluttered visual environment, top-down control is needed to efficiently process behaviorally relevant information while filtering out irrelevant or distracting information.&nbsp; Such top-down processing can be based on spatial cues, e.g., knowing where a relevant object is located.&nbsp; However, in many instances, we do not know where a relevant object is located but know its features, and therefore need to find it based on those features. </span><span>&nbsp;An example is finding the face of our child in a crowded room.&nbsp; While much is known about the mechanisms of spatial attention, the neural mechanisms underlying feature-based attention are unknown.&nbsp; Understanding more about the brain mechanisms that mediate attention to features will help us guide more intelligent artificial recognition systems in the future, which could aid people with low vision.&nbsp;&nbsp;</span></p> <p><span>In this project, we identified a new region in the prefrontal cortex that appears to be the source of feedback that guides the visual system during attention to features.&nbsp; When this region is temporarily deactivated, attention to features is severely degraded and distracting information in no longer effectively filtered out.&nbsp; We not only identified the critical source of this feedback, but we also uncovered some of the neuronal mechanisms in vision that are gated by attention, which become engaged when we attempt find an object in a cluttered scene.&nbsp; &nbsp;&nbsp;</span></p><br> <p>            Last Modified: 02/21/2019<br>      Modified by: Robert&nbsp;Desimone</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In a cluttered visual environment, top-down control is needed to efficiently process behaviorally relevant information while filtering out irrelevant or distracting information.  Such top-down processing can be based on spatial cues, e.g., knowing where a relevant object is located.  However, in many instances, we do not know where a relevant object is located but know its features, and therefore need to find it based on those features.  An example is finding the face of our child in a crowded room.  While much is known about the mechanisms of spatial attention, the neural mechanisms underlying feature-based attention are unknown.  Understanding more about the brain mechanisms that mediate attention to features will help us guide more intelligent artificial recognition systems in the future, which could aid people with low vision.    In this project, we identified a new region in the prefrontal cortex that appears to be the source of feedback that guides the visual system during attention to features.  When this region is temporarily deactivated, attention to features is severely degraded and distracting information in no longer effectively filtered out.  We not only identified the critical source of this feedback, but we also uncovered some of the neuronal mechanisms in vision that are gated by attention, which become engaged when we attempt find an object in a cluttered scene.           Last Modified: 02/21/2019       Submitted by: Robert Desimone]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
