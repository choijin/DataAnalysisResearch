<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>STTR Phase I:  An Assistive Tool to Locate People and Objects with a Multimodal Thermogram Interface</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2014</AwardEffectiveDate>
<AwardExpirationDate>12/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>224503.00</AwardTotalIntnAmount>
<AwardAmount>224503</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Muralidharan Nair</SignBlockName>
<PO_EMAI>mnair@nsf.gov</PO_EMAI>
<PO_PHON>7032927059</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This Small Business Technology Transfer Research (STTR) Phase I project will leverage past National Science Foundation Funded research to prove that a blind/low vision user can receive practical navigation and interaction information about their environment from a multimodal thermogram (thermal image) interface on a smartphone. There are no practical assistive technologies for blind or low vision users that allow them to locate people, objects, and the layout information of their surroundings other than exploring with a cane. This research will address the objective of creating an interface that provides both practical utility and will be accepted by the target demographic of blind users. This project represents an excellent translational path from NSF-sponsored research programs to a real-world system that is built from the ground up on solid theoretical underpinnings and empirical findings from multimodal human information processing. This research will use thermal radiation from people, machines, lighting and heat retention differences in building materials and convert this data into a user interface to facilitate blind navigation and environment interaction. The research result will be a multimodal (kinesthetic, vibro-tactile, and auditory) interface for blind users of a smartphone to interpret and gain useful value from thermal image information.&lt;br/&gt; &lt;br/&gt;The broader impact/commercial potential of this project will go beyond assistive use of thermal technology for blind users. This technology will have a societal impact by improving the quality of life and autonomy for the blind in the same way that Global Positioning System and handheld computing have. Information about the layout of an unfamiliar public space can be learned from the heat and shape of materials, which will assist blind users in work environments or everyday activities. With the global proliferation of smartphone usage and an aging population, the commercial market for this assistive product will continue to see strong growth in the future. After the technology for low cost thermal imaging on a smartphone has been developed, the market for sighted users could benefit from numerous related applications as well. For example, various commercial industries could benefit from low cost thermal imaging on a smartphone that can communicate data wirelessly through a cellular network. Industry examples might include: manufacturing, petrochemical installations, construction, electrical systems, food packaging, or agriculture applications. The commercial market for applications used by sighted people will likely exceed the commercial market for blind users.</AbstractNarration>
<MinAmdLetterDate>12/11/2013</MinAmdLetterDate>
<MaxAmdLetterDate>12/11/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1346292</AwardID>
<Investigator>
<FirstName>Nicholas</FirstName>
<LastName>Giudice</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nicholas Giudice</PI_FULL_NAME>
<EmailAddress>nicholas.giudice@maine.edu</EmailAddress>
<PI_PHON>2075812187</PI_PHON>
<NSF_ID>000503809</NSF_ID>
<StartDate>12/11/2013</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Hanzal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian Hanzal</PI_FULL_NAME>
<EmailAddress>brian_hanzal@yahoo.com</EmailAddress>
<PI_PHON>6124818723</PI_PHON>
<NSF_ID>000647355</NSF_ID>
<StartDate>12/11/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Moai Technologies L.L.C.</Name>
<CityName>Plymouth</CityName>
<ZipCode>554464595</ZipCode>
<PhoneNumber>6124818723</PhoneNumber>
<StreetAddress>18215 45th Ave N</StreetAddress>
<StreetAddress2><![CDATA[Suite B]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>078662436</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MOAI TECHNOLOGIES LLC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maine]]></Name>
<CityName>Orono</CityName>
<StateCode>ME</StateCode>
<ZipCode>044695711</ZipCode>
<StreetAddress><![CDATA[348 Broadman Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maine</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>ME02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1505</Code>
<Text>STTR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>1505</Code>
<Text>STTR PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>8035</Code>
<Text>Hardware Devices</Text>
</ProgramReference>
<ProgramReference>
<Code>9139</Code>
<Text>INFORMATION INFRASTRUCTURE &amp; TECH APPL</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~224503</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Smartphones now make possible the fusion of multiple image technologies to allow a blind or low vision person to locate people, objects, and interpret the layout of their surroundings. No devices have ever been produced that provide this range of information-gathering capabilities to the disabled user in a single, portable handheld assistive device. In addition to visible light images, there are several &ldquo;smartphone add on&rdquo; thermal imaging cameras that have recently come on the market. The fusion of these technologies can provide a blind user with highly detailed information about their surroundings. In this project Moai Technologies LLC worked with Dr. Nicholas Giudice at the University of Maine&rsquo;s School of Computing and Information Science for the goal of creating a smartphone human interface technology to utilize this fusion of imaging technology for blind users. Dr. Giudice is leading the NSF-sponsored research of multimodal (kinesthetic, vibro-tactile, and auditory) interfaces for blind and low vision users of a smartphone touchscreen graphic display. This project represents an excellent translational path from NSF-sponsored research programs to a real-world system that is built from the ground up on solid theoretical underpinnings and empirical findings from studies on multimodal human information processing. This NSF STTR goal is to develop the first smartphone multimodal interfaces to use these new imaging products to assist blind persons in every day applications.&nbsp;</p> <p>Thermal imaging data can help to differentiate people and objects from their background without the need for complex image analysis. Thermal imaging also improves problems with visible imaging in low light levels. The shape and the temperature of the human body and machines allows the location of these to be easily determined. For instance, in complex public spaces a blind person can haptically examine the thermogram image or a schematized representation created by the smartphone of people standing and sitting in front of them by feeling vibro-tactile cues on a smartphone&rsquo;s touchscreen. By highlighting the temperature range of the human body, the shapes of people standout from other temperatures in the background. This can be fused with the visible images from the smart phone camera to provide scene interpretation. Examples of this use would be determining a place to sit in a crowded classroom or food court. The system would also aid in figuring out whether the area they want to travel through is crowded or open. Whether there a line of people cuing and where does the line end? &nbsp;Some objects also give off heat that is unique from their background. Most machines generate waste heat that can be used to locate them. Examples would be vending machines, ATMs, and automated train ticket booths. The rectangular backlight of a vending display serves as a thermal signature of a machine with a graphical user interface. The product goal is to provide a voice or tactile narrative of the scenery by constructing a skeletonized dictionary of people, objects and their placements. The final product will be combined with a smartphone multimodal interface of interior and exterior layout databases to become a powerful independent navigation solution for low vision and blind users. &nbsp;&nbsp;</p> <p>The activities of this STTR phase I were designed to apply past NSF funded research to prove that a blind/low vision user can receive practical value from a multimodal thermogram interface on a smartphone. A prototype system was developed to capture a thermal image of a scene (which often is referred to as a thermogram), and via a custom image-processing algorithm convert the image to a form for actual and symbolic display on the touchscreen of a mobile device. &nbsp;Human testing of the evaluation system was done with blind/low-vision user to show that they can receive practical...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Smartphones now make possible the fusion of multiple image technologies to allow a blind or low vision person to locate people, objects, and interpret the layout of their surroundings. No devices have ever been produced that provide this range of information-gathering capabilities to the disabled user in a single, portable handheld assistive device. In addition to visible light images, there are several "smartphone add on" thermal imaging cameras that have recently come on the market. The fusion of these technologies can provide a blind user with highly detailed information about their surroundings. In this project Moai Technologies LLC worked with Dr. Nicholas Giudice at the University of MaineÆs School of Computing and Information Science for the goal of creating a smartphone human interface technology to utilize this fusion of imaging technology for blind users. Dr. Giudice is leading the NSF-sponsored research of multimodal (kinesthetic, vibro-tactile, and auditory) interfaces for blind and low vision users of a smartphone touchscreen graphic display. This project represents an excellent translational path from NSF-sponsored research programs to a real-world system that is built from the ground up on solid theoretical underpinnings and empirical findings from studies on multimodal human information processing. This NSF STTR goal is to develop the first smartphone multimodal interfaces to use these new imaging products to assist blind persons in every day applications.   Thermal imaging data can help to differentiate people and objects from their background without the need for complex image analysis. Thermal imaging also improves problems with visible imaging in low light levels. The shape and the temperature of the human body and machines allows the location of these to be easily determined. For instance, in complex public spaces a blind person can haptically examine the thermogram image or a schematized representation created by the smartphone of people standing and sitting in front of them by feeling vibro-tactile cues on a smartphoneÆs touchscreen. By highlighting the temperature range of the human body, the shapes of people standout from other temperatures in the background. This can be fused with the visible images from the smart phone camera to provide scene interpretation. Examples of this use would be determining a place to sit in a crowded classroom or food court. The system would also aid in figuring out whether the area they want to travel through is crowded or open. Whether there a line of people cuing and where does the line end?  Some objects also give off heat that is unique from their background. Most machines generate waste heat that can be used to locate them. Examples would be vending machines, ATMs, and automated train ticket booths. The rectangular backlight of a vending display serves as a thermal signature of a machine with a graphical user interface. The product goal is to provide a voice or tactile narrative of the scenery by constructing a skeletonized dictionary of people, objects and their placements. The final product will be combined with a smartphone multimodal interface of interior and exterior layout databases to become a powerful independent navigation solution for low vision and blind users.     The activities of this STTR phase I were designed to apply past NSF funded research to prove that a blind/low vision user can receive practical value from a multimodal thermogram interface on a smartphone. A prototype system was developed to capture a thermal image of a scene (which often is referred to as a thermogram), and via a custom image-processing algorithm convert the image to a form for actual and symbolic display on the touchscreen of a mobile device.  Human testing of the evaluation system was done with blind/low-vision user to show that they can receive practical value from a multimodal thermogram interface on the mobile device. Specifically it was shown that they can determine the l...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
