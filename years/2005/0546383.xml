<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Passive Vision -- What Can Be Learned by a Stationary Observer</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2006</AwardEffectiveDate>
<AwardExpirationDate>02/29/2012</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>516000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration> CAREER: Passive Vision -- What Can Be Learned by a Stationary  Observer&lt;br/&gt;&lt;br/&gt;Passive Vision is the analysis of video from a camera that is not moving.  Many cameras do not move, and continually watch a specific scene -- an ATM, an airport security desk, or a traffic intersection -- for months or years.  Much as Active Vision (the ability to intentionally control camera motion) simplifies problems in structure from motion, Passive Vision simplifies statistical image analysis by observing statistics of the same scene for very long time periods.&lt;br/&gt;&lt;br/&gt;This project develops a framework to study the statistics of fixed-viewpoint video.  General statistics of natural video underlie current models of image and video compression and provide a statistical context for general image processing.  But for video taken from a single viewpoint, the same analytic tools find much more specific statistical correlations, and these correlations relate to important scene features.  For example, image regions that share geometric features such as surface normal and depth have a correlated responses to natural lighting changes.  A tree waving in the wind tends to move all at the same time.&lt;br/&gt;&lt;br/&gt;Furthermore, automated tools that develop statistics of specific video sequences, accumulated over time, promise to ground a number of probabilistic algorithms in surveillance. Surprisingly simple, local statistics of image derivatives find anomalous objects in scenes with significant background motions and find complicated patterns of motions of objects in a scene.  Within surveillance, characterizing the statistics of background variations captured over weeks or months provides a foundation to more formally address questions of slow background drift (due to clouds, shadows, or seasons), and when or whether moving objects that stop should be included in the background.&lt;br/&gt;&lt;br/&gt;This research program formalizes heuristic approaches to key problems in surveillance and offers a broader understanding of the statistics of natural images.  This provides the foundation for a potentially large body of research in learning scene-specific algorithms for image representation and coding, image de-noising, object recognition, anomaly detection, and scene annotation --- key problems in using Computer Vision to address current Homeland Security needs.&lt;br/&gt;&lt;br/&gt;Project web page:&lt;br/&gt;http://www.cse.wustl.edu/~pless/PassiveVision.html&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>02/23/2006</MinAmdLetterDate>
<MaxAmdLetterDate>04/09/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0546383</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Pless</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Pless</PI_FULL_NAME>
<EmailAddress>pless@gwu.edu</EmailAddress>
<PI_PHON>3142390536</PI_PHON>
<NSF_ID>000269630</NSF_ID>
<StartDate>02/23/2006</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Washington University</Name>
<CityName>Saint Louis</CityName>
<ZipCode>631304862</ZipCode>
<PhoneNumber>3147474134</PhoneNumber>
<StreetAddress>CAMPUS BOX 1054</StreetAddress>
<StreetAddress2><![CDATA[1 Brookings Drive]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<StateCode>MO</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MO01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>068552207</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068552207</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Washington University]]></Name>
<CityName>Saint Louis</CityName>
<StateCode>MO</StateCode>
<ZipCode>631304862</ZipCode>
<StreetAddress><![CDATA[CAMPUS BOX 1054]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MO01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0106</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2006~70845</FUND_OBLG>
<FUND_OBLG>2007~70103</FUND_OBLG>
<FUND_OBLG>2008~101178</FUND_OBLG>
<FUND_OBLG>2009~126539</FUND_OBLG>
<FUND_OBLG>2010~147335</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project explored ways to understand long-term image data captured of the same scene. &nbsp;Many cameras do not move, and continually watch a specific scene - an airport security desk, a beach, a volcano.&nbsp; In either case, the fact that all the images are from the same scene makes it easier to answer questions about what happening in each image.</p> <p>One basic question is: "where is the camera?" There are many live webcams broadcasting online from unknown locations; these cameras can be geo-located because the lighting and weather changes they observe depends on where the camera is. We&nbsp;offered one of the first approaches to geo-locate a time-series of images. The algorithm uses the fact that outdoor scenes have changes that are very consistent, and we extended this to&nbsp;also geo-calibrate (i.e. find the orientation and the zoom level) of cameras.</p> <p>Another question is "what is in the scene?" &nbsp;Others have tried to recognize objects by their appearance in one image, but we have explored what can be learned by measuring the time scale over which images change. Long term time-lapses gives an approach to automatically labeling scene locations (like trees) that vary over annual time scales, locations (like eastward facing walls) that are consistently brighter in the morning, or segmenting objects in a scene based on very small motions.</p> <p>At shorter time scales, we demonstrate the ability to build a 3D model of a scene from a time-lapse of clouds passing overhead, and the ability&nbsp;to automatically key in on important events, even when the background of the scene includes complex motions like waves crashing on a beach or traffic following a typical traffic pattern. &nbsp;&nbsp;</p> <p>To support our research, and the larger community, we have built and actively share a dataset called the Archive of Many Outdoor Scenes (AMOS). This archives and organizes available webcam imagery&nbsp;by discovering publicly available webcam feeds, indexing what was in each scene, geo-locating the imagery so we know where in the world it was from, and calibrating these cameras so that image measurements can be related to real world quantities.&nbsp;</p> <p>Collectively this research helps address national needs in homeland security and the need to understand long term changes in the environment.</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/07/2012<br>      Modified by: Robert&nbsp;Pless</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project explored ways to understand long-term image data captured of the same scene.  Many cameras do not move, and continually watch a specific scene - an airport security desk, a beach, a volcano.  In either case, the fact that all the images are from the same scene makes it easier to answer questions about what happening in each image.  One basic question is: "where is the camera?" There are many live webcams broadcasting online from unknown locations; these cameras can be geo-located because the lighting and weather changes they observe depends on where the camera is. We offered one of the first approaches to geo-locate a time-series of images. The algorithm uses the fact that outdoor scenes have changes that are very consistent, and we extended this to also geo-calibrate (i.e. find the orientation and the zoom level) of cameras.  Another question is "what is in the scene?"  Others have tried to recognize objects by their appearance in one image, but we have explored what can be learned by measuring the time scale over which images change. Long term time-lapses gives an approach to automatically labeling scene locations (like trees) that vary over annual time scales, locations (like eastward facing walls) that are consistently brighter in the morning, or segmenting objects in a scene based on very small motions.  At shorter time scales, we demonstrate the ability to build a 3D model of a scene from a time-lapse of clouds passing overhead, and the ability to automatically key in on important events, even when the background of the scene includes complex motions like waves crashing on a beach or traffic following a typical traffic pattern.     To support our research, and the larger community, we have built and actively share a dataset called the Archive of Many Outdoor Scenes (AMOS). This archives and organizes available webcam imagery by discovering publicly available webcam feeds, indexing what was in each scene, geo-locating the imagery so we know where in the world it was from, and calibrating these cameras so that image measurements can be related to real world quantities.   Collectively this research helps address national needs in homeland security and the need to understand long term changes in the environment.          Last Modified: 06/07/2012       Submitted by: Robert Pless]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
