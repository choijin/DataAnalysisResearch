<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER:  Foundations for Ubiquitous Image-Based Appearance Capture</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2006</AwardEffectiveDate>
<AwardExpirationDate>01/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>457372.00</AwardTotalIntnAmount>
<AwardAmount>457372</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>CAREER: Foundations for Ubiquitous Image-based Appearance Capture&lt;br/&gt;PI: Todd Zickler&lt;br/&gt;&lt;br/&gt;Abstract:&lt;br/&gt;For many applications, the nature of visual content is rapidly changing. Two-dimensional images are being replaced by higher-dimensional appearance models that summarize the (very large)set of images of an object for many viewpoints and lighting conditions. These models enable an object to be virtually rotated, re-lit and composited into other scenes, and they improve our ability to automate visual tasks such as detection, recognition, and tracking. Yet, despite the growing number of applications, suitable appearance models of common objects are remarkably hard to find. This is due largely to the complexity of appearance, which makes the task of appearance capture quite formidable.  The goal of the proposed research is to develop a framework for appearance capture that can be applied to any opaque and non-refracting surface, and that is simultaneously accurate and practical.  Ultimately, we seek to enable "ubiquitous appearance capture": the widespread ability to acquire appearance models outside of the laboratory - in homes, offices, museums, hospitals and in the field. To achieve this goal, we take a physics-based approach that seeks to exploit common reflectance properties (e.g., Helmholtz reciprocity, reflectance separability and compressibility) that we believe have yet to be fully utilized. The benefits of this approach are two-fold.  First, it enables image-based acquisition of both shape and reflectance from a small number of uncalibrated cameras and light-sources, eliminating the need for laser range-scanners, projector-based structured lighting, or other specialized equipment. Second, it makes possible the modeling of a broad class of surfaces, including those with complex reflectance not necessarily well-represented by low-dimensional (i.e., parametric)models. This second property is essential for modeling real-world surfaces, and is very different from most existing image-based&lt;br/&gt;approaches that are predicated on restrictive assumptions about the nature of surface reflectance. This research activity is closely linked to an educational program that includes the development of courses in human and computer vision at both the undergraduate and graduate levels, and the creation of undergraduate and graduate research opportunities. The educational program extends beyond the university by making a mobile acquisition system available as a teaching tool in museums and classrooms in the Boston area, and by making appearance models and software available through the Internet.&lt;br/&gt;&lt;br/&gt;URL: http://www.eecs.harvard.edu/~zickler/research.html&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>01/06/2006</MinAmdLetterDate>
<MaxAmdLetterDate>02/26/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0546408</AwardID>
<Investigator>
<FirstName>Todd</FirstName>
<LastName>Zickler</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Todd Zickler</PI_FULL_NAME>
<EmailAddress>zickler@eecs.harvard.edu</EmailAddress>
<PI_PHON>6174954390</PI_PHON>
<NSF_ID>000118883</NSF_ID>
<StartDate>01/06/2006</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard University]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021385369</ZipCode>
<StreetAddress><![CDATA[1033 MASSACHUSETTS AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0106</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2006~81326</FUND_OBLG>
<FUND_OBLG>2007~87587</FUND_OBLG>
<FUND_OBLG>2008~101345</FUND_OBLG>
<FUND_OBLG>2009~92183</FUND_OBLG>
<FUND_OBLG>2010~94931</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>We developed efficient acquisition strategies, based on mathematical and computational foundations, for measuring three-dimensional shape and material information from digital photographs. In addition to providing the statistical and structural information about the visual world that is required for the success of future computer vision systems, these strategies are useful for digitally "capturing appearance" to enhance physical realism in computer graphics applications, including virtual and augmented reality.</p> <p>Our contributions can be divided into two categories:</p> <p>1. Active shape and reflectance capture. In many cases, we have the ability to manipulate scene lighting while capturing images, and this provides additional constraints that can be used to recover shape and material information. For accurate results, reflectance must be obtained along with shape from a single set of images. We have shown that certain imaging configurations allow one to decouple shape and reflectance information in image data, so that each can be inferred without making assumptions about the other. This includes methods that exploit isotropy and reciprocity (Zickler, 2006; Tan et al., 2011), tangent plane symmetries (Holroyd et al., 2008), and reciprocal images with structured lighting (Holroyd et al., 2010). Separately, we developed methods for inferring reflectance and shape at the same time, while relaxing the restrictions on both as much as possible (Alldrin et al., 2008; Sunkavalli et al., 2010). A general overview of the current state-of-the-art can be found in our tutorial and survey (Weyrich et al., 2008).</p> <p>2. Passive shape and reflectance capture. A separate and complimentary approach is to learn about the world's shape, materials, and lighting by exploiting passive observations of objects and scenes under naturally-occurring illumination. Among other things, this would enable the automatic learning of shape and material information from the billions of images and videos being shared online. Along these lines, we have developed mathematical and computational tools for inferring an object's intrinsic color (Chong et al., 2007; Sunkavalli et al., 2008; Owens et al., 2011), inferring glossiness and other reflectance properties (Mallick et al., 2006; Romeiro et al., 2008; Romeiro and Zickler, 2010), and recovering shape information from images of curved mirrors (Adato et al., 2007).</p><br> <p>            Last Modified: 03/18/2012<br>      Modified by: Todd&nbsp;Zickler</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ We developed efficient acquisition strategies, based on mathematical and computational foundations, for measuring three-dimensional shape and material information from digital photographs. In addition to providing the statistical and structural information about the visual world that is required for the success of future computer vision systems, these strategies are useful for digitally "capturing appearance" to enhance physical realism in computer graphics applications, including virtual and augmented reality.  Our contributions can be divided into two categories:  1. Active shape and reflectance capture. In many cases, we have the ability to manipulate scene lighting while capturing images, and this provides additional constraints that can be used to recover shape and material information. For accurate results, reflectance must be obtained along with shape from a single set of images. We have shown that certain imaging configurations allow one to decouple shape and reflectance information in image data, so that each can be inferred without making assumptions about the other. This includes methods that exploit isotropy and reciprocity (Zickler, 2006; Tan et al., 2011), tangent plane symmetries (Holroyd et al., 2008), and reciprocal images with structured lighting (Holroyd et al., 2010). Separately, we developed methods for inferring reflectance and shape at the same time, while relaxing the restrictions on both as much as possible (Alldrin et al., 2008; Sunkavalli et al., 2010). A general overview of the current state-of-the-art can be found in our tutorial and survey (Weyrich et al., 2008).  2. Passive shape and reflectance capture. A separate and complimentary approach is to learn about the world's shape, materials, and lighting by exploiting passive observations of objects and scenes under naturally-occurring illumination. Among other things, this would enable the automatic learning of shape and material information from the billions of images and videos being shared online. Along these lines, we have developed mathematical and computational tools for inferring an object's intrinsic color (Chong et al., 2007; Sunkavalli et al., 2008; Owens et al., 2011), inferring glossiness and other reflectance properties (Mallick et al., 2006; Romeiro et al., 2008; Romeiro and Zickler, 2010), and recovering shape information from images of curved mirrors (Adato et al., 2007).       Last Modified: 03/18/2012       Submitted by: Todd Zickler]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
