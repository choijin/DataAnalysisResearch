<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Modeling 3D Geologic Structures in a Multi-sensory (Touch and Sound) Virtual Environment</AwardTitle>
<AwardEffectiveDate>11/15/2005</AwardEffectiveDate>
<AwardExpirationDate>10/31/2010</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>338900</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
</ProgramOfficer>
<AbstractNarration>For many practical geoscientific tasks, such as natural resource exploration, geohazard prevention or aquifer-characterization, geologists need detailed information about the underlying structure (3D shape) of subsurface rock formations in the form of a computer 3D Earth model.  Advances in 3D computer graphics already allow geologists to effectively visualize geologic models in 3D stereo systems.  However, the actual creation of such models, by interpreting the 3D geologic raw data spatially, is still severely limited by the lack of direct 3D interaction with 3D geologic data.  This lack of interaction negatively impacts the quality of the Earth models and has, so far, limited their widespread use.  The hardware costs for multi-model interfaces (specifically touch and sound) have dropped considerably in the past and could become mainstream technology in the next years.  The project will investigate if virtual environments that use vision, touch and sound are potentially more effective for understanding and modeling 3D geologic shapes than the traditional, visual-only, systems.  The focus of the project is to design and implement a variety of different interaction methods for deforming and cutting triangle meshes representing geologic surfaces (rocks and faults), using a combination of 3D stereo vision, force-feedback (haptic) technology and interactive audio streams (sonification).  A thorough user evaluation of these interaction methods will test how well geologists are able to interactively correct spatial errors in the shapes of geologic surfaces i.e., overlaps or gaps with other surfaces.  The project will result in a novel multi-modal geologic virtual reality system and will lay the foundation for the next generation of mainstream computer systems for modeling 3D geologic structures.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  The ability to easily create and maintain 3D Earth models is fundamental to many areas of applied geology which in turn impact large parts of our society.  The result of this project will be knowledge about the use of multi-modal (sensory) interaction methods for shaping geologic surfaces and will aid their integration into future mainstream 3D geologic modeling systems. Providing the everyday geologist with more efficient interaction methods will lead to significant improvements in the creation of 3D computer models with regards to speed and quality.  Many geoscientific tasks are critically dependent on access to the best possible, qualitative information about geologic structure and so stand to benefit from the potential improvements resulting of this project, such as: exploration and exploitation of natural resources (e.g., minerals or petroleum), characterization of rock units for hydrogeologic tasks (such as groundwater flow modeling) and geotechnical applications (such as slope failure prediction).  This multi-modal VR system will be implemented as an open source project and will be made available via the web to any geoscientists interested in exploring its potential benefits for research or classroom teaching.</AbstractNarration>
<MinAmdLetterDate>11/04/2005</MinAmdLetterDate>
<MaxAmdLetterDate>03/04/2008</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0534681</AwardID>
<Investigator>
<FirstName>Carl</FirstName>
<LastName>Jacobson</LastName>
<EmailAddress>cejac@iastate.edu</EmailAddress>
<StartDate>11/04/2005</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Carolina</FirstName>
<LastName>Cruz-Neira</LastName>
<EmailAddress>cxcruz@ualr.edu</EmailAddress>
<StartDate>11/04/2005</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Chris</FirstName>
<LastName>Harding</LastName>
<EmailAddress>charding@iastate.edu</EmailAddress>
<StartDate>11/04/2005</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Iowa State University</Name>
<CityName>AMES</CityName>
<ZipCode>500112207</ZipCode>
<PhoneNumber>5152945225</PhoneNumber>
<StreetAddress>1138 Pearson</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<StateCode>IA</StateCode>
</Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>6845</Code>
<Text>HUMAN COMPUTER INTER PROGRAM</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramElement>
<Code>7496</Code>
<Text>COLLABORATIVE SYSTEMS</Text>
</ProgramElement>
<ProgramReference>
<Code>7496</Code>
<Text>COLLABORATIVE SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
