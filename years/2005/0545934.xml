<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER:   Motion Capabilities for Autonomous Mobile Manipulation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/15/2006</AwardEffectiveDate>
<AwardExpirationDate>12/31/2011</AwardExpirationDate>
<AwardTotalIntnAmount>480000.00</AwardTotalIntnAmount>
<AwardAmount>504000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>jeffrey trinkle</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>CAREER: Motion Capabilities for Autonomous Mobile Manipulation&lt;br/&gt;&lt;br/&gt;Abstract&lt;br/&gt;Concerted efforts in industry and academia have lead to the design and construction of highly advanced humanoid robots and mobile manipulators.  These mechanisms combine dexterous manipulation capabilities with mobility and are equipped with multi-modal sensor suites.  Autonomous motion of these complex mechanisms is subject to a multitude of constraints, each requiring the consideration of feedback at a specific frequency.  The investigator proposes to devise a novel motion generation generation approach that addresses motion constraints and their feedback constraints.  At the low end of the frequency spectrum, the investigator proposes the application of techniques from active learning and utility theory to global motion planning.  These techniques permit the exploitation of structure inherent in any real-world motion-planning problem to increase the efficiency of motion planning.  The resulting planning methods will satisfy the feedback requirements of autonomous mobile manipulation.  To obtain adequate feedback about the state of the environment, perceptual information is provided by novel sensing and modeling techniques, which will also be developed as part of this effort.  To address motion constraints that require high-frequency feedback, the investigator proposes to combine multi-objective control methods with planning approaches for constraint satisfaction.  These planning methods overcome the susceptibility of control-based motion generation to local minima.  The resulting overall approach to motion generation addresses the motion requirements of autonomous mobile manipulation, offering a significant advance towards the autonomous operation of humanoid robots and mobile manipulators in unstructured environments.&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>01/06/2006</MinAmdLetterDate>
<MaxAmdLetterDate>03/09/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0545934</AwardID>
<Investigator>
<FirstName>Oliver</FirstName>
<LastName>Brock</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Oliver Brock</PI_FULL_NAME>
<EmailAddress>oli@cs.umass.edu</EmailAddress>
<PI_PHON>4135450698</PI_PHON>
<NSF_ID>000488719</NSF_ID>
<StartDate>01/06/2006</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName>Hadley</CityName>
<StateCode>MA</StateCode>
<ZipCode>010359450</ZipCode>
<StreetAddress><![CDATA[Research Administration Building]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0106</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2006~103114</FUND_OBLG>
<FUND_OBLG>2007~109492</FUND_OBLG>
<FUND_OBLG>2008~113870</FUND_OBLG>
<FUND_OBLG>2009~112250</FUND_OBLG>
<FUND_OBLG>2010~65274</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As robots are moving from factory floors to everyday environments shared with humans, many novel skills are required to make these robots move about in a safe, competent, and robust manner.&nbsp; The project has developed a general approach to generate the motion of robots in these everyday enviornments.&nbsp; While moving, the robots can avoid obstances that move in an unpredictable manner; this makes the motion safe.&nbsp; At the same time, the approach esures that the robot reaches its goal location.&nbsp; During this motion, the robot can perform a task with its hand or end-effector, such as carrying a glass while keeping it upright, shining a flashlight at a pipe to be inspected, or while wiping a surface with a cloth.&nbsp; The motion can also be influenced so as to be most economical (with respect to power consumption), to look most natural, and to respect mechanical limits of the robot.&nbsp; The approach thus integrates all aspects of robot motion into a single framework.</p> <p>In a separate line of work, work in this project led to the development of a new view of the combined manipulation and perception problem, called interactive perception (IP).&nbsp; IP is an approach to perception and manipulation that does not distinguish between these two components.&nbsp; Manipulation is performed in service of achieving a manipulation goal, of course. At the same time, to achieve that manipulation goal, certain aspects of the environment must be perceived.&nbsp; So perception must be performed.&nbsp; Perception in turn can be facilitated by manipulation (closing the loop between manipulation and perception, and establishing a second purpose for the manipulation action), as manipulation is able to reveal sensor signals that could otherwise not be perceived.&nbsp; This &ldquo;dissolution&rdquo; of the boundary between manipulation and perception leads foremost to a different view of the problem.&nbsp; This view enables the discovery of solutions.&nbsp; These solutions ideally exploit characteristics of the problem that lie in the combined manipulation/perception space, i.e., in a space that was not considered when one regarded IP and manipulation as separate problems.</p><br> <p>            Last Modified: 08/02/2015<br>      Modified by: Oliver&nbsp;Brock</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2015/0545934/0545934_10021563_1438546883277_er--rgov-214x142.jpg" original="/por/images/Reports/POR/2015/0545934/0545934_10021563_1438546883277_er--rgov-800width.jpg" title="Motion execution for mobile manipulation"><img src="/por/images/Reports/POR/2015/0545934/0545934_10021563_1438546883277_er--rgov-66x44.jpg" alt="Motion execution for mobile manipulation"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The robot performs a task: it keeps the end-effector on a line given by a laser beam, shown in the top right of the images. The robot also avoids stationary and moving obstacles (orange cylinders) and reaches a globally specified goal position.</div> <div class="imageCredit">Springer - Autonomous Robots</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Oliver&nbsp;Brock</div> <div class="imageTitle">Motion execution for mobile manipulation</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As robots are moving from factory floors to everyday environments shared with humans, many novel skills are required to make these robots move about in a safe, competent, and robust manner.  The project has developed a general approach to generate the motion of robots in these everyday enviornments.  While moving, the robots can avoid obstances that move in an unpredictable manner; this makes the motion safe.  At the same time, the approach esures that the robot reaches its goal location.  During this motion, the robot can perform a task with its hand or end-effector, such as carrying a glass while keeping it upright, shining a flashlight at a pipe to be inspected, or while wiping a surface with a cloth.  The motion can also be influenced so as to be most economical (with respect to power consumption), to look most natural, and to respect mechanical limits of the robot.  The approach thus integrates all aspects of robot motion into a single framework.  In a separate line of work, work in this project led to the development of a new view of the combined manipulation and perception problem, called interactive perception (IP).  IP is an approach to perception and manipulation that does not distinguish between these two components.  Manipulation is performed in service of achieving a manipulation goal, of course. At the same time, to achieve that manipulation goal, certain aspects of the environment must be perceived.  So perception must be performed.  Perception in turn can be facilitated by manipulation (closing the loop between manipulation and perception, and establishing a second purpose for the manipulation action), as manipulation is able to reveal sensor signals that could otherwise not be perceived.  This "dissolution" of the boundary between manipulation and perception leads foremost to a different view of the problem.  This view enables the discovery of solutions.  These solutions ideally exploit characteristics of the problem that lie in the combined manipulation/perception space, i.e., in a space that was not considered when one regarded IP and manipulation as separate problems.       Last Modified: 08/02/2015       Submitted by: Oliver Brock]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
