<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Interpreting Human Behaviour in Video using FSA's and Object Context</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2006</AwardEffectiveDate>
<AwardExpirationDate>02/28/2010</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>PI: David Forsyth&lt;br/&gt;Title: Interpreting Human Behavior in Video using FSA's and Object Context&lt;br/&gt;&lt;br/&gt;Understanding what people are doing in a video is one of the great unsolved problems of computer vision.  A fair solution opens tremendous application possibilities.  The proposed work will use existing tools from the speech and object recognition community --- in particular, finite state automata or FSA's --- to obtain an understanding of activities that depend on detailed information about the body.&lt;br/&gt;&lt;br/&gt;The particular focus is everyday activity.  In this case, a fixed vocabulary either doesn't exist, or isn't appropriate.  For example,  one does not know words for behaviors that appear familiar. One way to deal with this is to work  with a notation (for example, laban notation); but such notations typically work in terms that are  difficult to map to visual observables (for example, the weight of a motion). The alternatives are either to develop a vocabulary, or to develop expressive tools for authoring models.&lt;br/&gt;&lt;br/&gt;This project will explore the third approach of building tools for authoring models of behavior quickly and expressively using finite-state methods. Research will explore a class of models that are easy to author from existing, or easily available, data.  The interpretation of what someone is doing is affected by the objects&lt;br/&gt;nearby  --- a person standing near a bus stop is doing something different from a person standing near an office door.  The models studied make it practical to investigate this phenomenon of object context, using recent advances from the object recognition literature.&lt;br/&gt;&lt;br/&gt;Evaluating models for everyday behaviors is hard, because there is no prospect of obtaining a large collection of marked up video (among  other things, there isn't a vocabulary in which to mark it up).  This project will use proxies --- statistics that are hard to measure from video without accurate inferences of behavior,  but easy to measure in other ways --- to evaluate behavior representations.  These will make it possible to tell whether, for example, a model of buying a beverage represents the concept accurately.&lt;br/&gt;&lt;br/&gt;Intellectual merits:  This project will produce very large finite state models of behavior using the same hierarchical authoring methods used in speech.  There will be a particular emphasis on behaviors which require one to understand the kinematic configuration of the body, a topic that  has been very difficult to study to date, with an  intention of identifying basic building blocks of a vocabulary of everyday behavior.  The results should include datasets of public behavior that can be disseminated, without encountering privacy concerns. New insights into the structure of human motion and behavior should emerge from (a) observations of people in public; (b) the process of  authoring models; and (c) methods for identifying and modelling compositional structure in motion.&lt;br/&gt;&lt;br/&gt;Broader impact:   This project should make substantial progress on one of the key open and applicable problems in computer vision.  Methods that can search video for particular behaviors and compute statistics of behaviors have a wide range of applications, including human-computer interfaces built around computers that can watch the body; an improved understanding of what people do in public which will result in better architectural planning; more efficient management of surveillance data, allowing searches for dangerous behaviors while preserving privacy.  Education and access: This project will contribute to the graduate training of several students, and work described will contribute to a planned text on computing with human motion.&lt;br/&gt;&lt;br/&gt;URL: http://luthuli.cs.uiuc.edu/~daf/action.html&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>03/07/2006</MinAmdLetterDate>
<MaxAmdLetterDate>03/10/2009</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0534837</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Forsyth</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David A Forsyth</PI_FULL_NAME>
<EmailAddress>daf@cs.uiuc.edu</EmailAddress>
<PI_PHON>2172656851</PI_PHON>
<NSF_ID>000391155</NSF_ID>
<StartDate>03/07/2006</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<CountyName>CHAMPAIGN</CountyName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Champaign</CityName>
<CountyName>CHAMPAIGN</CountyName>
<StateCode>IL</StateCode>
<ZipCode>618207406</ZipCode>
<StreetAddress><![CDATA[1901 South First Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7339</Code>
<Text>COMPUTER VISION</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0106</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2006~96812</FUND_OBLG>
<FUND_OBLG>2007~100164</FUND_OBLG>
<FUND_OBLG>2008~103024</FUND_OBLG>
</Award>
</rootTag>
