<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Multi-Channel Information Processing for Intelligent Collaboration</AwardTitle>
<AwardEffectiveDate>01/01/2006</AwardEffectiveDate>
<AwardExpirationDate>12/31/2009</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>600000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
</ProgramOfficer>
<AbstractNarration>In collaborative sessions, information presented or generated is usually unstructured so that, for example, a traditional tape recording of a meeting does not present a clear picture of who said what and when, thereby limiting the usability of such information in contexts other than a simple replay.  Furthermore, in face-to-face interactions participants make full use of human sensory capabilities such as binaural hearing and binocular (stereo) vision to conduct information sharing and exchange.  To enable intelligent reuse of collaborative session information at a future time, or to support real time collaborations involving parties located remotely, we need to be able to acquire, process, integrate and organize speech, audio and multimedia information so as to provide a sense of full-dimensional immersive interaction.  Among other things, this implies that individual sources must be accessible for life-like playback free of interference from other information streams.  In this project the PIs will develop technologies to these end, focusing in particular on achieving spatialized audio with little or no interference due to reverberation or noise, individual tracking of information sources, and rich and accurate annotation that supports efficient storage and retrieval of the information in a session.  The PIs plan to meet these challenges through use of multi-channel acoustic signal processing, which involves multi-channel signal acquisition, processing, representation and synthesis.  Their approach to multi-channel signal acquisition will exploit the ubiquity of individual access (and transduction) devices such as the cell phones and PDAs that most people carry nowadays.  During a meeting, these devices can be fitted with necessary interfaces to form a distributed information acquisition network such that the signal generated in the meeting can be captured and transmitted to a wireless base stations for processing.  The project will result in the following key technical innovations: a fundamental formulation of acoustic signal processing as a multi-input-multi-output (MIMO) problem; a practical solution to "sound objectization" that takes ubiquitous acoustic signals as input and produces as output an array of signals with known source identities; effective organization of the multi-channel acoustic signals in a collaborative session for efficient retrieval; and modeling and mitigation of non-linearity due to speech and audio coding in distributed acoustic signal processing.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  This research will lead to a modern multi-channel signal processing theory for acoustic signal processing, along with  a new generation of technology for remote collaboration and conferencing that allows participants to conduct full-dimensional information sharing, as well as a new generation of technology for collaborative session recording and playback that revolutionize the meeting room facility for efficient retrieval of precise information.  Because collaborative applications of all kinds lie at the heart of the emerging information society, this work will ultimately lead to substantial enhancements in productivity.</AbstractNarration>
<MinAmdLetterDate>11/22/2005</MinAmdLetterDate>
<MaxAmdLetterDate>08/13/2007</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0534221</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>West</LastName>
<EmailAddress>jimwest@jhu.edu</EmailAddress>
<StartDate>11/22/2005</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Biing-Hwang</FirstName>
<LastName>Juang</LastName>
<EmailAddress>juang@ece.gatech.edu</EmailAddress>
<StartDate>11/22/2005</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
</Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramElement>
<Code>7496</Code>
<Text>COLLABORATIVE SYSTEMS</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>7496</Code>
<Text>COLLABORATIVE SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
