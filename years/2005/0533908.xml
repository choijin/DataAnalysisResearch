<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Sensory Integration of Multimodal Human Computer Interfaces</AwardTitle>
<AwardEffectiveDate>11/01/2005</AwardEffectiveDate>
<AwardExpirationDate>10/31/2009</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>481052</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Although there appear to be many clear benefits to using multimodal human-machine interfaces in terms of increased information transfer, there are also potential performance costs that may result from inter-sensory biases, degraded processing associated with attending to more than one sensory modality, and inhibitory interactions in crossmodal reorienting.  The creation of effective and intuitive multimodal interfaces hinges on the ability of the designer to recreate the natural integration between sensory modalities that occurs in the real world (e.g., when we look towards a voice in a crowd).  Unfortunately for the interface designer, the conditions that distinguish between "good" and "bad" inter-sensory interactions have not been clearly elucidated.  There remain many basic research questions in the realm of sensory integration that must be answered to optimize multimodal human-machine interfaces.  In this project the PIs will conduct a series of basic psychophysical experiments to investigate several crucial outstanding issues in the development of multimodal human-computer interfaces.  First, the spatial and temporal parameters needed to achieve faster response times will be identified through a systematic parametric investigation of crossmodal interactions using visual, auditory and tactile signals.  Second, because for many human-machine interfaces it is necessary to present signals from distinct areas of the display, the PIs will explore the crossmodal mapping between proximal and distal surfaces by extending their pilot research which has demonstrated that tactile cues presented to a user's back can be used to speed responses to visual targets presented on a monitor in front of the user.  Knowledge gained from the aforementioned experiments will be applied to the development of a novel haptic directional display for automobile driver assistance, whose effectiveness will be tested in a series of driving simulator experiments.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  This project will significantly advance our understanding of human multi-sensory integration, and will provide a framework that can be used to guide both future research and the engineering of multimodal human-machine interfaces.  The haptic driver assistance display to be developed will help transform automobile information technology, reducing the serious problem of driver distraction.</AbstractNarration>
<MinAmdLetterDate>10/25/2005</MinAmdLetterDate>
<MaxAmdLetterDate>08/21/2007</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0533908</AwardID>
<Investigator>
<FirstName>Hong</FirstName>
<LastName>Tan</LastName>
<EmailAddress>hongtan@purdue.edu</EmailAddress>
<StartDate>10/25/2005</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Rob</FirstName>
<LastName>Gray</LastName>
<EmailAddress>robgray@asu.edu</EmailAddress>
<StartDate>10/25/2005</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Arizona State University</Name>
<CityName>TEMPE</CityName>
<ZipCode>852816011</ZipCode>
<PhoneNumber>4809655479</PhoneNumber>
<StreetAddress>ORSPA</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
</Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramElement>
<Code>7496</Code>
<Text>COLLABORATIVE SYSTEMS</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>7496</Code>
<Text>COLLABORATIVE SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
