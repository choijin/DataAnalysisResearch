<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Memory-Constrained Predictive Data Mining</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/15/2006</AwardEffectiveDate>
<AwardExpirationDate>06/30/2012</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>478448</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>In the environment where new large-scale problems are emerging in various disciplines and pervasive computing applications are becoming more common, there is an urgent need for techniques that provide efficient and accurate knowledge discovery by limited-capacity computing devices. The objective of this project is to address this need by developing memory-constrained predictive data mining algorithms that operate when data size exceeds the available memory capacity. The approach is based on the integration of data mining and data compression techniques to optimally utilize the memory for data and model storage, learning, and ancillary operations. The methodology will be thoroughly evaluated on a range of real-life problems that includes learning from large sorted databases, biased data and nonstationary data. Various memory constraints will be considered pertaining to devices ranging from powerful workstations to handheld computers and cell phones to small, inexpensive sensors. This research will reveal the memory lower bounds for accurate learning from different types of data and by different types of learning algorithms. The educational component of the project seeks to integrate research into computer science instruction by designing exciting courses, exploring effective teaching techniques, introducing research to undergraduate and graduate students, and involving underrepresented student groups in research. Broader impacts of the project will be in extending the frontiers of computer and information science and in facilitating knowledge discovery in various scientific, engineering, and business disciplines. Teaching materials and research results, including developed software and databases, will be widely disseminated via Internet (http://www.ist.temple.edu/~vucetic/CAREER.htm) to promote learning and enhance scientific understanding. &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>01/25/2006</MinAmdLetterDate>
<MaxAmdLetterDate>06/10/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0546155</AwardID>
<Investigator>
<FirstName>Slobodan</FirstName>
<LastName>Vucetic</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Slobodan Vucetic</PI_FULL_NAME>
<EmailAddress>vucetic@temple.edu</EmailAddress>
<PI_PHON>2152045773</PI_PHON>
<NSF_ID>000427104</NSF_ID>
<StartDate>01/25/2006</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Temple University</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191226003</ZipCode>
<PhoneNumber>2157077547</PhoneNumber>
<StreetAddress>1801 N. Broad Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>057123192</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEMPLE UNIVERSITY-OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>057123192</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Temple University]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191226003</ZipCode>
<StreetAddress><![CDATA[1801 N. Broad Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0104000</Code>
<Name>Information Systems</Name>
</FoaInformation>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramElement>
<Code>7496</Code>
<Text>COLLABORATIVE SYSTEMS</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>6855</Code>
<Text>INFORMATION &amp; KNOWLEDGE MANAGE</Text>
</ProgramReference>
<ProgramReference>
<Code>7218</Code>
<Text>RET SUPP-Res Exp for Tchr Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0106</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2006~90000</FUND_OBLG>
<FUND_OBLG>2007~90000</FUND_OBLG>
<FUND_OBLG>2008~90000</FUND_OBLG>
<FUND_OBLG>2009~106000</FUND_OBLG>
<FUND_OBLG>2010~90000</FUND_OBLG>
<FUND_OBLG>2011~12448</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>CAREER: Memory-Constrained Predictive Data Mining</strong><strong>&nbsp;</strong></p> <p>In the environment where new large-scale problems are emerging in various disciplines and pervasive computing applications are becoming more common, there is an increasing need for techniques that provide efficient and accurate knowledge discovery by limited-capacity computing devices. The objective of this project was to address this need by developing novel learning algorithms that operate when data size exceeds the available memory. The activities were also aimed at revealing the memory lower bounds for accurate learning from different types of data and by different types of learning algorithms. The overall approach was based on the integration of novel machine learning and data compression techniques to optimally utilize the memory for data and model storage, learning, and ancillary operations.</p> <p>&nbsp;</p> <p>&nbsp;&nbsp;&nbsp; This project covered a range of topics and it resulted in several novel memory-constrained algorithms. Most of the learning algorithms we studied were prototype-based, where the resulting prediction model can be represented as a subset of training data points with the associated weights. The most popular examples of such algorithms are learning vector quantization, kernel perceptrons, and support vector machines. Kernel perceptrons and support vector machines, in particular, are among the most powerful known learning algorithms able to efficiently and accurately learn very complex nonlinear classification problems. However, they are also known to suffer from the curse of kernelization, which causes unbounded growth in model size with training data size and makes them unsuitable for learning from very large data sets or from unlimited data streams. To address this issue, we proposed several budgeted prototype-based algorithms that bound model size to a constant number of prototypes, regardless of training data size. In addition to being memory-constrained, the resulting algorithms are typically very fast, with time cost growing only linearly with data size. The combination of constrained model size and linear time cost in principle allows <span style="text-decoration: underline;">learning from arbitrarily large data sets using algorithms implemented on resource-constrained computational devices</span>. By combining data compression techniques and our algorithms, we were able to empirically explore the lower bounds on memory resources needed for accurate learning. In particular, we implemented our budgeted kernel perceptron on ATTiny2313 microcontroller, one of the most primitive computational devices with only 128 bytes of data memory and 2 kilobytes of program memory. The algorithm was able to process thousands of training data points per second and the resulting classifier was comparable in accuracy to classifiers trained on the most powerful computers. This result reveals that powerful online learning algorithms could be implemented even on the most resource-constrained computational devices. Considering that modern data sets often exceed memory of even the most powerful computers by orders of magnitude, our result is also relevant for learning from huge data sets, which is one of the grand challenges of contemporary data mining.</p> <p>&nbsp;</p> <p>&nbsp;&nbsp;&nbsp; Activities for integration of research and education involved supervising 6 undergraduate independent studies where students have been involved in data collection, data processing, and implementation of memory-unconstrained data mining algorithms. Two of them got prizes on regional research competitions for undergraduate students. Seven graduate students supported by the project gained valuable research experience and presented their work at numerous computer science conferences.&nbsp;The PI has been actively involved in an effort to introduce data mining to science and...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ CAREER: Memory-Constrained Predictive Data Mining   In the environment where new large-scale problems are emerging in various disciplines and pervasive computing applications are becoming more common, there is an increasing need for techniques that provide efficient and accurate knowledge discovery by limited-capacity computing devices. The objective of this project was to address this need by developing novel learning algorithms that operate when data size exceeds the available memory. The activities were also aimed at revealing the memory lower bounds for accurate learning from different types of data and by different types of learning algorithms. The overall approach was based on the integration of novel machine learning and data compression techniques to optimally utilize the memory for data and model storage, learning, and ancillary operations.         This project covered a range of topics and it resulted in several novel memory-constrained algorithms. Most of the learning algorithms we studied were prototype-based, where the resulting prediction model can be represented as a subset of training data points with the associated weights. The most popular examples of such algorithms are learning vector quantization, kernel perceptrons, and support vector machines. Kernel perceptrons and support vector machines, in particular, are among the most powerful known learning algorithms able to efficiently and accurately learn very complex nonlinear classification problems. However, they are also known to suffer from the curse of kernelization, which causes unbounded growth in model size with training data size and makes them unsuitable for learning from very large data sets or from unlimited data streams. To address this issue, we proposed several budgeted prototype-based algorithms that bound model size to a constant number of prototypes, regardless of training data size. In addition to being memory-constrained, the resulting algorithms are typically very fast, with time cost growing only linearly with data size. The combination of constrained model size and linear time cost in principle allows learning from arbitrarily large data sets using algorithms implemented on resource-constrained computational devices. By combining data compression techniques and our algorithms, we were able to empirically explore the lower bounds on memory resources needed for accurate learning. In particular, we implemented our budgeted kernel perceptron on ATTiny2313 microcontroller, one of the most primitive computational devices with only 128 bytes of data memory and 2 kilobytes of program memory. The algorithm was able to process thousands of training data points per second and the resulting classifier was comparable in accuracy to classifiers trained on the most powerful computers. This result reveals that powerful online learning algorithms could be implemented even on the most resource-constrained computational devices. Considering that modern data sets often exceed memory of even the most powerful computers by orders of magnitude, our result is also relevant for learning from huge data sets, which is one of the grand challenges of contemporary data mining.         Activities for integration of research and education involved supervising 6 undergraduate independent studies where students have been involved in data collection, data processing, and implementation of memory-unconstrained data mining algorithms. Two of them got prizes on regional research competitions for undergraduate students. Seven graduate students supported by the project gained valuable research experience and presented their work at numerous computer science conferences. The PI has been actively involved in an effort to introduce data mining to science and engineering undergraduate students. In October 2008 the PI attended the two-day NSF conference on Broadening Participation in Computing Disciplines in Virginia Beach, VA and in February 2009 the Computational Thinking for Ev...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
