<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII:SCH:A Generative Deep Learning (GDL) based Platform for Super-resolution, Virtual-Pathological Visualization of Coronary Images</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2020</AwardEffectiveDate>
<AwardExpirationDate>05/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>199000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Fay Cobb Payton</SignBlockName>
<PO_EMAI>fpayton@nsf.gov</PO_EMAI>
<PO_PHON>7032927939</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Coronary artery disease (CAD) has been influencing a market over $2.8 billion with roughly 1,000,000 treatment procedures performed annually. Existing guidance for CAD treatment suffers from limited spatial resolution and lacks real-time detailed pathological identification. This project is to investigate novel computational techniques for pathological, super-resolution visualization of coronary images. The project, if successful, will contribute towards a new generation of clinical guidance for the treatment of cardiovascular disease, which is currently the leading cause of human deaths in the United States. Technically, the method of encoding multi-domain image representations into a single-domain image acquisition could benefit other fields, such as multi-camera surveillance monitoring, multimodal biomedical imaging, etc., in terms of greatly reducing hardware cost and medical labor. The educational plan in this project emphasizes activities designed to guide senior designs, enrich curriculum in deep learning courses, and facilitate outreach for minority students in multicultural engineering programs.&lt;br/&gt;&lt;br/&gt;This project aims to develop a data-driven approach to use off-line data and training process, without any hardware modifications, to generatively produce new information that could not be acquired previously or can only be obtained ex vivo. This project will develop generative deep learning algorithms to produce additional information for low-resolution optical coherence tomography (OCT) images by aggregating image information from high-resolution OCT images and histological microscopic images during off-line training. This project will investigate on improving the resolution of OCT while maintaining fast scanning rate via a volumetric generative adversarial network (GAN) for super-resolution. This project will develop a novel unpaired training scheme to map OCT image to a histopathology image by using a GAN-based image translation framework. The approach will be validated using both objective and subjective analysis on OCT images and histopathology images. This project is expected to generate academic outcomes in both computer science and biomedical informatics. This project will provide a deep learning solution for cross-platform volumetric super-resolution and a generative learning approach to address cross-modality image translation.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/17/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/30/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1948540</AwardID>
<Investigator>
<FirstName>Yu</FirstName>
<LastName>Gan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yu Gan</PI_FULL_NAME>
<EmailAddress>ygan6@eng.ua.edu</EmailAddress>
<PI_PHON>2052486105</PI_PHON>
<NSF_ID>000795988</NSF_ID>
<StartDate>03/17/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Alabama Tuscaloosa</Name>
<CityName>Tuscaloosa</CityName>
<ZipCode>354870001</ZipCode>
<PhoneNumber>2053485152</PhoneNumber>
<StreetAddress>801 University Blvd.</StreetAddress>
<StreetAddress2><![CDATA[152 Rose Admin. / Box 870104]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Alabama</StateName>
<StateCode>AL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>045632635</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ALABAMA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>808245794</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The University of Alabama]]></Name>
<CityName>Tuscaloosa</CityName>
<StateCode>AL</StateCode>
<ZipCode>354860001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Alabama</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramElement>
<ProgramReference>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~175000</FUND_OBLG>
<FUND_OBLG>2021~24000</FUND_OBLG>
</Award>
</rootTag>
