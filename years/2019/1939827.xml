<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Using Imperfect Predictions to Make Good Decisions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2019</AwardEffectiveDate>
<AwardExpirationDate>06/30/2022</AwardExpirationDate>
<AwardTotalIntnAmount>301255.00</AwardTotalIntnAmount>
<AwardAmount>301255</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Roger Mailler</SignBlockName>
<PO_EMAI>rmailler@nsf.gov</PO_EMAI>
<PO_PHON>7032927982</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As humans and other animals navigate the world they demonstrate remarkable flexibility in encountering unfamiliar systems, spaces and phenomena, learning to make predictions about how they will behave, and making good decisions based on those predictions. Crucial to this ability is the fact that one does not need to make perfectly accurate or fully detailed predictions to make good decisions. Though, due to our natural limitations, our predictions about the future are necessarily flawed, they are nevertheless sufficiently useful to make reasonable decisions. For artificial agents, in contrast, imperfect predictions often lead to catastrophic failures in decision making. Many existing approaches fundamentally assume that the agent will eventually learn to make perfect predictions and make perfect decisions, which is unreasonable in sufficiently rich, complex environments. This work considers the problem of developing artificial agents that are more aware of and more robust to their own limitations. Agents that can more robustly and flexibly learn from experience in truly complex environments have the potential to impact nearly any application in which decisions are made over time, for instance autonomous robots/vehicles, personal assistants, and medical/legal decision support. Furthermore, as the project will be undertaken at an undergraduate-only liberal arts college, undergraduate researchers will play an integral role in the work. The PI will also build on the strength of the liberal arts setting to enhance instruction of key discipline-specific research and writing skills throughout the Computer Science curriculum. Explicit development of these skills will not only improve students' preparation for a wide variety of career paths (including basic research) but is also aligned with best practices for broadening participation in the discipline. &lt;br/&gt;&lt;br/&gt;This project studies model-based reinforcement learning (MBRL) under the assumption that the agent has fundamental limitations that prevent it from learning a perfect model or from producing optimal plans. The central hypothesis is that in this context the MBRL problem cannot be decomposed into separate model-learning and planning problems, each treating the other as an idealized black box. Rather the optimization process for each component must be aware of its role in the overall architecture and of the limitations of its partner. One key aim of the work is to derive novel measures of model quality that are more tightly related to the true objective of control performance than standard measures of one-step prediction accuracy adapted from supervised learning settings. Another is to investigate how model learning objectives/algorithms can be adapted to account for the limitations of the specific planner that will use the model. Further, control algorithms will be investigated that can make effective use of models of non-homogeneous quality by mediating between model-based and model-free knowledge. The ultimate goal is to integrate these principles into novel MBRL agents that are significantly more robust to limitations in the model class and/or planner and are able to succeed in environments that are too complex and high-dimensional to be modeled or solved exactly.</AbstractNarration>
<MinAmdLetterDate>07/30/2019</MinAmdLetterDate>
<MaxAmdLetterDate>05/06/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1939827</AwardID>
<Investigator>
<FirstName>Erin</FirstName>
<LastName>Talvitie</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Erin J Talvitie</PI_FULL_NAME>
<EmailAddress>erik.talvitie@fandm.edu</EmailAddress>
<PI_PHON>7173585869</PI_PHON>
<NSF_ID>000700376</NSF_ID>
<StartDate>07/30/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvey Mudd College</Name>
<CityName>CLAREMONT</CityName>
<ZipCode>917115901</ZipCode>
<PhoneNumber>9096218121</PhoneNumber>
<StreetAddress>301 Platt Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>27</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA27</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>066691130</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>HARVEY MUDD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>066691130</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvey Mudd College]]></Name>
<CityName>Claremont</CityName>
<StateCode>CA</StateCode>
<ZipCode>917115901</ZipCode>
<StreetAddress><![CDATA[301 Platt Boulevard 91711-5901]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>27</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA27</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~24963</FUND_OBLG>
<FUND_OBLG>2018~91343</FUND_OBLG>
<FUND_OBLG>2019~92092</FUND_OBLG>
<FUND_OBLG>2020~92857</FUND_OBLG>
</Award>
</rootTag>
