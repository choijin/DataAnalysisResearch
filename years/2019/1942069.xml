<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Deep Architectures for Ppredicting 3D Object Motion</AwardTitle>
<AwardEffectiveDate>09/01/2019</AwardEffectiveDate>
<AwardExpirationDate>08/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>175406.00</AwardTotalIntnAmount>
<AwardAmount>175406</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Our everyday living environments are populated with lots of functional objects with which we can interact through their moving parts (e.g., swivel chairs, laptops, bikes and cars, to name just a few). For autonomous agents to correctly interact with these objects in real-world settings, the agents must be equipped with algorithms that are able to parse the objects into their moving parts. But that is not enough.  Through the widespread use of commodity 3D sensors and modern 3D modeling techniques, large repositories (such as ShapeNet) containing millions of digital representations of everyday objects are now available, but these representations are for the most part currently static, that is to say they represent single snapshots of objects. To make use of these object representations in dynamic, virtual environments and in animation applications, methods that automatically segment them into moving parts and synthesize plausible motions for them are needed. This project will explore the design, implementation, and testing of new deep learning architectures that accomplish this, and thereby bring large portions of static 3D datasets "to life." The new algorithms will have broad industrial impact by advancing 3D modeling and animation software, while the generated motion data will be useful for training new computer vision algorithms for object motion recognition and tracking in videos.&lt;br/&gt;&lt;br/&gt;Achieving the project goals will require development of new algorithms to convert static digital representations of 3D objects into dynamic ones by automatically recognizing their moving parts and animating them based on input reference videos of similar objects from the real world and through incorporation of novel methods for estimating partial 2D-3D correspondences, for lifting 2D motion cues to 3D, and for inferring motion rigs for 3D shapes. The project will be organized into two main thrusts, each of which will present its own research challenges.  The first thrust will investigate new deep learning architectures for performing mobility-based segmentation of 3D objects and predicting the underlying motion of their parts. The architecture will be applied to man-made objects with rigidly moving parts.  This part of the research will be carried out in the first year of the project.  The second thrust will extend the previous work to animate 3D models representing living organisms such as quadrupeds, birds and fish (i.e., animals from the DigitalLife dataset). These models undergo non-rigid deformations, so the architecture will have to be modified to estimate and control more sophisticated deformation primitives. This part of the work will be executed in the second year of the project.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/16/2019</MinAmdLetterDate>
<MaxAmdLetterDate>08/16/2019</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1942069</AwardID>
<Investigator>
<FirstName>Evangelos</FirstName>
<LastName>Kalogerakis</LastName>
<EmailAddress>kalo@cs.umass.edu</EmailAddress>
<StartDate>08/16/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
</Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
</Award>
</rootTag>
