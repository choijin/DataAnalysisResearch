<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Randomized Algorithms for Matrix Computations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2018</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>120749.00</AwardTotalIntnAmount>
<AwardAmount>120749</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Leland Jameson</SignBlockName>
<PO_EMAI>ljameson@nsf.gov</PO_EMAI>
<PO_PHON>7032924883</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will develop mathematical techniques for accelerating computational tasks such as simulating electromagnetic scattering,  medical imaging, extracting useful information from large datasets, machine learning, and many others. In all these computations, the step that tends to be the most time-consuming, and which therefore limits how large problems can be solved, concerns the manipulation of large square or rectangular arrays of numbers, called "matrices". Many of the matrices that arise in practical applications have redundancies, and can be compressed to enable them to be stored using less space. Using the compressed format, computations involving the matrix can also be greatly accelerated. The problems that will be addressed are deterministic in nature, but the algorithms that will be developed are probabilistic. It turns out that by exploiting certain mathematical properties of large ensembles of independent random numbers, one can build algorithms for compressing matrices that are much faster than traditional deterministic techniques. The new randomized algorithms can in theory fail, but the likelihood of failure can be shown to  be lower than 1 time out of 10,000,000,000 runs in typical applications. Randomized algorithms of this type have recently attracted much interest due to the fact that they perform  particularly well on emerging computing platforms such as mobile computing (where conserving energy is the key priority), computing using graphical processor units (where the vast numbers of computational cores create challenges), and distributed memory parallel computers. The methods also perform very well when applied  to massively large datasets that must be stored on hard drives, or on large server farms. The project will train one doctoral student, and will lead to the release of a publicly available software package that implements the methods that will be developed. &lt;br/&gt;&lt;br/&gt;From a technical point of view, the objective of the project is to develop efficient algorithms for factorizing matrices and for solving large linear systems of algebraic equations. The algorithms will be based on randomized sampling, and will exploit remarkable mathematical properties of random matrices and random orthogonal projections. Such randomized algorithms require less communication  than traditional methods, which makes them particularly attractive for modern applications involving multicore processors, distributed computing, out-of-core computing, etc. Specifically, the project will address the following problems: (1) Computing full matrix factorizations (e.g. the so called "column pivoted QR factorization") which are core building blocks in scientific computing. Preliminary numerical experiments demonstrate speed-ups of close to an order of magnitude compared to state-of-the-art software packages. (2) Solving linear systems involving many unknowns and many equations. We expect to achieve substantial practical acceleration, and are cautiously optimistic about the possibility to develop solvers with substantially better asymptotic complexity than the cubic complexity achieved by standard techniques. (3) Developing randomized methods for accelerating computational simulations of phenomena such as electro-statics, composite materials, biochemical processes, slow fluid flows, Gaussian processes in 2 and 3 dimensions, etc. Technically, this will be achieved by developing randomized methods for compressing so called "data-sparse" or "rank-structured" matrices.</AbstractNarration>
<MinAmdLetterDate>03/28/2019</MinAmdLetterDate>
<MaxAmdLetterDate>03/28/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1929568</AwardID>
<Investigator>
<FirstName>Per-Gunnar</FirstName>
<LastName>Martinsson</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Per-Gunnar J Martinsson</PI_FULL_NAME>
<EmailAddress>pgm@ices.utexas.edu</EmailAddress>
<PI_PHON>5122322612</PI_PHON>
<NSF_ID>000257904</NSF_ID>
<StartDate>03/28/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787595316</ZipCode>
<StreetAddress><![CDATA[3925 W Braker Lane]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~120749</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project objectives were to develop faster and more energy-efficient algorithms for fundamental tasks in computational science. To be precise, the project addressed the problem of how to efficiently extract information from a rectangular array of real numbers called a "matrix". Such arrays of numbers are used to organize, e.g., statistical data, measurements from sensors, or financial data. They also form essential modeling tools in physics and chemistry, in machine learning, in medical imaging, and many other applications of essential importance to society.<br /><br />The reason we sought to develop better algorithms for analyzing matrices was in part to accelerate computations that can already be performed, and in part to enable computations that are beyond the reach of existing methods. Faster and more energy efficient algorithms will help to reduce the power consumption of data centers, to make desktop computers faster, and to extend the battery life of mobile devices such as cell phones and tablet computers.<br /><br />The fundamental innovation behind our approach was to harness mathematical properties of large collections of random numbers to<br />build new stochastic algorithms that dramatically outperform existing deterministic ones. We used randomized sampling, and randomized averaging, to reduce the effective dimensionality of intermediate problems. This enabled us to make data sets fit in fast memory that is close to the computer processor.<br /><br />In addition to the development of new algorithms, we developed new analytic tools that help researchers understand how these methods perform, and how they compare to classical deterministic techniques. Such analytic&nbsp; tools provide an essential help in the development of the next generation of algorithms.<br /><br />Five doctoral students were involved in the supported research. These students have been trained on how to develop and analyze algorithms, how to write efficient software that performs well on modern hardware, and how to communicate their work to a broader audience. In their training, high priority has been placed on developing an understanding for how mathematics, computer science, and knowledge from domain science (say electrical engineering, or medical imaging) all interact in the development of new computational techniques.<br /><br />The project has resulted in several traditional research articles, but also in a wealth of educational resources such as tutorial articles, videotaped lectures, and tutorial software. The purpose is to maximize the impact of the new methods by making them accessible to potential users from a broad range of backgrounds.</p><br> <p>            Last Modified: 02/01/2021<br>      Modified by: Per-Gunnar&nbsp;J&nbsp;Martinsson</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1929568/1929568_10451571_1612217229279_randomsketch--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1929568/1929568_10451571_1612217229279_randomsketch--rgov-800width.jpg" title="Random sketch of a matrix"><img src="/por/images/Reports/POR/2021/1929568/1929568_10451571_1612217229279_randomsketch--rgov-66x44.jpg" alt="Random sketch of a matrix"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Given an array of data A to be analyzed, we extract a small "sketch" Y by evaluating the action of A on a sparse array of random numbers Omega. Important information about the original data can be extracted from the information in the compact sketch in a provably reliable way.</div> <div class="imageCredit">Per-Gunnar Martinsson</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Per-Gunnar&nbsp;J&nbsp;Martinsson</div> <div class="imageTitle">Random sketch of a matrix</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project objectives were to develop faster and more energy-efficient algorithms for fundamental tasks in computational science. To be precise, the project addressed the problem of how to efficiently extract information from a rectangular array of real numbers called a "matrix". Such arrays of numbers are used to organize, e.g., statistical data, measurements from sensors, or financial data. They also form essential modeling tools in physics and chemistry, in machine learning, in medical imaging, and many other applications of essential importance to society.  The reason we sought to develop better algorithms for analyzing matrices was in part to accelerate computations that can already be performed, and in part to enable computations that are beyond the reach of existing methods. Faster and more energy efficient algorithms will help to reduce the power consumption of data centers, to make desktop computers faster, and to extend the battery life of mobile devices such as cell phones and tablet computers.  The fundamental innovation behind our approach was to harness mathematical properties of large collections of random numbers to build new stochastic algorithms that dramatically outperform existing deterministic ones. We used randomized sampling, and randomized averaging, to reduce the effective dimensionality of intermediate problems. This enabled us to make data sets fit in fast memory that is close to the computer processor.  In addition to the development of new algorithms, we developed new analytic tools that help researchers understand how these methods perform, and how they compare to classical deterministic techniques. Such analytic  tools provide an essential help in the development of the next generation of algorithms.  Five doctoral students were involved in the supported research. These students have been trained on how to develop and analyze algorithms, how to write efficient software that performs well on modern hardware, and how to communicate their work to a broader audience. In their training, high priority has been placed on developing an understanding for how mathematics, computer science, and knowledge from domain science (say electrical engineering, or medical imaging) all interact in the development of new computational techniques.  The project has resulted in several traditional research articles, but also in a wealth of educational resources such as tutorial articles, videotaped lectures, and tutorial software. The purpose is to maximize the impact of the new methods by making them accessible to potential users from a broad range of backgrounds.       Last Modified: 02/01/2021       Submitted by: Per-Gunnar J Martinsson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
