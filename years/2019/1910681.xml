<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF:  RI: Small: Barriers in Adversarially Robust Learning</AwardTitle>
<AwardEffectiveDate>10/01/2019</AwardEffectiveDate>
<AwardExpirationDate>09/30/2022</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>A. Funda Ergun</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Learning algorithms are increasingly taking on roles that were previously held by humans. Examples include face recognition, malware detection, making decisions about loans or bail, etc. Learning algorithms, however, are usually sensitive to adversarial manipulations happening during training or decision time. Due to the sensitivity of the contexts in which these algorithms are used, it is crucial to understand the power and limitations of provably robust methods in such adversarial contexts. The goal of this project is to study adversarial robustness from a provable perspective and identify the barriers that might exist against it. The project will build connections to other areas such as computational complexity as well as cryptography. The project also involves mentoring PhD students. The findings will be incorporated into newly designed courses and will be disseminated via workshops, conferences, and journals.&lt;br/&gt;&lt;br/&gt;The project, more specifically, will focus on two parts that enable the main goals outlined above. The first part is to model adversarially robust learning formally to enable a provable approach. Indeed, Cryptography has benefited tremendously from such mathematically rigorous approach to security, and to reach similar results, adversarially robust learning needs a similar definitional approach that models subtle aspects of the attack such as: the computational complexity of the attacker, its precise knowledge, and the role of randomness. The second part of this project aims at identifying barriers that exist against provable robustness for adversarial learning. This project will study barriers against both information theoretic (a.k.a. statistic) as well as computational security. Information theoretic security models the adversary as an all powerful entity, while the more realistic model of computational security, which is widely used in Cryptography, models the attacker as a polynomial-time algorithm. Identifying these barriers is an essential part of designing optimally robust learning methods.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/27/2019</MinAmdLetterDate>
<MaxAmdLetterDate>06/27/2019</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1910681</AwardID>
<Investigator>
<FirstName>Mohammad</FirstName>
<LastName>Mahmoody Ghidary</LastName>
<EmailAddress>mohammad@virginia.edu</EmailAddress>
<StartDate>06/27/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Virginia Main Campus</Name>
<CityName>CHARLOTTESVILLE</CityName>
<ZipCode>229044195</ZipCode>
<PhoneNumber>4349244270</PhoneNumber>
<StreetAddress>P.O.  BOX 400195</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
</Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>025Z</Code>
<Text>SaTC: Secure and Trustworthy Cyberspace</Text>
</ProgramReference>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
</Appropriation>
</Award>
</rootTag>
