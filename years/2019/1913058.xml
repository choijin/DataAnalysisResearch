<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Development of Collaborative and Interpretable Machine Learning Platform</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2019</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>225000.00</AwardTotalIntnAmount>
<AwardAmount>225000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Atherton</SignBlockName>
<PO_EMAI>patherto@nsf.gov</PO_EMAI>
<PO_PHON>7032928772</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This SBIR Phase I project aims to design and develop a collaborative and interpretable machine learning platform for key machine learning stakeholders to work together to deliver trusted machine learning and artificial intelligence capabilities. This project will address the critical commercial and societal problem of lack of trust due to inability to provide meaningful interpretation, explanation &amp; collaborative oversight for machine generated results. This problem has become one of the biggest challenges for broader adoption of Machine Learning (ML) and Artificial Intelligence (AI), especially, in highly regulated industries, where reasonable degree of traceability, auditability and rationale as to how the machine algorithms arrived at the outcomes and predictions is necessary and mandated by law. This project aims to initially address specific and critical use cases in the healthcare and insurance areas with a plan to expand to other sectors like financial services, pharma and self-driving auto industry. The project intends to capitalize on AI driven growth in the economy by becoming the ML platform of choice in key regulatory market verticals while providing safeguards against perpetuating negative impacts due to incorrect ML and AI predictions. &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The project key innovation focuses on combining the benefits of Machine Learning's ability to mass process at fast rates, find patterns which are hard to find with human collaboration, cognition and oversight to achieve transparency and trust in the ML outcomes. The project aims to advance past and ongoing scientific research in Interpretable ML (IML) and Explainable AI (XAI) and commercialize it. It also significantly speeds up adoption and commercialization of this research by applying it to critical business use cases in specific industry domain verticals. This project provides a novel collaborative interface that evaluates, augments and applies the explanations to specific business use cases in highly regulated industries. This project also uses an innovative hybrid machine and human in the loop design to make the ML interactions more meaningful and a human at the center authority for evaluation and oversight of the ML explanations prior to use in business decision-making. In future phases, this project also aims to provide continuous closed loop feedback and improvement of interpretability of ML models using its gold standard explanations knowledgebase.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/28/2019</MinAmdLetterDate>
<MaxAmdLetterDate>06/28/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1913058</AwardID>
<Investigator>
<FirstName>Sam</FirstName>
<LastName>Nayak</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sam Nayak</PI_FULL_NAME>
<EmailAddress>sam@lorykeet.com</EmailAddress>
<PI_PHON>5123466563</PI_PHON>
<NSF_ID>000788999</NSF_ID>
<StartDate>06/28/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>LORYKEET CORPORATION</Name>
<CityName>AUSTIN</CityName>
<ZipCode>787597765</ZipCode>
<PhoneNumber>5123466563</PhoneNumber>
<StreetAddress>9404 SCHUG CV</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>081558226</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LORYKEET CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[LORYKEET CORPORATION]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787597765</ZipCode>
<StreetAddress><![CDATA[9404 Schug Cv.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~225000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span>Lack of trust and inability to provide meaningful explanations and collaborative oversight for machine generated results is becoming one of the biggest challenges for adoption of Machine Learning (ML) and Artificial Intelligence (AI). Current ML and AI platforms are limited in use for highly regulated industries, where complete traceability, auditability and rationale of how the machine algorithms arrived at the outcomes and decisions is necessary and mandated by law. Phase 1 of this project has developed a&nbsp;<strong><span>C</span></strong>ollaborative and&nbsp;<strong><span>I</span></strong>nterpretable&nbsp;<strong><span>M</span></strong>achine<strong><span>&nbsp;L</span></strong>earning (CIML) platform for key ML stakeholders to work together to deliver trusted ML and AI capability for the business. Some of the major outcomes of the CIML platform are: 1) More trust in the ML results due to more transparency 2) Clear rationale and common understanding of the ML outcomes across key stakeholders, hence quicker and wider adoption 3) Compliance with regulatory audit requirements. Specifically, the CIML platform provides a novel approach to combine the benefits of machine automation with humans in the loop to create trustworthy explanations for machine learning generated outcomes. The CIML platform addresses the black box nature of current ML solutions via a hybrid design that augments human intelligence with ML and vice versa and compensates for each other's limitations. The result is 1) use of the natural benefits of machine learning's ability to mass process at fast rates, find patterns etc. and 2) combining it with human collaboration, cognition and oversight to achieve transparency and trust in the ML outcomes 3) continuous improvement of ML models for better and more accurate ML predictions over time.</span></p> <p><span>Below are the highlights on research objectives from phase 1:</span></p> <p><span>1)&nbsp;&nbsp;&nbsp;&nbsp; Unified access to multiple selectable XAI methods that are automatically mapped to ML algorithms and models based on relevance and best fit.</span></p> <p><span>2)&nbsp;&nbsp;&nbsp;&nbsp; A collaborative interface for evaluating and ranking the explanations by defining a novel standard explanation format specification.</span></p> <p><span>3)&nbsp;&nbsp;&nbsp;&nbsp; A collaborative interface for enriching and augmenting the explanations with user notes, concept graphs and domain knowledge-base.</span></p> <p><span>4)&nbsp;&nbsp;&nbsp;&nbsp; A hybrid Machine and human in the loop collaborative workflow for review and oversight of ML results and explanations.</span></p> <p><span>The use case selected by Lorykeet for its initial implementation and release is based on numerous studies and has significant broader impact. The economic burden associated with prescription opioid abuse is substantial. Opioid misuse members (patients) cost private Payers (large insurers) approximately $15,000 on average annually above non-abusers. There are an estimated 2 million members in the private Payer market in this category. That is a staggering $30 billion cost burden in the private Payer industry alone. The Lorykeet CIML platform will have a direct commercial impact in lowering these costs by reducing the high cost of care member risk pool size for Payer and Providers. Additionally, the economic burden of prescription opioid abuse is characterized not only by direct healthcare costs and medical utilization, but also by considerable indirect costs and societal burden. According to the CDC, more than 183,000 people have died in the United States from overdoses related to prescription opioids from 1999 to 2015.</span></p> <p><span>With a focus on its first use case for Opioid Risk Management, Lorykeet has developed technology to tackle this multidimensional health crisis by providing innovation in collaboration between key stakeholders by providing explainable insights and reasoning into black box machine driven predictions. This is creating a trustworthy network effect to reduce both the commercial and societal burden of Opioid misuse. The platform is designed to expand to other areas of Risk Management in the future.</span></p><br> <p>            Last Modified: 09/17/2020<br>      Modified by: Sam&nbsp;Nayak</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Lack of trust and inability to provide meaningful explanations and collaborative oversight for machine generated results is becoming one of the biggest challenges for adoption of Machine Learning (ML) and Artificial Intelligence (AI). Current ML and AI platforms are limited in use for highly regulated industries, where complete traceability, auditability and rationale of how the machine algorithms arrived at the outcomes and decisions is necessary and mandated by law. Phase 1 of this project has developed a Collaborative and Interpretable Machine Learning (CIML) platform for key ML stakeholders to work together to deliver trusted ML and AI capability for the business. Some of the major outcomes of the CIML platform are: 1) More trust in the ML results due to more transparency 2) Clear rationale and common understanding of the ML outcomes across key stakeholders, hence quicker and wider adoption 3) Compliance with regulatory audit requirements. Specifically, the CIML platform provides a novel approach to combine the benefits of machine automation with humans in the loop to create trustworthy explanations for machine learning generated outcomes. The CIML platform addresses the black box nature of current ML solutions via a hybrid design that augments human intelligence with ML and vice versa and compensates for each other's limitations. The result is 1) use of the natural benefits of machine learning's ability to mass process at fast rates, find patterns etc. and 2) combining it with human collaboration, cognition and oversight to achieve transparency and trust in the ML outcomes 3) continuous improvement of ML models for better and more accurate ML predictions over time.  Below are the highlights on research objectives from phase 1:  1)     Unified access to multiple selectable XAI methods that are automatically mapped to ML algorithms and models based on relevance and best fit.  2)     A collaborative interface for evaluating and ranking the explanations by defining a novel standard explanation format specification.  3)     A collaborative interface for enriching and augmenting the explanations with user notes, concept graphs and domain knowledge-base.  4)     A hybrid Machine and human in the loop collaborative workflow for review and oversight of ML results and explanations.  The use case selected by Lorykeet for its initial implementation and release is based on numerous studies and has significant broader impact. The economic burden associated with prescription opioid abuse is substantial. Opioid misuse members (patients) cost private Payers (large insurers) approximately $15,000 on average annually above non-abusers. There are an estimated 2 million members in the private Payer market in this category. That is a staggering $30 billion cost burden in the private Payer industry alone. The Lorykeet CIML platform will have a direct commercial impact in lowering these costs by reducing the high cost of care member risk pool size for Payer and Providers. Additionally, the economic burden of prescription opioid abuse is characterized not only by direct healthcare costs and medical utilization, but also by considerable indirect costs and societal burden. According to the CDC, more than 183,000 people have died in the United States from overdoses related to prescription opioids from 1999 to 2015.  With a focus on its first use case for Opioid Risk Management, Lorykeet has developed technology to tackle this multidimensional health crisis by providing innovation in collaboration between key stakeholders by providing explainable insights and reasoning into black box machine driven predictions. This is creating a trustworthy network effect to reduce both the commercial and societal burden of Opioid misuse. The platform is designed to expand to other areas of Risk Management in the future.       Last Modified: 09/17/2020       Submitted by: Sam Nayak]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
