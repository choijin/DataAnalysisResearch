<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research in DRMS: Judgments of Answerers and the Answers They Give</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/15/2020</AwardEffectiveDate>
<AwardExpirationDate>01/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>23420.00</AwardTotalIntnAmount>
<AwardAmount>23420</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Claudia Gonzalez-Vallejo</SignBlockName>
<PO_EMAI>clagonza@nsf.gov</PO_EMAI>
<PO_PHON>7032927836</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A basic feature of human social life is asking and answering questions, a complex task that requires one to decide whom to ask, how to phrase the question, and how to interpret the answer and judge its worth. Despite the centrality of the activity, however, little is understood about the general principles involved in each step of the process, making it difficult to design better ways to help people ask questions and find the information they need. The problem becomes even more pressing in the information age: people ask more questions than ever before, on a wider range of topics, and they take those questions not only to friends and family, but also to strangers and acquaintances on the wider world of online bulletin boards and social media. This dissertation work addresses gaps in our understanding, using a combination of methods from psychology and computer science. Tools from machine learning and artificial intelligence will be used to analyze large-scale collections of questions and answers that people produce on online bulletin board systems and results will be used to build general theories that explain and predict how and when people find good answers to their questions. In laboratory experiments to test these theories, participants will be shown questions and answers with varying underlying properties, and the results will identify features that are particularly crucial for information gathering. The large-scale nature of the data science investigation affords the development of theories based on subtle patterns in the relationship between question and answer, while the laboratory work allows investigation into the causal nature of the problem: what, for the question-asker, makes an answer good?&lt;br/&gt;&lt;br/&gt;One of the most common ways people explore and learn about the world is by seeking information from others through asking questions. This behavior requires two kinds of judgments: (1) from whom one should seek answers and (2) how should one judge whether the answer satisfies the question? These two judgments are inter-dependent and require that people make decisions under uncertainty. This two-pronged investigation into how such judgments are made includes not only the lab-based experiments that are a traditional strength of social and decision sciences, but also a data science component that looks at how these decisions are made in the wild. This mix of methodologies will be used to examine the defining features of good answers and those who give them. The research focuses on questions and answers that are given in human-to-human linguistic dialogue. Techniques from data science and computational linguistics will be used to analyze collections of questions and answers produced on online discussion forums, ranging from requests for technical help on Stackexchange to more nuanced, social questions asked by parents on Mumsnet. Two key properties of the relationship between questions and answers—the extent to which they overlap, and the degree to which answers focus the solution space posed by the question—are hypothesized to correspond to how answers are evaluated. Controlled laboratory experiments will be used to elucidate preferences for those who answer questions. The types of preferences a questioner has for potential answerers has direct consequences for the relevance and quality of information received, and in turn affects the utility of subsequent answers and further decisions. A Bayesian framework is used to formalize the task facing the questioner: to infer what type of answerer an individual is, given their past answering behavior. Behavioral experiments will test key predictions of the model.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>01/24/2020</MinAmdLetterDate>
<MaxAmdLetterDate>01/24/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1948887</AwardID>
<Investigator>
<FirstName>Gretchen</FirstName>
<LastName>Chapman</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gretchen B Chapman</PI_FULL_NAME>
<EmailAddress>gchapman@andrew.cmu.edu</EmailAddress>
<PI_PHON>4122687380</PI_PHON>
<NSF_ID>000208911</NSF_ID>
<StartDate>01/24/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Simon</FirstName>
<LastName>DeDeo</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Simon DeDeo</PI_FULL_NAME>
<EmailAddress>sdedeo@andrew.cmu.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000742148</NSF_ID>
<StartDate>01/24/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christina</FirstName>
<LastName>Boyce-Jacino</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christina Boyce-Jacino</PI_FULL_NAME>
<EmailAddress>cboyceja@andrew.cmu.edu</EmailAddress>
<PI_PHON>4122683665</PI_PHON>
<NSF_ID>000806669</NSF_ID>
<StartDate>01/24/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<CountyName/>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName/>
<CountyName/>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1321</Code>
<Text>Decision, Risk &amp; Mgmt Sci</Text>
</ProgramElement>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~23420</FUND_OBLG>
</Award>
</rootTag>
