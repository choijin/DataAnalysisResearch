<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Measurable Program Analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/11/2018</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>219660.00</AwardTotalIntnAmount>
<AwardAmount>227579</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Nina Amla</SignBlockName>
<PO_EMAI>namla@nsf.gov</PO_EMAI>
<PO_PHON>7032927991</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Software is everywhere and its correct operation plays an increasingly important role in the health, productivity, and safety of society and in the lives of individuals.  Consequently, there is a need for techniques that can cost-effectively measure software correctness to establish a well-founded basis for making judgments about whether software is ready for deployment and wide spread use.  The availability of such measures provides an evidentiary basis  for balancing the rewards of using a software system against the risks of its failure.  This type of evidence has the potential to transform the expectations of consumers of software and to enhance their understanding of software behavior and how to place their trust in that behavior.  Evidence of correctness has obvious value for safety critical software, but more broadly it will help shape how society views software as critical infrastructure and the professionalism that it expects of its manufacture.&lt;br/&gt;&lt;br/&gt;This project blends the outcomes of decades of work on abstraction-based program analysis and symbolic execution with recent results in quantifying the solution space of a logical formula.  The project explores novel combinations and staging of scalable non-quantitative analyses, to identify sub-spaces of program behavior that may be erroneous, followed by quantitative analyses focused on those sub-spaces.  This offers an approach to measurable program analysis (MPA) that promises scalability while yielding safe and accurate results.  The project produces theory and tools that realize a variety of MPA, empirically evaluates the cost and benefit of these analyses, and openly shares all results and artifacts with the research community.</AbstractNarration>
<MinAmdLetterDate>11/01/2018</MinAmdLetterDate>
<MaxAmdLetterDate>11/01/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1901769</AwardID>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Dwyer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew Dwyer</PI_FULL_NAME>
<EmailAddress>md3cn@virginia.edu</EmailAddress>
<PI_PHON>4349247604</PI_PHON>
<NSF_ID>000103915</NSF_ID>
<StartDate>11/01/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Virginia Main Campus</Name>
<CityName>CHARLOTTESVILLE</CityName>
<ZipCode>229044195</ZipCode>
<PhoneNumber>4349244270</PhoneNumber>
<StreetAddress>P.O.  BOX 400195</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>065391526</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RECTOR &amp; VISITORS OF THE UNIVERSITY OF VIRGINIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>065391526</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Virginia Main Campus]]></Name>
<CityName>Charlottesville</CityName>
<StateCode>VA</StateCode>
<ZipCode>229044195</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8206</Code>
<Text>Formal Methods and Verification</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~219659</FUND_OBLG>
<FUND_OBLG>2017~7920</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Software is at the heart of technology upon which modern society depends.&nbsp;&nbsp; To determine whether software behaves as intended current practice is to test it - to run the software on sample inputs and observe how it performs.&nbsp;&nbsp; This is an important step in assuring software, but it can only demonstrate flaws in software not their absence.&nbsp;&nbsp; In contrast static analysis techniques seek to prove the absence of errors and this has the potential to provide much higher degrees of assurance that software behavior can be trusted.</p> <p>Static analysis is generally forced to make a tradeoff between efficiency and accuracy.&nbsp;&nbsp; This project developed a new framework, called alternating conditional analysis (ACA), that can effectively balance these opposing forces.&nbsp; It achieves this by employing a portfolio of algorithmically diverse static analyzers.&nbsp; Each analyzer considers a portion of the software behavior and ACA consolidates that information to formulate a new analysis problem on the portion of the software behavior that remains unanalyzed.&nbsp;&nbsp; This progressively narrows the scope of analysis thereby allowing subsequent application of analyzers to be faster and more accurate.</p> <p>ACA has been realized in a toolset called ALPACA that can incorporates more than a dozen underlying analyzers to target programs written in C.&nbsp;&nbsp; ALPACA has, in turn, been applied as a building block to produce state-of-the-art software analyzers that are capable of computing quantitative characterizations of program behavior, e.g., the probability of failure given an input probability distribution.&nbsp; Ongoing work is exploring further applications of the framework to realize new forms of compositional analysis that can scale to large software systems while producing accurate results.</p> <p>ACA established a new paradigm in static analysis and the open-source ALPACA toolset allows the broader research community to build on the outcomes of this project to further broaden its impact.</p><br> <p>            Last Modified: 07/22/2020<br>      Modified by: Matthew&nbsp;Dwyer</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Software is at the heart of technology upon which modern society depends.   To determine whether software behaves as intended current practice is to test it - to run the software on sample inputs and observe how it performs.   This is an important step in assuring software, but it can only demonstrate flaws in software not their absence.   In contrast static analysis techniques seek to prove the absence of errors and this has the potential to provide much higher degrees of assurance that software behavior can be trusted.  Static analysis is generally forced to make a tradeoff between efficiency and accuracy.   This project developed a new framework, called alternating conditional analysis (ACA), that can effectively balance these opposing forces.  It achieves this by employing a portfolio of algorithmically diverse static analyzers.  Each analyzer considers a portion of the software behavior and ACA consolidates that information to formulate a new analysis problem on the portion of the software behavior that remains unanalyzed.   This progressively narrows the scope of analysis thereby allowing subsequent application of analyzers to be faster and more accurate.  ACA has been realized in a toolset called ALPACA that can incorporates more than a dozen underlying analyzers to target programs written in C.   ALPACA has, in turn, been applied as a building block to produce state-of-the-art software analyzers that are capable of computing quantitative characterizations of program behavior, e.g., the probability of failure given an input probability distribution.  Ongoing work is exploring further applications of the framework to realize new forms of compositional analysis that can scale to large software systems while producing accurate results.  ACA established a new paradigm in static analysis and the open-source ALPACA toolset allows the broader research community to build on the outcomes of this project to further broaden its impact.       Last Modified: 07/22/2020       Submitted by: Matthew Dwyer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
