<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: SMALL: Inducing Answer Set Programs to Provide Accurate and Concise Explanation of Machine-learned Models</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2019</AwardEffectiveDate>
<AwardExpirationDate>06/30/2022</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Roger Mailler</SignBlockName>
<PO_EMAI>rmailler@nsf.gov</PO_EMAI>
<PO_PHON>7032927982</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Artificial Intelligence (AI)/Machine Learning is gaining prominence as an important technology that will have significant impact on our economy, industry, society, and academia. A major problem with modern machine learning methods is their inability to explain their decisions to human users. Statistical machine learning methods produce models that are complex algebraic solutions to optimization problems such as risk minimization or data likelihood maximization. Lack of intuitive descriptions makes it hard for users to understand, verify or trust the underlying rules that govern the model. Also, these methods cannot produce a justification for a prediction they compute for a new data sample. As a result, there is significant research interest in what is termed as Explainable AI. This project will develop methods to capture the logic behind machine learning models, making the models explainable to users. This will allow users to improve the models and will enhance users' trust in these models.     &lt;br/&gt;&lt;br/&gt;Inductive Logic Programming (ILP) is an established technique to find the rules underlying a machine-learned model that are comprehensible to humans. The rules learned are represented as logic programs or Horn clauses. However, due to lack of negation-as-failure, Horn clauses offer limited expressiveness for representation and reasoning when the background knowledge about the domain being studied is incomplete. Additionally, ILP learns rules under the assumption that there are no exceptions to them. This results in exceptions and noise in the data being indistinguishable from each other. Often, the exceptions to the rules themselves follow a pattern, and these exceptions can be (recursively) learned as a default theory. It is hypothesized that a learned program that includes such a default theory describes the underlying model more accurately. This project extends heuristics-based, scalable ILP algorithms that learn default theories as answer set programs given background knowledge as well as positive and negative examples. These answer-set programs aim to capture the logic underlying a learned model to provide justifications for its decisions and to improve users' trust in it, discover any biases in the model, and comply with outside requirements such as governmental regulations.  The aim of this project is to advance the state-of-the-art in ILP research and to contribute to the general area of machine learning and explainable AI. Results of the project will be open-sourced, with the aim to enabling industries that make use of machine learning to develop better understanding of and trust in the learned models they use.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/15/2019</MinAmdLetterDate>
<MaxAmdLetterDate>07/15/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1910131</AwardID>
<Investigator>
<FirstName>Gopal</FirstName>
<LastName>Gupta</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gopal Gupta</PI_FULL_NAME>
<EmailAddress>gupta@utdallas.edu</EmailAddress>
<PI_PHON>4698781991</PI_PHON>
<NSF_ID>000249848</NSF_ID>
<StartDate>07/15/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Dallas</Name>
<CityName>Richardson</CityName>
<ZipCode>750803021</ZipCode>
<PhoneNumber>9728832313</PhoneNumber>
<StreetAddress>800 W. Campbell Rd., AD15</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>32</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX32</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>800188161</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT DALLAS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Dallas]]></Name>
<CityName>Richardson</CityName>
<StateCode>TX</StateCode>
<ZipCode>750803021</ZipCode>
<StreetAddress><![CDATA[800 W. Campbell Rd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>32</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX32</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~450000</FUND_OBLG>
</Award>
</rootTag>
