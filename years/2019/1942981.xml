<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Toward Video2Sim: Turning Real World Videos into Simulations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2020</AwardEffectiveDate>
<AwardExpirationDate>03/31/2025</AwardExpirationDate>
<AwardTotalIntnAmount>549999.00</AwardTotalIntnAmount>
<AwardAmount>209459</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops new technology toward Video2Sim: automatically converting a video into a virtual world, where scenes are reconstructed, actions are re-enacted, and alternative outcomes are simulated by a computer. Such a system does not yet exist due to the limitations of existing technology, and as a result, virtual worlds need to be manually and laboriously constructed. Video2Sim is useful because virtual worlds can be used to train and evaluate AI systems. For example, videos of traffic accidents can be converted into simulations to test autonomous cars, or videos of kitchen scenes to test home robots. Simulation is more scalable and cost-effective than real world experiments and is particularly suited for machine learning algorithms that require a lot of training data. Furthermore, such an automated system can leverage a large number of videos to provide a comprehensive coverage of rare events, which is essential for evaluating and assuring the safety of autonomous systems. Therefore, Video2Sim has the potential to benefit a broad range of applications including robotics, healthcare, and transportation. Research in this project is integrated with K12, undergraduate, and graduate education through research training, course development and outreach events. &lt;br/&gt;&lt;br/&gt;This research develops key techniques toward a Video2Sim system with a focus on 3D shape and motion. This effort is organized into two thrusts: (1) reconstructing 3D shape and motion and (2) simulating dynamics and behavior. The goal of thrust 1 is to recover 3D shape and motion of a full scene from a monocular video, such that we can re-render the scene and re-enact the events from an arbitrary view. The focus is on developing methods to recover detailed 3D shape and 3D motion from arbitrary unconstrained videos. The goal of thrust 2 is to recover the underlying dynamics of a scene, such that we can not only re-enact the actual events but also simulate alternative outcomes. The focus is on developing methods to infer not only physical properties of passive objectives but also behavior models of agents, that is, entities that do not just move passively according to external forces but can plan and initiate their own actions.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/19/2020</MinAmdLetterDate>
<MaxAmdLetterDate>04/26/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1942981</AwardID>
<Investigator>
<FirstName>Jia</FirstName>
<LastName>Deng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jia Deng</PI_FULL_NAME>
<EmailAddress>jiadeng@princeton.edu</EmailAddress>
<PI_PHON>6092581203</PI_PHON>
<NSF_ID>000662553</NSF_ID>
<StartDate>03/19/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<CountyName/>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<CountyName/>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[87 Prospect Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~103054</FUND_OBLG>
<FUND_OBLG>2021~106405</FUND_OBLG>
</Award>
</rootTag>
