<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Non-convex Variational Image Processing: Boosting Classical Methods with Machine Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2019</AwardEffectiveDate>
<AwardExpirationDate>07/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>198152.00</AwardTotalIntnAmount>
<AwardAmount>198152</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuliya Gorb</SignBlockName>
<PO_EMAI>ygorb@nsf.gov</PO_EMAI>
<PO_PHON>7032922113</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Recent advances in machine learning and AI, particularly those based on artificial neural networks, have enabled us to build systems that solve difficult information processing problems with human-like accuracy.  For example, neural networks can recognize objects, predict how proteins fold, automate manufacturing processing, and use computer vision to navigate a vehicle or analyze satellite imagery. Unfortunately, these advanced AI systems come with their own unique problems.  Like humans, neural networks can behave erratically, sometimes making strange and unexplainable decisions when asked to perform tasks that differ even a little from their training. For this reason, classifical image and signal processing methods are still the go-to solution when reliability, interpretability, and computational speed at needed.  The goal of this research project is to mash up the performance and power of neural networks with the speed and reliability and classical algorithms. This research project also features an integrated teaching plan involving graduate students and undergraduate interns.&lt;br/&gt; &lt;br/&gt;To achieve this goal, we consider three interrelated research thrusts.  First, we consider ways that deep networks can help to automate and improve classical algorithms.  For example, networks can be used to automate the selection of hyper-parameters, choose objective functions to minimize, identify noise types and levels that are present in data, and make other decisions that are needed to optimally tune the performance of classical imaging system.  Second, we consider ways that neural networks can be 'plugged in' to classical variational imaging methods. For example, classical image priors (such as wavelet sparsity or total variation), can be replaced with more sophisticated priors defined by neural networks. Third, we consider efficient algorithms for solving minimization problems that arise when complex neural networks are used as components in classical optimization problems.  Better algorithms will allow us to solve these complex problems efficiently, and without human oversight. This new suite of approaches has the potential to improve that state of the art for a range of important practical problems that have been studied by the PI. This includes enhancing deblurring problems of the type used for microscopy of new materials, boosting segmentation algorithms used to identify faults in semiconductor manufacturing, and solving complex resource allocation problems for medical applications.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/29/2019</MinAmdLetterDate>
<MaxAmdLetterDate>07/29/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1912866</AwardID>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Goldstein</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas A Goldstein</PI_FULL_NAME>
<EmailAddress>tomg@cs.umd.edu</EmailAddress>
<PI_PHON>3149225841</PI_PHON>
<NSF_ID>000546658</NSF_ID>
<StartDate>07/29/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<CountyName/>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<CountyName/>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~198152</FUND_OBLG>
</Award>
</rootTag>
