<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>STTR Phase I:  FlatCam: Inexpensive, Compact Lensless Cameras for IoT Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2019</AwardEffectiveDate>
<AwardExpirationDate>04/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>224995.00</AwardTotalIntnAmount>
<AwardAmount>224995</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Benaiah Schrag</SignBlockName>
<PO_EMAI>bschrag@nsf.gov</PO_EMAI>
<PO_PHON>7032928323</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact of this Small Business Technology Transfer (STTR) Phase I project is the development of a new imaging platform technology that has the potential to affect many areas including consumer imaging, medical imaging, spectroscopy, astronomy, surveillance, and defense.  Transitioning this technology into real applications will mean this technology can be used for personalized experiences, improved quality of life, and increased safety.&lt;br/&gt;&lt;br/&gt;The STTR Phase I proposed project will develop inexpensive, lensless imaging devices (contrary to the current state-of-art cameras, that rely on lenses to form a focused image), that can be integrated with internet-of-things (IoT) devices to gather visual data. Since the lens in a camera accounts for the vast majority of the cost and the weight, these devices can provide order of magnitude reductions in cost, allowing cameras to be integrated into a much larger array of home, auto, and city-scale smart devices. The research tasks in this project are: (1) developing fast, real-time algorithms for image reconstruction exploiting advances in optimization and machine learning (2) developing face detection, recognition, and tracking algorithms that operate with the lensless imaging platform for IoT applications like personalization, and (3) improving data communication (wired or wireless) to meet current and future IoT needs by exploring end-to-end system integration and optimization.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/08/2019</MinAmdLetterDate>
<MaxAmdLetterDate>11/04/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1914252</AwardID>
<Investigator>
<FirstName>Ashok</FirstName>
<LastName>Veeraraghavan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ashok Veeraraghavan</PI_FULL_NAME>
<EmailAddress>Ashok.Veeraraghavan@gmail.com</EmailAddress>
<PI_PHON>7133484820</PI_PHON>
<NSF_ID>000583333</NSF_ID>
<StartDate>07/08/2019</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jesse</FirstName>
<LastName>Adams</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jesse K Adams</PI_FULL_NAME>
<EmailAddress>jesse@synopic.com</EmailAddress>
<PI_PHON>9045544138</PI_PHON>
<NSF_ID>000790955</NSF_ID>
<StartDate>07/08/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Synopic Inc.</Name>
<CityName>Berkeley</CityName>
<ZipCode>947082105</ZipCode>
<PhoneNumber>8325480422</PhoneNumber>
<StreetAddress>91 Fairlawn Dr Unit 1</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>116816155</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>SYNOPIC INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rice University]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>770051827</ZipCode>
<StreetAddress><![CDATA[6100 Main St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1505</Code>
<Text>STTR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>8033</Code>
<Text>Hardware Software Integration</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~224995</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Through this STTR Phase I project we were able to further develop our algorithms in order to achieve real-time speeds for processing data to display both high-resolution images and accompanying 3D information using a single camera device. We implemented these algorithms in a stand-alone prototype using commercially available hardware to demonstrate this real-time capability with a custom designed camera assembly (including our integrated lens modification). Additionally, we were able to supplement the simulated data previously used for training our network with real biological data from stereo endoscopy datasets. The addition of this real data improves the robustness of our system for depth analysis in real biological systems such as those encountered during endoscopic procedures.</p> <p>The results of&nbsp;this project have shown the early steps toward offering physicians &amp; surgeons measurable spatial and depth information while retaining the high-resolution imaging required for successful endoscopic procedures. This could tremendously impact multiple fields in medical endoscopy by&nbsp; directly improving procedure outcomes through better visualization, but also through increased understanding of the progression of diseases (e.g., cancer) by acquiring more detailed information regarding tissue abnormalities and growth. Additionally, these results show&nbsp;the potential to impact other medical fields where measurable depth information can have a profound influence (e.g., ophthalmology, dermatology). Other opportunities also exist in the fields of microscopy, autonomous navigation, and health monitoring where&nbsp;3D imaging is showing the potential to play a more prominent role.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/12/2021<br>      Modified by: Jesse&nbsp;K&nbsp;Adams</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Through this STTR Phase I project we were able to further develop our algorithms in order to achieve real-time speeds for processing data to display both high-resolution images and accompanying 3D information using a single camera device. We implemented these algorithms in a stand-alone prototype using commercially available hardware to demonstrate this real-time capability with a custom designed camera assembly (including our integrated lens modification). Additionally, we were able to supplement the simulated data previously used for training our network with real biological data from stereo endoscopy datasets. The addition of this real data improves the robustness of our system for depth analysis in real biological systems such as those encountered during endoscopic procedures.  The results of this project have shown the early steps toward offering physicians &amp; surgeons measurable spatial and depth information while retaining the high-resolution imaging required for successful endoscopic procedures. This could tremendously impact multiple fields in medical endoscopy by  directly improving procedure outcomes through better visualization, but also through increased understanding of the progression of diseases (e.g., cancer) by acquiring more detailed information regarding tissue abnormalities and growth. Additionally, these results show the potential to impact other medical fields where measurable depth information can have a profound influence (e.g., ophthalmology, dermatology). Other opportunities also exist in the fields of microscopy, autonomous navigation, and health monitoring where 3D imaging is showing the potential to play a more prominent role.             Last Modified: 05/12/2021       Submitted by: Jesse K Adams]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
