<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Collaborative Research: Inexactness and Data-Awareness in Network Stacks for Distributed Machine Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2019</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>150000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Darleen Fisher</SignBlockName>
<PO_EMAI>dlfisher@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The architectures underlying modern network hardware and software have their roots in designs that were developed decades ago. Even though these architectures have evolved in many ways over the years, they remain unchanged in two key aspects: (1) They support ?exact? or complete/absolute reliable communication (either hop-by-hop, or end-to-end, or both); (2) They adhere to strict layering, and the resulting encapsulation and interfaces hide from lower network layers the semantics of the data applications transmit over the network.&lt;br/&gt;&lt;br/&gt;These design principles place serious impediments for emerging distributed machine learning (ML) training and inference applications. These applications are seeing adoption in a wide variety of important domains, such as, computer vision, robotics, data science, graphics, and speech recognition. Two distinguishing attributes of these applications are: (1) their computations are intrinsically inexact in nature, because these applications rely on computing or utilizing statistical models, and (2) their input and intermediate data have well-defined structure, i.e., tensors, or multi-dimensional arrays of typed data. Give these attributes, enforcing exact communication in a data semantics-unaware fashion limits the potentially enormous benefits of embracing inexactness in these approximate applications.&lt;br/&gt;&lt;br/&gt;This project explores co-designing ML applications with layers of the network software and hardware stack to allow application-driven cross-layer optimization for energy efficiency, hardware density/capacity, and performance. Given an application-provided overall inexactness budget, this research will explore both how to systematically apportion the budget across network layers, and how different layers can reconfigure their functionality to achieve different levels of approximation.&lt;br/&gt; &lt;br/&gt;This project will develop strawman approaches to encoding structured data and to achieving budget-driven inexact computation over it. The research will use experiments, simulations, and analysis to identify performance benefits to ML applications, and fundamental trade-offs that determine the feasibility of this approach. The resulting inexactness-aware ML software stack could drive hitherto unseen performance and accuracy improvements, and potentially drive future innovations in ML algorithms, systems, and applications.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/22/2019</MinAmdLetterDate>
<MaxAmdLetterDate>08/22/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1940216</AwardID>
<Investigator>
<FirstName>Rachit</FirstName>
<LastName>Agarwal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rachit Agarwal</PI_FULL_NAME>
<EmailAddress>ra625@cornell.edu</EmailAddress>
<PI_PHON>6072555014</PI_PHON>
<NSF_ID>000728107</NSF_ID>
<StartDate>08/22/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148537501</ZipCode>
<StreetAddress><![CDATA[107 Hoy Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span> </span>Traditionally, computer networks have offered two abstractions to applications: one that guarantees perfect, in-order, reliable delivery of all the data sent by the sender; and, second that provides no reliability guarantees whatsoever. These two abstractions are a good match for applications that are either willing to incur the overheads of achieving perfect reliability or could tolerate arbitrarily unreliable errors in data transmission. However, for many modern applications that are already statistical in nature and can tolerate some (but not arbitrarily many) errors in data delivery, existing network abstractions result in high tail latencies, unnecessary complexity and poor application-level performance.</p> <p><span> </span><span> </span>Motivated by such statistical applications, this project explores a new abstraction --- Approximately Reliable Channel (ARC) --- that allows applications to specify any desirable error guarantee for data transmission, and guarantees in-order delivery of at least (1-e) fraction of the data. This project also designed Approximate Transport Protocol (ATP), a datacenter protocol that realizes ARC on modern datacenter networks. Preliminary evaluation suggests that ATP improves the job execution time of machine learning applications by 25% without any impact on concurrently running applications that use the perfectly reliable abstraction.</p> <p>Intellectual Merits:&nbsp;<span id="docs-internal-guid-c3d6318c-7fff-1368-f4fc-511df164515e"><span>This project explores&nbsp;</span><span>the novel idea of alleviating the mismatch between the semantics required by the applications, and those supported by the network protocols and algorithms, by exploiting approximate networking.</span><span> It sheds initial light on a variety of fundamental issues, such as, how much inexactness is tolerable for machine learning applications; how to optimally apportion approximation budget between applications and network; and, how much can the application benefit as a result. The project lays the foundation for faster, more flexible, and more energy and cost efficient machine learning applications and infrastructure, and spur innovation in machine learning.</span></span></p> <p><span><span>Broader Impacts:&nbsp;<span id="docs-internal-guid-b7006c17-7fff-65f8-f18c-cdfc5b865e87"><span>This research is conducted collaboratively by a postdoctoral researcher and a senior graduate student, making them the intellectual leads for a potentially exciting new area of computer systems. We have already incorporated the results from this research into a graduate class at Cornell.</span></span></span></span></p><br> <p>            Last Modified: 02/09/2021<br>      Modified by: Rachit&nbsp;Agarwal</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Traditionally, computer networks have offered two abstractions to applications: one that guarantees perfect, in-order, reliable delivery of all the data sent by the sender; and, second that provides no reliability guarantees whatsoever. These two abstractions are a good match for applications that are either willing to incur the overheads of achieving perfect reliability or could tolerate arbitrarily unreliable errors in data transmission. However, for many modern applications that are already statistical in nature and can tolerate some (but not arbitrarily many) errors in data delivery, existing network abstractions result in high tail latencies, unnecessary complexity and poor application-level performance.    Motivated by such statistical applications, this project explores a new abstraction --- Approximately Reliable Channel (ARC) --- that allows applications to specify any desirable error guarantee for data transmission, and guarantees in-order delivery of at least (1-e) fraction of the data. This project also designed Approximate Transport Protocol (ATP), a datacenter protocol that realizes ARC on modern datacenter networks. Preliminary evaluation suggests that ATP improves the job execution time of machine learning applications by 25% without any impact on concurrently running applications that use the perfectly reliable abstraction.  Intellectual Merits: This project explores the novel idea of alleviating the mismatch between the semantics required by the applications, and those supported by the network protocols and algorithms, by exploiting approximate networking. It sheds initial light on a variety of fundamental issues, such as, how much inexactness is tolerable for machine learning applications; how to optimally apportion approximation budget between applications and network; and, how much can the application benefit as a result. The project lays the foundation for faster, more flexible, and more energy and cost efficient machine learning applications and infrastructure, and spur innovation in machine learning.  Broader Impacts: This research is conducted collaboratively by a postdoctoral researcher and a senior graduate student, making them the intellectual leads for a potentially exciting new area of computer systems. We have already incorporated the results from this research into a graduate class at Cornell.       Last Modified: 02/09/2021       Submitted by: Rachit Agarwal]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
