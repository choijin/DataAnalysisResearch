<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>STTR Phase I:  A Self-Learning Approach for In-Vehicle Driver and Passenger Monitoring Through a Sensor Fusion Approach</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2020</AwardEffectiveDate>
<AwardExpirationDate>03/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>225000.00</AwardTotalIntnAmount>
<AwardAmount>225000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Atherton</SignBlockName>
<PO_EMAI>patherto@nsf.gov</PO_EMAI>
<PO_PHON>7032928772</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact of this Small Business Technology Transfer (STTR) Phase I project will result from the introduction of a state-of-the-art driver monitoring system using artificial intelligence to detect distracted driving or poor driving practices. It can also be used for driver coaching and education, as well as to improve driver attention. The system will help minimize accidents and create safer roads and work environments. End users include automotive original equipment manufacturers (OEMs), commercial fleet operators, taxi and ride-sharing companies, heavy machinery and crane operators, rail and aviation operators, and operators of specialized transportation systems, such as school bus services and charter vehicles.&lt;br/&gt;&lt;br/&gt;This Small Business Technology Transfer (STTR) Phase I project will exploit data from different camera and inertial sensors inside a vehicle to monitor and assess the attention of the driver. The driverâ€™s gaze and upper body pose will be evaluated separately using artificial intelligence (AI) methods and the results combined to generate an overall estimate of the level of driver distraction. The proposed framework is expected to generate reliable results even in cases of high face occlusion. The technical objectives of the project include to: 1) Explore supervised and unsupervised methods to track the driver's body movement using depth and RGB sensors, addressing the challenges and drawbacks of current vision-based algorithms in real-world driving conditions; 2) Design a novel deep learning framework to integrate the driver's body pose with his/her attention level to infer driver's activities (e.g., such as using portable devices, eating, drinking, and other activities); 3) Develop new models of driver visual attention to obtain confidence levels in the estimated driver's gaze, estimated shoulder pose and joints positions; 4) Develop multi-modal end-to-end deep learning frameworks that integrate multiple sensors to provide important features for monitoring and assisting the driver; 5) Implement the system on low-power commodity hardware that is cost-effective and scalable.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/02/2020</MinAmdLetterDate>
<MaxAmdLetterDate>03/02/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1950249</AwardID>
<Investigator>
<FirstName>Steve</FirstName>
<LastName>Burns</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Steve Burns</PI_FULL_NAME>
<EmailAddress>steveburns@edgetensor.com</EmailAddress>
<PI_PHON>9493347670</PI_PHON>
<NSF_ID>000807750</NSF_ID>
<StartDate>03/02/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Rajesh</FirstName>
<LastName>Narasimha</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rajesh Narasimha</PI_FULL_NAME>
<EmailAddress>rajeshnarasimha@edgetensor.com</EmailAddress>
<PI_PHON>4042592928</PI_PHON>
<NSF_ID>000805679</NSF_ID>
<StartDate>03/02/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>EDGETENSOR TECHNOLOGIES INC.</Name>
<CityName>PLANO</CityName>
<ZipCode>750246320</ZipCode>
<PhoneNumber>4042592928</PhoneNumber>
<StreetAddress>6708 ALCOVE LN</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>111348554</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>EDGETENSOR TECHNOLOGIES INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas]]></Name>
<CityName>Richardson</CityName>
<StateCode>TX</StateCode>
<ZipCode>750800800</ZipCode>
<StreetAddress><![CDATA[800 W. Campbell Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1505</Code>
<Text>STTR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>8033</Code>
<Text>Hardware Software Integration</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~225000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-d3612b2c-7fff-a6c6-d2d3-21911efa124e"><span>In the Phase I project, we used our newly-developed car simulator platform (under COVID-19 restrictions) to capture the driver's face and upper-body posture for different drivers using depth and global shutter infrared cameras. We explored supervised and unsupervised methods to track the driver's body movement using RGB and infrared sensors, addressing the challenges and drawbacks of current vision-based algorithms in real-world driving conditions. We designed a novel multi-modal end-to-end deep learning framework that can integrate driver's body pose with his/her attention level estimated from eyes and face features to infer driver activities (e.g., use of portable devices, smartphone usage, eating, drinking and smoking etc.) and&nbsp; for driver drowsiness monitoring and assistance. The proposed solution is robust to illumination variations in practical scenarios, large face occlusions, facial accessories such as masks, caps, makeup, beard and extreme body and head/body motions. Distracted driving events are aggregated and provided to vehicle supervisors/managers to proactively advise the drivers of safety concerns and take necessary steps to improve the driving habits. The proposed solution improves the accuracy and robustness compared to existing solutions and makes it more generic and addresses the pain points of the broader market such as automotive/truck OEMs, commercial fleets, insurance, taxis and ride sharing, last-mile delivery, heavy machinery and cranes, trains and flights and specialized transportation such as school buses and charter vehicles. The solution is also designed to work on low power commodity hardware that is cost effective and thus scalable to the mass market. The other applications where the current solution would be a fit are robotics, education technology, video security systems and smart city applications.</span></span></p><br> <p>            Last Modified: 04/05/2021<br>      Modified by: Rajesh&nbsp;Narasimha</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658253747_Edgetensor_Deck_Automotive_v1-1--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658253747_Edgetensor_Deck_Automotive_v1-1--rgov-800width.jpg" title="Edge AI Platform"><img src="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658253747_Edgetensor_Deck_Automotive_v1-1--rgov-66x44.jpg" alt="Edge AI Platform"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Data to deploy pipeline for distracted driver monitoring</div> <div class="imageCredit">Edgetensor</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Rajesh&nbsp;Narasimha</div> <div class="imageTitle">Edge AI Platform</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658114650_SIM2--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658114650_SIM2--rgov-800width.jpg" title="DRIVING SIMULATOR"><img src="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658114650_SIM2--rgov-66x44.jpg" alt="DRIVING SIMULATOR"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The driving simulator set up in the laboratory for data collection</div> <div class="imageCredit">Simulator</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Rajesh&nbsp;Narasimha</div> <div class="imageTitle">DRIVING SIMULATOR</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658162244_SIM3--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658162244_SIM3--rgov-800width.jpg" title="DRIVING SIMULATOR"><img src="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658162244_SIM3--rgov-66x44.jpg" alt="DRIVING SIMULATOR"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Camera placement for the driving simulator</div> <div class="imageCredit">Simulator</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Rajesh&nbsp;Narasimha</div> <div class="imageTitle">DRIVING SIMULATOR</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658075259_SIM1--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658075259_SIM1--rgov-800width.jpg" title="DRIVING SIMULATOR"><img src="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658075259_SIM1--rgov-66x44.jpg" alt="DRIVING SIMULATOR"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The driving simulator set up in the laboratory for data collection</div> <div class="imageCredit">Simulator</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Rajesh&nbsp;Narasimha</div> <div class="imageTitle">DRIVING SIMULATOR</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658360457_Edgetensor_Deck_Automotive_v1-3--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658360457_Edgetensor_Deck_Automotive_v1-3--rgov-800width.jpg" title="Fleet/Automotive Safety Solution"><img src="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658360457_Edgetensor_Deck_Automotive_v1-3--rgov-66x44.jpg" alt="Fleet/Automotive Safety Solution"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Fleet/Automotive Safety Edge Solution, Market Segments</div> <div class="imageCredit">Edgetensor</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Rajesh&nbsp;Narasimha</div> <div class="imageTitle">Fleet/Automotive Safety Solution</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658438791_Edgetensor_Deck_Automotive_v1-2--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658438791_Edgetensor_Deck_Automotive_v1-2--rgov-800width.jpg" title="Product Card"><img src="/por/images/Reports/POR/2021/1950249/1950249_10655653_1617658438791_Edgetensor_Deck_Automotive_v1-2--rgov-66x44.jpg" alt="Product Card"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Distracted Driving Product</div> <div class="imageCredit">Edgetensor</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Rajesh&nbsp;Narasimha</div> <div class="imageTitle">Product Card</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In the Phase I project, we used our newly-developed car simulator platform (under COVID-19 restrictions) to capture the driver's face and upper-body posture for different drivers using depth and global shutter infrared cameras. We explored supervised and unsupervised methods to track the driver's body movement using RGB and infrared sensors, addressing the challenges and drawbacks of current vision-based algorithms in real-world driving conditions. We designed a novel multi-modal end-to-end deep learning framework that can integrate driver's body pose with his/her attention level estimated from eyes and face features to infer driver activities (e.g., use of portable devices, smartphone usage, eating, drinking and smoking etc.) and  for driver drowsiness monitoring and assistance. The proposed solution is robust to illumination variations in practical scenarios, large face occlusions, facial accessories such as masks, caps, makeup, beard and extreme body and head/body motions. Distracted driving events are aggregated and provided to vehicle supervisors/managers to proactively advise the drivers of safety concerns and take necessary steps to improve the driving habits. The proposed solution improves the accuracy and robustness compared to existing solutions and makes it more generic and addresses the pain points of the broader market such as automotive/truck OEMs, commercial fleets, insurance, taxis and ride sharing, last-mile delivery, heavy machinery and cranes, trains and flights and specialized transportation such as school buses and charter vehicles. The solution is also designed to work on low power commodity hardware that is cost effective and thus scalable to the mass market. The other applications where the current solution would be a fit are robotics, education technology, video security systems and smart city applications.       Last Modified: 04/05/2021       Submitted by: Rajesh Narasimha]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
