<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Highly power efficient and scalable hardware accelerator for AI applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/15/2019</AwardEffectiveDate>
<AwardExpirationDate>02/28/2021</AwardExpirationDate>
<AwardTotalIntnAmount>224996.00</AwardTotalIntnAmount>
<AwardAmount>224996</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Steven Konsek</SignBlockName>
<PO_EMAI>skonsek@nsf.gov</PO_EMAI>
<PO_PHON>7032927021</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact of this Small Business Innovation Research (SBIR) Phase I project is providing faster, cheaper and lower power alternatives to central processing units (CPUs) and graphic processing units (GPUs),  making machine learning more accessible to students, engineers and scientists. In general, this will lead to faster product development and shorter time-to-market in the artificial intelligence market. Highly power-efficient machine learning accelerators make training and complex inferences possible on so-called "Edge" devices and can revolutionize the way machine learning tasks are performed for end users. By enabling fast and power-efficient Edge computing, this innovation benefits society by reducing data traffic while preserving privacy and data security since data never leave the device. The Total Addressable Market for hardware accelerators for machine learning applications was estimated to be around $1B in 2017 but will likely grow at a 50% Compound Annual Growth Rate (CAGR) until 2025 to $66 B. High power-efficiency and scalability of this innovation gives it an immense competitive advantage to penetrate different segments within this market.&lt;br/&gt;&lt;br/&gt;The proposed project aims to develop a fast, scalable and area- and power-efficient matrix multiplier for machine learning applications. Matrix multiplication is at the heart of all machine learning algorithms and is the most computationally expensive task in these applications. Most hardware accelerator solutions store inputs, weights and partial sums in memory and retrieve them sequentially in order to perform matrix multiplication. The data movements between memory and computational units dominate the overall power consumption and latency of the system. By performing computations in memory, a significant power and area savings can be achieved. This SBIR project seeks to develop a technology to perform mixed-signal matrix multiplication in memory to significantly improve the speed and power- and area-efficiency of machine learning accelerators. Phase I will involve the design and verification of a matrix multiplier that can perform machine learning tasks more efficiently.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/28/2019</MinAmdLetterDate>
<MaxAmdLetterDate>12/07/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1938256</AwardID>
<Investigator>
<FirstName>Seyed Behdad</FirstName>
<LastName>Youssefi Azarbayjani</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Seyed Behdad Youssefi Azarbayjani</PI_FULL_NAME>
<EmailAddress>behdadyoussefi@yahoo.com</EmailAddress>
<PI_PHON>5105907305</PI_PHON>
<NSF_ID>000780594</NSF_ID>
<StartDate>08/28/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>AREANNA, INC.</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947021139</ZipCode>
<PhoneNumber>5105907305</PhoneNumber>
<StreetAddress>1224 ROSE ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>080961475</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>AREANNA, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[AREANNA, INC.]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>947021139</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>8033</Code>
<Text>Hardware Software Integration</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~224996</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>There has been an explosion of AI applications in almost every industry in the past decade. Today, in many cases, the computational complexity of these AI models requires that they be processed in the Cloud where computational resources are essentially limitless. However, the processing of these AI workloads in the cloud is very energy intensive and incurs a heavy data communication cost. It has been famously quoted that the energy needed to process all AI workloads globally in the next few years will be equivalent to boiling the entire oceans' water. This has resulted in the emergence of Edge Computing solutions to bring the AI processing closer to the sources of data generation, which are usually sensors, to reduce the data communication cost.</p> <p>However, Edge Computing hasn't really lived up to its promise. Edge AI processors are either deployed on Edge Servers where sophisticated<em> </em>AI models run on a power budget of about 10<strong> </strong>Watts or on energy-constrained Edge Devices where simple<em> </em>AI models run on milliwatts of power. A solution capable of bridging this gap requires an ultra-low power and scalable AI processor to be able to run sophisticated<em> </em>AI models with milliwatt power budget. This constitutes a majority of Edge Computing application space but unfortunately today's hardware platforms are not capable of addressing this market need.</p> <p>Research has shown that power consumption in Edge AI processors is dominated by data communication between memory and processor. To minimize data movement, the Compute-In-Memory (CIM) architecture has been explored by companies and academics to embed computation within memory. CIM is inherently a mixed signal architecture and hence requires data converters to interface between layers of network. However, data converters have proven to be the Achilles' heel of this architecture as they take up to 98% of overall chip area and consume more than 85% of overall power consumption, defeating the whole purpose of CIM architecture. CIM also suffers from analog nonidealities which can degrade AI performance. Furthermore, the extra processing steps needed to fabricate the memory array in CIM limits the scalability of this architecture.</p> <p>Areanna's architecture addresses these issues using our proprietary Compute-and-Quantize-In-Memory (CQIM) architecture where SRAM bit-cells are repurposed to construct data converters; improving power and area efficiency by over an order of magnitude. Using logic gates as its building blocks, CQIM is inherently a digital architecture and scales well with the latest process nodes. High power efficiency and scalability of this architecture brings deployment of sophisticated real-time AI models with mW power budget within reach. As result of Phase I project, a CQIM prototype has been implemented and taped out in standard CMOS process. Further, FPGA software stack was developed to communicate with the prototype.</p><br> <p>            Last Modified: 05/31/2021<br>      Modified by: Seyed Behdad&nbsp;Youssefi Azarbayjani</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ There has been an explosion of AI applications in almost every industry in the past decade. Today, in many cases, the computational complexity of these AI models requires that they be processed in the Cloud where computational resources are essentially limitless. However, the processing of these AI workloads in the cloud is very energy intensive and incurs a heavy data communication cost. It has been famously quoted that the energy needed to process all AI workloads globally in the next few years will be equivalent to boiling the entire oceans' water. This has resulted in the emergence of Edge Computing solutions to bring the AI processing closer to the sources of data generation, which are usually sensors, to reduce the data communication cost.  However, Edge Computing hasn't really lived up to its promise. Edge AI processors are either deployed on Edge Servers where sophisticated AI models run on a power budget of about 10 Watts or on energy-constrained Edge Devices where simple AI models run on milliwatts of power. A solution capable of bridging this gap requires an ultra-low power and scalable AI processor to be able to run sophisticated AI models with milliwatt power budget. This constitutes a majority of Edge Computing application space but unfortunately today's hardware platforms are not capable of addressing this market need.  Research has shown that power consumption in Edge AI processors is dominated by data communication between memory and processor. To minimize data movement, the Compute-In-Memory (CIM) architecture has been explored by companies and academics to embed computation within memory. CIM is inherently a mixed signal architecture and hence requires data converters to interface between layers of network. However, data converters have proven to be the Achilles' heel of this architecture as they take up to 98% of overall chip area and consume more than 85% of overall power consumption, defeating the whole purpose of CIM architecture. CIM also suffers from analog nonidealities which can degrade AI performance. Furthermore, the extra processing steps needed to fabricate the memory array in CIM limits the scalability of this architecture.  Areanna's architecture addresses these issues using our proprietary Compute-and-Quantize-In-Memory (CQIM) architecture where SRAM bit-cells are repurposed to construct data converters; improving power and area efficiency by over an order of magnitude. Using logic gates as its building blocks, CQIM is inherently a digital architecture and scales well with the latest process nodes. High power efficiency and scalability of this architecture brings deployment of sophisticated real-time AI models with mW power budget within reach. As result of Phase I project, a CQIM prototype has been implemented and taped out in standard CMOS process. Further, FPGA software stack was developed to communicate with the prototype.       Last Modified: 05/31/2021       Submitted by: Seyed Behdad Youssefi Azarbayjani]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
