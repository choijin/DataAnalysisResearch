<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RII Track-2 FEC: The Visual Experience Database: A Large-Scale Point-of-View Video Database for Vision Research</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2019</AwardEffectiveDate>
<AwardExpirationDate>07/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>3974003.00</AwardTotalIntnAmount>
<AwardAmount>2975629</AwardAmount>
<AwardInstrument>
<Value>Cooperative Agreement</Value>
</AwardInstrument>
<Organization>
<Code>01060100</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OIA</Abbreviation>
<LongName>Office of Integrative Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Eric Lindquist</SignBlockName>
<PO_EMAI>elindqui@nsf.gov</PO_EMAI>
<PO_PHON>7032927127</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Current artificial intelligence (AI) systems that recognize visual content require millions of training examples to achieve good performance. However, the databases used to train such systems often take photos and videos from the internet, and thus do not represent the content that humans see on a daily basis. This introduces substantial biases into the AI systems that can have serious implications for AI-based applications such as self-driving cars. This project, a collaboration between Bates College, the University of Nevada, Reno, and North Dakota State University, Fargo will create the Visual Experience Database (VED), a database of over 240 hours of video shot from the perspective of a diverse set of observers engaged in common, everyday activities such as shopping, eating, or walking. Along with these videos, we will track each observer's head and eye position in order to understand how people look at the world, and how this changes with environment, age, and task. Our goal is to make this database open and accessible to all. Having the computer skills to use the database is key to accessibility, so we will be releasing a suite of software tools for using the database, as well as implement a summer workshop in basic computer programming skills to grow a workforce that is prepared for a variety of scientific occupations. By making the database open to the public, we will enable scientists, historians, and even artists to benefit from this rich resource. &lt;br/&gt;&lt;br/&gt;Progress in both human visual neuroscience and computer vision are limited by the availability of representative visual data. However, currently available image and movie databases are not representative of typical first-person visual experience. This project, a collaboration between Bates College, the University of Nevada, Reno, and North Dakota State University, Fargo, will create the Visual Experience Database (VED), a database of over 240 hours of first-person video complete with eye- and head-tracking. We will record from people of diverse ages (5-70 years) across three geographically distinct sites as they engage in common, everyday activities such as shopping, eating, or walking. With these data, we will be able to assess how observers sample their visual environments, and how gaze patterns change with environment, age, and task. Further, these data can be used as training data for next-generation computer vision systems. In order to develop a workforce with the skills necessary to work with big data, we will teach a Big Data Skills Summer Workshop to provide undergraduate and graduate students with the basic skills of computer programming and computational literacy to make contributions to this project and to prepare them for a variety of STEM occupations. The VED will be of broad use across several academic communities (cognitive science, neuroscience, computer vision, and possibly digital humanities and art). By creating a database that represents common, human experiences, we bypass the many biases of extant datasets, which will increase the efficacy of computer vision algorithms. By making these data fully open, we will enable advances in these fields to be accessible to all.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/08/2019</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>CoopAgrmnt</TRAN_TYPE>
<CFDA_NUM>47.083</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1920896</AwardID>
<Investigator>
<FirstName>Benjamin</FirstName>
<LastName>Balas</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Benjamin Balas</PI_FULL_NAME>
<EmailAddress>benjamin.balas@ndsu.edu</EmailAddress>
<PI_PHON>7012316105</PI_PHON>
<NSF_ID>000597131</NSF_ID>
<StartDate>08/08/2019</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>MacNeilage</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul MacNeilage</PI_FULL_NAME>
<EmailAddress>pmacneilage@unr.edu</EmailAddress>
<PI_PHON>7757844040</PI_PHON>
<NSF_ID>000736991</NSF_ID>
<StartDate>08/08/2019</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michelle</FirstName>
<LastName>Greene</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michelle R Greene</PI_FULL_NAME>
<EmailAddress>mgreene2@bates.edu</EmailAddress>
<PI_PHON>2077866243</PI_PHON>
<NSF_ID>000742245</NSF_ID>
<StartDate>08/08/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Lescroart</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark D Lescroart</PI_FULL_NAME>
<EmailAddress>mlescroart@unr.edu</EmailAddress>
<PI_PHON>7756826807</PI_PHON>
<NSF_ID>000795229</NSF_ID>
<StartDate>08/08/2019</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Bates College</Name>
<CityName>Lewiston</CityName>
<ZipCode>042406028</ZipCode>
<PhoneNumber>2077868375</PhoneNumber>
<StreetAddress>2 Andrews Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maine</StateName>
<StateCode>ME</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>ME02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>058951401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT &amp; TRUSTEES OF BATES COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>058951401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Bates College]]></Name>
<CityName>Lewiston</CityName>
<StateCode>ME</StateCode>
<ZipCode>042406030</ZipCode>
<StreetAddress><![CDATA[2 Andrews Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maine</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>ME02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7217</Code>
<Text>EPSCoR Research Infrastructure</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~2004695</FUND_OBLG>
<FUND_OBLG>2021~970934</FUND_OBLG>
</Award>
</rootTag>
