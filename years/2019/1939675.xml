<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>FAI: Quantifying Direct and Indirect Consequences of Racial Disparities in Outcomes Following Cardiac Surgery</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2020</AwardEffectiveDate>
<AwardExpirationDate>12/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>169734.00</AwardTotalIntnAmount>
<AwardAmount>169734</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wendy Nilsen</SignBlockName>
<PO_EMAI>wnilsen@nsf.gov</PO_EMAI>
<PO_PHON>7032922568</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As learning algorithms become ubiquitous in our lives, both observers and insiders have expressed concerns about the potentially harmful or discriminatory biases and disparities. These issues may arise when algorithms use sensitive features in the data, such as race, age, gender, or sexual orientation, in inappropriate ways.  Troubling racial disparities have been discovered for many kinds of health outcomes.  In particular, it is known that African Americans have a higher prevalence of coronary heart disease compared to other ethnic groups, and are known to suffer higher rates of post-operative morbidity and mortality, after undergoing surgical interventions.  While these disparities are well established in the literature, the extent to which they are due to biological factors, socioeconomic factors, or differences in offered care is not known. This proposal will address the conceptual, methodological, and practical gaps in assessing and addressing reasons for disparities in health outcomes by a combination of tools from causal mediation analysis and fairness-aware algorithms, and a rich dataset obtained from electronic health records. This will ensure the benefits of learning algorithms used for prediction and decision support in healthcare settings apply fairly and equitably to all. In addition, as part of the project, the project will  allow for the introduction of disparities and algorithmic fairness into the data science curriculum at the university. Methodological and practical innovations for quantifying and addressing disparities developed in this research are crucial to make sure the benefits of learning algorithms used for prediction and decision support in healthcare settings apply fairly and equitably to all.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The perspective on disparities and fairness in the proposed project builds on the team's preliminary work where fairness constraints correspond to vanishing causal effects along certain (domain-specific) "impermissible" pathways in a causal model. The formal framework of causal modeling has allowed the team to mathematize (un)fairness criteria in terms of causal path-specific effects (PSEs) that can be estimated from observed data, and then imposed as constraints on the optimization task. The project will rigorously justify the proposed framework, making precise how the proposed formalization of fairness constraints (unlike previous proposals) is designed to intervene on cycles of injustice. Importantly we will draw on the relevant literature from moral philosophy and philosophy of science here, since the crucial concepts -- fairness, systemic injustice, causal explanation -- have been the subject of much debate and analysis in philosophy for decades. To address the limitations of prior work, which only allowed high quality solutions for relatively simple parametric models, or entailed intractable methods such as rejection sampling, this project develops novel methodology that will use techniques from structural nested models in causal inference and empirical likelihood in statistics to rephrase the problem in the framework of maximum likelihood.  These methods will be easier to reliably scale to high dimensional data, and yield much higher quality solutions to both prediction and policy learning problems than previously possible.  This will make our methodology for assessing and satisfying fairness constraints applicable to complex data found in healthcare. Finally, this project will apply the developed methodology to data on patients that have undergone heart surgery, and perform preliminary analyses that aim to assess the extent to which disparities are attributable to pathways associated with biology, socioeconomic status, and differences in care.  The clinical team will begin to  validate the models and the resulting findings. While algorithmic fairness is a topic of considerable interest to the machine learning community, with multiple approaches already explored, this proposal is unique in three ways.  First, the proposed framework is well-motivated, and provides a systematic way to evaluate disparate and sometimes conflicting intuitions that underly previous proposals.  Second, the project is designed to break the cycles of injustice in a formal sense.  Finally, the proposed approach to fair inference is not an incremental extension of a single method to the problem, but draws on insights from multiple communities, and can be viewed as a novel combination of tools from analytic philosophy, causal inference, semi-parametric statistics, and optimization.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>12/23/2019</MinAmdLetterDate>
<MaxAmdLetterDate>12/23/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1939675</AwardID>
<Investigator>
<FirstName>Ilya</FirstName>
<LastName>Shpitser</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ilya Shpitser</PI_FULL_NAME>
<EmailAddress>ilyas@cs.jhu.edu</EmailAddress>
<PI_PHON>9166671382</PI_PHON>
<NSF_ID>000727572</NSF_ID>
<StartDate>12/23/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<StreetAddress2><![CDATA[Suite B001]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001910777</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>JOHNS HOPKINS UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001910777</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Johns Hopkins University]]></Name>
<CityName>Baltimore</CityName>
<StateCode>MD</StateCode>
<ZipCode>212182683</ZipCode>
<StreetAddress><![CDATA[3400 N Charles Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>114Y</Code>
<Text>Fairness in Artificial Intelli</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~169734</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As learning algorithms become ubiquitous in our lives, both observers and insiders have expressed concerns about the potentially harmful or discriminatory biases encoded in such algorithms. Such biases arise when sensitive features in the data, such as race, age, gender, or sexual orientation, are used in inappropriate ways. Even in the absence of such features, ?proxy? variables may be present that are highly correlated with sensitive features. Resulting predictions or decisions made based on such proxies may thus still exhibit significant disparities.</p> <p>Troubling disparities have been discovered for many kinds of outcomes. In particular, it is known that African Americans have a higher prevalence of coronary heart disease compared to other ethnic groups, and are known to suffer higher rates of post-operative morbidity and mortality, after undergoing surgical interventions. While these disparities are well established in the literature, the extent to which they are due to biological factors, socioeconomic factors, or differences in offered care is not known.<br /><br />This project developed new statistical methods for quantifying disparities with respect to a sensitivity feature on an outcome using effects along causal pathways.&nbsp; In this framework, the existence of an impermissible disparity is defined as an outcome of a hypothetical study where a component of a cause pertaining to a pathway is varied in different study arms, while other components are held fixed.&nbsp; For instance, this framework may quantify a disparity associated with a direct effect of gender on a hiring decision by varying&nbsp; the direct perception of gender by the decision maker (such as recorded gender and pronouns on a resume) while keeping other variables, such as qualifications listed on the resume, the same.</p> <p>The project used tools from causal inference to estimate disparities quantified in this way from observed data.&nbsp; In addition, the project developed new methods for building predictive and decision support tools that are able to use data as efficiently as possible while simultaneously ensuring that fairness constraints pertaining to absence of disparities along impermissible causal pathways are absent.&nbsp; These methods were based on an innovative combination of causal inference, semi-parametric statistics, empirical likelihood, and constrained optimization tools.<br /><br />The project's longer term aim is to use the resulting methods to assess the extent of disparities in outcomes among cardiac surgery patients at Johns Hopkins hospital.&nbsp; In addition, while outside of the original scope of the project, the developed methods have also been used to assess potential mechanisms behind the well-documented racial disparities in outcomes among COVID-19 patients, using data from Johns Hopkins.<br /><br />To this end, a number of open source data pipelining tools have been developing in SQL and Python.<br /><br />Finally, the PI has incorporated a module on disparities and algorithmic fairness to his courses on causal inference, and disinformation.&nbsp; In addition, the PI has organized a symposium on disparities in COVID-19 that took place at Johns Hopkins University.</p><br> <p>            Last Modified: 04/30/2021<br>      Modified by: Ilya&nbsp;Shpitser</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As learning algorithms become ubiquitous in our lives, both observers and insiders have expressed concerns about the potentially harmful or discriminatory biases encoded in such algorithms. Such biases arise when sensitive features in the data, such as race, age, gender, or sexual orientation, are used in inappropriate ways. Even in the absence of such features, ?proxy? variables may be present that are highly correlated with sensitive features. Resulting predictions or decisions made based on such proxies may thus still exhibit significant disparities.  Troubling disparities have been discovered for many kinds of outcomes. In particular, it is known that African Americans have a higher prevalence of coronary heart disease compared to other ethnic groups, and are known to suffer higher rates of post-operative morbidity and mortality, after undergoing surgical interventions. While these disparities are well established in the literature, the extent to which they are due to biological factors, socioeconomic factors, or differences in offered care is not known.  This project developed new statistical methods for quantifying disparities with respect to a sensitivity feature on an outcome using effects along causal pathways.  In this framework, the existence of an impermissible disparity is defined as an outcome of a hypothetical study where a component of a cause pertaining to a pathway is varied in different study arms, while other components are held fixed.  For instance, this framework may quantify a disparity associated with a direct effect of gender on a hiring decision by varying  the direct perception of gender by the decision maker (such as recorded gender and pronouns on a resume) while keeping other variables, such as qualifications listed on the resume, the same.  The project used tools from causal inference to estimate disparities quantified in this way from observed data.  In addition, the project developed new methods for building predictive and decision support tools that are able to use data as efficiently as possible while simultaneously ensuring that fairness constraints pertaining to absence of disparities along impermissible causal pathways are absent.  These methods were based on an innovative combination of causal inference, semi-parametric statistics, empirical likelihood, and constrained optimization tools.  The project's longer term aim is to use the resulting methods to assess the extent of disparities in outcomes among cardiac surgery patients at Johns Hopkins hospital.  In addition, while outside of the original scope of the project, the developed methods have also been used to assess potential mechanisms behind the well-documented racial disparities in outcomes among COVID-19 patients, using data from Johns Hopkins.  To this end, a number of open source data pipelining tools have been developing in SQL and Python.  Finally, the PI has incorporated a module on disparities and algorithmic fairness to his courses on causal inference, and disinformation.  In addition, the PI has organized a symposium on disparities in COVID-19 that took place at Johns Hopkins University.       Last Modified: 04/30/2021       Submitted by: Ilya Shpitser]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
