<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Computational Model of Perceived Color and Appearance in Augmented Reality</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2025</AwardExpirationDate>
<AwardTotalIntnAmount>549892.00</AwardTotalIntnAmount>
<AwardAmount>222928</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Balakrishnan Prabhakaran</SignBlockName>
<PO_EMAI>bprabhak@nsf.gov</PO_EMAI>
<PO_PHON>7032924847</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Augmented reality (AR) allows a viewer to visualize digital, virtual objects mixed into their real-world environment. Different from virtual reality, which displays virtual content but blocks out the view of the real world, AR enables mixtures such as a virtual representation of a remote person sitting in a real conference room with other people. Other examples of AR applications include: (i) science students visualizing the invisible flow of electricity moving through wires in their circuits or visualize unseen forces such as magnetism or atmospheric currents in their real-world environment; (ii) doctors overlaying an X-ray-like view of internal organs during exams or laparoscopic surgery. This project aims to answer a complex and important question: how does human visual system perceive the mix of virtual AR content and the real world? The researchers will build a model of perception of the visual characteristics of AR systems, including color, brightness, depth, and graphics quality, with data from experiments that test visual responses. The model will mimic visual adaptation to different lighting environments, such as indoors versus outdoors, as well as compensations for reflections and other interactions with the environment. The project will result in improved AR system designs, including responsive algorithms that react to changes in the environment and anticipate the visual perception of the user. It will also result in a better scientific understanding of the human visual system, which has the potential to improve many other visual interfaces in addition to AR. Integrating the project with education, the researchers will develop and test AR learning modules for university courses in science, math, and art, leading to improved learning. By improving the understanding of visual perception in AR, this project will help enable visually accurate, more comfortable, and more responsive AR display systems. These have the potential to enhance the visual sense in applications such as education, medicine, and transportation.&lt;br/&gt;&lt;br/&gt;This project aims to build a robust computational model of visual appearance in AR systems that takes into account visual adaptation, cognitive interpretations, and the optical interaction between virtually displayed content and real objects and illumination in the environment. Visual experiments will employ psychophysical scaling, color matching via adjustment, and constant stimuli tasks to understand the influence of the luminance, color, contrast, complexity, and depth of both the AR virtual foreground and the real-world background. A model of color and material appearance in transparent AR environments will build on the working hypothesis that the perceived color is a non-physical addition of foreground and background whose weightings depend on cognitive discounting of the layers. Additional experiments will measure visual luminance and chromatic adaptation in temporally changing AR viewing environments, for instance, asking observers to adjust displayed AR and real-world stimuli to be achromatic in different lighting situations and at different adaptation times. The physical lighting environment, including the location, intensity, and color of light sources and bright objects, will be sensed with cameras and color sensors as input to responsive display algorithms utilizing the developed model of visual adaptation and appearance in AR. The responsive algorithms will ensure robust, predictable color appearance and display. During the project, several AR education applications incorporating the research results will be developed for validation and testing. Classroom assessments by students and faculty will evaluate these applications for validating the research results. AR will be adopted in the research group’s graduate color science courses as a tool for demonstrating adaptation and surround effects and as an environment for practicing psychophysical methods with experiments related to the ongoing research. The researchers will share their computational model and experimental findings via publication and professional organizations, with the goal of meaningful implementation both in AR system design and AR applications. Further, the impact of AR and the fascinating topic of color science will be shared with the community through the University’s open houses and the recruiting of students and faculty.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>02/28/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/08/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1942755</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Murdoch</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Murdoch</PI_FULL_NAME>
<EmailAddress>mmpocs@rit.edu</EmailAddress>
<PI_PHON>5854754134</PI_PHON>
<NSF_ID>000727316</NSF_ID>
<StartDate>02/28/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rochester Institute of Tech</Name>
<CityName>ROCHESTER</CityName>
<ZipCode>146235603</ZipCode>
<PhoneNumber>5854757987</PhoneNumber>
<StreetAddress>1 LOMB MEMORIAL DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY25</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002223642</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ROCHESTER INSTITUTE OF TECHNOLOGY (INC)</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002223642</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rochester Institute of Tech]]></Name>
<CityName>ROCHESTER</CityName>
<StateCode>NY</StateCode>
<ZipCode>146235603</ZipCode>
<StreetAddress><![CDATA[141 LOMB MEMORIAL DR]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~105597</FUND_OBLG>
<FUND_OBLG>2021~117331</FUND_OBLG>
</Award>
</rootTag>
