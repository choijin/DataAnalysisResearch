<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Collaborative Research: Structured Methods for Multi-Task Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/16/2018</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>178564.00</AwardTotalIntnAmount>
<AwardAmount>178564</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wei Ding</SignBlockName>
<PO_EMAI>weiding@nsf.gov</PO_EMAI>
<PO_PHON>7032928017</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The ability of human to learn from and transfer knowledge across related learning tasks enables us to grasp complex concepts from only a few examples. For instance, a three-year old child is able to discriminate chairs from tables without having been exposed to hundreds of different examples. In contrast, computer learning programs typically require training on a large number of examples in order to achieve similar levels of recognition. This prompts the study of multi-task learning in which multiple related tasks are learned simultaneously, thereby facilitating inter-task knowledge transfer. However, most multi-task learning studies are restricted to problems with well-defined tasks and structures. This project aims at developing algorithms and tools (including open source software) to attack problems that are not traditionally treated, but can potentially be reformulated and solved more effectively by multi-task learning. This allows a broad class of challenging machine learning problems to benefit from multi-task learning techniques. This project also develops a new curriculum that incorporates the proposed research into classroom. In addition, this project will allow the PIs to continue the ongoing efforts of actively recruiting and advising students from under-represented groups. &lt;br/&gt;&lt;br/&gt;To achieve these goals, this project focuses on an innovative, integrated research and education plan that includes the following components: (1) providing principled guidelines for reformulating problems into the multi-task learning formalism; (2) developing robust and clustered multi-task learning models to identify and prevent false interactions among unrelated tasks; (3) developing sparsity-inducing multi-task learning models to capture richly structured task interactions; (4) developing high-order multi-task learning models to capture task relatedness from interactions between features; and (5) investigating computational algorithms and theoretical properties of multi-task learning. The outcome of this project includes the capabilities of reformulating diverse machine learning problems into the multi-task learning framework and providing radically new ways to attack challenging problems that cannot be solved effectively by traditional methods. The systematic study of multi-task learning in this project is expected to generate novel reformulations, structured mathematical models, efficient optimization algorithms, and principled theoretical analyses, which will lead to significant practical and theoretical advances in multi-task learning.</AbstractNarration>
<MinAmdLetterDate>12/18/2018</MinAmdLetterDate>
<MaxAmdLetterDate>12/18/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1908166</AwardID>
<Investigator>
<FirstName>Shuiwang</FirstName>
<LastName>Ji</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shuiwang Ji</PI_FULL_NAME>
<EmailAddress>sji@tamu.edu</EmailAddress>
<PI_PHON>9794581547</PI_PHON>
<NSF_ID>000572148</NSF_ID>
<StartDate>12/18/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas A&amp;M Engineering Experiment Station</Name>
<CityName>College Station</CityName>
<ZipCode>778454645</ZipCode>
<PhoneNumber>9798626777</PhoneNumber>
<StreetAddress>400 Harvey Mitchell Pkwy S</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>847205572</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEXAS A&amp;M ENGINEERING EXPERIMENT STATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042915991</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Texas A&M University Main Campus]]></Name>
<CityName>College Station</CityName>
<StateCode>TX</StateCode>
<ZipCode>778433112</ZipCode>
<StreetAddress><![CDATA[410C Harvey R. Bright Building]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~178564</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The primary goal of this project is to develop novel and significant advances in multi-task machine learning methods and apply these methods to solve problems in a variety of fields, including biology, neuroscience, and medicine. One of the challenging multi-task learning (MTL) problems is dense prediction, in which a prediction is required for each input unit, and the predictions of different units are correlated. The key idea of current methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this project, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained.</p> <p>Multi-task learning in the multi-modality settings is especially challenging. Multi-modality data are widely used in clinical applications, such as tumor detection and brain disease diagnosis. Different modalities can usually provide complementary information, which commonly leads to improved performance. However, some modalities are commonly missing for some subjects due to various technical and practical reasons. As a result, multi-modality data are usually incomplete, raising the multi-modality missing data completion problem. In this project, we formulate the problem as a conditional image generation task and propose an encoder-decoder deep neural network to tackle this problem. Specifically, the model takes the existing modality as input and generates the missing modality. By employing an auxiliary adversarial loss, our model is able to generate high-quality missing modality images. At the same time, we propose to incorporate the available category information of subjects in training to enable the model to generate more informative images.</p> <p>Deep attention networks can be applied for multi-task learning and are becoming increasingly powerful in solving challenging tasks in various fields, including natural language processing, and computer vision. Compared to convolution layers and recurrent neural layers, attention operators are able to capture long-range dependencies and relationships among input elements, thereby boosting performance. In addition to images and texts, attention operators are also applied on graphs. In graph attention operators (GAOs), each node in a graph attend to all neighboring nodes, including itself. By employing attention mechanism, GAOs enable learnable weights for neighboring feature vectors when aggregating information from neighbors. However, a practical challenge of using GAOs on graph data is that they consume excessive computational resources, including computational cost and memory usage. In this project, we propose novel hard graph attention operator (hGAO). hGAO performs attention operation by requiring each query node to only attend to part of neighboring nodes in graphs. By attending to the most important nodes, the responses of the query node are more accurate, thereby leading to better performance than methods based on soft attention. Compared to GAO, hGAO also saves computational cost by reducing the number of nodes to attend.</p> <p>We apply machine learning and multi-task learning to solve problems in brain science. By using the recently published Allen Brain Observatory dataset of large-scale calcium imaging of mouse V1 activities under visual stimuli, we were able to obtain unprecedented pictures of simultaneous neuronal activities at multiple sub-divisions and cortical depths of V1. By using multi-task learning, we conducted a comprehensive survey of the coding ability of multiple cortical locations toward different stimulus attributes. Specifically, we focused on orientations and spatial frequencies (for static stimuli), as well as moving directions and speed (for in-motion stimuli). By using results produced by a prediction model, we quantified the decoding performance profile at different sub-areas and layers of V1, revealing the structural map for holding specific visual information.</p><br> <p>            Last Modified: 08/01/2020<br>      Modified by: Shuiwang&nbsp;Ji</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The primary goal of this project is to develop novel and significant advances in multi-task machine learning methods and apply these methods to solve problems in a variety of fields, including biology, neuroscience, and medicine. One of the challenging multi-task learning (MTL) problems is dense prediction, in which a prediction is required for each input unit, and the predictions of different units are correlated. The key idea of current methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this project, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained.  Multi-task learning in the multi-modality settings is especially challenging. Multi-modality data are widely used in clinical applications, such as tumor detection and brain disease diagnosis. Different modalities can usually provide complementary information, which commonly leads to improved performance. However, some modalities are commonly missing for some subjects due to various technical and practical reasons. As a result, multi-modality data are usually incomplete, raising the multi-modality missing data completion problem. In this project, we formulate the problem as a conditional image generation task and propose an encoder-decoder deep neural network to tackle this problem. Specifically, the model takes the existing modality as input and generates the missing modality. By employing an auxiliary adversarial loss, our model is able to generate high-quality missing modality images. At the same time, we propose to incorporate the available category information of subjects in training to enable the model to generate more informative images.  Deep attention networks can be applied for multi-task learning and are becoming increasingly powerful in solving challenging tasks in various fields, including natural language processing, and computer vision. Compared to convolution layers and recurrent neural layers, attention operators are able to capture long-range dependencies and relationships among input elements, thereby boosting performance. In addition to images and texts, attention operators are also applied on graphs. In graph attention operators (GAOs), each node in a graph attend to all neighboring nodes, including itself. By employing attention mechanism, GAOs enable learnable weights for neighboring feature vectors when aggregating information from neighbors. However, a practical challenge of using GAOs on graph data is that they consume excessive computational resources, including computational cost and memory usage. In this project, we propose novel hard graph attention operator (hGAO). hGAO performs attention operation by requiring each query node to only attend to part of neighboring nodes in graphs. By attending to the most important nodes, the responses of the query node are more accurate, thereby leading to better performance than methods based on soft attention. Compared to GAO, hGAO also saves computational cost by reducing the number of nodes to attend.  We apply machine learning and multi-task learning to solve problems in brain science. By using the recently published Allen Brain Observatory dataset of large-scale calcium imaging of mouse V1 activities under visual stimuli, we were able to obtain unprecedented pictures of simultaneous neuronal activities at multiple sub-divisions and cortical depths of V1. By using multi-task learning, we conducted a comprehensive survey of the coding ability of multiple cortical locations toward different stimulus attributes. Specifically, we focused on orientations and spatial frequencies (for static stimuli), as well as moving directions and speed (for in-motion stimuli). By using results produced by a prediction model, we quantified the decoding performance profile at different sub-areas and layers of V1, revealing the structural map for holding specific visual information.       Last Modified: 08/01/2020       Submitted by: Shuiwang Ji]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
