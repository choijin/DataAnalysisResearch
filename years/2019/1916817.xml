<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER:Cross-Core Learning in Future Manycore Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2019</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>126608.00</AwardTotalIntnAmount>
<AwardAmount>126608</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anindya Banerjee</SignBlockName>
<PO_EMAI>abanerje@nsf.gov</PO_EMAI>
<PO_PHON>7032927885</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As computing devices solve increasingly complex and diverse problems, engineers seek to design processors that provide higher performance, while remaining energy-efficient for environmental reasons. To achieve this, processor vendors have embraced manycore devices, where thousands of cores cooperate on a single chip to solve large-scale problems in a parallel manner. They have further incorporated heterogeneity, combining cores with different architectures on a single chip in a bid to provide ever-increasing performance per watt. This project boosts the search for higher energy-efficient performance by inventing novel cross-core learning techniques. Cores in current chips individually learn about the behavior of parallel programs in order to run programs more efficiently in the future, devoting complex and power-hungry hardware structures to do this. However, this research observes that parallel programs tend to exercise the hardware structures of different cores in correlated ways, meaning that the behavior of the program run on one core can be communicated to other cores for various performance and power benefits. As such, this form of intelligent cross-core information exchange is effective in achieving high performance per watt across computing domains from datacenters to embedded systems&lt;br/&gt;&lt;br/&gt;In this light, this research provides techniques to deduce how similarly a parallel program's various threads exercise their cores' hardware structures (looking at a range of different programmer, compiler, and architectural mechanisms to do so). When this is detected, cross-core learning hardware gleans the information that is most useful to exchange to improve performance or power, and then transmits this information among heterogeneous cores using low-overhead hardware/software techniques. This project develops a lightweight runtime software layer to orchestrate this information exchange, relying on dedicated hardware support when necessary. Through developing this framework, cross-core learning is applied to a number of specific cases, ranging from higher-performance manycore cache prefetching and branch prediction, to performance and power-management techniques for interrupts and exceptions in scale-out systems, as well as thread and instruction scheduling.  Furthermore, this project heavily disseminates knowledge on how to design and program large-scale manycore systems (or scale-out systems) by involving students at the graduate, undergraduate, and high-school levels through active research and coursework. Overall, this work impacts the engineering community and broader society by: (1) helping to achieve high-performance, but also energy-efficient and environmentally-friendly computing systems; (2) providing academics and chip designers a design methodology and infrastructure to study manycore design; (3) broadening the participation of underrepresented groups in computer science; (4) educating graduate, undergraduate, and high-school students on parallel programming for manycore systems.</AbstractNarration>
<MinAmdLetterDate>02/11/2019</MinAmdLetterDate>
<MaxAmdLetterDate>02/11/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1916817</AwardID>
<Investigator>
<FirstName>Abhishek</FirstName>
<LastName>Bhattacharjee</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Abhishek Bhattacharjee</PI_FULL_NAME>
<EmailAddress>abhishek.bhattacharjee@yale.edu</EmailAddress>
<PI_PHON>2034326403</PI_PHON>
<NSF_ID>000580469</NSF_ID>
<StartDate>02/11/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Yale University</Name>
<CityName>New Haven</CityName>
<ZipCode>065208327</ZipCode>
<PhoneNumber>2037854689</PhoneNumber>
<StreetAddress>Office of Sponsored Projects</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 208327]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<StateCode>CT</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CT03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043207562</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>YALE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>043207562</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Department of Computer Science]]></Name>
<CityName>New Haven</CityName>
<StateCode>CT</StateCode>
<ZipCode>065118937</ZipCode>
<StreetAddress><![CDATA[51 Prospect Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CT03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~24324</FUND_OBLG>
<FUND_OBLG>2017~102283</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1">As Moore's Law and Dennard scaling have waned, the computer systems community has turned away from single-thread performance as a primary means of performance scaling, and has instead embraced multi-core architectures. Today, all major hardware vendors build platforms that integrate multiple cores in dual-, quad-, eight-core formations, even ranging in the tens of cores. Indeed, researchers have already begun actively exploring the challenges of building&nbsp;platforms with hundreds of cores. While this multi- and many-core era offers continued performance scaling, it does require software refactoring to take advantage of the parallel hardware resources. In other words, a single program must be refactored into multiple threads of execution to concurrently extract performance from different cores.</p> <p class="p1">This project's goal is to take advantage of the fact that a single program is decomposed into multiple threads, so that information gleaned by a single core about a thread of the program can be used to infer execution properties of other threads of the same program running on other cores. These cross-core learning techniques can then be used to accelerate performance of the overall system.<span>&nbsp;</span>&nbsp;</p> <p class="p1">Over the lifetime of this project, several opportunities for cross-core learning were investigated and leveraged for performance improvement. Most of these cross-core learning opportunities focused on hardware optimizations, but some of them also focused on operating system improvements. Moreover, the majority of techniques focused on the address translation stack of modern server systems. Address translation is vital to supporting modern virtual memory abstractions, but has become a significant performance, power, and area consumer, especially with the advent of big-data workloads that use ever-increasing memory footprints with increasingly poor locality of reference. Via this project, we present several innovations that contribute to better address translation efficiency, as measured along a number of these metrics. A running theme through these techniques is that cross-core learning is leveraged so that performance is improved not just on a single core, but also on other cores in the same socket and across sockets. And looking forward, although we focus on single platforms, we believe that these techniques can be extended to enable cross-server learning in order to improve the performance of large-scale data centers.</p> <p class="p1">Beyond the technical content of this work, this project has also led to the education of several generations of PhD and MS students, as well as undergraduates and even high-school students. Relevant training has been offered via research projects, theses, and course work at the graduate and undergraduate levels.<span>&nbsp;</span></p><br> <p>            Last Modified: 01/11/2021<br>      Modified by: Abhishek&nbsp;Bhattacharjee</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[As Moore's Law and Dennard scaling have waned, the computer systems community has turned away from single-thread performance as a primary means of performance scaling, and has instead embraced multi-core architectures. Today, all major hardware vendors build platforms that integrate multiple cores in dual-, quad-, eight-core formations, even ranging in the tens of cores. Indeed, researchers have already begun actively exploring the challenges of building platforms with hundreds of cores. While this multi- and many-core era offers continued performance scaling, it does require software refactoring to take advantage of the parallel hardware resources. In other words, a single program must be refactored into multiple threads of execution to concurrently extract performance from different cores. This project's goal is to take advantage of the fact that a single program is decomposed into multiple threads, so that information gleaned by a single core about a thread of the program can be used to infer execution properties of other threads of the same program running on other cores. These cross-core learning techniques can then be used to accelerate performance of the overall system.   Over the lifetime of this project, several opportunities for cross-core learning were investigated and leveraged for performance improvement. Most of these cross-core learning opportunities focused on hardware optimizations, but some of them also focused on operating system improvements. Moreover, the majority of techniques focused on the address translation stack of modern server systems. Address translation is vital to supporting modern virtual memory abstractions, but has become a significant performance, power, and area consumer, especially with the advent of big-data workloads that use ever-increasing memory footprints with increasingly poor locality of reference. Via this project, we present several innovations that contribute to better address translation efficiency, as measured along a number of these metrics. A running theme through these techniques is that cross-core learning is leveraged so that performance is improved not just on a single core, but also on other cores in the same socket and across sockets. And looking forward, although we focus on single platforms, we believe that these techniques can be extended to enable cross-server learning in order to improve the performance of large-scale data centers. Beyond the technical content of this work, this project has also led to the education of several generations of PhD and MS students, as well as undergraduates and even high-school students. Relevant training has been offered via research projects, theses, and course work at the graduate and undergraduate levels.        Last Modified: 01/11/2021       Submitted by: Abhishek Bhattacharjee]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
