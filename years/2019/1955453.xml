<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Two-dimensional Synaptic Array for Advanced Hardware Acceleration of Deep Neural Networks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>299996.00</AwardTotalIntnAmount>
<AwardAmount>299996</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Lawrence Goldberg</SignBlockName>
<PO_EMAI>lgoldber@nsf.gov</PO_EMAI>
<PO_PHON>7032928339</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Nontechnical:&lt;br/&gt;&lt;br/&gt;The big data revolution has created a critical need for new computing paradigms to efficiently extract valuable information from large datasets. In existing computing systems, data is constantly transferred between the computation and memory units. This so-called memory bottleneck limits their energy efficiency and speed. In contrast, computation and memory in the human brain (neurons and synapses) are closely and densely interconnected. This gives rise to the brainâ€™s extremely low power consumption at ~20W. Inspired by the brain, neuromorphic computing and artificial neural networks have recently attracted immense interest. In particular, deep neural networks (DNNs) can execute complex processing tasks such as pattern recognition and image reconstruction. However, DNNs are computationally intensive and power hungry. This makes it impractical for them to be scaled up to the level of the complexity for true artificial intelligence (AI). In this project, the team will develop a novel artificial synapse for deep neural networks. This prototypical synapse will offer low power consumption, high precision, good scalability, and great potential for large-scale integration. This work can lead to significant improvement in energy efficiency, bandwidth, and performance for deep learning algorithms. The research outcome can lead to the wide use of AI for both high-performance computing and low-power flexible electronics. This project can revolutionize society through advances in healthcare, self-driving vehicles, and autonomous manufacturing. The team will work closely with their local communities to attract students to pursue engineering careers, especially those  from underrepresented groups. Activities will include laboratory demonstrations, design projects, summer internships, and career workshops.&lt;br/&gt;&lt;br/&gt;Technical:&lt;br/&gt;&lt;br/&gt;The objective of this project is to develop scalable electrochemical two-dimensional (2D) synaptic arrays with high-precision and low-power for advanced hardware acceleration of deep neural networks (DNNs) with orders of magnitude improvements in energy and speed. While binary SRAM cells have shown promising performance for DNN hardware acceleration, its inherent limitations in power and area make it impractical to scale up to the complexity level required for large-scale problems and/or datasets. In this project, the team will take a holistic approach to develop scalable electrochemical 2D synaptic arrays with high precision, lower-power, good linearity, low variations, and CMOS compatibility for large-scale integration. The team will carry out the following three research tasks: (1) device-level optimization in device precision, dynamic range, and scaling; (2) array-level demonstration by building synaptic arrays, lowering device variations, and designing peripheral circuits; (3) system-level integration via building device models, implementing computing-in-memory (CIM), and demonstrating on-chip learning for pixel-to-pixel applications. This work will provide a low-power and scalable framework for the hardware acceleration of DNNs, paving the ways towards the ubiquitous use of artificial intelligence (AI) in both high-performance computers and low-power embedded systems.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>05/05/2020</MinAmdLetterDate>
<MaxAmdLetterDate>05/05/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1955453</AwardID>
<Investigator>
<FirstName>Feng</FirstName>
<LastName>Xiong</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Feng Xiong</PI_FULL_NAME>
<EmailAddress>f.xiong@pitt.edu</EmailAddress>
<PI_PHON>4123835360</PI_PHON>
<NSF_ID>000729579</NSF_ID>
<StartDate>05/05/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pittsburgh</Name>
<CityName>Pittsburgh</CityName>
<ZipCode>152133203</ZipCode>
<PhoneNumber>4126247400</PhoneNumber>
<StreetAddress>300 Murdoch Building</StreetAddress>
<StreetAddress2><![CDATA[3420 Forbes Avenue]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>004514360</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF PITTSBURGH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004514360</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pittsburgh]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152132303</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1517</Code>
<Text>EPMD-ElectrnPhoton&amp;MagnDevices</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>086Z</Code>
<Text>Neuromorphic Computing</Text>
</ProgramReference>
<ProgramReference>
<Code>100E</Code>
<Text>Novel devices &amp; vacuum electronics</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~299996</FUND_OBLG>
</Award>
</rootTag>
