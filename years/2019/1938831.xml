<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Activating Voice Journaling for Mental Health with Voice Biomarkers</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/15/2019</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>224772.00</AwardTotalIntnAmount>
<AwardAmount>249772</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Atherton</SignBlockName>
<PO_EMAI>patherto@nsf.gov</PO_EMAI>
<PO_PHON>7032928772</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to transform voice intonations into voice biomarkers to predict the existence of disease, monitor progression or deterioration of disease or chronic condition, predict hospitalization and mortality, focus on patients in need, and as a result, optimize care and cost. Stress, anxiety, and depression cost American employers an estimated $500 B annually in lost productivity.  Furthermore, eight risks and behaviors associated with mental health drive 15 chronic conditions, accounting for 80% the total costs for all chronic illnesses worldwide and representing a projected $47 T problem by 2030.  Voice signals indicate a variety of health conditions, emotions, and diseases.  While wearables are becoming a ubiquitous tool to assess physical variables, their ability to measure psychological variables remains limited.The company has developed a neural-network model specifically to analyze raw text and audio from natural conversation, discovering speech patterns indicative of depression. The company is advancing research on human emotion classifiers, the first of its study across international geographies; this project will combine sensor inputs for state-of-the-art machine learning language-based models, design a hyper-individualized behavioral recommendation system for stress triggers, and develop a quantitative measurement on mental health that is both scalable and personalized to address the marketplace gap between simple apps and advanced neuropsychiatric treatment. &lt;br/&gt;&lt;br/&gt;This Small Business Innovation Research (SBIR) Phase I project is dedicated to building a smart voice journaling platform utilizing voice biomarkers to measure and predict well-being.  The major research objectives in this proposal include (1) intuitive human-computer voice interactions through various smart devices including phones, earbuds, watches, home, and in-car audio, (2) developing, training, refining, and scaling custom neural networks, (3) creating deep reinforcement learning models to serve relevant recommendations and actions to users, and (4) building visual representations of progress from individual journal entries. The anticipated outcome of this innovation is a personalized deep learning-based system that can be scaled to smart devices on-demand and robust enough to cover diverse, multicultural backgrounds worldwide. This company is using sensors to deliver objective measurements, algorithms to support the physician and psychologist in their assessments and care delivery, and non-pharmacological health-supportive tools as an emerging category of digital therapeutics.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/26/2019</MinAmdLetterDate>
<MaxAmdLetterDate>05/04/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1938831</AwardID>
<Investigator>
<FirstName>Grace</FirstName>
<LastName>Chang</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Grace C Chang</PI_FULL_NAME>
<EmailAddress>grace@kintsugihello.com</EmailAddress>
<PI_PHON>3105981553</PI_PHON>
<NSF_ID>000801591</NSF_ID>
<StartDate>08/26/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Kintsugi Mindful Wellness, Inc.</Name>
<CityName>Berkeley</CityName>
<ZipCode>947051346</ZipCode>
<PhoneNumber>3105981553</PhoneNumber>
<StreetAddress>2737 Garber Street</StreetAddress>
<StreetAddress2><![CDATA[No. 8]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>117046652</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>KINTSUGI MINDFUL WELLNESS, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[CHANG, GRACE]]></Name>
<CityName>San Francisco</CityName>
<StateCode>CA</StateCode>
<ZipCode>941103320</ZipCode>
<StreetAddress><![CDATA[2790 Harrison Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramElement>
<Code>8091</Code>
<Text>SBIR Outreach &amp; Tech. Assist</Text>
</ProgramElement>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~224772</FUND_OBLG>
<FUND_OBLG>2020~25000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-05aeeaf6-7fff-0260-8fee-22b5b91e1c63"> </span></p> <p dir="ltr"><span>Kintsugi is the first talk therapy software to detect clinical anxiety and depression from 2-3 minutes of speaking and provides community support for its current users across 250 international cities through its voice journaling consumer application. Stressed individuals have a reliable tool to feel heard and understood, and healthcare practitioners are augmented with a real-time voice diagnostic via the Kintsugi API endpoint.&nbsp;</span></p> <p dir="ltr"><span>Kintsugi is utilizing voice biomarkers to measure and predict well-being, and the anticipated outcome of our Phase I project is creating a personalized deep learning-based system that can be scaled to smart devices on-demand and robust enough to cover diverse, multicultural backgrounds worldwide. Voice detects a variety of health conditions, emotions, and diseases. Transforming voice intonations into voice biomarkers could allow us to predict the existence of disease, monitor progression or deterioration of disease or chronic condition, predict hospitalization and mortality, focus on patients in need, and as a result, optimize care and cost.</span></p> <p dir="ltr"><span>Kintsugi has developed a neural-network model specifically for voice entries that can analyze audio signals from natural conversation to discover speech patterns indicative of depression. Kintsugi is advancing research on human emotion classifiers, the first of its study across international geographies, combining sensor inputs in creating state-of-the-art machine learning language-based models, designing a hyper-individualized behavioral recommendation system for stress triggers, and developing a quantitative measurement on mental health that is both scalable and personalized.</span></p> <p dir="ltr"><span>The major technical challenges in Phase I include the following:</span></p> <p>- Intuitive and engaging human-computer voice interactions through various smart devices including phones, earbuds, watches, home, and in-car audio<br />- Developing, training, refining, and scaling custom neural networks<br />- Creating deep reinforcement learning models to serve relevant recommendations and actions to users<br />- Building simplified visual representations of progress from individual journal entries</p> <ol> </ol> <p dir="ltr"><span>The evolution of our assumptions, hypotheses, and findings led us to a few new discoveries.&nbsp;</span></p> <p dir="ltr"><span>First, our focus on delivering a solid user experience for the iOS application was critical in executing on rapid build iterations and purposefully less on platform parity. We did not need to expend unnecessary resources just to maintain feature equivalence across multiple devices and inputs. We shipped 83 builds in the last year and 50 new builds in the last 10-months. We had substantial insights from the users on the platform that necessitated shipping new versions weekly. We grew users 100X in that duration.</span></p> <p dir="ltr"><span>Second, our assumption that a multimodal LSTM would yield more accurate classifications for depression and anxiety were disproven when we grew our annotated, longitudinal voice repository to the largest in the world. We developed an incredibly elegant model, pre-processing and analyzing voice intonations alone and beat state-of-the-art results with a fraction of the parameters in the thousands, compared to heftier language models like BERT with 300m parameters--minimizing latency and producing real-time predictions. Another benefit of our models is that they are language-agnostic. We are in the process of FDA De Novo, and our technology is U.S. patent-pending.</span></p> <p dir="ltr"><span>Third, deep reinforcement learning models in serving relevant clinical and nonclinical recommendations requires further exploration. Rules-based models are easier to deploy for learning users' preferences, and we've learned that people are more motivated by other people in the Kintsugi Community than even their own preferences. This has led us to exploring new features to refine a system of reward functions.</span></p> <p dir="ltr"><span>Fourth, simplified visual representations of progress are only meaningful if it is both informative and delightful. Gaming mechanics provide a goal and achievement marker; however, the efficacy and engagement wane over time. As we continue research into reward functions tied more closely with peer-to-peer interactions, this will continue to shape the visual representations of progress for our users.&nbsp;</span></p> <p dir="ltr"><span>Our team is pleased with the set of insights learned through scientific rigor, a deep curiosity for the truth, and genuine empathy to help others through difficult times, especially in light of today's rise of COVID-19 cases across the world. As we pursue Phase II, our team will continue efforts in developing the consumer application for improved access to mental health support outside doctor's visits, advance machine learning models in voice biomarker diagnostics to create a new standard of care, and integrate more fully across healthcare providers and stakeholders across the $47T chronic conditions market. We are grateful to have the opportunity to serve our community in a meaningful way that amplifies our spirit in scaling technology to augment human-efforts.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 06/16/2020<br>      Modified by: Grace&nbsp;C&nbsp;Chang</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1938831/1938831_10637645_1592275359850_Kintsugi--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1938831/1938831_10637645_1592275359850_Kintsugi--rgov-800width.jpg" title="Kintsugi App"><img src="/por/images/Reports/POR/2020/1938831/1938831_10637645_1592275359850_Kintsugi--rgov-66x44.jpg" alt="Kintsugi App"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Kintsugi in the Apple iTunes Store</div> <div class="imageCredit">Kintsugi</div> <div class="imageSubmitted">Grace&nbsp;C&nbsp;Chang</div> <div class="imageTitle">Kintsugi App</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Kintsugi is the first talk therapy software to detect clinical anxiety and depression from 2-3 minutes of speaking and provides community support for its current users across 250 international cities through its voice journaling consumer application. Stressed individuals have a reliable tool to feel heard and understood, and healthcare practitioners are augmented with a real-time voice diagnostic via the Kintsugi API endpoint.  Kintsugi is utilizing voice biomarkers to measure and predict well-being, and the anticipated outcome of our Phase I project is creating a personalized deep learning-based system that can be scaled to smart devices on-demand and robust enough to cover diverse, multicultural backgrounds worldwide. Voice detects a variety of health conditions, emotions, and diseases. Transforming voice intonations into voice biomarkers could allow us to predict the existence of disease, monitor progression or deterioration of disease or chronic condition, predict hospitalization and mortality, focus on patients in need, and as a result, optimize care and cost. Kintsugi has developed a neural-network model specifically for voice entries that can analyze audio signals from natural conversation to discover speech patterns indicative of depression. Kintsugi is advancing research on human emotion classifiers, the first of its study across international geographies, combining sensor inputs in creating state-of-the-art machine learning language-based models, designing a hyper-individualized behavioral recommendation system for stress triggers, and developing a quantitative measurement on mental health that is both scalable and personalized. The major technical challenges in Phase I include the following:  - Intuitive and engaging human-computer voice interactions through various smart devices including phones, earbuds, watches, home, and in-car audio - Developing, training, refining, and scaling custom neural networks - Creating deep reinforcement learning models to serve relevant recommendations and actions to users - Building simplified visual representations of progress from individual journal entries   The evolution of our assumptions, hypotheses, and findings led us to a few new discoveries.  First, our focus on delivering a solid user experience for the iOS application was critical in executing on rapid build iterations and purposefully less on platform parity. We did not need to expend unnecessary resources just to maintain feature equivalence across multiple devices and inputs. We shipped 83 builds in the last year and 50 new builds in the last 10-months. We had substantial insights from the users on the platform that necessitated shipping new versions weekly. We grew users 100X in that duration. Second, our assumption that a multimodal LSTM would yield more accurate classifications for depression and anxiety were disproven when we grew our annotated, longitudinal voice repository to the largest in the world. We developed an incredibly elegant model, pre-processing and analyzing voice intonations alone and beat state-of-the-art results with a fraction of the parameters in the thousands, compared to heftier language models like BERT with 300m parameters--minimizing latency and producing real-time predictions. Another benefit of our models is that they are language-agnostic. We are in the process of FDA De Novo, and our technology is U.S. patent-pending. Third, deep reinforcement learning models in serving relevant clinical and nonclinical recommendations requires further exploration. Rules-based models are easier to deploy for learning users' preferences, and we've learned that people are more motivated by other people in the Kintsugi Community than even their own preferences. This has led us to exploring new features to refine a system of reward functions. Fourth, simplified visual representations of progress are only meaningful if it is both informative and delightful. Gaming mechanics provide a goal and achievement marker; however, the efficacy and engagement wane over time. As we continue research into reward functions tied more closely with peer-to-peer interactions, this will continue to shape the visual representations of progress for our users.  Our team is pleased with the set of insights learned through scientific rigor, a deep curiosity for the truth, and genuine empathy to help others through difficult times, especially in light of today's rise of COVID-19 cases across the world. As we pursue Phase II, our team will continue efforts in developing the consumer application for improved access to mental health support outside doctor's visits, advance machine learning models in voice biomarker diagnostics to create a new standard of care, and integrate more fully across healthcare providers and stakeholders across the $47T chronic conditions market. We are grateful to have the opportunity to serve our community in a meaningful way that amplifies our spirit in scaling technology to augment human-efforts.          Last Modified: 06/16/2020       Submitted by: Grace C Chang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
