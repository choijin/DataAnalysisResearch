<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Visual Training in the Geosciences by Training Visual Working Memory</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>05/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>608087.00</AwardTotalIntnAmount>
<AwardAmount>608087</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Michael Ford</SignBlockName>
<PO_EMAI>miford@nsf.gov</PO_EMAI>
<PO_PHON>7032925153</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project identify ways to improve education and training in the geosciences, building on fundamental research in cognitive science. Geoscience is a STEM discipline that is of growing importance to several national and global issues, including climate change, energy resources, and understanding earthquake activity. Expertise in geoscience depends heavily upon unconscious perceptual skills that are difficult or impossible to impart through traditional classroom education. Consequently there is a great need to improve or develop new methods for perceptual training (i.e., training students to visually detect, identify, and interpret geologic processes) and to study how educational practices in this area may be optimized. This project will apply a cognitive science approach to meet this goal, by conducting behavioral experiments involving geoscience students, and developing computational models of human cognition. This project is supported by NSF's EHR Core Research (ECR) program. The ECR program emphasizes fundamental STEM education research that generates foundational knowledge in the field.&lt;br/&gt;&lt;br/&gt;The proposed research will focus on the role of visual working memory (VWM) in perceptual learning and perceptual expertise in the geosciences. VWM is a cognitive system that is critical to the geosciences. For example, when a geologist views or studies a landscape, he or she acquires visual information from multiple points in the scene by making numerous eye movements. The geologist can then (unconsciously) use his or her VWM to integrate information across eye movements to develop a coherent representation of the landscape. While there is much existing research on VWM, very little is known about the relationship between visual working memory on the one hand, and fundamental changes in perceptual ability in specific domains such as geoscience on the other hand. The proposed research will build upon, and extend a recently developed model of human perception that has the potential to connect these two domains. This model is based upon information theory, or the mathematical study of how physical systems can efficiently communicate information.  This model suggests that training and expertise fundamentally change a person?s perceptual system in a way that leads to more efficient use of cognitive resources. In order to test this model, a series of behavioral experiments will be conducted involving novice and experienced students in the geosciences. These experiments and computational modeling efforts will contribute fundamental knowledge that can be used to improve geoscience and education.</AbstractNarration>
<MinAmdLetterDate>02/06/2019</MinAmdLetterDate>
<MaxAmdLetterDate>07/02/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1915874</AwardID>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Sims</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher R Sims</PI_FULL_NAME>
<EmailAddress>simsc3@rpi.edu</EmailAddress>
<PI_PHON>5182762963</PI_PHON>
<NSF_ID>000693990</NSF_ID>
<StartDate>02/06/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rensselaer Polytechnic Institute</Name>
<CityName>Troy</CityName>
<ZipCode>121803522</ZipCode>
<PhoneNumber>5182766000</PhoneNumber>
<StreetAddress>110 8TH ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002430742</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RENSSELAER POLYTECHNIC INSTITUTE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002430742</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rensselaer Polytechnic Institute]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>121803522</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7980</Code>
<Text>ECR-EHR Core Research</Text>
</ProgramElement>
<ProgramReference>
<Code>8817</Code>
<Text>STEM Learning &amp; Learning Environments</Text>
</ProgramReference>
<Appropriation>
<Code>0416</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0417</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0419</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0420</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~159271</FUND_OBLG>
<FUND_OBLG>2017~148516</FUND_OBLG>
<FUND_OBLG>2019~149594</FUND_OBLG>
<FUND_OBLG>2020~150706</FUND_OBLG>
</Award>
</rootTag>
