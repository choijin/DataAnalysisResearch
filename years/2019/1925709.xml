<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CCRI: Planning: Establishing A Hand-Gesture Research Platform for Behavior Biometrics and Cognitive Robotics (HGRP)</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2019</AwardEffectiveDate>
<AwardExpirationDate>03/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>100000.00</AwardTotalIntnAmount>
<AwardAmount>100000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The hand is one of the most complex and beautiful pieces of natural engineering in the human body, and it represents a triumph of complex engineering, exquisitely evolved to perform a range of tasks. Hand-gestures have been used for many areas and how to model hand-gestures has broad and profound impact on addressing the nation's priorities and societal needs, e.g., manipulators and co-robot in manufacturing, natural human-computer interaction for virtual reality, wearable platforms in consumer electronics, hand-gesture biometrics in cybersecurity, etc.  In this project, one of the research goals is to build a hand-gesture research platform, Hand-Gesture Research Platform (HGRP). HGRP targets at enabling researchers to easily access various hand-gesture data to validate their hand-gesture recognition models, benchmark the performance of newly developed algorithms, and compare with research outcomes from others. Moreover, HGRP is used to gather research communities' feedback based on existing cutting-edge hand-gesture research to prioritize the need on establishing a hand-gesture focused computing research infrastructure.&lt;br/&gt;&lt;br/&gt;The HGRP framework is based on a cloud computing platform and is used to enable research capabilities in the following areas: (a) hand-gesture biometrics; (b) cognitive Robotics; and (c) programmable interfaces for gesture-based data processing and visualization. HGRP is composed by the following salient features:&lt;br/&gt;&lt;br/&gt;*Data collection based on two major types of sensors: (a) motion detection sensors, e.g., wearable sensors such as watch, wrist band, on figure sensors, data motion gloves, infrared motion detection sensors, etc., and (b) video sensors such as leap motion sensors, video recorders, etc. The detected hand-gesture data is sent to data storage for processing and storing. &lt;br/&gt;&lt;br/&gt;*Data are collected and stored on an objective storage service, and frequently used data is stored in memory storage. &lt;br/&gt;&lt;br/&gt;*A GPU-based private cloud is established to allow researchers to implement well- known hand-gesture data processing models and establish benchmarking models. &lt;br/&gt;&lt;br/&gt;HGRP also allows researchers to submit computation tasks for evaluations and comparative studies. HGRP provides a web-based data collection, processing, sharing, and storing APIs that allow researchers remotely to access the hand-gesture repository for data retrieval, processing, sharing, storing through web services APIs.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/16/2019</MinAmdLetterDate>
<MaxAmdLetterDate>08/16/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1925709</AwardID>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Gould</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard A Gould</PI_FULL_NAME>
<EmailAddress>ragould@asu.edu</EmailAddress>
<PI_PHON>6026844767</PI_PHON>
<NSF_ID>000797078</NSF_ID>
<StartDate>08/16/2019</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dijiang</FirstName>
<LastName>Huang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dijiang Huang</PI_FULL_NAME>
<EmailAddress>dijiang@asu.edu</EmailAddress>
<PI_PHON>4809652776</PI_PHON>
<NSF_ID>000398949</NSF_ID>
<StartDate>08/16/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Yezhou</FirstName>
<LastName>Yang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yezhou Yang</PI_FULL_NAME>
<EmailAddress>yz.yang@asu.edu</EmailAddress>
<PI_PHON>4809655479</PI_PHON>
<NSF_ID>000733585</NSF_ID>
<StartDate>08/16/2019</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Arizona State University</Name>
<CityName>TEMPE</CityName>
<ZipCode>852816011</ZipCode>
<PhoneNumber>4809655479</PhoneNumber>
<StreetAddress>ORSPA</StreetAddress>
<StreetAddress2><![CDATA[660 South Mill Avenue, Suite 310]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>943360412</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ARIZONA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>806345658</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Arizona State University]]></Name>
<CityName>Tempe</CityName>
<StateCode>AZ</StateCode>
<ZipCode>852816011</ZipCode>
<StreetAddress><![CDATA[PO Box 876011]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~100000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The proposed HGRP framework is established based on a cloud computing platform and it is used to enable research capabilities in the following areas: (a) hand-gesture biometrics; (b) cognitive Robotics; and (c) programmable interfaces for gesture-based data processing and visualization. HGRP is composed of the following major components:</p> <ul> <li>Data collection based on two major      types of sensors: (a) motion detection sensors, e.g., wearable sensors      such as a watch, wrist band, on figure sensors, data motion gloves,      infrared motion detection sensors, etc., and (b) video sensors such as      leap motion sensors, video recorders, etc. The detected hands-gesture data      is sent to data storage for processing and storing.</li> <li>A GPU-based private cloud is      established to allow researchers to implement well- known hand-gesture      data processing models and establish benchmarking models. HGRP also allows      researchers to submit computation tasks for evaluations and comparative      studies.</li> <li>HGRP provides a web-based      data repository that allow researchers remotely to access the hand-gesture      data for research.</li> </ul> <p>We organized&nbsp;the outcomes of this project in a more user-friendly way. The generated datasets and tool kits are collectively called FMKit. It includes a code library and data repository for finger motion-based in-air-handwriting analysis. For details of the FMKit project, please refer to the project home page and a 2-minute short video introduction:</p> <ul> <li> Project homepage:&nbsp;<a href="https://urldefense.com/v3/__https:/github.com/duolu/fmkit__;!!IKRxdwAv5BmarQ!KBgZFjQAPurhJ__RUgLf19bX1uKJQE43kfFmlQLFRobxak8QjJ8qzi0c6JklR3x4BULxQA$">https://github.com/duolu/fmkit</a></li> <li> Introduction video:&nbsp;<a href="https://urldefense.com/v3/__https:/youtu.be/O3Jqq9yqJSE__;!!IKRxdwAv5BmarQ!KBgZFjQAPurhJ__RUgLf19bX1uKJQE43kfFmlQLFRobxak8QjJ8qzi0c6JklR3zrpwDiLw$">https://youtu.be/O3Jqq9yqJSE</a></li> <li>Research publication      describing the dataset: Duo Lu, Linzhen Luo, Dijiang Huang, Yezhou Yang,      "FMKit: An In-Air-Handwriting Analysis Library and Data      Repository."&nbsp;CVPR Workshop on Computer Vision for Augmented and      Virtual Reality,<em>&nbsp;2020.</em>&nbsp;(<a href="https://github.com/duolu/fmkit/blob/master/papers/fmkit.pdf">https://github.com/duolu/fmkit/blob/master/papers/fmkit.pdf</a>&nbsp;)&nbsp;<a href="https://mixedreality.cs.cornell.edu/workshop/2020/papers#block-93cead2afaf5f6895a67">[link]</a>&nbsp;</li> <li>FMKit project      documentation:&nbsp;<a href="https://urldefense.com/v3/__https:/duolu-fmkit.github.io/__;!!IKRxdwAv5BmarQ!KBgZFjQAPurhJ__RUgLf19bX1uKJQE43kfFmlQLFRobxak8QjJ8qzi0c6JklR3yjB0WFag$">https://duolu-fmkit.github.io/</a></li> </ul> <ul> </ul> <ul> </ul> <p>FMKit contains five datasets of in-air-handwriting signals collected from over 200 participants with two different hand motion capture devices. All datasets are openly and freely available to researchers. For details of the datasets, please refer to:</p> <ul> <li>FMKits data repository      description&nbsp;<a href="https://github.com/duolu/fmkit#the-data-repository">https://github.com/duolu/fmkit#the-data-repository</a></li> </ul> <p>To download the data repository for research and evaluation purposes, the project team provides a dataset access procedure by submitting a data acquisition letter to&nbsp;<a href="mailto:fmkit@googlegroups.com">fmkit@googlegroups.com</a>&nbsp;to request access. The datasets are free of access and use for research and evaluation purposes.</p> <ul> <li>Application letter: &nbsp;<a href="https://urldefense.com/v3/__https:/docs.google.com/document/d/1AHX3lj1mjm4ZZEZTHNdm3xDmJAAWi7P6bIdLlBNZ8PA/edit?usp=sharing__;!!IKRxdwAv5BmarQ!KBgZFjQAPurhJ__RUgLf19bX1uKJQE43kfFmlQLFRobxak8QjJ8qzi0c6JklR3xY5EYdOA$">https://docs.google.com/document/d/1AHX3lj1mjm4ZZEZTHNdm3xDmJAAWi7P6bIdLlBNZ8PA/edit?usp=sharing</a></li> </ul> <p>A new dataset, called the word-210 dataset, can be downloaded directly without needing the data access application. Word-210 dataset includes 10 users? in-air handwriting of 210 English words and 210 Chinese words and each with 5 repetitions:&nbsp;</p> <ul> <li>Word-210 dataset:&nbsp;<a href="https://www.thothlab.com/getgooglefile/1RXj0t8NMYt_Jr5lW-BIeAIxw4aX5VshV">https://www.thothlab.com/getgooglefile/1RXj0t8NMYt_Jr5lW-BIeAIxw4aX5VshV</a></li> </ul> <p>FMKit also provides a Python code library to process the signals and interface with other Python tools. You can download the FMKit codes at the following links:</p> <ul> <li><a href="https://github.com/duolu/fmkit/tree/master/code_fmkit">https://github.com/duolu/fmkit/tree/master/code_fmkit</a></li> </ul> <p>Hand-gesture research is an emerging inter-disciplinary research area. It has been actively involved research areas such as motion detection, visualization, robotics, ML/AI, and cybersecurity. Newly developed models and solutions including behavioral modeling and ML/AI-based solutions to make the hand-gesture applications more adaptive and easier to use.&nbsp; The developed education materials will significantly promote the research in these directions.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/03/2021<br>      Modified by: Dijiang&nbsp;Huang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The proposed HGRP framework is established based on a cloud computing platform and it is used to enable research capabilities in the following areas: (a) hand-gesture biometrics; (b) cognitive Robotics; and (c) programmable interfaces for gesture-based data processing and visualization. HGRP is composed of the following major components:  Data collection based on two major      types of sensors: (a) motion detection sensors, e.g., wearable sensors      such as a watch, wrist band, on figure sensors, data motion gloves,      infrared motion detection sensors, etc., and (b) video sensors such as      leap motion sensors, video recorders, etc. The detected hands-gesture data      is sent to data storage for processing and storing. A GPU-based private cloud is      established to allow researchers to implement well- known hand-gesture      data processing models and establish benchmarking models. HGRP also allows      researchers to submit computation tasks for evaluations and comparative      studies. HGRP provides a web-based      data repository that allow researchers remotely to access the hand-gesture      data for research.   We organized the outcomes of this project in a more user-friendly way. The generated datasets and tool kits are collectively called FMKit. It includes a code library and data repository for finger motion-based in-air-handwriting analysis. For details of the FMKit project, please refer to the project home page and a 2-minute short video introduction:   Project homepage: https://github.com/duolu/fmkit  Introduction video: https://youtu.be/O3Jqq9yqJSE Research publication      describing the dataset: Duo Lu, Linzhen Luo, Dijiang Huang, Yezhou Yang,      "FMKit: An In-Air-Handwriting Analysis Library and Data      Repository." CVPR Workshop on Computer Vision for Augmented and      Virtual Reality, 2020. (https://github.com/duolu/fmkit/blob/master/papers/fmkit.pdf ) [link]  FMKit project      documentation: https://duolu-fmkit.github.io/       FMKit contains five datasets of in-air-handwriting signals collected from over 200 participants with two different hand motion capture devices. All datasets are openly and freely available to researchers. For details of the datasets, please refer to:  FMKits data repository      description https://github.com/duolu/fmkit#the-data-repository   To download the data repository for research and evaluation purposes, the project team provides a dataset access procedure by submitting a data acquisition letter to fmkit@googlegroups.com to request access. The datasets are free of access and use for research and evaluation purposes.  Application letter:  https://docs.google.com/document/d/1AHX3lj1mjm4ZZEZTHNdm3xDmJAAWi7P6bIdLlBNZ8PA/edit?usp=sharing   A new dataset, called the word-210 dataset, can be downloaded directly without needing the data access application. Word-210 dataset includes 10 users? in-air handwriting of 210 English words and 210 Chinese words and each with 5 repetitions:   Word-210 dataset: https://www.thothlab.com/getgooglefile/1RXj0t8NMYt_Jr5lW-BIeAIxw4aX5VshV   FMKit also provides a Python code library to process the signals and interface with other Python tools. You can download the FMKit codes at the following links:  https://github.com/duolu/fmkit/tree/master/code_fmkit   Hand-gesture research is an emerging inter-disciplinary research area. It has been actively involved research areas such as motion detection, visualization, robotics, ML/AI, and cybersecurity. Newly developed models and solutions including behavioral modeling and ML/AI-based solutions to make the hand-gesture applications more adaptive and easier to use.  The developed education materials will significantly promote the research in these directions.           Last Modified: 06/03/2021       Submitted by: Dijiang Huang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
