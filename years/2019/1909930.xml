<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Collaborative Research: Improving Mobile Device Input for Users who are Blind or Low Vision</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2019</AwardEffectiveDate>
<AwardExpirationDate>09/30/2022</AwardExpirationDate>
<AwardTotalIntnAmount>274000.00</AwardTotalIntnAmount>
<AwardAmount>290000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Smartphones are an essential part of everyday life. But for people with visual impairments, basic tasks like composing text messages or browsing the web can be prohibitively slow and difficult. The goal of this project is to develop accessible text entry methods that will enable people with visual impairments to enter text at rates comparable to sighted people. This project will design new algorithms and feedback methods for today's standard text entry approaches of tapping on individual keys, gesturing across keys, or dictating via speech. The project aims to help users avoid errors by enabling more accurate input via audio and tactile feedback, help users find errors by providing audio and visual annotation of uncertain portions of the text, and help users correct errors by combining the probabilistic information from the original input, the correction, and approximate information about an error's location. Improving text entry methods for people who are blind or have low vision will enable them to use their mobile devices more effectively for work and leisure. Thus, this project represents an important step to achieving equity for people with visual impairments. &lt;br/&gt;&lt;br/&gt;This project will contribute novel interface designs to the accessibility and human-computer interaction literature. It will advance the state-of-the-art in mobile device accessibility by, first, studying text entry accessibility for low vision in addition to blind people. Next, the researchers will study and develop accessible gesture typing input methods.  Finally, the project will develop accessible speech input methods.  This project will produce design guidelines, feedback methods, input techniques, recognition algorithms, user study results, and software prototypes that will guide improvements to research and commercial input systems for users who are blind or low-vision. Further, the project's work on the error correction and revision process will improve the usability and performance of touchscreen and speech input methods for everyone.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/26/2019</MinAmdLetterDate>
<MaxAmdLetterDate>04/10/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1909930</AwardID>
<Investigator>
<FirstName>Shiri</FirstName>
<LastName>Azenkot</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shiri Azenkot</PI_FULL_NAME>
<EmailAddress>shiri.azenkot@cornell.edu</EmailAddress>
<PI_PHON>6072555014</PI_PHON>
<NSF_ID>000690878</NSF_ID>
<StartDate>08/26/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<CountyName>TOMPKINS</CountyName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell Tech]]></Name>
<CityName>New York</CityName>
<CountyName>NEW YORK</CountyName>
<StateCode>NY</StateCode>
<ZipCode>100441501</ZipCode>
<StreetAddress><![CDATA[2 West Loop Rd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~274000</FUND_OBLG>
<FUND_OBLG>2020~16000</FUND_OBLG>
</Award>
</rootTag>
