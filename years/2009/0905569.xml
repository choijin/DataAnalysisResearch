<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Medium: Collaborative Research: Generating Effective Dynamic Explanations in Augmented Reality</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2009</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>803216.00</AwardTotalIntnAmount>
<AwardAmount>803216</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>To survive and flourish, people must interact with their environment in an organized fashion. To do so, they need to learn, imagine, and perform an assortment of transformations on and in the world. Primary among these are manipulation of objects and navigation in space. This project integrates research in computer science and cognitive science to develop and evaluate augmented reality tools to create effective dynamic explanations that enhance manipulation and navigation, in conjunction with identification and visualization. Augmented reality refers to user interfaces in which virtual material is integrated with and overlaid on the user?s experience of the real world; for example, by using tracked head-worn and hand-held displays. Dynamic explanations are task-appropriate sequences of actions, presented interactively, with appropriate added information. The tools will be created in collaboration with subject matter experts for exploratory use in indoor and outdoor real world domains: navigating and identifying landmarks in a wooded park area, assembling a piece of furniture, and navigating and visualizing for planning the site of a new urban campus. Cognitive science research will determine the best ways to convey explanations and information to people. Computer science research will address the design and implementation of systems that embody the best candidate approaches for identifying objects and locations, specifying actions, and adding non-visible information. In situ experiments will be used to assess and refine the systems.  &lt;br/&gt;&lt;br/&gt;Manipulation, navigation, identification, and visualization are representative of important things that people do every day, ranging from fixing broken equipment to reaching a desired destination in an unfamiliar environment. The ways in which we perform these tasks could potentially be improved significantly through augmented reality systems designed using the principles to be developed by this project. Both the cognitive principles and the augmented reality tools will have broad applicability. The systems developed will inform the design of future systems that can aid the general public, for educational and recreational ends, as well as systems that can assist people with auditory, visual, or physical impairments.</AbstractNarration>
<MinAmdLetterDate>08/06/2009</MinAmdLetterDate>
<MaxAmdLetterDate>07/23/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0905569</AwardID>
<Investigator>
<FirstName>Steven</FirstName>
<LastName>Feiner</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Steven K Feiner</PI_FULL_NAME>
<EmailAddress>feiner@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397083</PI_PHON>
<NSF_ID>000121899</NSF_ID>
<StartDate>08/06/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName>NEW YORK</CityName>
<StateCode>NY</StateCode>
<ZipCode>100276902</ZipCode>
<StreetAddress><![CDATA[2960 Broadway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~137853</FUND_OBLG>
<FUND_OBLG>2010~216086</FUND_OBLG>
<FUND_OBLG>2011~224251</FUND_OBLG>
<FUND_OBLG>2012~225026</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Augmented Reality (AR) overlays graphics and other media on our view of the world, using devices ranging from existing hand-held displays, to head-worn displays that will someday become commonplace.&nbsp; This can make it possible to present people with needed information about important tasks indoors and outdoors that the surrounding environment does not provide. But, what information should be added and how? Added information must be timely, important, and easily understood. Developing effective tools for augmenting reality relies on insights into aspects of human cognition and communication that this project has investigated. The tasks that we have addressed reflect many of the everyday and technical tasks that people need to understand and perform sequences of purposeful actions in context. Therefore, the cognitive design principles derived from the research are applicable to a broad range of domains.</p> <p>&nbsp;</p> <p><strong>Intellectual Merit</strong></p> <p>Our work resulted in the development of a number of novel interaction and visualization techniques, embodied and evaluated in working research prototypes, which we describe here and illustrate in the accompanying figures:</p> <p><strong>A research testbed for assisting users in performing maintenance and assembly tasks using AR. </strong>Our testbed tracks the user and multiple components in a maintenance and assembly task, and provides dynamic, prescriptive, overlaid instructions on a head-tracked, see-through, head-worn display, in response to the user&rsquo;s ongoing activity. A formal user study showed participants were able to complete important parts of a realistic assembly task significantly faster and with significantly greater accuracy than when using 3D-graphics&ndash;based assistance presented on a stationary LCD. Qualitative questionnaire results indicated that participants overwhelmingly preferred the AR approach, and ranked it as more intuitive than using the LCD.</p> <p><strong>Methods by which a user can be efficiently guided to assume one of a range of acceptable viewing positions and orientations.</strong> We introduced ParaFrustum, a way to geometrically represent the set of strategic viewpoints and orientations from which a task can be accomplished. We developed a set of visualizations that communicate this information to a user in AR, and conducted a study that showed the advantages of visualizing loose constraints on position and orientation for tasks for which they are appropriate.</p> <p><strong>An exploration of how AR can be used to augment and virtually revisit views other than a user&rsquo;s first-person view.</strong> We developed SnapAR, a new approach combining AR and users&rsquo; snapshots. It allows users to take snapshots of augmented scenes that can be virtually revisited at later times. The system stores still images of scenes along with camera poses, so that augmentations remain dynamic and interactive. This makes it possible for users to manipulate virtual objects while viewing snapshots taken at different perspectives, instead of physically moving to the real-world viewpoints. We conducted a formal user study comparing performance in virtual snapshot and physical travel conditions in a task in which a virtual object must be aligned with two pairs of physical objects. Proper alignment requires sequentially visiting two viewpoints, either vicariously through snapshots or physically through actual travel. Participants completed the alignment task significantly faster and more accurately using snapshots than when physically traveling. Moreover, participants preferred manipulating virtual objects using snapshots over physical travel.</p> <p><strong>Methods for collaborative task performance among co-located users and remote users.</strong> We developed approaches for communicating references to and operations on physical objects between pairs of co-located users and p...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Augmented Reality (AR) overlays graphics and other media on our view of the world, using devices ranging from existing hand-held displays, to head-worn displays that will someday become commonplace.  This can make it possible to present people with needed information about important tasks indoors and outdoors that the surrounding environment does not provide. But, what information should be added and how? Added information must be timely, important, and easily understood. Developing effective tools for augmenting reality relies on insights into aspects of human cognition and communication that this project has investigated. The tasks that we have addressed reflect many of the everyday and technical tasks that people need to understand and perform sequences of purposeful actions in context. Therefore, the cognitive design principles derived from the research are applicable to a broad range of domains.     Intellectual Merit  Our work resulted in the development of a number of novel interaction and visualization techniques, embodied and evaluated in working research prototypes, which we describe here and illustrate in the accompanying figures:  A research testbed for assisting users in performing maintenance and assembly tasks using AR. Our testbed tracks the user and multiple components in a maintenance and assembly task, and provides dynamic, prescriptive, overlaid instructions on a head-tracked, see-through, head-worn display, in response to the userÆs ongoing activity. A formal user study showed participants were able to complete important parts of a realistic assembly task significantly faster and with significantly greater accuracy than when using 3D-graphics&ndash;based assistance presented on a stationary LCD. Qualitative questionnaire results indicated that participants overwhelmingly preferred the AR approach, and ranked it as more intuitive than using the LCD.  Methods by which a user can be efficiently guided to assume one of a range of acceptable viewing positions and orientations. We introduced ParaFrustum, a way to geometrically represent the set of strategic viewpoints and orientations from which a task can be accomplished. We developed a set of visualizations that communicate this information to a user in AR, and conducted a study that showed the advantages of visualizing loose constraints on position and orientation for tasks for which they are appropriate.  An exploration of how AR can be used to augment and virtually revisit views other than a userÆs first-person view. We developed SnapAR, a new approach combining AR and usersÆ snapshots. It allows users to take snapshots of augmented scenes that can be virtually revisited at later times. The system stores still images of scenes along with camera poses, so that augmentations remain dynamic and interactive. This makes it possible for users to manipulate virtual objects while viewing snapshots taken at different perspectives, instead of physically moving to the real-world viewpoints. We conducted a formal user study comparing performance in virtual snapshot and physical travel conditions in a task in which a virtual object must be aligned with two pairs of physical objects. Proper alignment requires sequentially visiting two viewpoints, either vicariously through snapshots or physically through actual travel. Participants completed the alignment task significantly faster and more accurately using snapshots than when physically traveling. Moreover, participants preferred manipulating virtual objects using snapshots over physical travel.  Methods for collaborative task performance among co-located users and remote users. We developed approaches for communicating references to and operations on physical objects between pairs of co-located users and pairs of physically remote users in AR.  A research testbed for developing hybrid user interfaces that combine multiple complementary displays. We investigated how an urban environment, presented on a shared horizontal...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
