<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Medium:  New Directions in Computational Complexity</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>599990.00</AwardTotalIntnAmount>
<AwardAmount>599990</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Studies in computational complexity in three directions are proposed: &lt;br/&gt;holographic algorithms, Darwinian evolution, and multicore algorithms.&lt;br/&gt;In the first of these areas, holographic reductions have been shown to&lt;br/&gt;be a fruitful source of new efficient algorithms for certain problems,&lt;br/&gt;and evidence of intractability for othrs. In this research the aim is to&lt;br/&gt;arrive at a better understanding of the possibilities and limitations of&lt;br/&gt;holographic algorithms, by exploring ways in which specific currently&lt;br/&gt;known limitations of this class of methods can be circumvented. For&lt;br/&gt;evolution the goal is to understand better what classes of mechanisms&lt;br/&gt;can evolve through the Darwinian processes of variation and selection&lt;br/&gt;when only feasible resources in terms of population sizes and numbers of&lt;br/&gt;generations are available. In the area of multi-core algorithms, a&lt;br/&gt;methodology will be developed for expressing and analyzing parallel&lt;br/&gt;algorithms that are optimal for a wide range of hardware performance&lt;br/&gt;parameters. Such algorithms would make possible portable software, that&lt;br/&gt;is aware of the parameters of the machine on which it executes, and can&lt;br/&gt;run efficiently on all such machines.&lt;br/&gt;&lt;br/&gt;The work on multi-core algorithms aims to have the practical goal of&lt;br/&gt;increasing the effective exploitation of multi-core computers as these&lt;br/&gt;become more pervasive. The work on evolution will highlight the fact&lt;br/&gt;that the question of how complex mechanisms could have evolved within&lt;br/&gt;the resources available, is a question that is resolvable by the methods&lt;br/&gt;of computational complexity, and aims to provide more precise&lt;br/&gt;mathematical specifications of what the Darwinian process can achieve.&lt;br/&gt;The work on holographic algorithms aims to make progress in our&lt;br/&gt;understanding of what are widely regarded as the most fundamental&lt;br/&gt;questions regarding the power of practical computation.</AbstractNarration>
<MinAmdLetterDate>04/29/2010</MinAmdLetterDate>
<MaxAmdLetterDate>04/29/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0964401</AwardID>
<Investigator>
<FirstName>Leslie</FirstName>
<LastName>Valiant</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Leslie G Valiant</PI_FULL_NAME>
<EmailAddress>valiant@seas.harvard.edu</EmailAddress>
<PI_PHON>6174955817</PI_PHON>
<NSF_ID>000207249</NSF_ID>
<StartDate>04/29/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard University]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021385369</ZipCode>
<StreetAddress><![CDATA[1033 MASSACHUSETTS AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7927</Code>
<Text>COMPLEXITY &amp; CRYPTOGRAPHY</Text>
</ProgramElement>
<ProgramElement>
<Code>7934</Code>
<Text>PARAL/DISTRIBUTED ALGORITHMS</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~599990</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><br /><br />Computational complexity is the study of the resources needed for realizing computations. The most frequently studied resource is the number of computational steps, but other important resources are storage space and the number of operations that can be done in parallel. For any computational task one seeks to understand the resources needed. A positive result shows, for example, that for that task a certain number of steps are sufficient. The development of a new algorithm is often justified by a proof that it takes fewer steps than any previously known algorithm. In the opposite direction, negative results that show that any algorithm for a task needs infeasibly many steps can be also of great practical relevance if the task is one we do not wish anyone to be able to accomplish, such as breaking a cryptographic code.<br /><br />One thrust of the work performed is in the area of holographic algorithms. This is an approach to algorithms, suggested by this investigator, that is partially inspired by the mathematics of quantum computation, but aimed at conventional computers. In this framework algorithms have been found for certain problems that are exponentially faster than any previously known. In a different direction this framework has been shown to be useful for proving that pairs of apparently dissimilar tasks are in fact identical, or at least of equal computational difficulty. Further, these two aspects can be sometimes put together to prove dichotomy theorems, that show that for certain classes of tasks, each task is either feasibly computable, or equivalent to a single task that is suspected to be intractable. Such dichotomy theorems enable the problem of identifying the computational complexity of a new task to be resolved routinely, without requiring new research.<br /><br />A second thrust has been in biology. Biological processes at every level can be regarded as being logically complex, and as realizing complex computations. This investigator has suggested that any mechanism that might realize biological evolution can be viewed as a learning mechanism, in the same technical sense as in the field of machine learning. Thus the quantitative capabilities and limitations of any particular realization of the general Darwinian mechanism, might be explored within the better understood framework of machine learning. In neuroscience the fact that the biological processes can be regarded as computations have been appreciated for much longer, but no quantitatively satisfactory theory of how the human brain accomplishes basic learning and memory in the manner that it does has wide acceptance. In this project a new suggestion is made of how the hippocampal system might be helping the cortex in realizing these tasks.</p><br> <p>            Last Modified: 10/05/2015<br>      Modified by: Leslie&nbsp;G&nbsp;Valiant</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Computational complexity is the study of the resources needed for realizing computations. The most frequently studied resource is the number of computational steps, but other important resources are storage space and the number of operations that can be done in parallel. For any computational task one seeks to understand the resources needed. A positive result shows, for example, that for that task a certain number of steps are sufficient. The development of a new algorithm is often justified by a proof that it takes fewer steps than any previously known algorithm. In the opposite direction, negative results that show that any algorithm for a task needs infeasibly many steps can be also of great practical relevance if the task is one we do not wish anyone to be able to accomplish, such as breaking a cryptographic code.  One thrust of the work performed is in the area of holographic algorithms. This is an approach to algorithms, suggested by this investigator, that is partially inspired by the mathematics of quantum computation, but aimed at conventional computers. In this framework algorithms have been found for certain problems that are exponentially faster than any previously known. In a different direction this framework has been shown to be useful for proving that pairs of apparently dissimilar tasks are in fact identical, or at least of equal computational difficulty. Further, these two aspects can be sometimes put together to prove dichotomy theorems, that show that for certain classes of tasks, each task is either feasibly computable, or equivalent to a single task that is suspected to be intractable. Such dichotomy theorems enable the problem of identifying the computational complexity of a new task to be resolved routinely, without requiring new research.  A second thrust has been in biology. Biological processes at every level can be regarded as being logically complex, and as realizing complex computations. This investigator has suggested that any mechanism that might realize biological evolution can be viewed as a learning mechanism, in the same technical sense as in the field of machine learning. Thus the quantitative capabilities and limitations of any particular realization of the general Darwinian mechanism, might be explored within the better understood framework of machine learning. In neuroscience the fact that the biological processes can be regarded as computations have been appreciated for much longer, but no quantitatively satisfactory theory of how the human brain accomplishes basic learning and memory in the manner that it does has wide acceptance. In this project a new suggestion is made of how the hippocampal system might be helping the cortex in realizing these tasks.       Last Modified: 10/05/2015       Submitted by: Leslie G Valiant]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
