<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HECURA: Collaborative Research: QoS-driven Storage Management for High-end Computing Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2009</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>384343.00</AwardTotalIntnAmount>
<AwardAmount>456343</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>     In today's high-end computing (HEC) systems, the parallel file system (PFS) is at the core of the storage infrastructure. PFS deployments are shared by many users and applications, but currently there are no provisions for differentiation of service - data access is provided in a best-effort manner. As systems scale, this limitation can prevent applications from efficiently utilizing the HEC resources while achieving their desired performance and it presents a hurdle to support a large number of data-intensive applications concurrently. This NSF HECURA project tackles the challenges in quality of service (QoS) driven HEC storage management, aiming to support I/O bandwidth guarantees in PFSs by addressing the following four research aspects: 1. Per-application I/O bandwidth allocation based on PFS virtualization, where each application gets its specific I/O bandwidth share through its dynamically created virtual PFS. 2. PFS management services that control the lifecycle and configuration of per-application virtual PFSs as well as support application I/O monitoring and storage resource reservation. 3. Efficient I/O bandwidth allocation through autonomic, fine-grained resource scheduling across applications that incorporate coordinated scheduling and optimizations based on profiling and prediction. 4. Scalable application checkpointing based on performance isolation and optimization on virtual PFSs customized for checkpointing I/Os.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/31/2009</MinAmdLetterDate>
<MaxAmdLetterDate>04/26/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0938045</AwardID>
<Investigator>
<FirstName>Ming</FirstName>
<LastName>Zhao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ming Zhao</PI_FULL_NAME>
<EmailAddress>mingzhao@asu.edu</EmailAddress>
<PI_PHON>4809652783</PI_PHON>
<NSF_ID>000511914</NSF_ID>
<StartDate>08/31/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Florida International University</Name>
<CityName>Miami</CityName>
<ZipCode>331990001</ZipCode>
<PhoneNumber>3053482494</PhoneNumber>
<StreetAddress>11200 SW 8TH ST</StreetAddress>
<StreetAddress2><![CDATA[MARC 430]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL26</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>071298814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>FLORIDA INTERNATIONAL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Florida International University]]></Name>
<CityName>Miami</CityName>
<StateCode>FL</StateCode>
<ZipCode>331990001</ZipCode>
<StreetAddress><![CDATA[11200 SW 8TH ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL26</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramElement>
<ProgramElement>
<Code>7952</Code>
<Text>HECURA</Text>
</ProgramElement>
<ProgramReference>
<Code>1800</Code>
<Text>Research Experience for Vets</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>7952</Code>
<Text>HECURA</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~384343</FUND_OBLG>
<FUND_OBLG>2010~16000</FUND_OBLG>
<FUND_OBLG>2011~16000</FUND_OBLG>
<FUND_OBLG>2012~16000</FUND_OBLG>
<FUND_OBLG>2013~24000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>High-performance computing (HPC) systems are important platforms for solving challenging computational problems in many disciplines. They deliver high performance to applications through parallel computing on large numbers of processors and parallel I/O accesses across large numbers of storage devices. However, HPC applications are becoming increasingly data intensive. On one hand, there is a rapidly growing number of data-driven applications which rely on the processing and analysis of large volumes of data. On the other hand, as applications employ more processors to solve larger and harder problems, they have to checkpoint more data to tolerate the more frequent failures. At the same time, HPC applications are increasingly deployed to shared computing and storage infrastructures because of the significant economic benefits brought by consolidation to both HPC users and providers. In addition, hosting large datasets on shared infrastructure allow the data to be efficiently shared by applications.</p> <p>The combination of the above trends makes HPC resource management, particularly the management of shared storage I/O resources a critical and challenging problem. Although several techniques exist to partition processors in an HPC system, parallel storage bandwidth is difficult to allocate because it has to be time-shared by applications with varying I/O demands. Without proper isolation of competing I/Os, an application&rsquo;s performance may degrade in unpredictable ways under contention. Nonetheless, the support of such storage management is generally lacking in HPC systems. In fact, today&rsquo;s HPC storage stacks are unable to recognize different applications&rsquo; I/O workloads&mdash;they only see generic I/O requests arriving from compute nodes; they are also incapable of satisfying different storage bandwidth needs from different applications&mdash;they are often architected to meet the throughput target for the entire HPC system. These prevent applications from achieving their desired performance while making efficient use of the HPC resources.</p> <p>This project presents a multi-faceted approach to addressing the aforementioned research problems and providing application Quality-of-Service (QoS) driven storage management to HPC applications. First, it provides an application-specific storage bandwidth management framework based on the virtualization of parallel file systems commonly used in HPC systems. The virtualization layer, named vPFS, is able to transparently interpose parallel file system I/Os, differentiate them on a per-application basis, and schedule them according to the applications&rsquo; I/O demands. Second, based on vPFS, the project enables the study of various I/O scheduling algorithms for different storage management objectives. Specifically, it is among the first to study proportional-share schedulers for managing both data and metadata I/Os in HPC storage systems and a two-level scheduling architecture for achieving both I/O throughput and latency objectives. Finally, this project has also contributed a novel parallel file system simulator (PFSsim) capable of simulating different I/O scheduling algorithms for current and futuristic HPC architectures.</p> <p>A prototype of vPFS which virtualizes PVFS2, a widely used parallel file system implementation, has been developed and evaluated with experiments using typical parallel computing and I/O benchmarks as well as real MPI applications. The results demonstrate that the overhead of the parallel file system virtualization framework is small (less than 3% in terms of I/O throughput) compared to native PVFS2. The results also show that the I/O schedulers enabled by vPFS achieve good proportional sharing of both data and metadata services (at least 96% of any given target sharing ratio) for competing applications with diverse I/O patterns.</p> <p>The results of this project have generate...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ High-performance computing (HPC) systems are important platforms for solving challenging computational problems in many disciplines. They deliver high performance to applications through parallel computing on large numbers of processors and parallel I/O accesses across large numbers of storage devices. However, HPC applications are becoming increasingly data intensive. On one hand, there is a rapidly growing number of data-driven applications which rely on the processing and analysis of large volumes of data. On the other hand, as applications employ more processors to solve larger and harder problems, they have to checkpoint more data to tolerate the more frequent failures. At the same time, HPC applications are increasingly deployed to shared computing and storage infrastructures because of the significant economic benefits brought by consolidation to both HPC users and providers. In addition, hosting large datasets on shared infrastructure allow the data to be efficiently shared by applications.  The combination of the above trends makes HPC resource management, particularly the management of shared storage I/O resources a critical and challenging problem. Although several techniques exist to partition processors in an HPC system, parallel storage bandwidth is difficult to allocate because it has to be time-shared by applications with varying I/O demands. Without proper isolation of competing I/Os, an applicationÆs performance may degrade in unpredictable ways under contention. Nonetheless, the support of such storage management is generally lacking in HPC systems. In fact, todayÆs HPC storage stacks are unable to recognize different applicationsÆ I/O workloads&mdash;they only see generic I/O requests arriving from compute nodes; they are also incapable of satisfying different storage bandwidth needs from different applications&mdash;they are often architected to meet the throughput target for the entire HPC system. These prevent applications from achieving their desired performance while making efficient use of the HPC resources.  This project presents a multi-faceted approach to addressing the aforementioned research problems and providing application Quality-of-Service (QoS) driven storage management to HPC applications. First, it provides an application-specific storage bandwidth management framework based on the virtualization of parallel file systems commonly used in HPC systems. The virtualization layer, named vPFS, is able to transparently interpose parallel file system I/Os, differentiate them on a per-application basis, and schedule them according to the applicationsÆ I/O demands. Second, based on vPFS, the project enables the study of various I/O scheduling algorithms for different storage management objectives. Specifically, it is among the first to study proportional-share schedulers for managing both data and metadata I/Os in HPC storage systems and a two-level scheduling architecture for achieving both I/O throughput and latency objectives. Finally, this project has also contributed a novel parallel file system simulator (PFSsim) capable of simulating different I/O scheduling algorithms for current and futuristic HPC architectures.  A prototype of vPFS which virtualizes PVFS2, a widely used parallel file system implementation, has been developed and evaluated with experiments using typical parallel computing and I/O benchmarks as well as real MPI applications. The results demonstrate that the overhead of the parallel file system virtualization framework is small (less than 3% in terms of I/O throughput) compared to native PVFS2. The results also show that the I/O schedulers enabled by vPFS achieve good proportional sharing of both data and metadata services (at least 96% of any given target sharing ratio) for competing applications with diverse I/O patterns.  The results of this project have generated broader impacts in several key aspects. First, the QoS-driven storage management framework is contributed to...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
