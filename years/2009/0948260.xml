<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Designing Touch-Sound Interfaces to Geospatial Information for Visually-Impaired Users</AwardTitle>
<AwardEffectiveDate>09/01/2009</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>117951.00</AwardTotalIntnAmount>
<AwardAmount>117951</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
</ProgramOfficer>
<AbstractNarration>The increasing use of geospatial information (that is to say, data connected to a location on Earth) in applications such as Google Maps and Google Earth underscores its growing importance to mainstream computing.  However, nearly all such geospatial applications use a highly visual interface and are therefore largely inaccessible to visually-impaired users.  Although today's most widely used solutions for touch interfaces (point haptic devices) are rudimentary when compared to the richness and complexity of human touch, ongoing developments suggest that it will eventually become technically possible to design and implement a system that uses touch and sound to convey many different forms of geospatial data.  The PI's ultimate goal is to create such a system that will allow visually-impaired users to effectively access and interact with geospatial information.  In this exploratory project, he will conduct research on how to design such touch/sound based systems. The work will cut across disciplines and actively involve visually-impaired persons as users and designers, in order to leverage the unique perspective on the use and design of non-visual interfaces that visually-impaired users have; a significant part of the research will be carried out by a visually-impaired graduate research assistant.  The expected outcomes of the PI's four step research plan will include an analysis of the goals and needs of visually-impaired users and their support personnel regarding geospatial information, a list of potential tasks for this context, a catalog of how different touch/sound methods could be used for these tasks, and an evaluation of some of these methods with visually-impaired users.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  The interdisciplinary human-computer interaction program at the PI's institution has recently admitted its first visually-impaired graduate student, who will contribute his technical skills and his connections to the visually-impaired community to this project.  The results of this work will impact the quality of touch/sound user interfaces by providing designers with a broader overview, by helping them consider multiple alternatives, and by enabling them to make more informed and inclusive design decisions.  While these new user interfaces will initially benefit primarily persons with visual impairments in accessing and interacting with geospatial data, it is likely that some of the tasks and methods explored in this research will have relevance for enhancing future interfaces to geospatial data for sighted users as well.</AbstractNarration>
<MinAmdLetterDate>09/03/2009</MinAmdLetterDate>
<MaxAmdLetterDate>09/03/2009</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0948260</AwardID>
<Investigator>
<FirstName>Chris</FirstName>
<LastName>Harding</LastName>
<EmailAddress>charding@iastate.edu</EmailAddress>
<StartDate>09/03/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Iowa State University</Name>
<CityName>AMES</CityName>
<ZipCode>500112207</ZipCode>
<PhoneNumber>5152945225</PhoneNumber>
<StreetAddress>1138 Pearson</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<StateCode>IA</StateCode>
</Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
