<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Keeneland: National Institute for Experimental Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2009</AwardEffectiveDate>
<AwardExpirationDate>04/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>12000000.00</AwardTotalIntnAmount>
<AwardAmount>12000000</AwardAmount>
<AwardInstrument>
<Value>Cooperative Agreement</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Robert Chadduck</SignBlockName>
<PO_EMAI>rchadduc@nsf.gov</PO_EMAI>
<PO_PHON>7032922247</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many-core processor architectures are rapidly emerging in many computing environments.  One of their attractions is the ability to speed up significantly computation for certain classes of algorithm.  In addition, they typically offer lower energy consumption per unit of computation.  Several recent studies have identified the development of effective methods for efficiently programming many-core architectures as a major challenge.  This project will make available, as experimental platforms, two systems in which one form of many-core processor with very high memory bandwidth, a graphics processing unit, is deployed at scale for use as an accelerator for high-performance parallel computing.  &lt;br/&gt;&lt;br/&gt;The Georgia Institute of Technology (Georgia Tech) and its partners, the University of Tennessee at Knoxville and the Oak Ridge National Laboratory will initially acquire and deploy a small, experimental, high-performance computing system consisting of an HP system with NVIDIA Tesla accelerators attached.  This will be integrated into the TeraGrid.  The project team will use this system to develop scientific libraries and programming tools to facilitate the development of science and engineering research applications.  The project team will also provide consulting support to researchers who wish to develop applications for the system using OpenCL or to port applications to the system.  &lt;br/&gt;&lt;br/&gt;In 2012, the project will upgrade the heterogeneous system to a larger and more powerful system based on a next-generation platform and NVIDIA accelerators.  It is anticipated that the final system will have a peak performance of roughly 2 petaflops/s.  The project will operate the upgraded system as a TeraGrid resource for a further two years.&lt;br/&gt;&lt;br/&gt;The final system has the potential to support many different science areas.  Possible areas of impact include some of the scientific domains in which GPU-based acceleration has already been demonstrated to have an impact at smaller scale; for example, chemistry and biochemistry, materials science, atmospheric science and combustion science.  &lt;br/&gt;&lt;br/&gt;In addition to providing infrastructure for science and engineering research and education, the project partners will educate and train the next-generation of computational scientists on cutting-edge computing architectures and emerging programming environments, using the experimental computing resource as one example.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/21/2009</MinAmdLetterDate>
<MaxAmdLetterDate>02/11/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>CoopAgrmnt</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0910735</AwardID>
<Investigator>
<FirstName>Jack</FirstName>
<LastName>Dongarra</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jack J Dongarra</PI_FULL_NAME>
<EmailAddress>dongarra@icl.utk.edu</EmailAddress>
<PI_PHON>8659748295</PI_PHON>
<NSF_ID>000299281</NSF_ID>
<StartDate>09/21/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Karsten</FirstName>
<LastName>Schwan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karsten Schwan</PI_FULL_NAME>
<EmailAddress>schwan@cc.gatech.edu</EmailAddress>
<PI_PHON>4048942589</PI_PHON>
<NSF_ID>000216799</NSF_ID>
<StartDate>09/21/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Fujimoto</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard M Fujimoto</PI_FULL_NAME>
<EmailAddress>fujimoto@cc.gatech.edu</EmailAddress>
<PI_PHON>4048539384</PI_PHON>
<NSF_ID>000296204</NSF_ID>
<StartDate>09/21/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Schulthess</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas Schulthess</PI_FULL_NAME>
<EmailAddress>schulthesstc@ornl.gov</EmailAddress>
<PI_PHON>8655744344</PI_PHON>
<NSF_ID>000489975</NSF_ID>
<StartDate>09/21/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Vetter</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeffrey S Vetter</PI_FULL_NAME>
<EmailAddress>vetter@tennessee.edu</EmailAddress>
<PI_PHON>8659743461</PI_PHON>
<NSF_ID>000246878</NSF_ID>
<StartDate>09/21/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 NORTH AVE NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>7619</Code>
<Text>Innovative HPC</Text>
</ProgramElement>
<ProgramElement>
<Code>7684</Code>
<Text>CESER-Cyberinfrastructure for</Text>
</ProgramElement>
<ProgramReference>
<Code>7619</Code>
<Text>EQUIPMENT ACQUISITIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~4967118</FUND_OBLG>
<FUND_OBLG>2011~7032882</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;The Keeneland Project was a five-year cooperative agreement awarded by the National Science Foundation (NSF) in 2009 for the deployment of an innovative high performance computing system in order to bring emerging architectures to the open science community. The Georgia Institute of Technology (Georgia Tech) and its partners - Oak Ridge National Lab, University of Tennessee-Knoxville, and the National Institute for Computational Sciences - managed the facility, performed education and outreach activities for advanced architectures, developed and deployed software tools for this emerging class of architectures to ensure productivity, and teamed with early adopters to map their applications to Keeneland architectures.</p> <p>&nbsp;</p> <p>In 2010, the Keeneland project procured and deployed its initial delivery system (KIDS): a 201 Teraflop, 120-node HP SL390 system with 240 Intel Xeon CPUs and 360 NVIDIA Fermi graphics processors, with the nodes connected by an InfiniBand QDR network. KIDS was being used to develop programming tools and libraries in order to ensure that the project can productively accelerate important scientific and engineering applications. The system was also available to a select group of users to port and tune their codes to a scalable GPU-accelerated system. In the spring of 2012 KIDS was upgraded from NVIDIA M2070 to M2090 GPUs for total peak performance of 255 TFLOPS.</p> <p>&nbsp;</p> <p>In October of 2012, the Keeneland Full Scale (KFS) system was accepted by the NSF and went into production.&nbsp; KFS was a 264-node cluster based on HP SL250 servers.&nbsp; Each node had 32 GB of host memory, two Intel Sandy Bridge CPU&rsquo;s, three NVIDIA M2090 GPUs, and a Mellanox FDR InfiniBand interconnect.&nbsp; The total peak double precision performance was 615 TF.&nbsp;</p> <p>&nbsp;</p> <p>During its lifetime, the Keeneland project</p> <p>&nbsp;</p> <ul> <li>Served 942 total users of KIDS and KFS for research, development and educational purposes;</li> <li>Contributed to at least 367 publications, presentations, and other software artifacts;</li> <li>Provided over 50 million CPU hours were used on KFS and over 10 million SUs were provided to XSEDE (equivalent to ~50 million SUs on a CPU-centric system);</li> <li>Averaged 81.2% GPU utilization over Keeneland&rsquo;s Full Scale 2-year production run;</li> <li>Contributed to the development of many early software packages for GPU heterogeneous computing: Scalable Heterogeneous Computing (SHOC) Benchmarks, MAGMA BLAS libraries, Ocelot emulator, and many others;</li> <li>Provided Advanced Application Support that increased GPU and system utilization and assisted with application development for BEAST/Beagle, improved batch matrix multiplication, and improvements to many large-scale applications; and,</li> <li>Employed 10 staff members, four faculty members, and nearly 40 students and postdoctoral research associates.</li> </ul><br> <p>            Last Modified: 06/25/2015<br>      Modified by: Jeffrey&nbsp;S&nbsp;Vetter</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2015/0910735/0910735_10127482_1435240578324_keeneland_xsede_711x_beauty_shot393--rgov-214x142.jpg" original="/por/images/Reports/POR/2015/0910735/0910735_10127482_1435240578324_keeneland_xsede_711x_beauty_shot393--rgov-800width.jpg" title="Keeneland Initial Delivery System"><img src="/por/images/Reports/POR/2015/0910735/0910735_10127482_1435240578324_keeneland_xsede_711x_beauty_shot393--rgov-66x44.jpg" al...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The Keeneland Project was a five-year cooperative agreement awarded by the National Science Foundation (NSF) in 2009 for the deployment of an innovative high performance computing system in order to bring emerging architectures to the open science community. The Georgia Institute of Technology (Georgia Tech) and its partners - Oak Ridge National Lab, University of Tennessee-Knoxville, and the National Institute for Computational Sciences - managed the facility, performed education and outreach activities for advanced architectures, developed and deployed software tools for this emerging class of architectures to ensure productivity, and teamed with early adopters to map their applications to Keeneland architectures.     In 2010, the Keeneland project procured and deployed its initial delivery system (KIDS): a 201 Teraflop, 120-node HP SL390 system with 240 Intel Xeon CPUs and 360 NVIDIA Fermi graphics processors, with the nodes connected by an InfiniBand QDR network. KIDS was being used to develop programming tools and libraries in order to ensure that the project can productively accelerate important scientific and engineering applications. The system was also available to a select group of users to port and tune their codes to a scalable GPU-accelerated system. In the spring of 2012 KIDS was upgraded from NVIDIA M2070 to M2090 GPUs for total peak performance of 255 TFLOPS.     In October of 2012, the Keeneland Full Scale (KFS) system was accepted by the NSF and went into production.  KFS was a 264-node cluster based on HP SL250 servers.  Each node had 32 GB of host memory, two Intel Sandy Bridge CPUÆs, three NVIDIA M2090 GPUs, and a Mellanox FDR InfiniBand interconnect.  The total peak double precision performance was 615 TF.      During its lifetime, the Keeneland project     Served 942 total users of KIDS and KFS for research, development and educational purposes; Contributed to at least 367 publications, presentations, and other software artifacts; Provided over 50 million CPU hours were used on KFS and over 10 million SUs were provided to XSEDE (equivalent to ~50 million SUs on a CPU-centric system); Averaged 81.2% GPU utilization over KeenelandÆs Full Scale 2-year production run; Contributed to the development of many early software packages for GPU heterogeneous computing: Scalable Heterogeneous Computing (SHOC) Benchmarks, MAGMA BLAS libraries, Ocelot emulator, and many others; Provided Advanced Application Support that increased GPU and system utilization and assisted with application development for BEAST/Beagle, improved batch matrix multiplication, and improvements to many large-scale applications; and, Employed 10 staff members, four faculty members, and nearly 40 students and postdoctoral research associates.        Last Modified: 06/25/2015       Submitted by: Jeffrey S Vetter]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
