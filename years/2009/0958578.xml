<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CI-P: The "Poor Quality" Meetings Corpus</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>99308.00</AwardTotalIntnAmount>
<AwardAmount>99308</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Go to any meeting or lecture with the younger generation of researchers, business people, or government, and there is a laptop or smartphone at every seat. Each laptop and smartphone is capable not only of recording and transmitting video and audio in real time, but also of advanced analytics on the data (e.g. speech recognition, speaker identification, face detection, etc.). Yet this rich resource goes largely unexploited mostly due to lack of good training data for machine learning algorithms.&lt;br/&gt;&lt;br/&gt;The first step in exploiting this resource is to collect a corpus of audio, video, and annotations of "natural" meetings using the participants' own laptops and cellphones, allowing both analysis of the meetings and training of machine learning algorithms. This one year planning project involves design of the corpus, including collection protocols, signals, formats, and annotations, collection of a small pilot corpus, and ongoing interaction with the community through mailing lists, forums, wikis, and a workshop hosted at ICSI in Berkeley, California. The annotations include the words spoken, events such as laughter, a telephone ringing, or a new participant entering the room, who is speaking, who appears on which camera, head and hand gestures, the participants' focus of attention, and summaries and topics.&lt;br/&gt;&lt;br/&gt;Successful planning for the collection of a significant number natural meetings from a variety of settings using the participants' own laptops and cellphones allows for a new generation of analytic tools for meetings including browsing, search, collaboration tools, and teleremote aids.</AbstractNarration>
<MinAmdLetterDate>07/19/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/19/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0958578</AwardID>
<Investigator>
<FirstName>Adam</FirstName>
<LastName>Janin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Adam Janin</PI_FULL_NAME>
<EmailAddress>janin@icsi.berkeley.edu</EmailAddress>
<PI_PHON>5106662900</PI_PHON>
<NSF_ID>000184175</NSF_ID>
<StartDate>07/19/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>International Computer Science Institute</Name>
<CityName>Berkeley</CityName>
<ZipCode>947041345</ZipCode>
<PhoneNumber>5106662900</PhoneNumber>
<StreetAddress>2150 Shattuck Ave, Suite 1100</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>187909478</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>INTERNATIONAL COMPUTER SCIENCE INSTITUTE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[International Computer Science Institute]]></Name>
<CityName>Berkeley</CityName>
<StateCode>CA</StateCode>
<ZipCode>947041345</ZipCode>
<StreetAddress><![CDATA[2150 Shattuck Ave, Suite 1100]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~99308</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Go to any meeting or lecture with the younger generation of researchers, business people, or government, and you will see a laptop or smartphone at every seat. Each laptop and smartphone is capable not only of recording and transmitting video and audio in real time, but also of advanced analytics on the data (e.g. speech recognition, speaker identification, face detection, etc.). Yet this rich resource goes largely unexploited.<br /><br />One reason is that the accuracy of algorithms can be quite poor when the signals are provided by an ad-hoc collection of sensors such as participants' laptops and cellphones. Algorithms have typically been trained, tuned, and tested on very high-quality signals collected in tightly controlled instrumented meeting rooms, often in so-called "scenario-driven" meetings held specifically in order to collect corpus data.<br /><br />This one-year project's goal was to plan for the collection of a corpus of natural meetings (ones that would have taken place regardless of the corpus collection task), record them using the participants' own laptops and smartphones, and establish protocols for annotating the meeting (e.g. what was said, who said it, events, etc.).<br /><br />At the time this project was conceived (July 2009), using existing software to record and then upload videos to sites such as YouTube was deemed infeasible for a number of reasons: 1). A smartphone's memory capacity was quite a bit smaller than today, limiting the length of a meeting that could be recorded; 2). YouTube limited uploads to 15 minutes; 3). YouTube had a limited choice for licensing; 4) Some users expressed hesitancy about uploading to publicly available sites. Therefore, we planned to use our own infrastructure for recording, streaming, and storing the audio and video from meetings.<br /><br />Times have changed dramatically since July 2009, especially in terms of users' attitude towards sites such as YouTube. Uploading videos to the web is now the norm, while using a specialized app is often viewed with suspicion. Also, YouTube lifted the 15 minute restriction in December 2010 and allowed licensing under the Creative Commons license in June 2011. The changes to YouTube's policies were not widely publicized, and unfortunately the proposers did not become aware of them until 2012. At that time, we redirected the project to finding suitable videos on the web rather than collection using our own infrastructure.<br /><br />A key outcome of the project is that the traditional methods of collecting a meeting corpus have been eclipsed due to the popularity of sites such as YouTube. If you want to analyze virtually any human activity involving audio or video (including meetings), it is far more fruitful to search YouTube rather than to collect it yourself. The difficulty now is in finding the quality and quantity of videos you require.<br /><br />We addressed this by first searching for videos over 20 minutes long; shorter videos on YouTube are typically entertainment rather than meetings. Next, we added a few keywords to the search (e.g. "meeting", "PTA", "club", "planning"). This yielded a large number of potential hits. From this set, we were able to find 10-15 appropriate videos within half an hour. Since we wanted a diverse collection, we limited ourselves about a dozen videos per keyword set. Just the process of collecting these videos would often suggest new keywords to use. We ended up with a diverse set of 100 videos of meetings suitable for later transcription and as input for training automatic systems. The methods scale easily to vastly larger collections.<br /><br /></p><br> <p>            Last Modified: 01/17/2014<br>      Modified by: Adam&nbsp;Janin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Go to any meeting or lecture with the younger generation of researchers, business people, or government, and you will see a laptop or smartphone at every seat. Each laptop and smartphone is capable not only of recording and transmitting video and audio in real time, but also of advanced analytics on the data (e.g. speech recognition, speaker identification, face detection, etc.). Yet this rich resource goes largely unexploited.  One reason is that the accuracy of algorithms can be quite poor when the signals are provided by an ad-hoc collection of sensors such as participants' laptops and cellphones. Algorithms have typically been trained, tuned, and tested on very high-quality signals collected in tightly controlled instrumented meeting rooms, often in so-called "scenario-driven" meetings held specifically in order to collect corpus data.  This one-year project's goal was to plan for the collection of a corpus of natural meetings (ones that would have taken place regardless of the corpus collection task), record them using the participants' own laptops and smartphones, and establish protocols for annotating the meeting (e.g. what was said, who said it, events, etc.).  At the time this project was conceived (July 2009), using existing software to record and then upload videos to sites such as YouTube was deemed infeasible for a number of reasons: 1). A smartphone's memory capacity was quite a bit smaller than today, limiting the length of a meeting that could be recorded; 2). YouTube limited uploads to 15 minutes; 3). YouTube had a limited choice for licensing; 4) Some users expressed hesitancy about uploading to publicly available sites. Therefore, we planned to use our own infrastructure for recording, streaming, and storing the audio and video from meetings.  Times have changed dramatically since July 2009, especially in terms of users' attitude towards sites such as YouTube. Uploading videos to the web is now the norm, while using a specialized app is often viewed with suspicion. Also, YouTube lifted the 15 minute restriction in December 2010 and allowed licensing under the Creative Commons license in June 2011. The changes to YouTube's policies were not widely publicized, and unfortunately the proposers did not become aware of them until 2012. At that time, we redirected the project to finding suitable videos on the web rather than collection using our own infrastructure.  A key outcome of the project is that the traditional methods of collecting a meeting corpus have been eclipsed due to the popularity of sites such as YouTube. If you want to analyze virtually any human activity involving audio or video (including meetings), it is far more fruitful to search YouTube rather than to collect it yourself. The difficulty now is in finding the quality and quantity of videos you require.  We addressed this by first searching for videos over 20 minutes long; shorter videos on YouTube are typically entertainment rather than meetings. Next, we added a few keywords to the search (e.g. "meeting", "PTA", "club", "planning"). This yielded a large number of potential hits. From this set, we were able to find 10-15 appropriate videos within half an hour. Since we wanted a diverse collection, we limited ourselves about a dozen videos per keyword set. Just the process of collecting these videos would often suggest new keywords to use. We ended up with a diverse set of 100 videos of meetings suitable for later transcription and as input for training automatic systems. The methods scale easily to vastly larger collections.         Last Modified: 01/17/2014       Submitted by: Adam Janin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
