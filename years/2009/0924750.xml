<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Neural Basis of the Perception of Sound Location</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2009</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>659405.00</AwardTotalIntnAmount>
<AwardAmount>659405</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Alumit Ishai</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>In our daily lives, we are confronted with many sounds.  Distinguishing among them--identifying them, and determining where they are coming from--is essential to many aspects of our existence from carrying on a conversation in a room full of people to avoiding cars when crossing the street. With funding from the National Science Foundation, Dr. Jennifer Groh and colleagues at Duke University are investigating how the locations of sounds are represented in the brain.  Unlike the visual system, the auditory system does not appear to use maps for encoding stimulus location.  Rather, emerging evidence suggests that at least the horizontal dimension of auditory space is encoded via populations of neurons that respond with monotonically increasing or decreasing responses the farther a sound is located to the right or left.  In theory, neurons with this kind of response pattern encode sound location via their firing rates, which exhibit a roughly one-to-one correspondence with a particular sound location.  A critical limitation of this kind of coding scheme, however, is that the representation of multiple simultaneous sound locations would appear to be impossible because neurons cannot discharge at more than one firing rate at a time.  And yet, humans can perceive multiple sound locations.  So how does this happen?  This project tests several competing hypotheses for how neurons might respond when more than one sound is present.  &lt;br/&gt;&lt;br/&gt;The answers to these questions will yield important new insights into how the neural representation of sound location subserves this critical component of auditory perception.  The findings generated by this research will be informative not only in the auditory domain, but will also help clarify the kinds of computational strategies the brain may employ more generally to compress information in the face of limited neural resources.  The funding from this project will be used to support a research group at Duke, providing training opportunities for undergraduate, graduate, and post-doctoral trainees in cognitive and computational neuroscience.  &lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/21/2009</MinAmdLetterDate>
<MaxAmdLetterDate>07/11/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0924750</AwardID>
<Investigator>
<FirstName>Jennifer</FirstName>
<LastName>Groh</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jennifer M Groh</PI_FULL_NAME>
<EmailAddress>jmgroh@duke.edu</EmailAddress>
<PI_PHON>9196816536</PI_PHON>
<NSF_ID>000088863</NSF_ID>
<StartDate>09/21/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277054010</ZipCode>
<StreetAddress><![CDATA[2200 W. Main St, Suite 710]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1699</Code>
<Text>Cognitive Neuroscience</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1699</Code>
<Text>COGNEURO</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~167397</FUND_OBLG>
<FUND_OBLG>2010~283774</FUND_OBLG>
<FUND_OBLG>2011~39012</FUND_OBLG>
<FUND_OBLG>2012~169222</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong><span style="text-decoration: underline;">Intellectual Merit:</span></strong>&nbsp;</p> <p>&nbsp;</p> <p>This project concerned how the locations of sounds are represented in the brain.&nbsp; Localizing each sound in the auditory scene is critical for detecting prey, avoiding predators, and communicating with other members of the species. Unlike the visual system, the auditory system does not appear to use maps for encoding stimulus location.&nbsp; Rather, emerging evidence from many mammals suggests that at least the horizontal dimension of auditory space is encoded via populations of neurons that respond with monotonically increasing or decreasing responses the farther a sound is located to the right or left.&nbsp; In theory, neurons with this kind of response pattern encode sound location via their firing rates, which exhibit a roughly one-to-one correspondence with a particular sound location.&nbsp;</p> <p>&nbsp;</p> <p>A critical limitation of this kind of coding scheme, however, is that the representation of multiple simultaneous sound locations would appear to be impossible because neurons cannot discharge at more than one firing rate at a time.&nbsp; We therefore investigated how the brain might accomplish the representation of multiple sounds simultaneously. We suspected that successful perception of each sound at its distinct location requires neurons to separate into two populations, with each population encoding one of the two sounds.&nbsp; This sorting may occur based on sound frequency (the two sounds must differ in frequency, thus activating a slightly different population of frequency tuned neurons), or it may occur in time, with neurons oscillating back and forth between encoding one sound or the other.&nbsp; We have found evidence suggesting that neurons do indeed show fluctuating activity patterns consistent with coding each sound separately across time.&nbsp; This discovery required developing a novel statistical approach to evaluating neural activity patterns across time.&nbsp;</p> <p>&nbsp;</p> <p><strong><span style="text-decoration: underline;">Broader Impact:</span></strong>&nbsp;</p> <p>&nbsp;</p> <p>In tandem with these research objectives, our educational objectives were to (a) develop educational materials on the role of the brain in the perception of sound location, for use at the primary and secondary as well as university levels; (b) disseminate scientific advances to the public through writing for a non-scientific audience, and (c) bring a diverse population of undergraduate students and graduate students into laboratory research.&nbsp; All three of these goals were achieved.&nbsp; PI Groh wrote a book entitled &ldquo;Making Space: How the Brain Knows Where Things Are&rdquo; for a general audience, and developed a Coursera course entitled &ldquo;The Brain and Space&rdquo;&nbsp; on the same topic.&nbsp; Numerous undergraduate and graduate students have participated in laboratory research during the life of the grant.&nbsp; &nbsp;&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/02/2015<br>      Modified by: Jennifer&nbsp;M&nbsp;Groh</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual Merit:      This project concerned how the locations of sounds are represented in the brain.  Localizing each sound in the auditory scene is critical for detecting prey, avoiding predators, and communicating with other members of the species. Unlike the visual system, the auditory system does not appear to use maps for encoding stimulus location.  Rather, emerging evidence from many mammals suggests that at least the horizontal dimension of auditory space is encoded via populations of neurons that respond with monotonically increasing or decreasing responses the farther a sound is located to the right or left.  In theory, neurons with this kind of response pattern encode sound location via their firing rates, which exhibit a roughly one-to-one correspondence with a particular sound location.      A critical limitation of this kind of coding scheme, however, is that the representation of multiple simultaneous sound locations would appear to be impossible because neurons cannot discharge at more than one firing rate at a time.  We therefore investigated how the brain might accomplish the representation of multiple sounds simultaneously. We suspected that successful perception of each sound at its distinct location requires neurons to separate into two populations, with each population encoding one of the two sounds.  This sorting may occur based on sound frequency (the two sounds must differ in frequency, thus activating a slightly different population of frequency tuned neurons), or it may occur in time, with neurons oscillating back and forth between encoding one sound or the other.  We have found evidence suggesting that neurons do indeed show fluctuating activity patterns consistent with coding each sound separately across time.  This discovery required developing a novel statistical approach to evaluating neural activity patterns across time.      Broader Impact:      In tandem with these research objectives, our educational objectives were to (a) develop educational materials on the role of the brain in the perception of sound location, for use at the primary and secondary as well as university levels; (b) disseminate scientific advances to the public through writing for a non-scientific audience, and (c) bring a diverse population of undergraduate students and graduate students into laboratory research.  All three of these goals were achieved.  PI Groh wrote a book entitled "Making Space: How the Brain Knows Where Things Are" for a general audience, and developed a Coursera course entitled "The Brain and Space"  on the same topic.  Numerous undergraduate and graduate students have participated in laboratory research during the life of the grant.              Last Modified: 11/02/2015       Submitted by: Jennifer M Groh]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
