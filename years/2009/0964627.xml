<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Medium: Collaborative Research: Scaling the Implicitly Parallel Programming Model with Lifelong Thread Extraction and Dynamic Adaptation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2010</AwardEffectiveDate>
<AwardExpirationDate>04/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>261679</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The microprocessor industry has moved toward multicore designs to leverage increasing transistor counts in the face of physical and micro-architectural limitations.  Unfortunately, providing multiple cores does not translate into performance for most applications. Rather than pushing all the burden onto programmers, this project advocates the use of the implicitly parallel programming model to eliminate the laborious and error-prone process of explicit parallel programming.  Implicit parallel programming leverages sequential languages to facilitate shorter development and debug cycles, and relies on automatic tools, both static compilers and run-time systems, to identify parallelism and customize it to the target platform.  Implicit parallelism can be systematically extracted using: (1) decoupled softwarepipelining, a technique to extract the pipeline parallelism found in many sequential applications; (2) low-frequency and high-confidence speculation to overcome limitations of memory dependence analysis; (3) whole-program scope for parallelization to eliminate analysis boundaries; (4) simple extensions to the sequential programming model that give the programmer the power to refine the meaning of a program; (5) dynamic adaptation to ensure efficiency is maintained across changing environments. This project is developing the set of technologies to realize an implicitly parallel programming system with scalable, lifelong thread extraction and dynamic adaptation.  At the broader level, the implicitly parallel programming approach will free programmers to consider the problems they are trying to solve, rather than forcing them to overcome the processor industry's failure to continue to scale performance.  This approach will keep computers accessible, helping computing to have the same increasingly positive impact on other fields.</AbstractNarration>
<MinAmdLetterDate>03/08/2010</MinAmdLetterDate>
<MaxAmdLetterDate>10/10/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0964627</AwardID>
<Investigator>
<FirstName>Mary Lou</FirstName>
<LastName>Soffa</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mary Lou Soffa</PI_FULL_NAME>
<EmailAddress>soffa@cs.virginia.edu</EmailAddress>
<PI_PHON>4349822277</PI_PHON>
<NSF_ID>000203853</NSF_ID>
<StartDate>10/10/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kim</FirstName>
<LastName>Hazelwood</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kim Hazelwood</PI_FULL_NAME>
<EmailAddress>hazelwood@virginia.edu</EmailAddress>
<PI_PHON>4349822228</PI_PHON>
<NSF_ID>000482856</NSF_ID>
<StartDate>03/08/2010</StartDate>
<EndDate>10/10/2012</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Virginia Main Campus</Name>
<CityName>CHARLOTTESVILLE</CityName>
<ZipCode>229044195</ZipCode>
<PhoneNumber>4349244270</PhoneNumber>
<StreetAddress>P.O.  BOX 400195</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>065391526</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RECTOR &amp; VISITORS OF THE UNIVERSITY OF VIRGINIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>065391526</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Virginia Main Campus]]></Name>
<CityName>CHARLOTTESVILLE</CityName>
<StateCode>VA</StateCode>
<ZipCode>229044195</ZipCode>
<StreetAddress><![CDATA[P.O.  BOX 400195]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~128452</FUND_OBLG>
<FUND_OBLG>2011~133227</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="MsoNormal" style="margin-top: 0in; margin-right: .1in; margin-bottom: .0001pt; margin-left: .1in; line-height: normal; mso-layout-grid-align: none; text-autospace: none;"><span>With  chip multiprocessors (CMPs) comes the promise of high-performance  computing on a desktop. CMPs impact the design, implementation and the  way that high performance applications execute. These applications,  which have become increasingly more complex, larger in scale, and handle  huge data sets, can benefit greatly from CMPs. These applications are  expected to use parallel systems with tens to several hundreds of nodes  to handle their ever growing problem sizes. For example, simulating  complex ocean circulation models requires exploiting significant  parallelism. Similarly, emerging applications in biomedical computing,  automated surgery, and data mining have inherent parallelism and CMPs  can increase their performance by several factors. With the shift to  CMPs, managing shared resources has become a critical issue in realizing  their full potential. In this research, we focused on the contention  for memory resources in a CMP.</span></p> <p class="MsoNormal" style="margin-top: 0in; margin-right: .1in; margin-bottom: .0001pt; margin-left: .1in; line-height: normal; mso-layout-grid-align: none; text-autospace: none;"><span>&nbsp;</span></p> <p class="MsoNormal" style="margin-top: 0in; margin-right: .1in; margin-bottom: 10.0pt; margin-left: 6.0pt; line-height: normal;"><span>To  develop approaches to reduce shared resource contention for emerging  multi-threaded applications, we studied how their performances are  affected by contention for a particular shared resource. We developed a  general methodology for characterizing multi-threaded applications by  determining the effect of shared-resource contention on performance.<span style="mso-spacerun: yes;">&nbsp; </span>We characterized the applications using the PARSEC benchmark suite for shared-memory resource contention.<span style="mso-spacerun: yes;">&nbsp; </span>The  characterization revealed several interesting aspects. Three of twelve  PARSEC benchmarks exhibit no contention for cache resources. Nine  exhibit contention for the L2-cache, with only three exhibiting  contention among their own threads&ndash;most contention is because of  competition with a co-runner. Interestingly, contention for the Front  Side Bus is a major factor and degrades performance by more than 11%</span></p> <p class="MsoNormal" style="margin-bottom: .0001pt; line-height: normal; mso-layout-grid-align: none; text-autospace: none;"><span>&nbsp;</span></p> <p class="MsoListParagraph" style="margin-top: 0in; margin-right: 0in; margin-bottom: .0001pt; margin-left: 6.0pt; mso-add-space: auto; line-height: normal; mso-layout-grid-align: none; text-autospace: none;"><span>Effective  resource and application management on CMPs requires consideration of  user specific requirements and dynamic adaption of management decisions  based on the actual run-time environment. However, designing an  algorithm to manage resources and applications that can dynamically  adapt based on the run-time environment is difficult because most  resource and application management and monitoring facilities are only  available at the OS level. We developed REEact, an infrastructure that  provides the capability to specify user-level management policies with  dynamic adaptation.<span style="mso-spacerun: yes;">&nbsp; </span>REEact  is a virtual execution environment that provides a framework and core  services to quickly enable the design of custom management policies for  dynamically managing resources and applications.<span style="mso-spacerun: yes;">&nbsp; </span>We  evaluated REEact on three case studies, each illustrating the use of  REEact to apply a specific dynamic management policy on a real CMP.  Through these case studies, we demonstrated that REEact can effective...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[With  chip multiprocessors (CMPs) comes the promise of high-performance  computing on a desktop. CMPs impact the design, implementation and the  way that high performance applications execute. These applications,  which have become increasingly more complex, larger in scale, and handle  huge data sets, can benefit greatly from CMPs. These applications are  expected to use parallel systems with tens to several hundreds of nodes  to handle their ever growing problem sizes. For example, simulating  complex ocean circulation models requires exploiting significant  parallelism. Similarly, emerging applications in biomedical computing,  automated surgery, and data mining have inherent parallelism and CMPs  can increase their performance by several factors. With the shift to  CMPs, managing shared resources has become a critical issue in realizing  their full potential. In this research, we focused on the contention  for memory resources in a CMP.   To  develop approaches to reduce shared resource contention for emerging  multi-threaded applications, we studied how their performances are  affected by contention for a particular shared resource. We developed a  general methodology for characterizing multi-threaded applications by  determining the effect of shared-resource contention on performance.  We characterized the applications using the PARSEC benchmark suite for shared-memory resource contention.  The  characterization revealed several interesting aspects. Three of twelve  PARSEC benchmarks exhibit no contention for cache resources. Nine  exhibit contention for the L2-cache, with only three exhibiting  contention among their own threads&ndash;most contention is because of  competition with a co-runner. Interestingly, contention for the Front  Side Bus is a major factor and degrades performance by more than 11%   Effective  resource and application management on CMPs requires consideration of  user specific requirements and dynamic adaption of management decisions  based on the actual run-time environment. However, designing an  algorithm to manage resources and applications that can dynamically  adapt based on the run-time environment is difficult because most  resource and application management and monitoring facilities are only  available at the OS level. We developed REEact, an infrastructure that  provides the capability to specify user-level management policies with  dynamic adaptation.  REEact  is a virtual execution environment that provides a framework and core  services to quickly enable the design of custom management policies for  dynamically managing resources and applications.  We  evaluated REEact on three case studies, each illustrating the use of  REEact to apply a specific dynamic management policy on a real CMP.  Through these case studies, we demonstrated that REEact can effectively  and efficiently implement policies to dynamically manage resources and  adapt application execution.   Previous  research has shown that thread mapping is a powerful tool for resource  management. However, the difficulty of simultaneously managing multiple  hardware resources and the varying nature of the workloads has impeded  the efficiency of thread mapping algorithms. We developed an in-depth  analysis of PARSEC benchmarks running under different thread mappings to  investigate the interaction of various thread mappings with  microarchitectural resources, including L1 I/D-caches, I/D TLBs, L2  caches, hardware prefetchers, off-chip memory interconnects, branch  predictors, memory disambiguation units and the cores. Our experiments  show that when only memory resources are considered, thread mapping  improves an applicationÆs performance by as much as 14% over the default  Linux scheduler. In contrast, when both memory and processor resources  are considered the mapping algorithm achieves performance improvements  by as much as 28%.    We  also developed ReSense, the first run-time system that uses application  characteristic...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
