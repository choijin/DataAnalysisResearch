<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Medium: Very Large Scale Consistent DHTs</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>1199242.00</AwardTotalIntnAmount>
<AwardAmount>1215242</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Modern distributed systems increasingly rely on distributed storage and lookup services.  Existing storage and lookup solutions, however, provide only a subset of high availability, high scalability, and strong consistency semantics. In this project, we develop Harmony, a distributed hash table (DHT) aimed at providing very large scale applications with highly scalable, consistent and available storage and lookup. We address many significant challenges in delivering on this goal. First, the Internet has unpredictable node and communication failures, and preserving consistency in this context is a difficult task.  To address this issue, we provide the abstraction of a collection of self-managing groups that coordinate to ensure atomic updates to distributed state.  Second, consistency often comes at the cost of reduced availability.  In our system, consistency is an inviolable safety property; availability is provided through replication.  Third, coordination mechanisms for consistent replication and atomic updates often result in performance penalties.  A key insight in our work is that one can improve performance with coordination mechanisms for delegation and autonomous execution.  Finally, by isolating most of the communication necessary to preserve consistency within a group, we both simplify our implementation and improve its scalability.  Further, adaptation to changing workloads and resource constraints is easier because it can take place within a framework for the consistent update of distributed state. If successful, the resulting storage abstraction should greatly simplify the development of complex distributed applications.</AbstractNarration>
<MinAmdLetterDate>03/11/2010</MinAmdLetterDate>
<MaxAmdLetterDate>05/31/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0963754</AwardID>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Anderson</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas E Anderson</PI_FULL_NAME>
<EmailAddress>tom@cs.washington.edu</EmailAddress>
<PI_PHON>2065439348</PI_PHON>
<NSF_ID>000196821</NSF_ID>
<StartDate>03/11/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Arvind</FirstName>
<LastName>Krishnamurthy</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Arvind Krishnamurthy</PI_FULL_NAME>
<EmailAddress>arvind@cs.washington.edu</EmailAddress>
<PI_PHON>2066160957</PI_PHON>
<NSF_ID>000488256</NSF_ID>
<StartDate>03/11/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981950001</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Ave NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7354</Code>
<Text>COMPUTER SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9152</Code>
<Text>EDUCATION PROGRAM EVALUATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~589394</FUND_OBLG>
<FUND_OBLG>2011~298307</FUND_OBLG>
<FUND_OBLG>2013~327541</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Modern distributed systems increasingly rely on distributed storage and lookup services. Existing services, however, only provide a subset of high availability, high scalability, and strong consistency semantics. In this project, we have been developing storage systems aimed at very large scale applications that simultaneously meets their scalability, consistency and availability requirements.&nbsp; Our first system (Scatter) provides scalable and consistent distributed hash table key-value storage.&nbsp; Scatter is robust to P2P churn, heterogeneous node capacities, and flaky and irregular network behavior.&nbsp; (We have left robustness to malicious behavior, such as against DDoS attacks and Byzantine faults, to future work.)&nbsp; In keeping with our goal of building an open system, an essential requirement for Scatter is that there be no central point of control for commercial interests to exploit.&nbsp; Our follow-on work (SpecPaxos) addresses performance concerns by building better network primitives.&nbsp; Distributed systems are traditionally designed independently from the underlying network, making worst-case assumptions (e.g., complete asynchrony) about its behavior.&nbsp; However, many distributed applications are today deployed in datacenters, where the network is more reliable, predictable, and extensible. In these environments, it is possible to codesign distributed systems with their network layer, and doing so can offer substantial benefits.&nbsp; This approach leads to substantial performance benefits under realistic datacenter conditions.&nbsp; Our most recent work, Tapir, shows that this approach can be generalized to distributed transactions in addition to replicated state machines.&nbsp; Here, we use a novel replication protocol, called inconsistent replication, that provides fault tolerance without consistency. By enforcing strong consistency only in the transaction protocol, TAPIR can commit transactions in a single round-trip and order distributed transactions without centralized coordination.</p> <p>&nbsp;</p> <p>The key thrusts of our work over the last year has been on:</p> <p>&nbsp;</p> <p>Codesigning the replication layer and the transaction processing protocol in order to achieve efficient and scalable distributed transactions: &nbsp;We designed and implemented TAPIR -- the Transactional Application Protocol for Inconsistent Replication. TAPIR uses a new replication technique, called inconsistent replication (IR), that provides fault tolerance without consistency.&nbsp; Despite IR's weak consistency guarantees, TAPIR provides and supports globally-consistent reads across the database at a timestamp -- the same guarantees as Spanner.&nbsp; TAPIR efficiently leverages IR to distribute read-write transactions in a single round-trip and order transactions globally across partitions and replicas with no centralized coordination.</p> <p>&nbsp;</p> <p>Consistent storage over untrusted storage providers:&nbsp; We designed, implemented, and evaluated MetaSync, a secure and reliable file synchronization service that uses multiple cloud synchronization services as untrusted storage providers. To make MetaSync work correctly, we devise a novel variant of Paxos that provides efficient and consistent updates on top of the unmodified APIs exported by existing services.&nbsp; Our system automatically redistributes files upon reconfiguration of providers.</p> <p>&nbsp;</p> <p>Our research addresses fundamental questions about how to design, build, and deploy a consistent storage system that provides high availability and consistency, without compromising on performance.&nbsp; There are many significant hurdles to delivering on our goal.&nbsp; First, preserving consistency in a system with unpredictable node and communication failures is a difficult task.&nbsp; To address these issue, we use classical distributed systems algorithms in order to provide ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Modern distributed systems increasingly rely on distributed storage and lookup services. Existing services, however, only provide a subset of high availability, high scalability, and strong consistency semantics. In this project, we have been developing storage systems aimed at very large scale applications that simultaneously meets their scalability, consistency and availability requirements.  Our first system (Scatter) provides scalable and consistent distributed hash table key-value storage.  Scatter is robust to P2P churn, heterogeneous node capacities, and flaky and irregular network behavior.  (We have left robustness to malicious behavior, such as against DDoS attacks and Byzantine faults, to future work.)  In keeping with our goal of building an open system, an essential requirement for Scatter is that there be no central point of control for commercial interests to exploit.  Our follow-on work (SpecPaxos) addresses performance concerns by building better network primitives.  Distributed systems are traditionally designed independently from the underlying network, making worst-case assumptions (e.g., complete asynchrony) about its behavior.  However, many distributed applications are today deployed in datacenters, where the network is more reliable, predictable, and extensible. In these environments, it is possible to codesign distributed systems with their network layer, and doing so can offer substantial benefits.  This approach leads to substantial performance benefits under realistic datacenter conditions.  Our most recent work, Tapir, shows that this approach can be generalized to distributed transactions in addition to replicated state machines.  Here, we use a novel replication protocol, called inconsistent replication, that provides fault tolerance without consistency. By enforcing strong consistency only in the transaction protocol, TAPIR can commit transactions in a single round-trip and order distributed transactions without centralized coordination.     The key thrusts of our work over the last year has been on:     Codesigning the replication layer and the transaction processing protocol in order to achieve efficient and scalable distributed transactions:  We designed and implemented TAPIR -- the Transactional Application Protocol for Inconsistent Replication. TAPIR uses a new replication technique, called inconsistent replication (IR), that provides fault tolerance without consistency.  Despite IR's weak consistency guarantees, TAPIR provides and supports globally-consistent reads across the database at a timestamp -- the same guarantees as Spanner.  TAPIR efficiently leverages IR to distribute read-write transactions in a single round-trip and order transactions globally across partitions and replicas with no centralized coordination.     Consistent storage over untrusted storage providers:  We designed, implemented, and evaluated MetaSync, a secure and reliable file synchronization service that uses multiple cloud synchronization services as untrusted storage providers. To make MetaSync work correctly, we devise a novel variant of Paxos that provides efficient and consistent updates on top of the unmodified APIs exported by existing services.  Our system automatically redistributes files upon reconfiguration of providers.     Our research addresses fundamental questions about how to design, build, and deploy a consistent storage system that provides high availability and consistency, without compromising on performance.  There are many significant hurdles to delivering on our goal.  First, preserving consistency in a system with unpredictable node and communication failures is a difficult task.  To address these issue, we use classical distributed systems algorithms in order to provide the abstraction of a collection of self-managing groups.  The groups coordinate with each other to ensure atomic updates to both the DHT topology and the key values.  Second, consistency often comes at the cost of reduce...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
