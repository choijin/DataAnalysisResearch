<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Small:Explorations in Computational Learning Theory</AwardTitle>
<AwardEffectiveDate>08/01/2009</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardAmount>338698</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Balasubramanian Kalyanasundaram</SignBlockName>
</ProgramOfficer>
<AbstractNarration>A primary goal of machine learning is to have computers "learn" from data, and to make predictions based on what they have learned.  Machine learning has been used in many applications, such as identification of spam emails, detection of suspicious computer network traffic, and detection of malignant tumors.  The use of machine learning is based on the implicit assumption that there is a mathematical function that describes, with some accuracy, the relation between the inputs to the prediction problem and the correct prediction.  The function is not arbitrary; instead, it is of a certain restricted type.  However, not all types of functions are efficiently learnable.  Also, learnability depends crucially on the type of data that is available.&lt;br/&gt;&lt;br/&gt;This project focuses on the learnability of Boolean functions, a central topic in computational learning theory.  The research in this project falls into three main categories: learning from random examples, learning with costs, and DNF learning and minimization.  Problems in the first category address core open questions in the standard PAC learning model and explore the extent to which access to data from different probability distributions can aid in learning.  Problems in the second category are motivated by concrete problems in protein engineering, databases, and cyber-security, where there are costs associated with determining the value of inputs, or in obtaining data.  The third category concerns problems of properly learning DNF formulas using DNF hypotheses, related complexity theoretic problems concerning DNF minimization, and problems concerning the complexity of certificates of DNF size.&lt;br/&gt;&lt;br/&gt;Broadly, this project seeks to expand our understanding of which types of functions are efficiently learnable by computers, and under what conditions.  The research on learning with costs can yield advances in the application areas that motivate it.  DNF minimization is a central problem in both complexity theory and in the design of logic circuits; the research on DNF has the potential for impact in both these areas.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/05/2009</MinAmdLetterDate>
<MaxAmdLetterDate>06/17/2013</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0917153</AwardID>
<Investigator>
<FirstName>Lisa</FirstName>
<LastName>Hellerstein</LastName>
<EmailAddress>lisa.hellerstein@nyu.edu</EmailAddress>
<StartDate>08/05/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
</Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
