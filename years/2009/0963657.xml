<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Collaborative Research:  Reconstructing Cities from Photographs</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/15/2010</AwardEffectiveDate>
<AwardExpirationDate>04/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>720000.00</AwardTotalIntnAmount>
<AwardAmount>720000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project is focused on research issues associated with producing extremely detailed and accurate 3D geometry and appearance (BRDF) models at city scale from internet collections of photos from various sources containing millions of photos of enormous diversity such as viewing range and conditions, time of day and weather conditions, to name a few.  The properties of urban scenes may include low-texture surfaces, reflective and transparent materials, and repeated structures that challenge existing reconstruction algorithms. The investigators will address these challenges with the aim of reconstructing several large US and foreign cities.   Historical photos and virtual models may also be incorporated.  There are a number of research topics associated with the project. To register photographs and recover sparse geometry at city-scale, a new, unified, structure-from-motion (SfM) algorithm will be designed to take advantage of large, parallel computing platforms.  With registered photographs and sparse 3D scene points recovered by SfM, multi-view stereo (MVS) algorithms can reconstruct detailed geometric models.  Novel MVS algorithms will then exploit the structure of architectural scenes and volumetric reconstruction methods will be employed to produce annotated models of exceptional accuracy and usability. Digital models are playing an increasingly important role in social, cultural and economic endeavor and are central to next-generation mapping and visualization applications.  All models and datasets will be made freely available to researchers and the general public.</AbstractNarration>
<MinAmdLetterDate>05/19/2010</MinAmdLetterDate>
<MaxAmdLetterDate>05/17/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0963657</AwardID>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Curless</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian L Curless</PI_FULL_NAME>
<EmailAddress>curless@cs.washington.edu</EmailAddress>
<PI_PHON>2066853796</PI_PHON>
<NSF_ID>000417360</NSF_ID>
<StartDate>05/19/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Steven</FirstName>
<LastName>Seitz</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Steven M Seitz</PI_FULL_NAME>
<EmailAddress>seitz@cs.washington.edu</EmailAddress>
<PI_PHON>2066169431</PI_PHON>
<NSF_ID>000195084</NSF_ID>
<StartDate>05/19/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sameer</FirstName>
<LastName>Agarwal</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Dr.</PI_SUFX_NAME>
<PI_FULL_NAME>Sameer Agarwal</PI_FULL_NAME>
<EmailAddress>sagarwal@cs.washington.edu</EmailAddress>
<PI_PHON>2065434043</PI_PHON>
<NSF_ID>000539932</NSF_ID>
<StartDate>05/19/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981950001</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Ave NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~254976</FUND_OBLG>
<FUND_OBLG>2011~227463</FUND_OBLG>
<FUND_OBLG>2012~237561</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Most popular cities are captured from millions of viewpoints and illumination conditions, covering interiors, exteriors, objects, statues, artifacts, landscapes and more. Virtually anything that people find interesting is densely sampled on photo sharing sites like Flickr and Picasa.&nbsp; How much of a city can be reconstructed from such photos?&nbsp; This project aimed to answer this question by producing extremely detailed and accurate 3D geometry and appearance (BRDF) models at city scale from online photo collections.&nbsp; The challenges are great: the scale is huge (millions of photos), the diversity is enormous given the range of viewing conditions across time of day and weather, and the properties of urban scenes include low-texture surfaces, reflective and transparent materials, and repeated structures that challenge existing reconstruction algorithms. The investigators addressed these challenges, with specific contributions in the following areas.</p> <p>&nbsp;</p> <p>Large scale scene modeling:&nbsp; we developed a method that produces the first high resolution fused 3D models from aerial and ground-based (Flickr) photos.&nbsp; We estimate not just geometry (via structure-from-motion and stereo), but also surface reflectance, which enables relighting the scene.&nbsp; We also introduce a new method for quantifying the visual appeal of such models through a "Visual Turing Test:"&nbsp;&nbsp; a very large scale user study conducted on Amazon Mechanical Turk, where users are asked to compare real and virtual photos to evaluate which is more realistic.</p> <p>&nbsp;</p> <p>3D sensing:&nbsp; we introduced a new technique that reasons about &ldquo;internal silhouettes,&rdquo; a powerful but previously unexploited cue to dramatically improve the performance of 3D reconstructions from multiple photos.&nbsp; While reconstructing 3D models from a large set of photos is challenging, it&rsquo;s even more difficult when only a single photo is available.&nbsp;&nbsp; We invented a new approach that exploits common properties of architectural scenes to handle both piece-wise planar and curved geometries.&nbsp;</p> <p>&nbsp;</p> <p>Computational photography:&nbsp; we addressed the problem of extending the field of view of a photo - an operation we call uncrop &ndash; by leveraging large collections of Internet photos captured at roughly the same location.&nbsp; &nbsp;Given a reference photograph to be uncropped, our approach selects, reprojects, and composites a subset of Internet imagery taken near the reference into a larger image around the reference using the underlying scene geometry. &nbsp;&nbsp;Our approach is capable of handling large Internet photo collections with arbitrary viewpoints, dramatic appearance variation, and complicated scene layout. We demonstrate results that are visually compelling on a wide range of real-world landmarks.&nbsp; Finally, we developed a novel approach for refocusing plenoptic images (e.g., from a Lytro camera), which improves significantly on the existing state of the art.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/18/2014<br>      Modified by: Steven&nbsp;M&nbsp;Seitz</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2014/0963657/0963657_10016207_1418955427030_uncrop--rgov-214x142.jpg" original="/por/images/Reports/POR/2014/0963657/0963657_10016207_1418955427030_uncrop--rgov-800width.jpg" title="Uncrop result"><img src="/por/images/Repo...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Most popular cities are captured from millions of viewpoints and illumination conditions, covering interiors, exteriors, objects, statues, artifacts, landscapes and more. Virtually anything that people find interesting is densely sampled on photo sharing sites like Flickr and Picasa.  How much of a city can be reconstructed from such photos?  This project aimed to answer this question by producing extremely detailed and accurate 3D geometry and appearance (BRDF) models at city scale from online photo collections.  The challenges are great: the scale is huge (millions of photos), the diversity is enormous given the range of viewing conditions across time of day and weather, and the properties of urban scenes include low-texture surfaces, reflective and transparent materials, and repeated structures that challenge existing reconstruction algorithms. The investigators addressed these challenges, with specific contributions in the following areas.     Large scale scene modeling:  we developed a method that produces the first high resolution fused 3D models from aerial and ground-based (Flickr) photos.  We estimate not just geometry (via structure-from-motion and stereo), but also surface reflectance, which enables relighting the scene.  We also introduce a new method for quantifying the visual appeal of such models through a "Visual Turing Test:"   a very large scale user study conducted on Amazon Mechanical Turk, where users are asked to compare real and virtual photos to evaluate which is more realistic.     3D sensing:  we introduced a new technique that reasons about "internal silhouettes," a powerful but previously unexploited cue to dramatically improve the performance of 3D reconstructions from multiple photos.  While reconstructing 3D models from a large set of photos is challenging, itÆs even more difficult when only a single photo is available.   We invented a new approach that exploits common properties of architectural scenes to handle both piece-wise planar and curved geometries.      Computational photography:  we addressed the problem of extending the field of view of a photo - an operation we call uncrop &ndash; by leveraging large collections of Internet photos captured at roughly the same location.   Given a reference photograph to be uncropped, our approach selects, reprojects, and composites a subset of Internet imagery taken near the reference into a larger image around the reference using the underlying scene geometry.   Our approach is capable of handling large Internet photo collections with arbitrary viewpoints, dramatic appearance variation, and complicated scene layout. We demonstrate results that are visually compelling on a wide range of real-world landmarks.  Finally, we developed a novel approach for refocusing plenoptic images (e.g., from a Lytro camera), which improves significantly on the existing state of the art.          Last Modified: 12/18/2014       Submitted by: Steven M Seitz]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
