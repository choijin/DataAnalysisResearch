<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Visual Characterization of I/O System Behavior for High-End Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2009</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>434000.00</AwardTotalIntnAmount>
<AwardAmount>434000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Frank Olken</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Abstract &lt;br/&gt;Modern supercomputers are complex, hierarchical systems &lt;br/&gt;consisting of huge numbers of cores, systems for disk storage, &lt;br/&gt;and nodes for I/O forwarding. These numbers continue to grow and &lt;br/&gt;the need for tools to understand the behavior of the system &lt;br/&gt;software becomes paramount: without these tools it is impossible &lt;br/&gt;to effectively tune that software, and high degrees of efficiency &lt;br/&gt;is unattainable by applications. This project addresses the &lt;br/&gt;challenge of understanding the behavior of complex system &lt;br/&gt;software on very large-scale compute platforms, like the current &lt;br/&gt;petascale computers. In particular, this project is developing &lt;br/&gt;software infrastructure to provide end-to-end analysis and &lt;br/&gt;visualization of I/O system software. Specifically, the &lt;br/&gt;objectives are to develop, improve, and deploy (1) end-to-end, &lt;br/&gt;scalable tracing integrated into the I/O system (MPI-IO, I/O &lt;br/&gt;forwarding, and file system); (2) information visualization tools &lt;br/&gt;for inspecting traces and extracting knowledge; (3) testing &lt;br/&gt;components that drive this system to generate example patterns, &lt;br/&gt;including a component to generate anomalies; and (4) tutorials &lt;br/&gt;and tools for helping other system software developers &lt;br/&gt;incorporate this analysis and visualization system into their &lt;br/&gt;production software. The software and techniques developed in &lt;br/&gt;this project will be directly applicable to and useful in other &lt;br/&gt;system software libraries which perform complex interactions on &lt;br/&gt;large systems. &lt;br/&gt;&lt;br/&gt;For further information see the project web site at the URL: &lt;br/&gt;http://vis.cs.ucdavis.edu/NSF/Jupiter &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/23/2009</MinAmdLetterDate>
<MaxAmdLetterDate>06/21/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0937928</AwardID>
<Investigator>
<FirstName>Kamil</FirstName>
<LastName>Iskra</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kamil A Iskra</PI_FULL_NAME>
<EmailAddress>iskra@uchicago.edu</EmailAddress>
<PI_PHON>6302527197</PI_PHON>
<NSF_ID>000528703</NSF_ID>
<StartDate>09/23/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Beckman</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter H Beckman</PI_FULL_NAME>
<EmailAddress>beckman@uchicago.edu</EmailAddress>
<PI_PHON>6302529020</PI_PHON>
<NSF_ID>000298331</NSF_ID>
<StartDate>09/23/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372612</ZipCode>
<PhoneNumber>7737028669</PhoneNumber>
<StreetAddress>6054 South Drexel Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005421136</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CHICAGO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005421136</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606372612</ZipCode>
<StreetAddress><![CDATA[6054 South Drexel Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7476</Code>
<Text>XD-Extreme Digital</Text>
</ProgramElement>
<ProgramElement>
<Code>7952</Code>
<Text>HECURA</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~144840</FUND_OBLG>
<FUND_OBLG>2010~289160</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Supercomputers have been described as machines that turn compute-bound problems into input/output-bound ones. Their massive&mdash;and rapidly increasing&mdash;computing subsystems demand equally capable input/output (I/O) infrastructure to supply them the input data and extract the results without undue delays. Unfortunately research into the performance analysis and optimization of I/O systems has lagged behind its computational counterparts. Yet, with the rapidly increasing complexity of new generations of high-end computing systems, we need improved methods to analyze the I/O performance, in order to identify bottlenecks and implement improvements that minimize the idle time caused by computing applications waiting for data transfers to complete. Our task in this project was to develop techniques for collecting the "raw" I/O performance data, which could then be studied using novel visual analysis techniques developed by our project partners from UC Davis.<br /><br />The challenge of instrumenting in detail (or, as we call it, tracing) a large system is that it consists of many thousands (possibly soon millions) of individual components, arranged in interdependent, hierarchical layers.&nbsp; In order to obtain a full view of the system behavior, all the relevant layers need to be instrumented and the activities at each layer be recorded with enough context that the progress of individual operations originating with the computing application can be followed through the layers all the way to the storage devices. Paradoxically, tracing the I/O subsystem can itself become a challenging I/O problem because of the large volumes of the trace data being generated. Yet, tracing must not put any significant additional load on the I/O system while the application is running, as it could then disturb the natural behavior of the system (the so-called observer effect). Meeting these conflicting goals requires careful planning and coordination.</p> <p>We were successful in obtaining I/0 event traces up to 16,000 compute processes on the Argonne Leadership Computing Facility system Intrepid, spanning application processes, file system clients, and file system servers. This was the first time that such correlated, multilayer data has been collected at this scale. The data continues to be analyzed and visualized by our project partners. Our work was also instrumental in improving the scalability of tracing workloads on the Oak Ridge Leadership Computing Facility system Jaguar up to 200,000 compute processes, in collaboration with Oak Ridge National Laboratory and TU Dresden, Germany. This achievement involved the introduction of an additional layer of staging nodes between the application and the file system, which could temporarily store the raw trace data and rearrange it in a more optimal fashion before writing it out to storage, while letting the application run undisturbed. Our improvements increased the scalability by an order of magnitude, setting a new benchmark on application tracing on high-end computing systems.</p><br> <p>            Last Modified: 10/29/2012<br>      Modified by: Kamil&nbsp;A&nbsp;Iskra</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Supercomputers have been described as machines that turn compute-bound problems into input/output-bound ones. Their massive&mdash;and rapidly increasing&mdash;computing subsystems demand equally capable input/output (I/O) infrastructure to supply them the input data and extract the results without undue delays. Unfortunately research into the performance analysis and optimization of I/O systems has lagged behind its computational counterparts. Yet, with the rapidly increasing complexity of new generations of high-end computing systems, we need improved methods to analyze the I/O performance, in order to identify bottlenecks and implement improvements that minimize the idle time caused by computing applications waiting for data transfers to complete. Our task in this project was to develop techniques for collecting the "raw" I/O performance data, which could then be studied using novel visual analysis techniques developed by our project partners from UC Davis.  The challenge of instrumenting in detail (or, as we call it, tracing) a large system is that it consists of many thousands (possibly soon millions) of individual components, arranged in interdependent, hierarchical layers.  In order to obtain a full view of the system behavior, all the relevant layers need to be instrumented and the activities at each layer be recorded with enough context that the progress of individual operations originating with the computing application can be followed through the layers all the way to the storage devices. Paradoxically, tracing the I/O subsystem can itself become a challenging I/O problem because of the large volumes of the trace data being generated. Yet, tracing must not put any significant additional load on the I/O system while the application is running, as it could then disturb the natural behavior of the system (the so-called observer effect). Meeting these conflicting goals requires careful planning and coordination.  We were successful in obtaining I/0 event traces up to 16,000 compute processes on the Argonne Leadership Computing Facility system Intrepid, spanning application processes, file system clients, and file system servers. This was the first time that such correlated, multilayer data has been collected at this scale. The data continues to be analyzed and visualized by our project partners. Our work was also instrumental in improving the scalability of tracing workloads on the Oak Ridge Leadership Computing Facility system Jaguar up to 200,000 compute processes, in collaboration with Oak Ridge National Laboratory and TU Dresden, Germany. This achievement involved the introduction of an additional layer of staging nodes between the application and the file system, which could temporarily store the raw trace data and rearrange it in a more optimal fashion before writing it out to storage, while letting the application run undisturbed. Our improvements increased the scalability by an order of magnitude, setting a new benchmark on application tracing on high-end computing systems.       Last Modified: 10/29/2012       Submitted by: Kamil A Iskra]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
