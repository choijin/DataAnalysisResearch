<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Managing Large-scale Uncertain Data Repositories</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2009</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>498538.00</AwardTotalIntnAmount>
<AwardAmount>498538</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Frank Olken</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Increasing numbers of real-world application domains are generating data that is&lt;br/&gt;inherently noisy, incomplete, and probabilistic in nature.  Examples of such&lt;br/&gt;data include measurement data collected by sensor networks, observation data in&lt;br/&gt;the context of social networks, scientific and biomedical data, and data&lt;br/&gt;collected by various online cyber-sources. The data uncertainties may be a&lt;br/&gt;result of the fundamental limitations of the underlying measurement&lt;br/&gt;infrastructures, the inherent ambiguity in the domain, or they may be a&lt;br/&gt;side-effect of the rich probabilistic modeling typically performed to extract&lt;br/&gt;high-level events from sensor and cyber data.  Similarly, when attempting to&lt;br/&gt;integrate heterogeneous data sources ("data integration") or extracting&lt;br/&gt;structured information from text ("information extraction"), the results are&lt;br/&gt;approximate and uncertain at best.  However, there is currently a lack of data&lt;br/&gt;management tools that can reason about large volumes of uncertain data, and&lt;br/&gt;hence the information about the uncertainty is often either discarded or&lt;br/&gt;reasoned about only superficially.&lt;br/&gt;&lt;br/&gt;In this project, we are building a complete probabilistic data management&lt;br/&gt;system, called PrDB, that can manage, store, and process large-scale&lt;br/&gt;repositories of uncertain data.  PrDB unifies ideas from "large-scale structured&lt;br/&gt;graphical models" like probabilistic relational models (PRMs), developed in the&lt;br/&gt;machine learning literature, and "probabilistic query processing", studied in&lt;br/&gt;the database literature. PrDB framework is based on the notion of "shared&lt;br/&gt;factors", which not only allows us to express and manipulate uncertainties at&lt;br/&gt;various levels of abstractions, but also supports capturing rich correlations&lt;br/&gt;among the uncertain data. PrDB supports a declarative SQL-like language for&lt;br/&gt;specifying uncertain data and the correlations among them. PrDB also supports&lt;br/&gt;exact and approximate evaluation of a wide range of queries including inference&lt;br/&gt;queries, SQL queries, and decision-support queries.&lt;br/&gt;&lt;br/&gt;The cross-disciplinary research undertaken during this project will enable us to&lt;br/&gt;simultaneously address the challenges in the areas of probabilistic databases&lt;br/&gt;and machine learning, and allow us to transfer the key technologies developed&lt;br/&gt;between those areas, thus advancing the research in both areas. It will enable&lt;br/&gt;the development of a significant and high-impact new class of real-world&lt;br/&gt;applications, in a variety of domains including health informatics, social media&lt;br/&gt;management, World Wide Web, and scientific databases. The PrDB system source&lt;br/&gt;code, and the datasets generated during the project, will be released using an&lt;br/&gt;appropriate open source license, at the project web site:&lt;br/&gt;http://www.cs.umd.edu/db/PrDB.html</AbstractNarration>
<MinAmdLetterDate>09/09/2009</MinAmdLetterDate>
<MaxAmdLetterDate>08/05/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0916736</AwardID>
<Investigator>
<FirstName>Lise</FirstName>
<LastName>Getoor</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lise Getoor</PI_FULL_NAME>
<EmailAddress>getoor@soe.ucsc.edu</EmailAddress>
<PI_PHON>8314591489</PI_PHON>
<NSF_ID>000465719</NSF_ID>
<StartDate>09/09/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Amol</FirstName>
<LastName>Deshpande</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amol V Deshpande</PI_FULL_NAME>
<EmailAddress>amol@cs.umd.edu</EmailAddress>
<PI_PHON>3014056269</PI_PHON>
<NSF_ID>000486255</NSF_ID>
<StartDate>09/09/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~161768</FUND_OBLG>
<FUND_OBLG>2010~336770</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This NSF-funded project was motivated by the observation that increasing numbers of real-world application domains are generating data that is inherently noisy, incomplete, and probabilistic in nature. Examples of such data include measurement data collected by sensor networks, observation data in the context of social networks and scientific databases, and data collected by various online cyber-sources. The data uncertainties may be a result of the fundamental limitations of the underlying measurement infrastructures, the inherent ambiguity in the domain, or they may be a side-effect of the rich probabilistic modeling typically performed to extract high-level events from sensor and Web data. Similarly, when attempting to integrate heterogeneous data sources ("data integration") or extracting structured information from text ("information extraction"), the results are approximate and uncertain at best. It is relatively straightforward to capture such uncertainties by adding appropriate annotations to the data, with "probabilities" being the most natural and intuitive form of such annotations. However, traditional data management tools cannot properly reason or operate upon such annotated data. Further, executing even simple analysis tasks on such data turns out to be intractable in many cases. The goal of this project was to develop a "probabilistic" data management system to manage, store, and query large-scale repositories of data annotated with probabilities. While there has been prior work on developing such probabilistic data management systems, most of that prior work considered simplistic correlation models that are insufficient to represent the uncertainties in many of the application domains.<br /><br />The key outcomes of this project can be summarized as follows. We built a unifying framework, and a prototype system called "PrDB", for combining the rich modeling power of structured graphical models, developed in the machine learning literature, and the large-scale data processing capabilities of database systems. PrDB framework is based on the notion of "shared factors", which not only allows one to express and manipulate uncertainties at various levels of abstractions, but also supports capturing rich correlations among the uncertain data. PrDB supports a high-level declarative SQL-like language for specifying uncertain data and the correlations among them. PrDB also supports exact and approximate evaluation of a wide range of queries including inference queries, SQL queries, and decision-support queries. To build PrDB and to make it efficient at handling large volumes of uncertain data, we developed a suite of novel algorithms and data structures. We showed how query evaluation in probabilistic databases is equivalent to "inference" in probabilistic graphical models, and developed several novel inference and query processing techniques. We designed and implemented indexing structures for querying large-scale correlated datasets that result in orders-of-magnitude performance improvements. We developed a learning-based approach to "ranking" over probabilistic data that combined many previous approaches in a single, elegant framework. Our algorithms work for both discrete and continuous uncertainties, can scale to very large datasets, and can handle correlations in the data. We developed the notion of "consensus answers" that promises to significantly increase the utility of probabilistic databases, by allowing us to systematically convert probabilistic query results into deterministic query results; the latter are better suited for further analysis and decision making by the users and applications. We proposed and developed the notions of "influence" and "explanation" to better understand query results, and to facilitate robust query processing over uncertain databases. We proposed a general uncertain graph model that captures a variety of different unce...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This NSF-funded project was motivated by the observation that increasing numbers of real-world application domains are generating data that is inherently noisy, incomplete, and probabilistic in nature. Examples of such data include measurement data collected by sensor networks, observation data in the context of social networks and scientific databases, and data collected by various online cyber-sources. The data uncertainties may be a result of the fundamental limitations of the underlying measurement infrastructures, the inherent ambiguity in the domain, or they may be a side-effect of the rich probabilistic modeling typically performed to extract high-level events from sensor and Web data. Similarly, when attempting to integrate heterogeneous data sources ("data integration") or extracting structured information from text ("information extraction"), the results are approximate and uncertain at best. It is relatively straightforward to capture such uncertainties by adding appropriate annotations to the data, with "probabilities" being the most natural and intuitive form of such annotations. However, traditional data management tools cannot properly reason or operate upon such annotated data. Further, executing even simple analysis tasks on such data turns out to be intractable in many cases. The goal of this project was to develop a "probabilistic" data management system to manage, store, and query large-scale repositories of data annotated with probabilities. While there has been prior work on developing such probabilistic data management systems, most of that prior work considered simplistic correlation models that are insufficient to represent the uncertainties in many of the application domains.  The key outcomes of this project can be summarized as follows. We built a unifying framework, and a prototype system called "PrDB", for combining the rich modeling power of structured graphical models, developed in the machine learning literature, and the large-scale data processing capabilities of database systems. PrDB framework is based on the notion of "shared factors", which not only allows one to express and manipulate uncertainties at various levels of abstractions, but also supports capturing rich correlations among the uncertain data. PrDB supports a high-level declarative SQL-like language for specifying uncertain data and the correlations among them. PrDB also supports exact and approximate evaluation of a wide range of queries including inference queries, SQL queries, and decision-support queries. To build PrDB and to make it efficient at handling large volumes of uncertain data, we developed a suite of novel algorithms and data structures. We showed how query evaluation in probabilistic databases is equivalent to "inference" in probabilistic graphical models, and developed several novel inference and query processing techniques. We designed and implemented indexing structures for querying large-scale correlated datasets that result in orders-of-magnitude performance improvements. We developed a learning-based approach to "ranking" over probabilistic data that combined many previous approaches in a single, elegant framework. Our algorithms work for both discrete and continuous uncertainties, can scale to very large datasets, and can handle correlations in the data. We developed the notion of "consensus answers" that promises to significantly increase the utility of probabilistic databases, by allowing us to systematically convert probabilistic query results into deterministic query results; the latter are better suited for further analysis and decision making by the users and applications. We proposed and developed the notions of "influence" and "explanation" to better understand query results, and to facilitate robust query processing over uncertain databases. We proposed a general uncertain graph model that captures a variety of different uncertainties, including "identity uncertainty" that was not captured by pri...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
