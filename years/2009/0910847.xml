<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Flash Gordon: A Data Intensive Computer</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2009</AwardEffectiveDate>
<AwardExpirationDate>03/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>20000000.00</AwardTotalIntnAmount>
<AwardAmount>21448911</AwardAmount>
<AwardInstrument>
<Value>Cooperative Agreement</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Edward Walker</SignBlockName>
<PO_EMAI>edwalker@nsf.gov</PO_EMAI>
<PO_PHON>7032924863</PO_PHON>
</ProgramOfficer>
<AbstractNarration>ABSTRACT UCSD 0910847 Norman, Michael L.&lt;br/&gt;&lt;br/&gt;This project supports the acquisition, deployment and operation of a new supercomputing system suitable for data-intensive applications.  The system, to be known as Flash Gordon, will be deployed by the University of California at San Diego at the San Diego Supercomputer Center and integrated into the TeraGrid.  The system, which has been designed by Appro International Incorporated, with partners Intel and ScaleMP, seeks to bridge the widening latency gap between main memory and rotating disk storage in modern computing systems.  It uses flash memory to provide a level of dense, affordable, low-latency storage that can be configured as either extended swap space or a very fast file system.  The system will consist of very large shared virtual-memory, cache-coherent "super-nodes" to support a versatile set of programming paradigms.   Peak performance will exceed 200 teraflops/s in double precision.&lt;br/&gt;&lt;br/&gt;Flash Gordon's large addressable virtual memory, low-latency flash memory, and user-friendly programming environment will provide a step-up in capability for data-intensive applications that scale poorly on current large-scale architectures, providing a resource that will enable transformative research in many research domains.  Even sequential codes will be able to address up to terabytes of fast virtual memory. &lt;br/&gt;&lt;br/&gt;Examples of scientific challenges, as described in the proposal, that this resource will allow researchers to tackle, include the following. &lt;br/&gt;&lt;br/&gt;De Novo Genome Assembly: Gene sequencers produce information about many small fragments of a genome.  Some recent assembly algorithms use a graph-based approach, much more readily executed on a shared-memory system.  Using Flash Gordon, researchers will be able to rapidly assemble complex genomes such as mammalian genomes.&lt;br/&gt;&lt;br/&gt;Astronomy: Modern astronomy databases can be large; for example, the Sloan Digital Sky Survey is approximately six terabytes in size.  Typically, the analysis algorithms that researchers use to perform complex searches for astronomical phenomena can be implemented more easily on shared-memory systems.  Flash Gordon will enable researchers to load a copy of the Sloan Digital Sky Survey into the flash memory associated with a super-node, greatly extending the types of analyses astronomers can make.&lt;br/&gt;&lt;br/&gt;Astrophysics: Cosmological simulations produce many terabytes of output describing the simulated universe.  Detailed analysis of the results of these simulations, to find features such as collapsed halos, galaxy mergers, dwarf galaxies, and galaxy clusters, often requires density-based cluster analysis that does not parallelize well.  With Flash Gordon, these analyses can be accelerated by exploiting the large SMP partitions and fast flash memory.&lt;br/&gt;&lt;br/&gt;Interaction Networks: Interaction networks, graphs representing the relationships between objects, are used in research in areas such as epidemiology, phylogenetics, systems biology, and population biology.  These interaction networks can represent relationships between types of data stored in different databases; for example, the combination of social network databases with medical records and genomic profiles to explore questions such as genetic resistance to disease.  Flash Gordon will speed analysis of large interaction networks because the databases can be stored on the solid-state disks, greatly reducing access time and permitting more complex types of analysis.&lt;br/&gt;&lt;br/&gt;The project team will leverage a number of ongoing educational activities at UCSD to expand and diversify the community of users that can utilize this computational resource, including successful outreach programs for women and minorities from underrepresented groups in science and engineering.  The project will also create a summer training program for undergraduates.</AbstractNarration>
<MinAmdLetterDate>09/16/2009</MinAmdLetterDate>
<MaxAmdLetterDate>06/07/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>CoopAgrmnt</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0910847</AwardID>
<Investigator>
<FirstName>Wayne</FirstName>
<LastName>Pfeiffer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wayne Pfeiffer</PI_FULL_NAME>
<EmailAddress>pfeiffer@sdsc.edu</EmailAddress>
<PI_PHON>8585345120</PI_PHON>
<NSF_ID>000410328</NSF_ID>
<StartDate>12/24/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Norman</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael L Norman</PI_FULL_NAME>
<EmailAddress>mlnorman@ucsd.edu</EmailAddress>
<PI_PHON>8588225461</PI_PHON>
<NSF_ID>000235680</NSF_ID>
<StartDate>09/16/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Allan</FirstName>
<LastName>Snavely</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Allan Snavely</PI_FULL_NAME>
<EmailAddress>allans@sdsc.edu</EmailAddress>
<PI_PHON>8585345158</PI_PHON>
<NSF_ID>000423313</NSF_ID>
<StartDate>09/16/2009</StartDate>
<EndDate>06/20/2012</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Steven</FirstName>
<LastName>Swanson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Steven Swanson</PI_FULL_NAME>
<EmailAddress>swanson@cs.ucsd.edu</EmailAddress>
<PI_PHON>8585340246</PI_PHON>
<NSF_ID>000069307</NSF_ID>
<StartDate>06/20/2012</StartDate>
<EndDate>12/24/2015</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Shawn</FirstName>
<LastName>Strande</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shawn M Strande</PI_FULL_NAME>
<EmailAddress>sstrande@ucsd.edu</EmailAddress>
<PI_PHON>8585346948</PI_PHON>
<NSF_ID>000451928</NSF_ID>
<StartDate>06/20/2012</StartDate>
<EndDate>04/14/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[Office of Contract &amp; Grant A]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramElement>
<Code>7619</Code>
<Text>Innovative HPC</Text>
</ProgramElement>
<ProgramReference>
<Code>7619</Code>
<Text>EQUIPMENT ACQUISITIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>9145</Code>
<Text>SPECIAL PROGRAMS-RESERVE</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~11007882</FUND_OBLG>
<FUND_OBLG>2010~9288560</FUND_OBLG>
<FUND_OBLG>2012~23750</FUND_OBLG>
<FUND_OBLG>2014~44910</FUND_OBLG>
<FUND_OBLG>2015~1083809</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The San Diego Supercomputer Center at the University of California, San Diego deployed the Gordon data-intensive supercomputer during 2011. After extensive testing, SDSC operated Gordon for allocated access by academic researchers, educators, and students through NSF&rsquo;s XSEDE project from March 2012 to March 2017. Some NSF-funded projects continue to use Gordon, with its operation currently funded by the Simons Foundation. During its 61 months of operation as an XSEDE resource, it ran 2.3 million jobs; it provided 450 million core hours for more than 9,000 users, most of whom gained access via a gateway rather than via the command line; and it enabled publication of hundreds of scientific papers.</p> <p>Gordon has a peak speed of 341 Tflop/s delivered by 16,384 cores in 1,024 compute nodes, each with 64 GB of DRAM. Its signature feature is 300 TB of flash memory served by 64 I/O nodes, with 300 GB accessible to each compute node in the normal configuration.</p> <p>Gordon was designed by SDSC in collaboration with vendor partners Appro, Intel, Mellanox, and ScaleMP. The design incorporated three significant technology innovations:</p> <ul> <li>Extensive use of flash memory via SSDs to bridge the latency gap in the memory hierarchy between DRAM and disk, thereby greatly improving performance for analyses that generate and reuse large amounts of intermediate data.</li> <li>Virtual shared-memory via the vSMP software, which enables large-memory analyses in a cost-effective manner by aggregating up to 1 TB of DRAM from 16 separate compute nodes.</li> <li>A dual-rail, 3D torus interconnect with 4x4x4 topology that uses separate rails for message passing and I/O to minimize interference between them.</li> </ul> <p>With faster processors and more DRAM per node than on most other XSEDE resources when operation of Gordon began, it was very attractive to users from a broad swath of science and engineering disciplines. The large amount of DRAM was used to obtain a better understanding of the structure of viruses in two studies. Researchers from Cornell and TSRI reconstructed the HK97 virus from microscopy data, while researchers from UCSD modeled the surface proteins of avian flu virus.</p> <p>Flash memory was widely used as an alternative to disk for I/O of intermediate data generated during data-intensive analyses. Researchers doing quantum chemistry and materials science analyses made extensive use of this capability to store and retrieve the large numbers of electron integrals that arise during such analyses. A research group at Harvard screened millions of candidate semiconductors to find better photovoltaic materials for solar cells. Each candidate was analyzed in a separate job, with hundreds of jobs running simultaneously, all using flash memory to improve performance and minimize the impact on other users that disk I/O would entail.</p> <p>Flash memory was also used as persistent storage for several projects that received dedicated allocations of an I/O node, and use by the following two projects continues. The IODA project, operated by CAIDA at SDSC, monitors Internet traffic data to detect hacker activity or connectivity disruptions. Deploying the IODA databases on the flash memory of an I/O node resulted in an 18x speedup compared to using disk. The OpenTopography gateway, another project hosted at SDSC, provides persistent access to topography data from high-resolution LiDAR and to tools for analyzing those data.</p> <p>Virtual shared memory enabled by the vSMP software was used by researchers who needed even more DRAM than in a single compute node or who could benefit from using more than 16 cores in a shared-memory analysis. Researchers from Cornell studied the stress response of a vertebra using a high-resolution, finite-element model. Storing the resulting 750-GB stiffness matrix in DRAM aggregated across multiple compute nodes allowed the analysis to complete more than 10x faster than splitting the matrix between 64 GB of DRAM in a single compute node and 700 GB of disk. Researchers from UC Irvine and UCSD analyzed a large network in mathematical anthropology using 256 cores in a shared-memory model, while other researchers at UCSD post-processed large cosmological simulations using both 256 cores and 256 GB of shared memory.</p> <p>Science gateways, especially the CIPRES gateway for phylogenetic research, accounted for an increasing percentage of users and enabled many significant results during the operation of Gordon as an XSEDE resource. A research team led from UC Berkeley generated a new tree of life that revealed the existence of a massive new superphylum of bacteria.</p> <p>Tens of outreach activities exposed thousands of researchers, educators, and students to the benefits of data-intensive computing, especially those enabled by Gordon. SDSC staff hosted tutorials at scientific meetings, workshops at SDSC and on other university campuses, and annual summer institutes. These activities targeted potential users of Gordon in varied disciplines, especially fields such as finance and ecology that had previously made little use of high-performance computing. University faculty also used Gordon for classes in parallel processing and data science.</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/28/2017<br>      Modified by: Michael&nbsp;L&nbsp;Norman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The San Diego Supercomputer Center at the University of California, San Diego deployed the Gordon data-intensive supercomputer during 2011. After extensive testing, SDSC operated Gordon for allocated access by academic researchers, educators, and students through NSF?s XSEDE project from March 2012 to March 2017. Some NSF-funded projects continue to use Gordon, with its operation currently funded by the Simons Foundation. During its 61 months of operation as an XSEDE resource, it ran 2.3 million jobs; it provided 450 million core hours for more than 9,000 users, most of whom gained access via a gateway rather than via the command line; and it enabled publication of hundreds of scientific papers.  Gordon has a peak speed of 341 Tflop/s delivered by 16,384 cores in 1,024 compute nodes, each with 64 GB of DRAM. Its signature feature is 300 TB of flash memory served by 64 I/O nodes, with 300 GB accessible to each compute node in the normal configuration.  Gordon was designed by SDSC in collaboration with vendor partners Appro, Intel, Mellanox, and ScaleMP. The design incorporated three significant technology innovations:  Extensive use of flash memory via SSDs to bridge the latency gap in the memory hierarchy between DRAM and disk, thereby greatly improving performance for analyses that generate and reuse large amounts of intermediate data. Virtual shared-memory via the vSMP software, which enables large-memory analyses in a cost-effective manner by aggregating up to 1 TB of DRAM from 16 separate compute nodes. A dual-rail, 3D torus interconnect with 4x4x4 topology that uses separate rails for message passing and I/O to minimize interference between them.   With faster processors and more DRAM per node than on most other XSEDE resources when operation of Gordon began, it was very attractive to users from a broad swath of science and engineering disciplines. The large amount of DRAM was used to obtain a better understanding of the structure of viruses in two studies. Researchers from Cornell and TSRI reconstructed the HK97 virus from microscopy data, while researchers from UCSD modeled the surface proteins of avian flu virus.  Flash memory was widely used as an alternative to disk for I/O of intermediate data generated during data-intensive analyses. Researchers doing quantum chemistry and materials science analyses made extensive use of this capability to store and retrieve the large numbers of electron integrals that arise during such analyses. A research group at Harvard screened millions of candidate semiconductors to find better photovoltaic materials for solar cells. Each candidate was analyzed in a separate job, with hundreds of jobs running simultaneously, all using flash memory to improve performance and minimize the impact on other users that disk I/O would entail.  Flash memory was also used as persistent storage for several projects that received dedicated allocations of an I/O node, and use by the following two projects continues. The IODA project, operated by CAIDA at SDSC, monitors Internet traffic data to detect hacker activity or connectivity disruptions. Deploying the IODA databases on the flash memory of an I/O node resulted in an 18x speedup compared to using disk. The OpenTopography gateway, another project hosted at SDSC, provides persistent access to topography data from high-resolution LiDAR and to tools for analyzing those data.  Virtual shared memory enabled by the vSMP software was used by researchers who needed even more DRAM than in a single compute node or who could benefit from using more than 16 cores in a shared-memory analysis. Researchers from Cornell studied the stress response of a vertebra using a high-resolution, finite-element model. Storing the resulting 750-GB stiffness matrix in DRAM aggregated across multiple compute nodes allowed the analysis to complete more than 10x faster than splitting the matrix between 64 GB of DRAM in a single compute node and 700 GB of disk. Researchers from UC Irvine and UCSD analyzed a large network in mathematical anthropology using 256 cores in a shared-memory model, while other researchers at UCSD post-processed large cosmological simulations using both 256 cores and 256 GB of shared memory.  Science gateways, especially the CIPRES gateway for phylogenetic research, accounted for an increasing percentage of users and enabled many significant results during the operation of Gordon as an XSEDE resource. A research team led from UC Berkeley generated a new tree of life that revealed the existence of a massive new superphylum of bacteria.  Tens of outreach activities exposed thousands of researchers, educators, and students to the benefits of data-intensive computing, especially those enabled by Gordon. SDSC staff hosted tutorials at scientific meetings, workshops at SDSC and on other university campuses, and annual summer institutes. These activities targeted potential users of Gordon in varied disciplines, especially fields such as finance and ecology that had previously made little use of high-performance computing. University faculty also used Gordon for classes in parallel processing and data science.          Last Modified: 06/28/2017       Submitted by: Michael L Norman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
