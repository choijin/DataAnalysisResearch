<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Efficient Stochastic Oracle Based Algorithms for Stochastic Programming and Large Scale Convex Optimization</AwardTitle>
<AwardEffectiveDate>09/15/2009</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>327417.00</AwardTotalIntnAmount>
<AwardAmount>327417</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Junping Wang</SignBlockName>
</ProgramOfficer>
<AbstractNarration>The proposed research is aimed at  developing  computational techniques for handling two classes of complex convex problems. The first is Stochastic Programming, where objective and the constraints are given implicitly, as expectations over stochastic uncertain data, and thus are difficult to compute.&lt;br/&gt;The second class is formed by large-scale well-structured deterministic convex programs like those of  Linear or Semidefinite Programming, the difficulty coming from huge problem sizes; e.g., LP's with just few thousands of variables and constraints and  dense constraint matrices (arising, e.g., in Compressed Sensing and Machine Learning), or typical Semidefinite programs of similar sizes, are often beyond the grasp of the state-of-the-art solvers. The proposed research is  intended to treat both these problem classes simultaneously, reducing them to variational inequalities with monotone operators represented by unbiased easy-to-compute stochastic oracles. These oracles arise naturally in the context  of Stochastic Programming and can be constructed (e.g., via randomizing matrix-vector multiplications) in the case of large-scale LP's and SDP's. In order to  solve the resulting stochastic variational inequalities it is proposed to employ  recently developed algorithms (Robust Mirror Descent Stochastic Approximation and Stochastic Mirror Prox) which, under favorable circumstances,  exhibit nearly dimension-independent and theoretically unimprovable in the large scale case rate of convergence.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The last two decades have witnessed dramatic progress in techniques for solving convex optimization problems -- progress which by some  estimates led to 1.000.000-fold  performance growth, the factor  nearly equally contributed by advances in algorithms and by increase in hardware power.&lt;br/&gt; In spite of this progress, a significant gap between what is needed  by applications and what can be routinely solved by the  state-of-the-art convex programming algorithms still persists,  especially when the problems to be processed are intrinsically  difficult. The proposed research is aimed at bridging the above gap  by developing novel, more powerful than the existing alternatives,  computational techniques based on systematic replacement of difficult  to compute in the large scale case quantities, like matrix-vector  products involving huge matrices, with relatively easy to compute  randomized estimates of these quantities. Coupled with carefully  designed stochastic type algorithms, this allows to solve huge  problems, unaccessible by currently available deterministic algorithms,  in a reasonable time with a reasonable accuracy by observing only a  small portion of the data. This can be of paramount importance, e.g., for various applications of machine learning techniques where the amount of available data by far too large for direct processing. &lt;br/&gt;Some  preliminary theoretical and numerical results are very encouraging and  suggest that the proposed avenue of research is&lt;br/&gt;highly promising, and if successful   the proposed research will advance &lt;br/&gt;optimization theory and practice by enriching   the algorithmic know-how&lt;br/&gt;related to processing complex optimization problems.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/14/2009</MinAmdLetterDate>
<MaxAmdLetterDate>09/14/2009</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0914785</AwardID>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Shapiro</LastName>
<EmailAddress>ashapiro@isye.gatech.edu</EmailAddress>
<StartDate>09/14/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Arkadi</FirstName>
<LastName>Nemirovski</LastName>
<EmailAddress>nemirovs@isye.gatech.edu</EmailAddress>
<StartDate>09/14/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
</Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
