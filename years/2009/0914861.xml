<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small-Collaborative: Efficient Bayesian Model Computation for Large and High Dimensional Data Sets</AwardTitle>
<AwardEffectiveDate>08/01/2009</AwardEffectiveDate>
<AwardExpirationDate>07/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>339023.00</AwardTotalIntnAmount>
<AwardAmount>339023</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
</ProgramOfficer>
<AbstractNarration>This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5.&lt;br/&gt;&lt;br/&gt;This grant supports research in adapting and optimizing Markov Chain Monte Carlo methods to compute Bayesian models on large data sets resident on secondary storage, exploiting database systems techniques. The work will seek to optimize computations, preserve model accuracy and accelerate sampling techniques from large and high dimensional data sets, exploiting &lt;br/&gt;different data set layouts and indexing data structures. The team will develop weighted sampling methods that can produce models of similar quality as traditional sampling methods, but which are much faster for large data sets that cannot fit on primary storage. One sub-goal will study how to compress a large data set preserving its statistical properties for parametric Bayesian models, and then adapting existing methods to handle compressed data sets. &lt;br/&gt;&lt;br/&gt;Intellectual Merit and Broader Impact&lt;br/&gt;&lt;br/&gt;This endeavor requires developing novel computational methods that can work efficiently with large data sets and numerically intensive computations. The main technical difficulty is that it is not possible to obtain accurate samples from subsamples of a large data set. Therefore, the team will focus on accelerating sampling from the posterior distribution based on the entire data set. This problem is unusually difficult because stochastic methods require a high number of iterations (typically thousands) over the entire data set to converge. However, if the data set is compressed it becomes necessary to generalize traditional methods to use weighted points combined with higher order statistics, beyond the well-known sufficient statistics for the Gaussian distribution. Developing optimizations combining primary and secondary storage is quite different from optimizing an algorithm that works only on primary storage. This research effort requires comprehensive statistical knowledge on both Bayesian models and stochastic methods, beyond traditional data mining methods. A strong database systems background in optimizing computations with large disk-resident matrices is also necessary. This research will enable a faster solution of larger scale problems compared to modern statistical packages to solve stochastic models. Bayesian analysis and model management will be easier, faster and more flexible. &lt;br/&gt;&lt;br/&gt;Broad Impact&lt;br/&gt;&lt;br/&gt;This research will occur within the context of three separate application areas: cancer, water pollution, and medical data sets with patients having cancer and heart disease. The educational component of this grant will enhance current teaching and research on data mining. In an advanced data mining course students will apply stochastic methods to compute complex Bayesian models on hundreds of variables and millions of records. Data mining research projects will be enhanced with Bayesian models, promoting interaction between statistics and computer science.&lt;br/&gt;&lt;br/&gt;Keywords: Bayesian model, stochastic method, database system</AbstractNarration>
<MinAmdLetterDate>07/30/2009</MinAmdLetterDate>
<MaxAmdLetterDate>07/30/2009</MaxAmdLetterDate>
<ARRAAmount>339023</ARRAAmount>
<AwardID>0914861</AwardID>
<Investigator>
<FirstName>Carlos</FirstName>
<LastName>Ordonez</LastName>
<EmailAddress>ordonez@cs.uh.edu</EmailAddress>
<StartDate>07/30/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Houston</Name>
<CityName>Houston</CityName>
<ZipCode>772042015</ZipCode>
<PhoneNumber>7137435773</PhoneNumber>
<StreetAddress>4800 Calhoun Boulevard</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
</Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>6890</Code>
<Text>RECOVERY ACT ACTION</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
