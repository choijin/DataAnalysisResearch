<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Large-Scale Structured Optimization:  Convex Relaxations and Gradient Methods</AwardTitle>
<AwardEffectiveDate>09/01/2009</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardAmount>0</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040100</Code>
<Directorate>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>rosemary renaut</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Optimization problems arise in signal/image denoising,&lt;br/&gt;compressed sensing, sensor network localization, multi-task learning, data&lt;br/&gt;mining/classification, are large-scale, nonlinear, but highly structured.&lt;br/&gt;An effective solution approach is to approximate the problems by&lt;br/&gt;convex relaxations that are computationally tractable and inherit the&lt;br/&gt;structures of the original problems.  Specialized computer algorithms&lt;br/&gt;are then developed to exploit the structures and&lt;br/&gt;efficiently solve the convex relaxations in real time.&lt;br/&gt;The investigator and his colleagues/students study the accuracy of&lt;br/&gt;different convex relaxations (i.e., how well they approximate the&lt;br/&gt;original problems) when data may be noisy, and develop new computer&lt;br/&gt;algorithms that are more efficient in theory and in practice.&lt;br/&gt;This includes coordinatewise and incremental gradient algorithms,&lt;br/&gt;possibly accelerated through extrapolation.&lt;br/&gt;Complexity and convergence rate are studied, as well as&lt;br/&gt;computer implementation and testing.&lt;br/&gt;Thus the twin topics of approximation by convex relaxations and&lt;br/&gt;algorithms for convex relaxations are integrated in their development.&lt;br/&gt;&lt;br/&gt;Measured data, from satellite images to biological images to stock indices&lt;br/&gt;to internet queries/usage, are inherently noisy and large size.&lt;br/&gt;How to process such large noisy data and extract key&lt;br/&gt;features/patterns is a major challenge, with important applications to&lt;br/&gt;image restoration/denoising, economic forecasting, pattern recognition,&lt;br/&gt;data classification, etc.  In the proposed research, parametrized&lt;br/&gt;mathematical models of the underlying data generating process are&lt;br/&gt;formulated and the parameters are tuned by specialized computer algorithms&lt;br/&gt;to optimize the model's predictive power.  The latter is achieved&lt;br/&gt;by optimizing a combination of the model's goodness-of-fit to data and a&lt;br/&gt;measure of model's simplicity.   This is motivated by Occam's Razor&lt;br/&gt;principle that the simplest is the best (in particular, for identifying&lt;br/&gt;underlying patterns and trends).  For example, a 1 Mega-pixel&lt;br/&gt;noisy image might be modeled by a weighted sum of 1000 "elementary"&lt;br/&gt;images from a dictionary.  The computer algorithm then finds weights&lt;br/&gt;that are sparse (i.e., few nonzeros) and&lt;br/&gt;  fit the model closely to the noisy image (say, in a least squares&lt;br/&gt;sense of the pixel values).  Predicting&lt;br/&gt;people's behavior based on past data is another&lt;br/&gt;example, such as the Netflix prize for predicting movie rating which&lt;br/&gt;has a training data set of over 100 million ratings from over 480 thousand&lt;br/&gt;people.  Owing to the large data size (and, in some cases, a need to&lt;br/&gt;process the data fast and in real time) and possibly high-dimensional&lt;br/&gt;parameter space, specialized algorithms based on new techniques (e.g.,&lt;br/&gt;work with small chunks of data or small number of parameters at each time)&lt;br/&gt;need to be developed.  This is the aim of the proposed research.</AbstractNarration>
<MinAmdLetterDate>08/18/2009</MinAmdLetterDate>
<MaxAmdLetterDate>09/16/2009</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0914215</AwardID>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Tseng</LastName>
<EmailAddress>tseng@math.washington.edu</EmailAddress>
<StartDate>08/18/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
</Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
