<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC:Small:Collaborative Research:Design and Evaluation of Socially Engaging Avatars</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2009</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>243147.00</AwardTotalIntnAmount>
<AwardAmount>259147</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will employ a cyclical two-step process to develop a computational model that embeds dynamic expression and socially engaging non-verbal gestures into talking avatars, and experimentally tests its usability within digital virtual environments involving human-digital agent interaction. Specifically, the research objectives of this project include: (1) synthesis of expressive talking faces and modeling of dynamic facial expressions, (2) synthesis of socially engaging non-verbal facial gestures, and (3) in-depth usability studies on resultant avatars. &lt;br/&gt;&lt;br/&gt;Digital immersive virtual environment technology has enormous implications for human-computer interaction. Many qualities of digital human representations, particularly those of human-appearing agents, are important for social engagement and social influence. In particular, non-verbal behaviors play a critical role. Among such behaviors, arguably the most important are facial expressions of emotion, which are critical for meaningful renderings of digital agents. To date, computational models that would permit such renderings are less than optimal. Indeed, an applicable and systematic computational model for rendering spontaneous, on-the-fly non-verbal facial gestures and integrating them with speech has not been created. &lt;br/&gt;&lt;br/&gt;The success of this proposed project will remove a major barrier to the widespread application of useful digital human representation technology for all applications in which computer-mediated communication can play a role, including commerce, education, health, engineering, and entertainment applications. In addition, it will have far-reaching scientific implications, providing a computationally tractable mechanism for embedding human qualities into computer-controlled entities that are used in other scientific and engineering fields. &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>07/07/2009</MinAmdLetterDate>
<MaxAmdLetterDate>06/22/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0914965</AwardID>
<Investigator>
<FirstName>Zhigang</FirstName>
<LastName>Deng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zhigang Deng</PI_FULL_NAME>
<EmailAddress>zhigang.deng@gmail.com</EmailAddress>
<PI_PHON>8324666116</PI_PHON>
<NSF_ID>000489047</NSF_ID>
<StartDate>07/07/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Houston</Name>
<CityName>Houston</CityName>
<ZipCode>772042015</ZipCode>
<PhoneNumber>7137435773</PhoneNumber>
<StreetAddress>4800 Calhoun Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>036837920</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF HOUSTON SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042916627</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Houston]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>772042015</ZipCode>
<StreetAddress><![CDATA[4800 Calhoun Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~243147</FUND_OBLG>
<FUND_OBLG>2010~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Intellectual merit outcomes</strong>:</p> <p>(1) <em><span style="text-decoration: underline;">Live speech driven facial animation techniques</span></em>: Live speech driven generation of lip-sync, head movement, and eye movement has been a very challenging problem in graphics and animation community for decades, due to: (i) &ldquo;Live speech driven&rdquo; implies the real-time performance of the algorithm; hence, its runtime performance needs to be highly efficient. (ii) Since only the prior and current information enclosed in live speech is available as the runtime input, many widely used dynamic programming schemes for generating lip-sync and facial gestures cannot be directly used for the task.</p> <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; In this project, the PI developed a novel, fully automated framework to generate realistic and synchronized eye and head movement on-the-fly based on live or prerecorded speech input (<strong><em>IEEE Transactions on Visualization and Computer Graphics, 2012, 18(11), pp. 1902-1914</em></strong>). In particular, the synthesized eye motion in the framework includes not only the traditional eye gaze but also the subtle eyelid movement and blink. Comparative user studies showed that the new approach outperformed various state-of-the-art facial gesture generation algorithms, and its results are measurably close to the ground truth. &nbsp;For live speech driven lip-sync, the PI developed a practical phoneme-based approach for live speech driven lip-sync (<strong><em>IEEE Computer Graphics and Applications, 2014 (accepted)</em></strong>). Besides generating realistic speech animation in real-time, the developed phoneme-based approach can straightforwardly handle speech input from different speakers. Compared to various existing lip-sync approaches, the main advantages of the developed approach are its efficiency, simplicity practicalness, and capability of handling live speech input in real-time. In addition, A long-standing problem in marker-based facial motion capture is what are the optimal facial mocap marker layouts. The PI developed an approach to compute optimized marker layouts for facial motion acquisition as optimization of characteristic control points from a set of high-resolution, ground-truth facial mesh sequences (<strong><em>IEEE Transactions on Visualization and Computer Graphics, 2013, 19(11), pp. 1859-1871</em></strong>).</p> <p>&nbsp;</p> <p>(2) <em><span style="text-decoration: underline;">Perceptual models for facial animation</span></em>: &nbsp;Although various efforts have been attempted to produce realistic facial animations with humanlike emotions; nevertheless, how to efficiently measure and synthesize believable and expressive facial animations is still a challenging research topic. The PI investigated how to build computational models to quantify the perceptual aspects of computer generated talking avatars. In order to address two unresolved research questions in data-driven speech animation: (i) can we automatically predict the quality of dynamically synthesized speech animations without conducting actual user studies? And (ii) can we dynamically compare and determine which algorithm (among them) can synthesize the best speech animation for specific text or speech input? The PI developed a novel statistical model to automatically predict the quality of synthesized speech animations on-the-fly generated by various data-driven algorithms (<strong>IEEE Transactions on Visualization and Computer graphics, 2012, 18(11), pp. 1915-1927</strong>). Second, previous studies were primarily focused on qualitative understanding of human perception on avatar head movements. The quantitative association between human perception and the audio-head motion characteristics of talking avatars remains to be uncovered. The PI quantified the correlation between perceptual user ratings (obtained via user study) and join...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual merit outcomes:  (1) Live speech driven facial animation techniques: Live speech driven generation of lip-sync, head movement, and eye movement has been a very challenging problem in graphics and animation community for decades, due to: (i) "Live speech driven" implies the real-time performance of the algorithm; hence, its runtime performance needs to be highly efficient. (ii) Since only the prior and current information enclosed in live speech is available as the runtime input, many widely used dynamic programming schemes for generating lip-sync and facial gestures cannot be directly used for the task.         In this project, the PI developed a novel, fully automated framework to generate realistic and synchronized eye and head movement on-the-fly based on live or prerecorded speech input (IEEE Transactions on Visualization and Computer Graphics, 2012, 18(11), pp. 1902-1914). In particular, the synthesized eye motion in the framework includes not only the traditional eye gaze but also the subtle eyelid movement and blink. Comparative user studies showed that the new approach outperformed various state-of-the-art facial gesture generation algorithms, and its results are measurably close to the ground truth.  For live speech driven lip-sync, the PI developed a practical phoneme-based approach for live speech driven lip-sync (IEEE Computer Graphics and Applications, 2014 (accepted)). Besides generating realistic speech animation in real-time, the developed phoneme-based approach can straightforwardly handle speech input from different speakers. Compared to various existing lip-sync approaches, the main advantages of the developed approach are its efficiency, simplicity practicalness, and capability of handling live speech input in real-time. In addition, A long-standing problem in marker-based facial motion capture is what are the optimal facial mocap marker layouts. The PI developed an approach to compute optimized marker layouts for facial motion acquisition as optimization of characteristic control points from a set of high-resolution, ground-truth facial mesh sequences (IEEE Transactions on Visualization and Computer Graphics, 2013, 19(11), pp. 1859-1871).     (2) Perceptual models for facial animation:  Although various efforts have been attempted to produce realistic facial animations with humanlike emotions; nevertheless, how to efficiently measure and synthesize believable and expressive facial animations is still a challenging research topic. The PI investigated how to build computational models to quantify the perceptual aspects of computer generated talking avatars. In order to address two unresolved research questions in data-driven speech animation: (i) can we automatically predict the quality of dynamically synthesized speech animations without conducting actual user studies? And (ii) can we dynamically compare and determine which algorithm (among them) can synthesize the best speech animation for specific text or speech input? The PI developed a novel statistical model to automatically predict the quality of synthesized speech animations on-the-fly generated by various data-driven algorithms (IEEE Transactions on Visualization and Computer graphics, 2012, 18(11), pp. 1915-1927). Second, previous studies were primarily focused on qualitative understanding of human perception on avatar head movements. The quantitative association between human perception and the audio-head motion characteristics of talking avatars remains to be uncovered. The PI quantified the correlation between perceptual user ratings (obtained via user study) and joint audio-head motion features as well as head motion patterns in the frequency-domain (ACM CHI Conference 2011, pp. 2699-2702).     (3) Example-based skinning decomposition and compression: Linear Blend Skinning (LBS) is the most widely used skinning model in entertainment industry practices to date. The PI developed a new Smooth Skinning Decomposition approach with Rig...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
