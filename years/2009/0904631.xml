<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Scalable Algorithms for Multiscale Modeling and Analysis of Turbulent Combustion</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2010</AwardEffectiveDate>
<AwardExpirationDate>02/29/2016</AwardExpirationDate>
<AwardTotalIntnAmount>1500000.00</AwardTotalIntnAmount>
<AwardAmount>1500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rajiv Ramnath</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Pascucci&lt;br/&gt;0904631&lt;br/&gt;&lt;br/&gt;Accurate simulation of turbulent combustion is a major open problem requiring petascale computing to resolve highly nonlinear coupling of physical processes over a wide range of length and time scales. The PIs approach to develop new modeling and algorithmic approaches for this problem to tackle effectively High Performance Computing (HPC) for combustion simulation at the Petascale. The PI's approach combines three techniques: automatic algorithm parallelization, multidimensional data analysis for model reduction, and multi-scale modeling with topological analysis to connect models at different scales. The algorithm parallelization is based on an algorithmic analysis that detects dependencies among computing stages, using graph theory to detect and exploit parallelism more effectively than current algorithms. This approach is independent from and complimentary to MPI distributed parallelism and allows achieving the finer grain parallelism necessary to exploit the multi core resources available on each computing node. The PIs also plan a powerful new approach to model multiphysics flows, such as turbulent combustion that leverages direct numerical simulation (DNS) and one-dimensional turbulence (ODT) to provide surrogate 'truth sets'. High-dimensional DNS data sets, containing terabytes of data, can be analyzed to extract lower-dimensional manifolds known to exist. Techniques such as principal component analysis can identify the optimal basis for representing manifolds in this high-dimensional data. Once a basis has been identified and extracted from the data sets generated by ODT, transport equations for the variables forming the basis may be derived and solved in a large-eddy simulation (LES). The LES can then be used to generate new ODT simulations which can feed back to the LES, thereby creating a dynamic modeling approach that uses down-scale, highly resolved statistical information to construct models to be used on larger scales (LES). This modeling approach is a prime candidate for early testing on petascale systems. The researchers in this study have already demonstrated the ability to scale DNS and LES to terascale computing systems, and availability of petascale computing will directly enable these modeling approaches. Application of the algorithmic and modeling advances will be made to oxyfuel combustion of natural gas. Oxyfuel combustion is one technique to facilitate carbon capture and sequestration to mitigate carbon dioxide emissions from power plants burning fossil fuels. While application will be made to natural gas systems, the techniques and algorithms developed here will apply directly to other systems including coal and transportation fuels such as diesel and gasoline. This project will provide unique educational experiences for students, including summer internships at national laboratories. Incorporating in regular classes the lessons learned in this project will help educate the future work force. Additionally, the research will strengthen collaborations between university researchers and national laboratory staff involved in simulation and model development, who will also participate in mentoring students.</AbstractNarration>
<MinAmdLetterDate>02/26/2010</MinAmdLetterDate>
<MaxAmdLetterDate>02/26/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0904631</AwardID>
<Investigator>
<FirstName>Valerio</FirstName>
<LastName>Pascucci</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Valerio Pascucci</PI_FULL_NAME>
<EmailAddress>pascucci@acm.org</EmailAddress>
<PI_PHON>8015851867</PI_PHON>
<NSF_ID>000294358</NSF_ID>
<StartDate>02/26/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>James</FirstName>
<LastName>Sutherland</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James C Sutherland</PI_FULL_NAME>
<EmailAddress>james.sutherland@utah.edu</EmailAddress>
<PI_PHON>8015851246</PI_PHON>
<NSF_ID>000425319</NSF_ID>
<StartDate>02/26/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Utah</Name>
<CityName>SALT LAKE CITY</CityName>
<ZipCode>841128930</ZipCode>
<PhoneNumber>8015816903</PhoneNumber>
<StreetAddress>75 S 2000 E</StreetAddress>
<StreetAddress2><![CDATA[Second Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009095365</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF UTAH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009095365</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName>SALT LAKE CITY</CityName>
<StateCode>UT</StateCode>
<ZipCode>841128930</ZipCode>
<StreetAddress><![CDATA[75 S 2000 E]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7691</Code>
<Text>PetaApps</Text>
</ProgramElement>
<ProgramReference>
<Code>1714</Code>
<Text>SPECIAL PROJECTS - CISE</Text>
</ProgramReference>
<ProgramReference>
<Code>7691</Code>
<Text>PetaApps</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~1500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project enabled the developed of new techniques that use efficiently large scale computational infrastructures which are increasingly deployed in high performance computing centers and enable scientific advances in a broad spectrum of disciplines. The main application examined has been the accurate and efficient simulation of computational fluidodynamics methods for the design of new clean energy production but has applicability to other disciplines such as climate modeling, material science, or medicine.</p> <p>The main technical advances achieved in this research have been on the following four fronts.</p> <ul> <li>Addressing the major challenge of using efficiently massively parallel supercomputers to solve large scale scientific simulations. To this end we have developed a new model of task-based parallelism that more easily takes advantage of heterogeneous computing environment to be used asynchronously. </li> <li>Development of improved algorithms for variable density computational fluidodynamics that allow the treatment of variable density flows particularly challenging in science applications. The methods have been demonstrated for density variation of several orders of magnitude, which makes them very useful in practice. </li> <li>Addressing the Big Data problem that arises often with large scale scientific research. In particular, we have provided a data movement and streaming infrastructure that allows scientists to deal easily with the massive amounts of data generated in their simulations or experiments. </li> <li>A set of in-situ data analytics components that allow scientists extracting from their data features of interest and understanding the science implications of their presence and trends over time. The in-situ character of such analytics allows them to be executed together with the simulation and therefore be used even if the data analyzed is too big to be saved.</li> </ul> <p>The research developed in this project has produced a number theoretical advances that have been published in top technical journals and presented at conferences in the fields of interest. Moreover, the work has resulted to a number of practical products that other groups can use directly in the form of software libraries. This is allowing the outcomes of the project to impact directly a broad class of scientific applications beyond the specific use case of clean energy simulations. These include for example large data models generated by other classes of simulations such as climate modeling, as well as experimental data generated by high resolution microscopes used in modern neuroscience.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/28/2016<br>      Modified by: Valerio&nbsp;Pascucci</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/0904631/0904631_10013691_1467119780944_Brain-Image-2--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/0904631/0904631_10013691_1467119780944_Brain-Image-2--rgov-800width.jpg" title="Brain data on powerwall"><img src="/por/images/Reports/POR/2016/0904631/0904631_10013691_1467119780944_Brain-Image-2--rgov-66x44.jpg" alt="Brain data on powerwall"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Visualization of high resolution brain data on a powerall display. The fine details captured with a 2-photon microscope are well represented for direct exploration by the neuroscientist who acquired the data.</div> <div ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project enabled the developed of new techniques that use efficiently large scale computational infrastructures which are increasingly deployed in high performance computing centers and enable scientific advances in a broad spectrum of disciplines. The main application examined has been the accurate and efficient simulation of computational fluidodynamics methods for the design of new clean energy production but has applicability to other disciplines such as climate modeling, material science, or medicine.  The main technical advances achieved in this research have been on the following four fronts.  Addressing the major challenge of using efficiently massively parallel supercomputers to solve large scale scientific simulations. To this end we have developed a new model of task-based parallelism that more easily takes advantage of heterogeneous computing environment to be used asynchronously.  Development of improved algorithms for variable density computational fluidodynamics that allow the treatment of variable density flows particularly challenging in science applications. The methods have been demonstrated for density variation of several orders of magnitude, which makes them very useful in practice.  Addressing the Big Data problem that arises often with large scale scientific research. In particular, we have provided a data movement and streaming infrastructure that allows scientists to deal easily with the massive amounts of data generated in their simulations or experiments.  A set of in-situ data analytics components that allow scientists extracting from their data features of interest and understanding the science implications of their presence and trends over time. The in-situ character of such analytics allows them to be executed together with the simulation and therefore be used even if the data analyzed is too big to be saved.   The research developed in this project has produced a number theoretical advances that have been published in top technical journals and presented at conferences in the fields of interest. Moreover, the work has resulted to a number of practical products that other groups can use directly in the form of software libraries. This is allowing the outcomes of the project to impact directly a broad class of scientific applications beyond the specific use case of clean energy simulations. These include for example large data models generated by other classes of simulations such as climate modeling, as well as experimental data generated by high resolution microscopes used in modern neuroscience.           Last Modified: 06/28/2016       Submitted by: Valerio Pascucci]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
