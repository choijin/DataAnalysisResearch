<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>DC: Large: Collaborative Research:  Mining a Million Scanned Books: Linguistic and Structure Analysis,  Fast Expanded Search, and Improved OCR</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2009</AwardEffectiveDate>
<AwardExpirationDate>09/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>281344.00</AwardTotalIntnAmount>
<AwardAmount>297344</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Center for Intelligent Information Retrieval at UMass Amherst, the Perseus Digital Library Project at Tufts, and the Internet Archive are investigating large-scale information extraction and retrieval technologies for digitized book collections. To provide effective analysis and search for scholars and the general public, and to handle the diversity and scale of these collections, this project focuses on improvements in seven interlocking technologies: improved OCR accuracy through word spotting, creating probabilistic models using joint distributions of features, and building topic-specific language models across documents; structural metadata extraction, to mine headers, chapters, tables of contents, and indices; linguistic analysis and information extraction, to perform syntactic analysis and entity extraction on noisy OCR output; inferred document relational structure, to mine citations, quotations, translations, and paraphrases; latent topic modeling  through time, to improve language modeling for OCR and retrieval, and to track the spread of ideas across periods and genres; query expansion for relevance models, to improve relevance in information retrieval by offline pre-processing of document comparisons; and interfaces for exploratory data analysis, to provide users of the document collection with efficient tools to update complex models of important entities, events, topics, and linguistic features. When applied across large corpora, these technologies reinforce each other: improved topic modeling enables more targeted language models for OCR; extracting structural metadata improves citation analysis; and entity extraction improves topic modeling and query expansion.The testbed for this project is the growing corpus of over one million open-access books from the Internet Archive.</AbstractNarration>
<MinAmdLetterDate>09/24/2009</MinAmdLetterDate>
<MaxAmdLetterDate>07/22/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0910165</AwardID>
<Investigator>
<FirstName>Gregory</FirstName>
<LastName>Crane</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gregory R Crane</PI_FULL_NAME>
<EmailAddress>gregory.crane@tufts.edu</EmailAddress>
<PI_PHON>6176273213</PI_PHON>
<NSF_ID>000113875</NSF_ID>
<StartDate>09/24/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Tufts University</Name>
<CityName>Boston</CityName>
<ZipCode>021111817</ZipCode>
<PhoneNumber>6176273696</PhoneNumber>
<StreetAddress>136 Harrison Ave</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073134835</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF TUFTS COLLEGE INC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073134835</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Tufts University]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>021111817</ZipCode>
<StreetAddress><![CDATA[136 Harrison Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7793</Code>
<Text>DATA-INTENSIVE COMPUTING</Text>
</ProgramElement>
<ProgramReference>
<Code>7793</Code>
<Text>DATA-INTENSIVE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~37867</FUND_OBLG>
<FUND_OBLG>2010~259477</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p id="docs-internal-guid-ab0abad8-9caf-6069-6f75-c1c2cbfba992" style="line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">Our work in the NSF 'Mining a Million Books' grant has focused on two complementary tasks, led by two different researchers. In years 1 and 2, we focused primarily upon working with raw OCR-generated text at scale. In years 3, 4 and 5, we focused on integrating the generation of crucial, but very expensive, linguistic data from large corpora with language learning.</span></p> <p>&nbsp;</p> <p style="line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">In the first two years, we focused on discovering linguistic information in the million book collection of the Internet Archive. This work has involved six levels of analysis: mining a corpus of 27,000 manually/automatically dated historical Latin texts drawn from a much larger collection of 1.2 million books in multiple languages; enhancing the metadata and enabling historical research by identifying the date of composition (as opposed to the print date) for a subset of those 27,000 books; analyzing changing lexical trends in Latin over that historical period; manually identifying a set of parallel Latin-English texts in the 1.2 million book collection to create a sense inventory and labeled training instances for automatic word sense disambiguation; using that trained model to automatically tag the word senses for all of the Latin words in the 27,000 work collection; and using that dated, sense-tagged collection to discover variation in Latin and English word senses over time.</span></p> <p>&nbsp;</p> <p style="line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">In the second phase, we focused upon the challenge of automatically producing linguistic data. The study of language in general and especially of historical languages must draw upon annotated corpora, where every word has one or more linguistic functions labelled. Morpho-syntactic annotation captures the form of individual words and their function. Nonetheless automated syntactic analysis is still imperfect and human annotators can be significantly more accurate. At the same time, syntactic analysis is challenging for human analysts as well. This makes syntactic analysis expensive in the best of cases. When we are working with historical languages for which, by definition, no native speakers are available, then the difficulties and costs go up accordingly. On the other hand, the ability to analyze syntax correlates well with the ability to translate a language, suggesting that the practice of syntactic annotation can help students perfect their knowledge of the language while they contribute to the development of new linguistic resources.</span></p> <p>&nbsp;</p> <p style="line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">One major goal was to compare errors made by students with the errors made by machines while dependency parsing ancient Greek. A </span><span style="font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-w...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Our work in the NSF 'Mining a Million Books' grant has focused on two complementary tasks, led by two different researchers. In years 1 and 2, we focused primarily upon working with raw OCR-generated text at scale. In years 3, 4 and 5, we focused on integrating the generation of crucial, but very expensive, linguistic data from large corpora with language learning.    In the first two years, we focused on discovering linguistic information in the million book collection of the Internet Archive. This work has involved six levels of analysis: mining a corpus of 27,000 manually/automatically dated historical Latin texts drawn from a much larger collection of 1.2 million books in multiple languages; enhancing the metadata and enabling historical research by identifying the date of composition (as opposed to the print date) for a subset of those 27,000 books; analyzing changing lexical trends in Latin over that historical period; manually identifying a set of parallel Latin-English texts in the 1.2 million book collection to create a sense inventory and labeled training instances for automatic word sense disambiguation; using that trained model to automatically tag the word senses for all of the Latin words in the 27,000 work collection; and using that dated, sense-tagged collection to discover variation in Latin and English word senses over time.    In the second phase, we focused upon the challenge of automatically producing linguistic data. The study of language in general and especially of historical languages must draw upon annotated corpora, where every word has one or more linguistic functions labelled. Morpho-syntactic annotation captures the form of individual words and their function. Nonetheless automated syntactic analysis is still imperfect and human annotators can be significantly more accurate. At the same time, syntactic analysis is challenging for human analysts as well. This makes syntactic analysis expensive in the best of cases. When we are working with historical languages for which, by definition, no native speakers are available, then the difficulties and costs go up accordingly. On the other hand, the ability to analyze syntax correlates well with the ability to translate a language, suggesting that the practice of syntactic annotation can help students perfect their knowledge of the language while they contribute to the development of new linguistic resources.    One major goal was to compare errors made by students with the errors made by machines while dependency parsing ancient Greek. A dependency graph is a tree in which every word is represented as a node and the directed arcs between the nodes show syntactic modifiers of the words. Dependency parsing is the task of generating dependency graph of that sentence. Data-driven dependency parsing techniques use an annotated corpus and learn to generate dependency graphs automatically from it. To compare errors made by students with errors made by automatic parsers, we first studied the errors made by students and different parsers and then finally compared those errors. Twenty-two students from an advanced Greek course at Tufts University participated in this research. Annotations were made according to the guidelines of the Perseus Project's Ancient Greek Dependency Treebank (AGDT). The texts chosen for the experiment were the Iliad and the Odyssey of Homer from the AGDT, which provides gold standard annotations for a number of Ancient Greek texts. For machine annotation, we used a group of three different state of the art dependency parsers.    Our experiments shows that the human annotators and parsers made very similar errors. Both methods produced very similar frequencies of different error types. What is hard for the students is hard for the parsers as well.    Future Work The Open Philology Project at Leipzig, led by PI Crane, is carrying forward both threads of the work begun in this project. The Open Greek and Latin subproject is building on the en...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
