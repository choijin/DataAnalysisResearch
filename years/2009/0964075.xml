<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Medium: Collaborative Research: Surface Haptics via Tractive Forces</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2010</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>949100.00</AwardTotalIntnAmount>
<AwardAmount>995100</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Surface Haptics, or the creation of virtual haptic effects on physical surfaces, is a topic of rapidly growing importance in human?]computer interaction because of the increasingly widespread use of touch screens.  Touch is at once an elegant and maddening interface modality.  It is elegant in its simplicity: one can make a selection or tap a button or key with no intervening mouse or joystick.  Moreover, touch (especially multi?]finger touch) supports gestures, such as swiping and expanding, which are satisfyingly natural. It is maddening, however, due to the lack of tactile and kinesthetic feedback that are so critical to natural touch.  Typing on a virtual keyboard, for instance, is typically an experience of visually guided hunt?]and?]peck with liberal use of the back?]space key.&lt;br/&gt;&lt;br/&gt;In this research the PIs will further develop a new class of surface haptic devices, called xPaDs, that promise to enrich the use of touch screen and touchpad interfaces for sighted as well as blind users.  xPaDs are notable because they provide controllable shear forces between the fingertips and an ordinary sheet of glass.  By controlling shear force in response to a measure of fingertip position (which may be obtained using a variety of existing technologies), it is possible to simulate a huge array of virtual effects; examples include toggle switches that flip from one state to another (each state is a "potential well" on the glass surface that pulls the finger to a given location), and contours that can be easily traced.&lt;br/&gt;&lt;br/&gt;The heart of the current project lies in the systems engineering that will lead to practical and effective devices capable of controlling a force at one or more fingertips, and in the psychophysical and application?]based studies that will teach us how these capabilities may best be used.  xPaDs are sophisticated dynamic systems that employ ultrasonic vibrations to modulate friction synchronized with in?]plane vibrations to produce controllable force vectors.  The PIs will address the challenges of controlling force individually at each fingertip, of producing xPaDs with large surface area, and of minimizing energy consumption and audible noise generation.  They will use the idea and methodology of "pop?]out" experiments to find haptic primitives, that is to say features the human perceptual system can extract with minimal or no perceptual load.  The PIs will measure the information transmission capacity of surface haptic devices treated as symbolic channels.  And they will explore the ability of the perceptual system to "bind" surface haptic features presented to different fingertips into a meaningful, coherent whole.  These studies will position the PIs for investigating a set of applications for the blind.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  Computer interfaces for the blind often rely heavily on speech, which necessarily presents information serially.  The PIs argue that a haptic surface can augment a speech?]based interface with critical spatial information.  They will study the editing and reading of mathematical expressions, of locating key content on a web page, of navigating intersections, and of planning routes with tools such as Google Maps.   In addition, the PIs will develop a low?]cost xPaD development kit, make the plans and code available on the Internet, and develop a high school enrichment unit based upon these materials.</AbstractNarration>
<MinAmdLetterDate>04/27/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/20/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0964075</AwardID>
<Investigator>
<FirstName>J. Edward</FirstName>
<LastName>Colgate</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>J. Edward Colgate</PI_FULL_NAME>
<EmailAddress>colgate@northwestern.edu</EmailAddress>
<PI_PHON>8474914264</PI_PHON>
<NSF_ID>000267549</NSF_ID>
<StartDate>04/27/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Peshkin</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael A Peshkin</PI_FULL_NAME>
<EmailAddress>peshkin@northwestern.edu</EmailAddress>
<PI_PHON>8474914630</PI_PHON>
<NSF_ID>000317165</NSF_ID>
<StartDate>04/27/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606114579</ZipCode>
<StreetAddress><![CDATA[750 N. Lake Shore Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~226962</FUND_OBLG>
<FUND_OBLG>2011~237597</FUND_OBLG>
<FUND_OBLG>2012~251055</FUND_OBLG>
<FUND_OBLG>2013~263486</FUND_OBLG>
<FUND_OBLG>2014~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to enable people to touch the screens of devices like tablet computers and smart phones and feel tangible features like edges, bumps, and holes.&nbsp; Although the surfaces of these devices are flat, it is possible to create the impression of tangible features by changing the friction on the glass or making the device vibrate.&nbsp; Our approach combined engineering with perceptual science.&nbsp; We developed the technology for creating tangible features on touch-screens, while we studied how humans perceive them.&nbsp;&nbsp;</p> <p>From a technology standpoint, we started with the idea of controlling the friction between the human fingertip and a touch surface such as a touch screen.&nbsp; We developed two approaches to friction control:&nbsp; one, ultrasonic vibration of the touch surface can be used to reduce friction; two, electric fields acting between the surface and the fingertip can be used to increase friction.&nbsp; &nbsp;We found that we could use friction modulation to produce a wide range of tangible features, from textures to illusory bumps and edges.&nbsp; However, we also moved beyond friction modulation to develop techniques for applying active forces to a fingertip, enabling an even greater breadth of tangible features to be produced.</p> <p>To understand perception, we performed a number of experiments with human subjects.&nbsp; In one series of experiments, we identified the kinds of features created from friction changes that are easiest to feel.&nbsp; We established that people can quickly detect an &ldquo;edge&rdquo;, that is, a straight line defined by a change in friction.&nbsp; We also determined how well people can tell apart edges that differ in orientation on the surface; for example, can they tell an edge pointed toward 1 o&rsquo;clock from one pointed toward 2 o&rsquo;clock?&nbsp; People&rsquo;s ability to discriminate edge orientations allowed us to measure how much spatial information the current displays can provide.</p> <p>Another direction for our research was to ask whether touching a display with more than one finger enhances the kinds of features people can feel.&nbsp; We discovered entirely new ways of conveying the impression of bumps and holes through multi-finger exploration.&nbsp;&nbsp; The data from experiments led to a theory of how the brain interprets the signals from multiple fingers at the same time.&nbsp; The key element is that the brain assumes the simplest model of the physical world.&nbsp; For example, if two fingers feel equal forces at the same time, they probably come from a continuous surface, rather than two objects that just happen to touch the fingers with equal force.&nbsp;</p> <p>Our research has many practical applications.&nbsp; An important one is to develop inexpensive, rapidly changeable displays for people with low vision or blindness. Such displays will be highly useful, for example, to enable blind people to &ldquo;read&rdquo; charts and graphs through the sense of touch.&nbsp; Adding tangible features to touch-screens can also enhance the experience of using tablets and phones for sighted people.&nbsp; Touch can be used to make games more entertaining, to create added security features, or to make locations on the screen easier to find.&nbsp;</p> <p>Through this description of outcomes, we intend to convey how our research advances knowledge and society.&nbsp; It has contributed to the basic understanding of the sense of touch and has guided engineers in their development of active touch screens.&nbsp; It has the potential to transform experiences for blind and sighted in their interactions with the devices they use virtually every day.</p><br> <p>            Last Modified: 07/20/2015<br>      Modified by: J. Edward&nbsp;Colgate</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCou...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to enable people to touch the screens of devices like tablet computers and smart phones and feel tangible features like edges, bumps, and holes.  Although the surfaces of these devices are flat, it is possible to create the impression of tangible features by changing the friction on the glass or making the device vibrate.  Our approach combined engineering with perceptual science.  We developed the technology for creating tangible features on touch-screens, while we studied how humans perceive them.    From a technology standpoint, we started with the idea of controlling the friction between the human fingertip and a touch surface such as a touch screen.  We developed two approaches to friction control:  one, ultrasonic vibration of the touch surface can be used to reduce friction; two, electric fields acting between the surface and the fingertip can be used to increase friction.   We found that we could use friction modulation to produce a wide range of tangible features, from textures to illusory bumps and edges.  However, we also moved beyond friction modulation to develop techniques for applying active forces to a fingertip, enabling an even greater breadth of tangible features to be produced.  To understand perception, we performed a number of experiments with human subjects.  In one series of experiments, we identified the kinds of features created from friction changes that are easiest to feel.  We established that people can quickly detect an "edge", that is, a straight line defined by a change in friction.  We also determined how well people can tell apart edges that differ in orientation on the surface; for example, can they tell an edge pointed toward 1 oÆclock from one pointed toward 2 oÆclock?  PeopleÆs ability to discriminate edge orientations allowed us to measure how much spatial information the current displays can provide.  Another direction for our research was to ask whether touching a display with more than one finger enhances the kinds of features people can feel.  We discovered entirely new ways of conveying the impression of bumps and holes through multi-finger exploration.   The data from experiments led to a theory of how the brain interprets the signals from multiple fingers at the same time.  The key element is that the brain assumes the simplest model of the physical world.  For example, if two fingers feel equal forces at the same time, they probably come from a continuous surface, rather than two objects that just happen to touch the fingers with equal force.   Our research has many practical applications.  An important one is to develop inexpensive, rapidly changeable displays for people with low vision or blindness. Such displays will be highly useful, for example, to enable blind people to "read" charts and graphs through the sense of touch.  Adding tangible features to touch-screens can also enhance the experience of using tablets and phones for sighted people.  Touch can be used to make games more entertaining, to create added security features, or to make locations on the screen easier to find.   Through this description of outcomes, we intend to convey how our research advances knowledge and society.  It has contributed to the basic understanding of the sense of touch and has guided engineers in their development of active touch screens.  It has the potential to transform experiences for blind and sighted in their interactions with the devices they use virtually every day.       Last Modified: 07/20/2015       Submitted by: J. Edward Colgate]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
