<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRAM:   A Congestion-Aware Resource and Allocation Manager for Data-Intensive High-Performance Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2009</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>495000.00</AwardTotalIntnAmount>
<AwardAmount>495000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will develop a job scheduling and resource allocation system for data-intensive high-performance computing (HPC) based on the congestion pricing of a systems' heterogeneous resources.  This extends the concept of resource management beyond processing: it allocates memory, disk I/O, and the network among jobs.  The research will overcome the critical shortcomings of processor-centric resource management, which wastes huge portions of cluster and supercomputer resources for data-intensive workloads, e.g. I/O bandwidth governs the performance of many modern HPC applications but, at present, it is neither allocated nor managed.  The research will develop techniques that (1) recon&amp;#64257;gure the degree of parallelism of HPC jobs to avoid congestion and wastage, (2) support lower-priority, allocation elastic jobs that can be scheduled on arbitrary numbers of nodes to consume unallocated resource fragments, and (3) co-schedule batch-processing workloads that use system resources that are unoccupied due to asymmetric utilization and temporal shifts in the foreground jobs.  These techniques will be implemented and supported for free public use as extensions to an open-source resource-management framework.  If used broadly, the software has the potential to provide much better utilization of the national investment in HPC facilities.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/10/2009</MinAmdLetterDate>
<MaxAmdLetterDate>08/23/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0937810</AwardID>
<Investigator>
<FirstName>Randal</FirstName>
<LastName>Burns</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Randal Burns</PI_FULL_NAME>
<EmailAddress>randal@cs.jhu.edu</EmailAddress>
<PI_PHON>4104936312</PI_PHON>
<NSF_ID>000461531</NSF_ID>
<StartDate>09/10/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>John</FirstName>
<LastName>Griffin</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John L Griffin</PI_FULL_NAME>
<EmailAddress>jlg@cs.jhu.edu</EmailAddress>
<PI_PHON>4105168668</PI_PHON>
<NSF_ID>000181362</NSF_ID>
<StartDate>09/10/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<StreetAddress2><![CDATA[Suite B001]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001910777</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>JOHNS HOPKINS UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001910777</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Johns Hopkins University]]></Name>
<CityName>Baltimore</CityName>
<StateCode>MD</StateCode>
<ZipCode>212182686</ZipCode>
<StreetAddress><![CDATA[1101 E 33rd St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7476</Code>
<Text>XD-Extreme Digital</Text>
</ProgramElement>
<ProgramElement>
<Code>7952</Code>
<Text>HECURA</Text>
</ProgramElement>
<ProgramElement>
<Code>I159</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~195634</FUND_OBLG>
<FUND_OBLG>2010~299366</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to create techniques for computing with big data that fully utilize the capabilities of modern hardware. &nbsp;Prior techniques for allocating resources are processor-centric; they distribute compute cycles to parallel jobs and do not account for memory and disk bottlenecks. &nbsp;We developed a suite of job scheduling and storage management tools that are data-centric and provide huge performance gains for big data computing.</p> <div class="page" title="Page 1"> <div class="layoutArea"> <div class="column"> <p><strong>High IOPS Storage Systems:</strong> <span>We built high IOPS (I/O operations per second) storage systems that overcome the write bottlenecks for random workloads. This scales single-system I/O to the extreme, building engines that fully utilize the capabilities of shared-memory hardware, specifically massive non-uniform memory architectures and arrays of solid-state storage devices (SSDs). The process overcame obstacles to the scalability of systems, such as remote memory performance, processor/device affinities, and operating system resource contention to realize more than 1 million IOPS.</span></p> <p><strong>Data-Driven Batch Scheduling:&nbsp;</strong>We built a data-driven scheduling framework for high-performance computing (HPC) applications that have overlapping data requirements. This embodies two principles: to schedule the execution of workload in the order that produces the most efficient I/O schedule and to identify shared I/O among different jobs and perform the I/O one time to meet the requirements of all jobs. This framework turns scheduling upside down: the HPC tradition schedules execution order and derives I/O requests from the execution order. Our techniques schedule I/O and derive a processing order from the preferred I/O schedule.</p> <p>Using data-driven scheduling, we compute queries to the Johns Hopkins Turbulence Database (http://turbulence.pha.jhu.edu) at the aggregate streaming I/O rate of disk array, improving performance by a factor of two to eight.</p> </div> </div> </div> <p><strong>Classroom Education:&nbsp;</strong>This grant also funded the development of two new undergraduate and graduate computer science courses that focus on big data. &nbsp;<em>Parallel programming </em>has taught more than 300 student to&nbsp;abandon the comfort of serial algorithmic thinking and to harness the power of superomputers, clouds, GPUs, and multi-core processors. &nbsp;<em>Data-intensive computing </em>&nbsp;is an experiential education course that uses 10 hours of classroom contact and team programming to build data systems and algorithms on the Amazon cloud.</p><br> <p>            Last Modified: 12/17/2013<br>      Modified by: Randal&nbsp;Burns</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to create techniques for computing with big data that fully utilize the capabilities of modern hardware.  Prior techniques for allocating resources are processor-centric; they distribute compute cycles to parallel jobs and do not account for memory and disk bottlenecks.  We developed a suite of job scheduling and storage management tools that are data-centric and provide huge performance gains for big data computing.     High IOPS Storage Systems: We built high IOPS (I/O operations per second) storage systems that overcome the write bottlenecks for random workloads. This scales single-system I/O to the extreme, building engines that fully utilize the capabilities of shared-memory hardware, specifically massive non-uniform memory architectures and arrays of solid-state storage devices (SSDs). The process overcame obstacles to the scalability of systems, such as remote memory performance, processor/device affinities, and operating system resource contention to realize more than 1 million IOPS.  Data-Driven Batch Scheduling: We built a data-driven scheduling framework for high-performance computing (HPC) applications that have overlapping data requirements. This embodies two principles: to schedule the execution of workload in the order that produces the most efficient I/O schedule and to identify shared I/O among different jobs and perform the I/O one time to meet the requirements of all jobs. This framework turns scheduling upside down: the HPC tradition schedules execution order and derives I/O requests from the execution order. Our techniques schedule I/O and derive a processing order from the preferred I/O schedule.  Using data-driven scheduling, we compute queries to the Johns Hopkins Turbulence Database (http://turbulence.pha.jhu.edu) at the aggregate streaming I/O rate of disk array, improving performance by a factor of two to eight.     Classroom Education: This grant also funded the development of two new undergraduate and graduate computer science courses that focus on big data.  Parallel programming has taught more than 300 student to abandon the comfort of serial algorithmic thinking and to harness the power of superomputers, clouds, GPUs, and multi-core processors.  Data-intensive computing  is an experiential education course that uses 10 hours of classroom contact and team programming to build data systems and algorithms on the Amazon cloud.       Last Modified: 12/17/2013       Submitted by: Randal Burns]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
