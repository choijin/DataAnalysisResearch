<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: RI: Developing a Framework for Automated Measurement of the Intensity of Spontaneous Facial Expressions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>11/01/2009</AwardEffectiveDate>
<AwardExpirationDate>10/31/2011</AwardExpirationDate>
<AwardTotalIntnAmount>79886.00</AwardTotalIntnAmount>
<AwardAmount>87886</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Spontaneous facial expressions are representative of facial expressions in daily life. Studying spontaneous (non-posed) facial expressions and Facial Action Coding System-AUs (FACS-AUs) requires spontaneous face expression data where the action units are manually and reliably coded. Currently, there is a high demand in computer vision society for such a database. The first objective of this research is to capture a 3D facial expression database and make it publically available to others interested in studying spontaneous facial expressions. As ground-truth, we also manually code the existence and the intensity of important action units (i.e., AU 1, 2, 4, 6, 9, 10, 12, 14, 15, 20, 25, and 26) in this dataset. These action units describe positive and negative emotional expressions. &lt;br/&gt;&lt;br/&gt;The second objective of this research is to develop a framework to automatically measure the intensity of spontaneous FACS action units in videos captured from adults? faces. We aim to study dictionary-based representation (e.g., combination of wavelet and shearlet) to model face appearance in facial images. This is due to the fact that human facial images contain of edges, wrinkles and texture. We study this type of combined representations to provide a sparse representation of images containing edges and textures and utilize it in measuring the AU intensities. &lt;br/&gt;&lt;br/&gt;The results of this research, the face database, the manually codes of AUs, and the developed framework in the measuring the AUs, are made publically available to other researchers in computer vision society.&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/23/2009</MinAmdLetterDate>
<MaxAmdLetterDate>03/22/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0957983</AwardID>
<Investigator>
<FirstName>Mohammad</FirstName>
<LastName>Mahoor</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mohammad Mahoor</PI_FULL_NAME>
<EmailAddress>mmahoor@du.edu</EmailAddress>
<PI_PHON>3038713745</PI_PHON>
<NSF_ID>000511471</NSF_ID>
<StartDate>09/23/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Denver</Name>
<CityName>Denver</CityName>
<ZipCode>802104711</ZipCode>
<PhoneNumber>3038712000</PhoneNumber>
<StreetAddress>2199 S. University Blvd.</StreetAddress>
<StreetAddress2><![CDATA[Ofc of Research & Sponsored Prog]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431760</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>COLORADO SEMINARY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431760</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Denver]]></Name>
<CityName>Denver</CityName>
<StateCode>CO</StateCode>
<ZipCode>802104711</ZipCode>
<StreetAddress><![CDATA[2199 S. University Blvd.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~79886</FUND_OBLG>
<FUND_OBLG>2011~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <p>The objective of this project was two-fold:</p> <p>1) Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver University Spontaneous Facial Action Intensity Database (DUS-FAID). Twenty-seven young adults were video-recorded using stereo camera, while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the Facial Action Unit Coding System; that is, a 6-point scale from absent to maximal intensity. Action units are the smallest visibly discriminable changes in facial action and may occur individually and in combinations to comprise more molar ( or moral) facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit detection and intensity are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.</p> <p>2) Lately, the theory and applications of sparse representation attracted much attention in computer vision and pattern recognition literature. Inspired by the idea of comprssed sensing, we developed a novel framework for recognition of facial action unit (AU) combinations by viewing the classification as a sparse representation problem. Based on this framework, we represented a facial image exhibiting the combination of AUs as a sparse linear combination of basis constituting an overcomplete dictionary.&nbsp; We build an overcomplete dictionary whose main elements are mean Gabor features of AU combinations under examination. The other elements of the dictionary are randomly sampled from a distribution (e.g., Gaussian distribution) that guarantees sparse signal recovery. Afterwards, by solving L1-norm minimization, a facial image is represented as a sparse vector which is used to distinguish various AU patterns. After calculating the sparse representation, the classification problem is simply viewed as a rank maximal problem. The index of the maximal value of the sparse vector is regarded as the class label of the facial image under test. Extensive experiments on the Cohn-Kanade facial expressions database demonstrate that this sparse learning framework is promising for recognition of AU combinations</p><br> <p>            Last Modified: 03/28/2012<br>      Modified by: Mohammad&nbsp;Mahoor</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    The objective of this project was two-fold:  1) Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver University Spontaneous Facial Action Intensity Database (DUS-FAID). Twenty-seven young adults were video-recorded using stereo camera, while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the Facial Action Unit Coding System; that is, a 6-point scale from absent to maximal intensity. Action units are the smallest visibly discriminable changes in facial action and may occur individually and in combinations to comprise more molar ( or moral) facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit detection and intensity are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.  2) Lately, the theory and applications of sparse representation attracted much attention in computer vision and pattern recognition literature. Inspired by the idea of comprssed sensing, we developed a novel framework for recognition of facial action unit (AU) combinations by viewing the classification as a sparse representation problem. Based on this framework, we represented a facial image exhibiting the combination of AUs as a sparse linear combination of basis constituting an overcomplete dictionary.  We build an overcomplete dictionary whose main elements are mean Gabor features of AU combinations under examination. The other elements of the dictionary are randomly sampled from a distribution (e.g., Gaussian distribution) that guarantees sparse signal recovery. Afterwards, by solving L1-norm minimization, a facial image is represented as a sparse vector which is used to distinguish various AU patterns. After calculating the sparse representation, the classification problem is simply viewed as a rank maximal problem. The index of the maximal value of the sparse vector is regarded as the class label of the facial image under test. Extensive experiments on the Cohn-Kanade facial expressions database demonstrate that this sparse learning framework is promising for recognition of AU combinations       Last Modified: 03/28/2012       Submitted by: Mohammad Mahoor]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
