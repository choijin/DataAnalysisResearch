<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Labor Supply Models and the External Validity of Randomized Welfare Experiments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2010</AwardEffectiveDate>
<AwardExpirationDate>05/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>434700.00</AwardTotalIntnAmount>
<AwardAmount>434700</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050100</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kwabena Gyimah-Brempong</SignBlockName>
<PO_EMAI>kgyimahb@nsf.gov</PO_EMAI>
<PO_PHON>7032927466</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project aims to enhance understanding of labor supply and program participation decisions by: 1) developing and estimating a behavioral model of labor supply using survey and administrative data from a randomized welfare experiment, 2) assessing the importance of precise measurement of agents' choices and incentives on the accuracy of the model's predictions, and 3) evaluating the ability of the estimated model to predict the results of other experiments in different populations.&lt;br/&gt;&lt;br/&gt;The intellectual merit of this research is to generate accurate quantitative models of labor supply and program participation. Such models can be used to evaluate the impact of reforms to the social insurance and tax system and to make quantitative statements about the welfare effects of proposed and existing policies. Our estimation approach combines observational and experimental variation in incentives and opportunities. The experimental variation in welfare rules allows us to relax many of the key assumptions typically employed when estimating behavioral responses to changes in program rules. Our ability (or inability) to accurately predict the results of markedly different randomized experiments provides a policy-relevant metric for evaluating our modeling framework in general and, more specifically, our ability to account for self-selection into program and labor market participation.&lt;br/&gt;&lt;br/&gt;The project will deliver two research papers. The first paper focuses on assessing the extent to which the ability of labor supply models to match experimental impacts depends upon measurement issues and modeling complexity. We will begin by developing and estimating a model of welfare participation and labor supply using data from the California Work Pays Demonstration Project (CWPDP) -- a large scale randomized welfare reform experiment implemented in California in the early 1990s. The longitudinally merged administrative and survey data available from this experiment allow us to measure the budgets (and hence incentives) of agents in substantially more detail than previous studies. Moreover, the various components of the data contain independent repeated measurements on a number of key variables allowing us to detect and model recording and reporting problems in a more satisfactory manner than has previously been attempted in this literature.&lt;br/&gt;&lt;br/&gt;We will then examine how our estimates change when we coarsen the choices available to agents, use approximate rather than exact policy rules, or ignore measurement problems. Our focus will be on how these changes influence our ability to match experimental impacts on quantities of direct policy interest such as total welfare payments and program participation. The analysis will also provide a quantitative unbundling of the incentive effects associated with the CWPDP treatment, allowing us to ascertain, for example, the relative importance of changes in welfare eligibility rules vs. changes in earnings disregards.&lt;br/&gt;&lt;br/&gt;The second paper is motivated by the concern that randomized experiments may have little external validity. The CWPDP experiment, for instance, was conducted on a sample of on-going welfare recipients residing in four California counties during a sustained boom in the state job market. To what extent can what we learn from the California experiment be generalized to other populations, time periods, and program mixes? The many state welfare experiments conducted during the 1990s provide us with the opportunity to answer this question. We will use our estimates from the CWPDP sample to generate predictions about the results of randomized experiments in two other states. This will entail developing methods to re-estimate distributions of unobservable preferences and skills from the control observations available in each state's sample. We will conclude with an assessment of the practical advantages (if any) of access to experimental variation in generating credible out of sample policy predictions.&lt;br/&gt;&lt;br/&gt;The broader impact of our project will be to develop methods for enhancing what can be learned from social experiments through use of a behavioral model. We seek to illustrate how to build and estimate economic models capable of "pooling together" and interpreting the results of multiple experiments in the presence of unobserved individual-level heterogeneity. We believe such methods will become increasingly important as social and field experiments continue to proliferate in economics.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>06/01/2010</MinAmdLetterDate>
<MaxAmdLetterDate>04/16/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0962352</AwardID>
<Investigator>
<FirstName>Patrick</FirstName>
<LastName>Kline</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Patrick Kline</PI_FULL_NAME>
<EmailAddress>pkline@econ.berkeley.edu</EmailAddress>
<PI_PHON>5106434153</PI_PHON>
<NSF_ID>000539169</NSF_ID>
<StartDate>06/01/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Melissa</FirstName>
<LastName>Tartari</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Melissa Tartari</PI_FULL_NAME>
<EmailAddress>melissa.tartari@yale.edu</EmailAddress>
<PI_PHON>2034329536</PI_PHON>
<NSF_ID>000540873</NSF_ID>
<StartDate>06/01/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>National Bureau of Economic Research Inc</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385398</ZipCode>
<PhoneNumber>6178683900</PhoneNumber>
<StreetAddress>1050 Massachusetts Avenue</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>054552435</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NATIONAL BUREAU OF ECONOMIC RESEARCH, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>054552435</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[National Bureau of Economic Research Inc]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021385398</ZipCode>
<StreetAddress><![CDATA[1050 Massachusetts Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1320</Code>
<Text>Economics</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1320</Code>
<Text>ECONOMICS</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~289800</FUND_OBLG>
<FUND_OBLG>2011~144900</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project investigated the ability of a nonparametric model of labor supply to rationalize earnings and program participation data from a large social experiment. We analyzed data from the Manpower Demonstration Research Corporation&rsquo;s &ldquo;Job&rsquo;s First&rdquo; public use files which were collected in an experimental evaluation of Connecticut&rsquo;s implementation of welfare reform. This experiment randomly assigned welfare participants and applicants to either a control group that was exposed to the pre-existing Aid to Families with Dependent Children welfare program, or a treatment group that was exposed to Connecticut&rsquo;s new Jobs First (JF) variety of workfare. The JF program entailed improved financial incentives to work but also created incentives for some women with greater earnings ability to work less.</p> <p>Previous analyses of this experiment demonstrated that it raised employment rates. But economists have debated whether such reforms lead to adjustments in the amount earned conditional on working &ndash; a question that cannot be addressed without stronger assumptions. We developed a nonparametric decision model with unrestricted heterogeneity and showed that many potential responses to the JF reform were inconsistent with basic tenets of rationality. Ruling these responses out, we then developed an approach to bounding empirically the frequency of various sorts of theoretically allowed responses to the experiment. Our bound estimates indicated that while the JF reform raised employment, it also lowered the earnings of some women who sought to maintain eligibility for benefits. We also found that reform led some women to stay on welfare who would otherwise have gotten off assistance and led others to under-report their earnings to the welfare agency in order to qualify for benefits.</p> <p>The methods developed here are applicable to many other settings where researchers seek to interpret the behavioral content of treatment effect estimates. Our analysis of the JF reform demonstrated that even very weak restrictions from economic theory can provide important insights into the behavioral responses generating experimental impacts. In our case, the analysis illustrated quantitatively some potentially important drawbacks of the JF program and established that workers do have some ability to adjust the amount they earn conditional on working.</p> <p>This research was published in the April issue of the <em>American Economic Review</em>. The reference is:</p> <p class="DefaultText">&ldquo;Bounding the Labor Supply Responses to a Randomized Welfare Experiment: A Revealed Preference Approach.&rdquo; (w/ Melissa Tartari)&nbsp; <em>American Economic Review</em>, 106 (April 2016), pp. 972-1014.</p> <p>A web archive of the computer program used in this project is available online at:</p> <p><a href="https://www.aeaweb.org/articles?id=10.1257/aer.20130824">https://www.aeaweb.org/articles?id=10.1257/aer.20130824</a></p><br> <p>            Last Modified: 04/27/2016<br>      Modified by: Patrick&nbsp;Kline</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project investigated the ability of a nonparametric model of labor supply to rationalize earnings and program participation data from a large social experiment. We analyzed data from the Manpower Demonstration Research CorporationÆs "JobÆs First" public use files which were collected in an experimental evaluation of ConnecticutÆs implementation of welfare reform. This experiment randomly assigned welfare participants and applicants to either a control group that was exposed to the pre-existing Aid to Families with Dependent Children welfare program, or a treatment group that was exposed to ConnecticutÆs new Jobs First (JF) variety of workfare. The JF program entailed improved financial incentives to work but also created incentives for some women with greater earnings ability to work less.  Previous analyses of this experiment demonstrated that it raised employment rates. But economists have debated whether such reforms lead to adjustments in the amount earned conditional on working &ndash; a question that cannot be addressed without stronger assumptions. We developed a nonparametric decision model with unrestricted heterogeneity and showed that many potential responses to the JF reform were inconsistent with basic tenets of rationality. Ruling these responses out, we then developed an approach to bounding empirically the frequency of various sorts of theoretically allowed responses to the experiment. Our bound estimates indicated that while the JF reform raised employment, it also lowered the earnings of some women who sought to maintain eligibility for benefits. We also found that reform led some women to stay on welfare who would otherwise have gotten off assistance and led others to under-report their earnings to the welfare agency in order to qualify for benefits.  The methods developed here are applicable to many other settings where researchers seek to interpret the behavioral content of treatment effect estimates. Our analysis of the JF reform demonstrated that even very weak restrictions from economic theory can provide important insights into the behavioral responses generating experimental impacts. In our case, the analysis illustrated quantitatively some potentially important drawbacks of the JF program and established that workers do have some ability to adjust the amount they earn conditional on working.  This research was published in the April issue of the American Economic Review. The reference is: "Bounding the Labor Supply Responses to a Randomized Welfare Experiment: A Revealed Preference Approach." (w/ Melissa Tartari)  American Economic Review, 106 (April 2016), pp. 972-1014.  A web archive of the computer program used in this project is available online at:  https://www.aeaweb.org/articles?id=10.1257/aer.20130824       Last Modified: 04/27/2016       Submitted by: Patrick Kline]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
