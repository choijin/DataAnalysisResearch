<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>US Egypt Cooperative Research: Computer Aided Pronunciation Learning Application</AwardTitle>
    <AwardEffectiveDate>10/01/2009</AwardEffectiveDate>
    <AwardExpirationDate>09/30/2013</AwardExpirationDate>
    <AwardAmount>75051</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>01090000</Code>
      <Directorate>
        <LongName>Office Of The Director</LongName>
      </Directorate>
      <Division>
        <LongName>Office Of Internatl Science &amp;Engineering</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Marjorie Lueck</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This collaborative research project is being undertaken by Dr. Ahmed Elgammal, Rutgers University, and Dr. Sherif Abdou, Cairo University in Egypt, in order to create an automated system that will input pronunciations from language learners along with dynamic images of their faces as they speak. Using this information, the proposed system will assess whether the user has produced an accurate pronunciation of a particular word or phrase in the target language. The system will then offer specific feedback to the user regarding the quality of their pronunciation, with suggestions for improvement. Specifically, the research will entail analyzing pronunciation errors that occur for non-native speakers of English and Arabic.&lt;br/&gt;&lt;br/&gt;The researchers anticipate that this speech recognition system will be robust enough to withstand large mispronunciations that may occur when the user has a non-native tongue. Additionally, the system will be designed to assign a pass or fail score to the user?s utterance, detect where the error occurred, and classify the error in order to give adequate feedback on how the pronunciation should be altered. In order to create such a system, novel algorithms will be developed for combining visual cues with audio cues for the task of mispronunciation detection. This will be achieved by deploying lip tracking algorithms and studying the correlation between lip movement and speech through style-dependent models of individual users. The research plan will also result in collection of a large audio-visual database of the phonemes by native and non-native Arabic and English speakers. The significance of initially developing the mispronunciation detection for English and Arabic is that these two languages are both phonetically rich and different from one another, leading to an assumption that the mispronunciation detection has a wide working spectrum.&lt;br/&gt;&lt;br/&gt;The societal benefits of efficient speech training systems are significant. In addition to numerous benefits for language learning, the expected outcome of this collaborative research is a tool that would be one of the core components for any Computer Aided Language Learning (CALL) system. Such an outcome could provide a cost effective and personalized tool for hearing-impaired speakers where the acquisition of an acceptable pronunciation can be challenging.</AbstractNarration>
    <MinAmdLetterDate>09/24/2009</MinAmdLetterDate>
    <MaxAmdLetterDate>09/24/2009</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0923658</AwardID>
    <Investigator>
      <FirstName>Ahmed</FirstName>
      <LastName>Elgammal</LastName>
      <EmailAddress>elgammal@cs.rutgers.edu</EmailAddress>
      <StartDate>09/24/2009</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Rutgers University New Brunswick</Name>
      <CityName>Piscataway</CityName>
      <ZipCode>088543925</ZipCode>
      <PhoneNumber>8489320150</PhoneNumber>
      <StreetAddress>33 Knightsbridge Road</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>New Jersey</StateName>
      <StateCode>NJ</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7298</Code>
      <Text>COLLABORATIVE RESEARCH</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>0000</Code>
      <Text>UNASSIGNED</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>5944</Code>
      <Text>EGYPT (COOP SCIENCE)</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>5976</Code>
      <Text>AFRICA, NEAR EAST, &amp; SO ASIA</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>OTHR</Code>
      <Text>OTHER RESEARCH OR EDUCATION</Text>
    </ProgramReference>
  </Award>
</rootTag>
