<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Development of a Gesture Based Virtual Reality System for Research in Virtual Worlds</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2009</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>782039.00</AwardTotalIntnAmount>
<AwardAmount>932000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rita Rodriguez</SignBlockName>
<PO_EMAI>rrodrigu@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Proposal #: CNS 09-23158&lt;br/&gt;PI(s):  Ilies, Horea T.; &lt;br/&gt;  Anderson, Amy; Kazerounian, Kazem; Marsh, Kerry L.; Nowak, Kristine&lt;br/&gt;Institution: University of Connecticut&lt;br/&gt;Title:    MRI/Dev.: Dev. of a Gesture Based Virtual Reality System for Research in Virtual Worlds&lt;br/&gt;Project Proposed:&lt;br/&gt;This project, developing an integrated virtual environment system capable of allowing not only 3D visualization of data, but also interaction with data through natural hand and finger gestured based on a dual interface, exploits a multi-touch interaction interface and a vision based hand-gesture interface. Virtual reality environments rely on a collection of technologies that allow the user to go through a coherent and unified perceptual experience involving multiple senses, such as vision, touch, and sound, while interacting with 3-dimensional data. These immersive, highly visual, 3D environments currently offer a fairly high level of performance for spatially visualizing data. However, the corresponding machinery providing user interaction with these systems has not kept up the same pace of development with the visualization tools. At present some advanced commercial environments offer some user interaction capabilities achieved through wired wearable hardware (such as wired gloves and head mounted displays). This promotes, in turn, an unnatural and cumbersome interaction between the user and the virtual reality environments, curbing the acceptance of the technologies. The syntax and semantics of the hand and finger gestures developed interacts with geometric data, while the implementation relies on the Multi-Touch Surface computing platform as well as on a newly developed gesture tracking and recognition system. The environment should lead to a potent open platform for interacting with virtual geometric data in an intuitive way, without the need for wearable hardware galvanizing the state of the art at the institution in Nano and Design Engineering, Psychology, Computer Science, Structural biology, as well as support research in the Center for Health, Intervention, and Prevention (CHIP). This project addresses the problem.&lt;br/&gt;Broader Impacts: &lt;br/&gt;This instrumentation will be open source and widely available with well documented set up procedures. The VR system will be networked with other VR-sites, including VRAC at Iowa State, to maximize the impact and stimulate technology transfer. Moreover, the instrument contributes to&lt;br/&gt;- Stimulate critical avenues of interdisciplinary research involving engineering, biology, computer science,  &lt;br/&gt;psychology, and Human Computer Interaction (HCI),&lt;br/&gt;- Strengthen the potential for educational, student recruiting, and outreach activities, and&lt;br/&gt;- Perform targeted outreach to K-12 students, teachers, and school districts serving groups that have&lt;br/&gt; traditionally been underrepresented in the engineering disciplines. &lt;br/&gt;The project also advances the state of the art in the teaching and practice of engineering design, as well as other fields in which geometry plays and important role. Contributing to the development of a new generation of professionals that use capabilities of virtual reality tools to augment traditional disciplines for improved engineering design, this work should have a long-lasting impact on the ability of scientists and engineers.</AbstractNarration>
<MinAmdLetterDate>08/31/2009</MinAmdLetterDate>
<MaxAmdLetterDate>07/17/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0923158</AwardID>
<Investigator>
<FirstName>Kazem</FirstName>
<LastName>Kazerounian</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kazem Kazerounian</PI_FULL_NAME>
<EmailAddress>kazem@engr.uconn.edu</EmailAddress>
<PI_PHON>8604862102</PI_PHON>
<NSF_ID>000295822</NSF_ID>
<StartDate>08/31/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kerry</FirstName>
<LastName>Marsh</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kerry L Marsh</PI_FULL_NAME>
<EmailAddress>Kerry.L.Marsh@uconn.edu</EmailAddress>
<PI_PHON>8604862452</PI_PHON>
<NSF_ID>000352941</NSF_ID>
<StartDate>08/31/2009</StartDate>
<EndDate>07/17/2014</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kristine</FirstName>
<LastName>Nowak</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kristine L Nowak</PI_FULL_NAME>
<EmailAddress>kristine.nowak@uconn.edu</EmailAddress>
<PI_PHON>8604864080</PI_PHON>
<NSF_ID>000438536</NSF_ID>
<StartDate>08/31/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Amy</FirstName>
<LastName>Anderson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amy Anderson</PI_FULL_NAME>
<EmailAddress>amy.anderson@uconn.edu</EmailAddress>
<PI_PHON>6036461110</PI_PHON>
<NSF_ID>000093410</NSF_ID>
<StartDate>08/31/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Horea</FirstName>
<LastName>Ilies</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Horea T Ilies</PI_FULL_NAME>
<EmailAddress>horea.ilies@uconn.edu</EmailAddress>
<PI_PHON>8604868813</PI_PHON>
<NSF_ID>000330248</NSF_ID>
<StartDate>08/31/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Connecticut</Name>
<CityName>Storrs</CityName>
<ZipCode>062691133</ZipCode>
<PhoneNumber>8604863622</PhoneNumber>
<StreetAddress>438 Whitney Road Ext.</StreetAddress>
<StreetAddress2><![CDATA[Unit 1133]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<StateCode>CT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>614209054</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CONNECTICUT</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004534830</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Connecticut]]></Name>
<CityName>Storrs</CityName>
<StateCode>CT</StateCode>
<ZipCode>062691133</ZipCode>
<StreetAddress><![CDATA[438 Whitney Road Ext.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>1189</Code>
<Text>Major Research Instrumentation</Text>
</ProgramElement>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7684</Code>
<Text>CESER-Cyberinfrastructure for</Text>
</ProgramElement>
<ProgramReference>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~782039</FUND_OBLG>
<FUND_OBLG>2010~149961</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><pre><pre><!--StartFragment-->Human interaction with spatial data in virtual environments is a ubiquitous task&nbsp;</pre> <pre>in many fields of science and engineering and is becoming a popular paradigm&nbsp;</pre> <pre>in consumer industries. New interaction paradigms that allow users to employ</pre> <pre>the full manipulative capabilities of their natural hand gestures hold the promise&nbsp;</pre> <pre>to change the way we interact with everything from computers to robots to&nbsp;</pre> <pre>consumer products.</pre> <pre><br /></pre> <pre>This research produced the theoretical foundations and computational platform for&nbsp;</pre> <pre>inferring user's intentions for 3D data manipulation directly from natural hand&nbsp;</pre> <pre>gestures. The key idea is the use characteristic behavioral cues from&nbsp;</pre> <pre>neuropsychology to infer the manipulative gestures in real time. It was&nbsp;</pre> <pre>demonstrated through user studies that, by coupling characteristic behavioral&nbsp;</pre> <pre>cues with 3D imaging-based body tracking techniques, one can robustly infer&nbsp;</pre> <pre>the manipulative intentions of the user from natural hand gestures in a&nbsp;</pre> <pre>workspace larger than the span of the human arms. Furthermore, it was&nbsp;</pre> <pre>demonstrated that the resulting methods are robust against variability in</pre> <pre>gestures and tracking uncertainties.&nbsp;</pre> <pre><br /></pre> <pre><br /></pre> <pre>The results of this research lead to a deeper understanding of the fundamental&nbsp;</pre> <pre>problem as well as its challenges and opportunities, and are poised to extend&nbsp;</pre> <pre>the practicality and usability of natural hand gestures for human interaction&nbsp;</pre> <pre>with spatial data in virtual environments.</pre> <br /></pre><br> <p>            Last Modified: 10/13/2015<br>      Modified by: Horea&nbsp;T&nbsp;Ilies</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Human interaction with spatial data in virtual environments is a ubiquitous task  in many fields of science and engineering and is becoming a popular paradigm  in consumer industries. New interaction paradigms that allow users to employ the full manipulative capabilities of their natural hand gestures hold the promise  to change the way we interact with everything from computers to robots to  consumer products.   This research produced the theoretical foundations and computational platform for  inferring user's intentions for 3D data manipulation directly from natural hand  gestures. The key idea is the use characteristic behavioral cues from  neuropsychology to infer the manipulative gestures in real time. It was  demonstrated through user studies that, by coupling characteristic behavioral  cues with 3D imaging-based body tracking techniques, one can robustly infer  the manipulative intentions of the user from natural hand gestures in a  workspace larger than the span of the human arms. Furthermore, it was  demonstrated that the resulting methods are robust against variability in gestures and tracking uncertainties.      The results of this research lead to a deeper understanding of the fundamental  problem as well as its challenges and opportunities, and are poised to extend  the practicality and usability of natural hand gestures for human interaction  with spatial data in virtual environments.         Last Modified: 10/13/2015       Submitted by: Horea T Ilies]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
