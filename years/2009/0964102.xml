<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Collaborative Research:  Semi-Supervised Discriminative Training of Language Models</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2010</AwardEffectiveDate>
<AwardExpirationDate>05/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>519050</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project is conducting fundamental research in statistical language modeling to improve human language technologies, including automatic speech recognition (ASR) and machine translation (MT).&lt;br/&gt;&lt;br/&gt;A language model (LM) is conventionally optimized, using text in the target language, to assign high probability to well-formed sentences.  This method has a fundamental shortcoming: the optimization does not explicitly target the kinds of distinctions necessary to accomplish the task at hand, such as discriminating (for ASR) between different words that are acoustically confusable or (for MT) between different target-language words that express the multiple meanings of a polysemous source-language word.&lt;br/&gt;&lt;br/&gt;Discriminative optimization of the LM, which would overcome this shortcoming, requires large quantities of paired input-output sequences: speech and its reference transcription for ASR or source-language (e.g. Chinese) sentences and their translations into the target language (say, English) for MT.  Such resources are expensive, and limit the efficacy of discriminative training methods.&lt;br/&gt;&lt;br/&gt;In a radical departure from convention, this project is investigating discriminative training using easily available, *unpaired* input and output sequences: un-transcribed speech or monolingual source-language text and unpaired target-language text.  Two key ideas are being pursued: (i) unlabeled input sequences (e.g. speech or Chinese text) are processed to learn likely confusions encountered by the ASR or MT system; (ii) unpaired output sequences (English text) are leveraged to discriminate between these well-formed sentences from the (supposed) ill-formed sentences the system could potentially confuse them with.&lt;br/&gt;&lt;br/&gt;This self-supervised discriminative training, if successful, will advance machine intelligence in fundamental ways that impact many other applications.</AbstractNarration>
<MinAmdLetterDate>06/09/2010</MinAmdLetterDate>
<MaxAmdLetterDate>03/18/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0964102</AwardID>
<Investigator>
<FirstName>Brian</FirstName>
<LastName>Roark</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Brian E Roark</PI_FULL_NAME>
<EmailAddress>roarkbr@gmail.com</EmailAddress>
<PI_PHON>5037481752</PI_PHON>
<NSF_ID>000434316</NSF_ID>
<StartDate>06/09/2010</StartDate>
<EndDate>05/23/2013</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Izhak</FirstName>
<LastName>Shafran</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Izhak Shafran</PI_FULL_NAME>
<EmailAddress>zakshafran@gmail.com</EmailAddress>
<PI_PHON>5037481158</PI_PHON>
<NSF_ID>000096166</NSF_ID>
<StartDate>07/30/2013</StartDate>
<EndDate>03/18/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Izhak</FirstName>
<LastName>Shafran</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Izhak Shafran</PI_FULL_NAME>
<EmailAddress>zakshafran@gmail.com</EmailAddress>
<PI_PHON>5037481158</PI_PHON>
<NSF_ID>000096166</NSF_ID>
<StartDate>06/09/2010</StartDate>
<EndDate>05/23/2013</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Kain</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander Kain</PI_FULL_NAME>
<EmailAddress>kaina@ohsu.edu</EmailAddress>
<PI_PHON>5037481539</PI_PHON>
<NSF_ID>000296505</NSF_ID>
<StartDate>03/18/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Sproat</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard W Sproat</PI_FULL_NAME>
<EmailAddress>rws@cslu.ogi.edu</EmailAddress>
<PI_PHON>5037481031</PI_PHON>
<NSF_ID>000515015</NSF_ID>
<StartDate>06/09/2010</StartDate>
<EndDate>07/30/2013</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Oregon Health &amp; Science University</Name>
<CityName>Portland</CityName>
<ZipCode>972393098</ZipCode>
<PhoneNumber>5034947784</PhoneNumber>
<StreetAddress>3181 S W Sam Jackson Park Rd</StreetAddress>
<StreetAddress2><![CDATA[Mail Code L106OPAM]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<StateCode>OR</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OR03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>096997515</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OREGON HEALTH &amp; SCIENCE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>096997515</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Oregon Health &amp; Science University]]></Name>
<CityName>Portland</CityName>
<StateCode>OR</StateCode>
<ZipCode>972393098</ZipCode>
<StreetAddress><![CDATA[3181 S W Sam Jackson Park Rd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OR03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7298</Code>
<Text>International Research Collab</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>5940</Code>
<Text>TURKEY</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~229618</FUND_OBLG>
<FUND_OBLG>2011~194782</FUND_OBLG>
<FUND_OBLG>2012~94650</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p> <div>This project was focused on new methods for learning statistical models that can discriminate between sentences that (a) were likely to be produced by a person and (b) were unlikely to be produced by a person.&nbsp; These sorts of language models are widely used in various common human language processing systems, such as automatic speech recognition, machine translation and optical character recognition.&nbsp; The models help choose between possible alternative system outputs by biasing the system towards sentences that were likely to be produced by a person.&nbsp; Certain advanced methods for training discriminative language models require large amounts of data that match human produced sentences with other alternative sentences that the system outputs incorrectly in that case.&nbsp; In many common scenarios, large amounts of such data is simply unavailable.&nbsp; In this project, we addressed training in scenarios where alternative system outputs either were not readily available, or the correct sentence from among the outputs is unknown.&nbsp; In these scenarios, new methods for discriminatively training statistical language models were required, and the central intellectual merit of the project resided in creating such methods and validating them in real language processing systems.&nbsp; Given the growing importance of speech recognition and automatic translation as applications used by millions of people in mobile computing interactions, improvements in language model training methods will broadly impact the quality of human computer interaction.&nbsp; In addition, a broader impact of the project is the training received by several Ph.D. students who worked this project, some of whom have already taken this expertise with them to excellent positions in the field.</div> <div></div> <div>New language model training methods were designed and applied to both machine translation and automatic speech recognition tasks, and the change in system accuracy was measured relative to baseline approaches.&nbsp; In both applications, one of the standard approaches investigated was to train from human produced language samples (i.e., text in the target language) with no available system outputs, by simulating alternative system outputs for each given sentence.&nbsp; For machine translation, this simulation can be performed by first translating the target language string (e.g., in English) to the source language (e.g., Chinese) then back to the target language again (so-called 'round trip' translation).&nbsp; In such a way, for a given English sentence, there are now a set of possible alternative system outputs.&nbsp; We found that augmenting fully supervised training data (i.e., sentence with actual system outputs) with this semi-supervised 'simulated' training data yielded improvements in a Chinese-English translation task.&nbsp; Interestingly, translation-based approaches also were useful for discriminative language modeling for speech recognition.&nbsp; In this case, we learned to automatically 'translate' from a given real human sentence to simulated system outputs.&nbsp; Using these machine-translation-derived methods, we were able to train language models that achieved significant speech recognition accuracy improvements over strong baseline systems.&nbsp; We found that simulation methods based on word or phrase correspondences yielded models that outperformed those trained with simulation methods based on phone-sequence correspondences in an English Broadcast News task.&nbsp; The methods were shown to yield gains in diverse tasks: similar methods using morpheme sequences instead of word sequences showed strong accuracy improvements in Turkish speech recognition.&nbsp; In addition, we found that deriving correspondences for such simulation systems from string pairs with no knowledge of which string was actually spoken still yielded useful semi-supervised trainin...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  This project was focused on new methods for learning statistical models that can discriminate between sentences that (a) were likely to be produced by a person and (b) were unlikely to be produced by a person.  These sorts of language models are widely used in various common human language processing systems, such as automatic speech recognition, machine translation and optical character recognition.  The models help choose between possible alternative system outputs by biasing the system towards sentences that were likely to be produced by a person.  Certain advanced methods for training discriminative language models require large amounts of data that match human produced sentences with other alternative sentences that the system outputs incorrectly in that case.  In many common scenarios, large amounts of such data is simply unavailable.  In this project, we addressed training in scenarios where alternative system outputs either were not readily available, or the correct sentence from among the outputs is unknown.  In these scenarios, new methods for discriminatively training statistical language models were required, and the central intellectual merit of the project resided in creating such methods and validating them in real language processing systems.  Given the growing importance of speech recognition and automatic translation as applications used by millions of people in mobile computing interactions, improvements in language model training methods will broadly impact the quality of human computer interaction.  In addition, a broader impact of the project is the training received by several Ph.D. students who worked this project, some of whom have already taken this expertise with them to excellent positions in the field.  New language model training methods were designed and applied to both machine translation and automatic speech recognition tasks, and the change in system accuracy was measured relative to baseline approaches.  In both applications, one of the standard approaches investigated was to train from human produced language samples (i.e., text in the target language) with no available system outputs, by simulating alternative system outputs for each given sentence.  For machine translation, this simulation can be performed by first translating the target language string (e.g., in English) to the source language (e.g., Chinese) then back to the target language again (so-called 'round trip' translation).  In such a way, for a given English sentence, there are now a set of possible alternative system outputs.  We found that augmenting fully supervised training data (i.e., sentence with actual system outputs) with this semi-supervised 'simulated' training data yielded improvements in a Chinese-English translation task.  Interestingly, translation-based approaches also were useful for discriminative language modeling for speech recognition.  In this case, we learned to automatically 'translate' from a given real human sentence to simulated system outputs.  Using these machine-translation-derived methods, we were able to train language models that achieved significant speech recognition accuracy improvements over strong baseline systems.  We found that simulation methods based on word or phrase correspondences yielded models that outperformed those trained with simulation methods based on phone-sequence correspondences in an English Broadcast News task.  The methods were shown to yield gains in diverse tasks: similar methods using morpheme sequences instead of word sequences showed strong accuracy improvements in Turkish speech recognition.  In addition, we found that deriving correspondences for such simulation systems from string pairs with no knowledge of which string was actually spoken still yielded useful semi-supervised training data.  In sum, we found that phrase or word-based transduction models consistently yielded discriminative language modeling training data that resulted in models that significan...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
