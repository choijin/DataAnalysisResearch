<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Dynamic Staging Architecture for Accelerating I/O Pipelines</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2010</AwardEffectiveDate>
<AwardExpirationDate>12/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>133933.00</AwardTotalIntnAmount>
<AwardAmount>133933</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Petascale applications are producing terabytes of data at a great rate. Storage systems in large-scale machines are significantly stressed as I/O rates are not growing as fast to cope with data production. A variety of HPC activities such as writing output and checkpoint data are all stymied by the I/O bandwidth bottleneck. Further to this, the post-processing and subsequent analysis/visualization of computational results is increasingly time consuming due to the widening gap between the storage/processing capacities of supercomputers and users' local clusters. &lt;br/&gt;&lt;br/&gt;This research focuses on building a novel in-job dynamic data staging architecture and in bringing it to bear on the looming petascale I/O crisis. To this end, the following objectives are investigated: (i) the concerted use of node-local memory and emerging hardware such as Solid State Disks (SSDs), from a dedicated set of nodes, as a means to alleviate the I/O bandwidth bottleneck, (ii) the multiplexing of traditional user post-processing pipelines and secondary computations with asynchronous I/O on the staging ground to perform scalable I/O and data analytics, (iii) bypassing memory to access the staging area, and (iv) enabling QoS both in the staging ground and in the communication channel connecting it to compute client and persistent storage. &lt;br/&gt;&lt;br/&gt;This study will have a wide-ranging impact on future provisioning of extreme-scale machines and will provide formative guidelines to this end. The result of this research will be a set of integrated techniques that can fundamentally change the current parallel I/O model and accelerate petascale I/O pipelines. Further, this research will help analyze the utility of SSDs in day-to-day supercomputing I/O and inform the wider HPC community of its viability.</AbstractNarration>
<MinAmdLetterDate>04/19/2010</MinAmdLetterDate>
<MaxAmdLetterDate>04/19/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0937690</AwardID>
<Investigator>
<FirstName>Xiaosong</FirstName>
<LastName>Ma</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Xiaosong Ma</PI_FULL_NAME>
<EmailAddress>ma@csc.ncsu.edu</EmailAddress>
<PI_PHON>9195137577</PI_PHON>
<NSF_ID>000300080</NSF_ID>
<StartDate>04/19/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Scott</FirstName>
<LastName>Klasky</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Scott A Klasky</PI_FULL_NAME>
<EmailAddress>klasky@tennessee.edu</EmailAddress>
<PI_PHON>8652419980</PI_PHON>
<NSF_ID>000529121</NSF_ID>
<StartDate>04/19/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>North Carolina State University</Name>
<CityName>Raleigh</CityName>
<ZipCode>276957514</ZipCode>
<PhoneNumber>9195152444</PhoneNumber>
<StreetAddress>2601 Wolf Village Way</StreetAddress>
<StreetAddress2><![CDATA[Admin. III, STE 240]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042092122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTH CAROLINA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[North Carolina State University]]></Name>
<CityName>Raleigh</CityName>
<StateCode>NC</StateCode>
<ZipCode>276957514</ZipCode>
<StreetAddress><![CDATA[2601 Wolf Village Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7952</Code>
<Text>HECURA</Text>
</ProgramElement>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~133933</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In this project, we experimented and evaluated several different ways of building intelligent staging architecture for more efficient parallel I/O on supercomputers.&nbsp;</p> <p>&nbsp;</p> <p>Parallel I/O has been a serious performance (how fast parallel applications run) and scalability (how much faster applications get when run on more computers) problem. It is a performance problem because hard disks are much slower than the CPU and memory components. It is a scalability problem because the I/O pathway does not have similar hardware parallelism compared to the computing units, and many concurrently-running jobs must share the common parallel file system.&nbsp;</p> <p>&nbsp;</p> <p>Our research in this funded project attacked the above problem by enabling efficient staging areas for I/O (and potentially other data processing tasks). The main idea of "staging" is to quickly move out data to a closer/faster temporary storage area, from where they can then be (slowly) drained to the eventual destiny: parallel file system. This is targeting the write-heavy nature of parallel scientific applications, who typically write out checkpoint data and/or intermediate results periodically during their execution. Stating enables the periodic output data to be quickly removed from the memory buffers, allowing the computation to move on, while the much slower I/O takes place in the background.&nbsp;</p> <p>&nbsp;</p> <p>The novel contributions made in our research can be highlighted by findings in two new approaches:&nbsp;</p> <p>1. We proposed and implemented a novel functional partitioning (FP) runtime environment, which allocates cores to specific application tasks -- checkpointing, de-duplication, and scientific data format transformation -- so that the deluge of cores can be brought to bear on the entire gamut of application activities. This approach attempts to ease the I/O pressure by trading computing capability (abundant in today's multi-core environments) to aid fast and more flexible I/O.&nbsp; The focus is on utilizing the extra cores to support HPC application I/O activities and also leverage solid-state disks in this context. For example, our evaluation shows that dedicating 1 core on an oct-core machine for checkpointing and its assist tasks using FP can improve overall execution time of a FLASH benchmark on 80 and 160 cores by 43.95% and 41.34%, respectively.</p> <p>&nbsp;</p> <p>2. We proposed and implemented ActiveFlash, a new way of utilizing powerful SSD devices in supercomputers. While traditionally used for storage, cache, or memory extension, SSDs today have rather significant amount of compute power within their multi-core controllers. We explored such compute power, often highly under-utilized, to expedite data analysis pipelines by migrating it to the location of the data, the flash device itself. This approach attempts to ease the I/O pressure by incorporating more data processing into storage devices, so that data can be "digested" close to where they are generated, avoid wasteful transfer or storage. We argue that Active Flash has the potential to enable true out-of-core data analytics by freeing up both the compute core and the associated main memory. By performing analysis locally, dependence on limited bandwidth to a central storage system is reduced, while allowing this analysis to proceed in parallel with the main application. In addition, offloading work from the host to the more power-efficient controller reduces peak system power usage, which is already in the megawatt range and poses a major barrier to HPC system scalability.</p> <p>We propose an architecture for Active Flash (see attached figure), explore energy and performance trade-offs in moving computation from host to storage, demonstrate the ability of appropriate embedded controllers to perform data analysis and reduction tasks at speeds sufficient for this application, and present a...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In this project, we experimented and evaluated several different ways of building intelligent staging architecture for more efficient parallel I/O on supercomputers.      Parallel I/O has been a serious performance (how fast parallel applications run) and scalability (how much faster applications get when run on more computers) problem. It is a performance problem because hard disks are much slower than the CPU and memory components. It is a scalability problem because the I/O pathway does not have similar hardware parallelism compared to the computing units, and many concurrently-running jobs must share the common parallel file system.      Our research in this funded project attacked the above problem by enabling efficient staging areas for I/O (and potentially other data processing tasks). The main idea of "staging" is to quickly move out data to a closer/faster temporary storage area, from where they can then be (slowly) drained to the eventual destiny: parallel file system. This is targeting the write-heavy nature of parallel scientific applications, who typically write out checkpoint data and/or intermediate results periodically during their execution. Stating enables the periodic output data to be quickly removed from the memory buffers, allowing the computation to move on, while the much slower I/O takes place in the background.      The novel contributions made in our research can be highlighted by findings in two new approaches:   1. We proposed and implemented a novel functional partitioning (FP) runtime environment, which allocates cores to specific application tasks -- checkpointing, de-duplication, and scientific data format transformation -- so that the deluge of cores can be brought to bear on the entire gamut of application activities. This approach attempts to ease the I/O pressure by trading computing capability (abundant in today's multi-core environments) to aid fast and more flexible I/O.  The focus is on utilizing the extra cores to support HPC application I/O activities and also leverage solid-state disks in this context. For example, our evaluation shows that dedicating 1 core on an oct-core machine for checkpointing and its assist tasks using FP can improve overall execution time of a FLASH benchmark on 80 and 160 cores by 43.95% and 41.34%, respectively.     2. We proposed and implemented ActiveFlash, a new way of utilizing powerful SSD devices in supercomputers. While traditionally used for storage, cache, or memory extension, SSDs today have rather significant amount of compute power within their multi-core controllers. We explored such compute power, often highly under-utilized, to expedite data analysis pipelines by migrating it to the location of the data, the flash device itself. This approach attempts to ease the I/O pressure by incorporating more data processing into storage devices, so that data can be "digested" close to where they are generated, avoid wasteful transfer or storage. We argue that Active Flash has the potential to enable true out-of-core data analytics by freeing up both the compute core and the associated main memory. By performing analysis locally, dependence on limited bandwidth to a central storage system is reduced, while allowing this analysis to proceed in parallel with the main application. In addition, offloading work from the host to the more power-efficient controller reduces peak system power usage, which is already in the megawatt range and poses a major barrier to HPC system scalability.  We propose an architecture for Active Flash (see attached figure), explore energy and performance trade-offs in moving computation from host to storage, demonstrate the ability of appropriate embedded controllers to perform data analysis and reduction tasks at speeds sufficient for this application, and present a simulation study of Active Flash scheduling policies. These results show the viability of the Active Flash model, and its capability to potentially have a transformat...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
