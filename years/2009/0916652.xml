<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CCF-SHF: CSR: Small: Compilation for Multi-core Processors with Limited Local Memories</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2009</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>499995.00</AwardTotalIntnAmount>
<AwardAmount>523776</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Multi-core processors provide the only way to continue improving peak performance without much increase in the power consumption. However, there are serious challenges not only in expressing all the parallelism in the application, but also in exploiting the available parallelism by efficient application management on modern multi-core architectures. These challenges are only compounded by the trend of the absence of memory virtualization in the hardware, that is observed in futuristic processors, like the IBM Cell, and the Intel experimental 80-core processor. Memory management cannot be supported in hardware, because cache coherency protocols do not scale to 100s or 1000s of cores, and also because memory management in hardware consumes significant energy. Memory management in software can exploit application and data characteristics to reduce the overhead, however, it increases the burden of the application programmer. This project aims to develop tools and techniques to automatically manage the limited local memories present in each of the cores of a multi-core processor. In addition to power-efficient execution, the main objective of the project is to relieve the application programmer of the burden of carefully crafting the application, dividing and mapping it onto the cores to ensure its correct execution and portability.</AbstractNarration>
<MinAmdLetterDate>08/07/2009</MinAmdLetterDate>
<MaxAmdLetterDate>04/07/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0916652</AwardID>
<Investigator>
<FirstName>Aviral</FirstName>
<LastName>Shrivastava</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Aviral Shrivastava</PI_FULL_NAME>
<EmailAddress>aviral.shrivastava@asu.edu</EmailAddress>
<PI_PHON>4807276509</PI_PHON>
<NSF_ID>000490268</NSF_ID>
<StartDate>08/07/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Arizona State University</Name>
<CityName>TEMPE</CityName>
<ZipCode>852816011</ZipCode>
<PhoneNumber>4809655479</PhoneNumber>
<StreetAddress>ORSPA</StreetAddress>
<StreetAddress2><![CDATA[660 South Mill Avenue, Suite 310]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>943360412</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ARIZONA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>806345658</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Arizona State University]]></Name>
<CityName>TEMPE</CityName>
<StateCode>AZ</StateCode>
<ZipCode>852816011</ZipCode>
<StreetAddress><![CDATA[ORSPA]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9217</Code>
<Text>NATNL RESERCH &amp; EDUCAT NETWORK</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~499995</FUND_OBLG>
<FUND_OBLG>2010~15781</FUND_OBLG>
<FUND_OBLG>2011~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 7"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>Multi&shy;core computing is everywhere: from large server farms, to handhelds. In order to continuously improve the performance, the number of cores in a multicore processor are increasing. Right now we have few core processors, 8 cores, 16 cores or 32 cores. However, when the number of cores increase to more than hundred, it will become increasinly difficult to scale the memory architecture. Existing processors have cache-coherent architectures, in which all the data management and communication between cores is performed by hardware (caches). But when the number of cores scale to hundreds, the overhead of doing this increases very significantly.&nbsp;</span>The memory architecture of manycore (more than hundred cores) processors will be very different from existing multicore processors.</p> <p>Software Managed Manycore (SMM) architectures are scalable memory manycore design. This is a cache-less manycore architecture, in which each core has a small scratchpad memory (SPM) instead of a cache. The difference is that in an SPM based processor, the program or software must contain instructions to move data between the memories. This must be explicitly present in the program -- the memory (unlikes caches) does not do this automatically. SMM architectures are much easier to design and verify, and are scalable to many number of cores. However, the only challenge is to add instructoins to manage data and communication explicitly in the application. This is extremely difficult for a programmer to do. This project proposed to analyze the applications and insert these instructions automatically in the program. This will be done in the compiler.</p> <p>&nbsp;</p> <p>The project was successful in meeting all the proposed outcomes, and develop compiler techniques to analyze the data requirements of the applications, and automatically inserted data management instructions in the program. The main question really is that how good (in terms of performance) is the automatic data management. If the solution performs significantly worse than a cache based system, then it is not so useful. So a lot of effort went into analyzing the data access patterns more aggressively, and inserting data management instructions more nimbly (so that there is minimal overhead). Instead of one scheme to manage all kind of data, we developed customized schemes for analysis and management of different kinds of data. This is because different kinds of data has different access pattern. For example, stack data is accessed in a very stack-like access pattern. Similar is code, but it is read-only. So we developed techniques to manage code at the function level of granularity. A simple scheme is to bring the code of the function before it is executed, and kick it out of SPM after it returns, but this has high overheads. It took as 4 papers -- continuously optimizing the technique, and making it better -- &nbsp;to finally reach a point, where we were able to show that the performance of code managenent using our approach was better than on a cache based system. Similar is the story with stack data.</p> <p>&nbsp;</p> <p>The project resulted in 15 conference publications -- all in top venues for "software for embedded systems", e.g., DAC, CODES+ISSS, CASES, LCTES, ASAP etc., 8 journal publications -- all in top IEEE and ACM journals, e.g., TVLSI, TCAD, TECS etc., 2 Ph.D. thesis, one more to finish, 5 masters thesis, 1 more to finish. This project also resulted in 4 patents. One is already granted, and 3 more are pending. This project has touched more than 10 students, and have trained them in state-of-the-art compiler technology. All these students are now working as compiler developers in large companies, like AMD, Intel, nVIDIA, Qualcomm, ARM, Microsoft etc. In fact, this is one of the major problems th...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[     Multi&shy;core computing is everywhere: from large server farms, to handhelds. In order to continuously improve the performance, the number of cores in a multicore processor are increasing. Right now we have few core processors, 8 cores, 16 cores or 32 cores. However, when the number of cores increase to more than hundred, it will become increasinly difficult to scale the memory architecture. Existing processors have cache-coherent architectures, in which all the data management and communication between cores is performed by hardware (caches). But when the number of cores scale to hundreds, the overhead of doing this increases very significantly. The memory architecture of manycore (more than hundred cores) processors will be very different from existing multicore processors.  Software Managed Manycore (SMM) architectures are scalable memory manycore design. This is a cache-less manycore architecture, in which each core has a small scratchpad memory (SPM) instead of a cache. The difference is that in an SPM based processor, the program or software must contain instructions to move data between the memories. This must be explicitly present in the program -- the memory (unlikes caches) does not do this automatically. SMM architectures are much easier to design and verify, and are scalable to many number of cores. However, the only challenge is to add instructoins to manage data and communication explicitly in the application. This is extremely difficult for a programmer to do. This project proposed to analyze the applications and insert these instructions automatically in the program. This will be done in the compiler.     The project was successful in meeting all the proposed outcomes, and develop compiler techniques to analyze the data requirements of the applications, and automatically inserted data management instructions in the program. The main question really is that how good (in terms of performance) is the automatic data management. If the solution performs significantly worse than a cache based system, then it is not so useful. So a lot of effort went into analyzing the data access patterns more aggressively, and inserting data management instructions more nimbly (so that there is minimal overhead). Instead of one scheme to manage all kind of data, we developed customized schemes for analysis and management of different kinds of data. This is because different kinds of data has different access pattern. For example, stack data is accessed in a very stack-like access pattern. Similar is code, but it is read-only. So we developed techniques to manage code at the function level of granularity. A simple scheme is to bring the code of the function before it is executed, and kick it out of SPM after it returns, but this has high overheads. It took as 4 papers -- continuously optimizing the technique, and making it better --  to finally reach a point, where we were able to show that the performance of code managenent using our approach was better than on a cache based system. Similar is the story with stack data.     The project resulted in 15 conference publications -- all in top venues for "software for embedded systems", e.g., DAC, CODES+ISSS, CASES, LCTES, ASAP etc., 8 journal publications -- all in top IEEE and ACM journals, e.g., TVLSI, TCAD, TECS etc., 2 Ph.D. thesis, one more to finish, 5 masters thesis, 1 more to finish. This project also resulted in 4 patents. One is already granted, and 3 more are pending. This project has touched more than 10 students, and have trained them in state-of-the-art compiler technology. All these students are now working as compiler developers in large companies, like AMD, Intel, nVIDIA, Qualcomm, ARM, Microsoft etc. In fact, this is one of the major problems that I faced in this project -- even masters students were rapidly hired by these companies, eventually resulting in high attrition rate from the project. This project also involved 4 undergraduate students through 2 REU prop...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
