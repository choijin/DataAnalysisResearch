<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Large: An Integrated Approach to Creating Context Enriched Speech Translation Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2009</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>2200000.00</AwardTotalIntnAmount>
<AwardAmount>2200000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The project creates robust, widely-deployable and cost-effective technologies for supporting cross-lingual spoken interaction between people who do not share a common language. The target application supports communication between healthcare personnel who speak English only and patients with limited-English proficiency.  The state-of-the-art technologies that enable such cross-lingual interactions are characterized by a pipelined architecture of speech recognition, machine translation and speech synthesis, that largely ignore the rich information present in spoken language beyond those conveyed by words.  They also do not take advantage of the humans in the loop for collaboratively managing the interaction. Overcoming these limitations requires improving robust intelligence at all levels ? signal, system, and human ? and set the research goals for this project. &lt;br/&gt;&lt;br/&gt;The project?s intellectual merit comes from the unique combination of theoretical, computational model-ing and empirical elements: The theoretical framework is centered on notions of social co-presence to de-velop new models for translation-mediated communication. The computational modeling focuses on capturing prosody, dialog and user state from spoken language for enriching the technology components.  The empirical work relies on a participatory approach to iterative design and evaluation of the system, working directly with the stakeholders.&lt;br/&gt;&lt;br/&gt;The broader impact can be seen in the potential for facilitating multilingual efforts ranging from disaster relief and global business operations to servicing diverse immigrant populations notably in health care. The effort brings together engineers, linguists, human communication experts, and medical professionals to tackle a broad range of problems, and offers integrated interdisciplinary research training and mentor-ing. &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/13/2009</MinAmdLetterDate>
<MaxAmdLetterDate>06/07/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0911009</AwardID>
<Investigator>
<FirstName>Shrikanth</FirstName>
<LastName>Narayanan</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shrikanth S Narayanan</PI_FULL_NAME>
<EmailAddress>shri@sipi.usc.edu</EmailAddress>
<PI_PHON>2137406432</PI_PHON>
<NSF_ID>000377152</NSF_ID>
<StartDate>08/13/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Margaret</FirstName>
<LastName>McLaughlin</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Margaret L McLaughlin</PI_FULL_NAME>
<EmailAddress>mmclaugh@usc.edu</EmailAddress>
<PI_PHON>2137403938</PI_PHON>
<NSF_ID>000354973</NSF_ID>
<StartDate>08/13/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Panayiotis</FirstName>
<LastName>Georgiou</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Panayiotis Georgiou</PI_FULL_NAME>
<EmailAddress>georgiou@sipi.usc.edu</EmailAddress>
<PI_PHON>2137404654</PI_PHON>
<NSF_ID>000487058</NSF_ID>
<StartDate>08/13/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890001</ZipCode>
<StreetAddress><![CDATA[University Park]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~1658964</FUND_OBLG>
<FUND_OBLG>2012~541036</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project contributed to creating foundational technologies that aim to enable robust spoken interaction between people who do not share a common language. The motivating use case was machine-mediated interpersonal communication between healthcare personnel who speak English only and patients with limited-English proficiency. The research focused on creating a tightly unified speech to speech system design by integrating automatic speech recognition, machine translation and text to speech synthesis that, importantly, went beyond just lexical information, by including rich information present in speech and spoken discourse such as prosody and affect. The research and development hence focused on a comprehensive set of topics in enabling robust and accurate speech processing for extracting linguistic and paralinguistic information, joint optimization of speech processing components from voice activity detection and language identification to speech recognition and computational modeling of prosody and affect from speech and language. &nbsp;</p> <p>&nbsp;</p> <p>The intellectual merit of the project was in its contribution to novel algorithm development for several aspects of speech to speech translation, including in bi- or multi-lingual subtitle extraction and alignment for training speech recognition and machine translation systems, transcript recombination for exploiting affordable corpora creation from found data sources, analysis and use of crowd-sourcing reliability for transcription and translation, long speech-text alignment for exploiting existing data sources such as transcribed broadcast news, overlapped speech detection especially for cross-talk in dyadic interactions, robust speech recognition using novel sparse signal processing methods, concept classification and clustering, statistical TTS, and user modeling in mediated interaction settings such as coping with communication errors and uncertainty. A significant research effort also focused on computational modeling of interaction including paralinguistic aspects such as for word boundary detection, overlap and interruption detection, detection of speech fragments and repetitions as well as modeling of speech rate, emphasis, intonation, prominence, and affect. These led to over fifty publications in peer reviewed conferences and journals, including contributions to an edited special issue focused on speech to speech translation in Computer Speech and Language. The project also created an open source, framework for concurrent speech processing called Barista released under the BSD license.</p> <p>&nbsp;</p> <p>The broader impact of developing communication augmentation systems can be seen in their potential for facilitating multilingual efforts ranging from disaster relief to global business operations to servicing diverse immigrant populations including the emblematic application environment considered, viz., health care settings. In the United States, lack of equal medical treatment for patients with limited English speaking capability is a considerable problem. Numerous studies have documented resultant significant barriers to health care delivery and access to this segment of the population--predominantly minority women, children and elders. Treatment that takes place at the individual level must be improved at the individual level. Technology based cross-linguistic mediation augmentation systems can help serve this purpose. This effort brings together engineers, linguists, human communication experts, and medical professionals, including healthcare experts on language and cultural diversity issues to tackle a broad range of scientific and technological problems. As a result, the project offered opportunities for integrated interdisciplinary research training of doctoral and undergraduate students. The strategic partnership between academic and industry experts also offered unique mentoring and internship possibilities for project students in the context of solving problems of direct societal relevance. Our research training focused on research skills in several directions such as machine learning, signal processing, computer algorithm theory, signals and systems, speech recognition, machine translation, affective speech processing, dialog systems, and dyadic interaction analysis and affective modeling. Many of the project alumni have gone on to assuming positions in academia and leading industry labs in speech and language processing.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/24/2017<br>      Modified by: Shrikanth&nbsp;S&nbsp;Narayanan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project contributed to creating foundational technologies that aim to enable robust spoken interaction between people who do not share a common language. The motivating use case was machine-mediated interpersonal communication between healthcare personnel who speak English only and patients with limited-English proficiency. The research focused on creating a tightly unified speech to speech system design by integrating automatic speech recognition, machine translation and text to speech synthesis that, importantly, went beyond just lexical information, by including rich information present in speech and spoken discourse such as prosody and affect. The research and development hence focused on a comprehensive set of topics in enabling robust and accurate speech processing for extracting linguistic and paralinguistic information, joint optimization of speech processing components from voice activity detection and language identification to speech recognition and computational modeling of prosody and affect from speech and language.       The intellectual merit of the project was in its contribution to novel algorithm development for several aspects of speech to speech translation, including in bi- or multi-lingual subtitle extraction and alignment for training speech recognition and machine translation systems, transcript recombination for exploiting affordable corpora creation from found data sources, analysis and use of crowd-sourcing reliability for transcription and translation, long speech-text alignment for exploiting existing data sources such as transcribed broadcast news, overlapped speech detection especially for cross-talk in dyadic interactions, robust speech recognition using novel sparse signal processing methods, concept classification and clustering, statistical TTS, and user modeling in mediated interaction settings such as coping with communication errors and uncertainty. A significant research effort also focused on computational modeling of interaction including paralinguistic aspects such as for word boundary detection, overlap and interruption detection, detection of speech fragments and repetitions as well as modeling of speech rate, emphasis, intonation, prominence, and affect. These led to over fifty publications in peer reviewed conferences and journals, including contributions to an edited special issue focused on speech to speech translation in Computer Speech and Language. The project also created an open source, framework for concurrent speech processing called Barista released under the BSD license.     The broader impact of developing communication augmentation systems can be seen in their potential for facilitating multilingual efforts ranging from disaster relief to global business operations to servicing diverse immigrant populations including the emblematic application environment considered, viz., health care settings. In the United States, lack of equal medical treatment for patients with limited English speaking capability is a considerable problem. Numerous studies have documented resultant significant barriers to health care delivery and access to this segment of the population--predominantly minority women, children and elders. Treatment that takes place at the individual level must be improved at the individual level. Technology based cross-linguistic mediation augmentation systems can help serve this purpose. This effort brings together engineers, linguists, human communication experts, and medical professionals, including healthcare experts on language and cultural diversity issues to tackle a broad range of scientific and technological problems. As a result, the project offered opportunities for integrated interdisciplinary research training of doctoral and undergraduate students. The strategic partnership between academic and industry experts also offered unique mentoring and internship possibilities for project students in the context of solving problems of direct societal relevance. Our research training focused on research skills in several directions such as machine learning, signal processing, computer algorithm theory, signals and systems, speech recognition, machine translation, affective speech processing, dialog systems, and dyadic interaction analysis and affective modeling. Many of the project alumni have gone on to assuming positions in academia and leading industry labs in speech and language processing.           Last Modified: 11/24/2017       Submitted by: Shrikanth S Narayanan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
