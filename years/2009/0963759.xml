<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CCF: Medium:  Routine Parallelism Enabled by Speculation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2010</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>1042469.00</AwardTotalIntnAmount>
<AwardAmount>1074469</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Parallel programming is notoriously difficult, but essential to the future of computing.  Much of the difficulty stems from the need to guarantee, in advance, that parallel computations will not conflict with one another.&lt;br/&gt;Speculation provides an attractive alternative.  By monitoring behavior at run time, and retrying computations that conflict, speculation can expose significant amounts of otherwise unexploitable parallelism, while imposing little or no conceptual burden on the programmer.&lt;br/&gt;&lt;br/&gt;The sponsored research aims to make thread- and process-level speculation a fundamental feature of future programming systems, and to employ it in multiple forms and for multiple purposes: to automatically or semi-automatically parallelize sequential applications; to check, dynamically, the independence of explicitly parallel computations; to isolate the execution of semantically atomic functions; to enable optimizations that are not always safe; to parallelize scripting languages with one-thread-at-a-time semantics; and to profile applications in parallel, for feedback-driven optimization.&lt;br/&gt;&lt;br/&gt;The project adopts a tiered approach that isolates the users of simpler programming idioms from the need to understand more complex alternatives.&lt;br/&gt;At the implementation level, it stresses the seamless integration of shared memory and cluster-level distribution, compiler- and binary translator-based software instrumentation, virtual memory, and hardware speculation/transactions where available.  Latter phases of the project place major emphasis on profiling tools to identify potentially independent program regions, which can then safely be executed in parallel (via speculation) in future runs.</AbstractNarration>
<MinAmdLetterDate>05/05/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/10/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0963759</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Scott</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael L Scott</PI_FULL_NAME>
<EmailAddress>scott@cs.rochester.edu</EmailAddress>
<PI_PHON>5852757745</PI_PHON>
<NSF_ID>000343030</NSF_ID>
<StartDate>05/05/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Chen</FirstName>
<LastName>Ding</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Chen Ding</PI_FULL_NAME>
<EmailAddress>cding@cs.rochester.edu</EmailAddress>
<PI_PHON>5852751373</PI_PHON>
<NSF_ID>000325741</NSF_ID>
<StartDate>05/05/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Rochester</Name>
<CityName>Rochester</CityName>
<ZipCode>146270140</ZipCode>
<PhoneNumber>5852754031</PhoneNumber>
<StreetAddress>518 HYLAN, RC BOX 270140</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY25</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041294109</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ROCHESTER</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041294109</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Rochester]]></Name>
<CityName>Rochester</CityName>
<StateCode>NY</StateCode>
<ZipCode>146270140</ZipCode>
<StreetAddress><![CDATA[518 HYLAN, RC BOX 270140]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~761724</FUND_OBLG>
<FUND_OBLG>2011~32000</FUND_OBLG>
<FUND_OBLG>2013~280745</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In computer software, as in the physical world, tasks can execute safely in parallel only if they are mutually independent -- that is, if they do not interfere with one another. &nbsp;Unfortunately, it is often difficult to tell in advance whether two tasks are truly independent. &nbsp;Conservative strategies that permit parallelism only when it is known to be safe miss many opportunities to improve computer performance.</p> <p>Speculation, in parallel computing, is the strategy of executing tasks in parallel in the hope that they will prove to be independent, while preparing to "back out" and retry sequentially if conflicts are encountered. &nbsp;Work under this grant has demonstrated the value of several different speculative strategies, embodying several different tradeoffs between ease of programming and potential performance improvement.</p> <p>&nbsp; - In <em>nonblocking concurrent data structures</em>, carefully crafted algorithms employ hand-orchestrated speculation for maximum performance and fault tolerance.</p> <p>&nbsp; - At the opposite extreme, <em>Behavior-Oriented Parallelism</em> (BOP) allows the programmer to provide "this might be parallel" hints; these are then used to drive speculation while maintaining the observable behavior of traditional sequential programs.</p> <p>&nbsp; - In <em>speculation-based automatic parallelization</em>, a specially designed compiler identifies code regions in a sequential program that are most likely to benefit from parallel execution, and inserts BOP hints automatically.</p> <p>&nbsp; - In <em>transactional memory</em> (TM), explicitly parallel programs specify operations that must appear to occur one at a time; automatic speculation arranges to execute such operations in parallel whenever possible. &nbsp;Work performed under this grant has led to several key advances in TM semantics and implementation techniques, influencing further developments in both academia and industry, and helping to shape the emerging TM standard for the C++ programming language.</p> <p>&nbsp; - In <em>compiler-aided manual speculation</em> (CSpec), the programmer provides more detailed hints as to what is most likely to be conflict free, thereby improving the performance of explicitly parallel programs, with or without transactions.</p> <p>As computing becomes ever more essential to science, industry, government, commerce, and the arts, parallelism will be crucial for future improvements in performance and functionality. &nbsp;Work performed under this grant demonstrates the power of speculation to help secure those improvements.</p><br> <p>            Last Modified: 09/28/2015<br>      Modified by: Michael&nbsp;L&nbsp;Scott</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In computer software, as in the physical world, tasks can execute safely in parallel only if they are mutually independent -- that is, if they do not interfere with one another.  Unfortunately, it is often difficult to tell in advance whether two tasks are truly independent.  Conservative strategies that permit parallelism only when it is known to be safe miss many opportunities to improve computer performance.  Speculation, in parallel computing, is the strategy of executing tasks in parallel in the hope that they will prove to be independent, while preparing to "back out" and retry sequentially if conflicts are encountered.  Work under this grant has demonstrated the value of several different speculative strategies, embodying several different tradeoffs between ease of programming and potential performance improvement.    - In nonblocking concurrent data structures, carefully crafted algorithms employ hand-orchestrated speculation for maximum performance and fault tolerance.    - At the opposite extreme, Behavior-Oriented Parallelism (BOP) allows the programmer to provide "this might be parallel" hints; these are then used to drive speculation while maintaining the observable behavior of traditional sequential programs.    - In speculation-based automatic parallelization, a specially designed compiler identifies code regions in a sequential program that are most likely to benefit from parallel execution, and inserts BOP hints automatically.    - In transactional memory (TM), explicitly parallel programs specify operations that must appear to occur one at a time; automatic speculation arranges to execute such operations in parallel whenever possible.  Work performed under this grant has led to several key advances in TM semantics and implementation techniques, influencing further developments in both academia and industry, and helping to shape the emerging TM standard for the C++ programming language.    - In compiler-aided manual speculation (CSpec), the programmer provides more detailed hints as to what is most likely to be conflict free, thereby improving the performance of explicitly parallel programs, with or without transactions.  As computing becomes ever more essential to science, industry, government, commerce, and the arts, parallelism will be crucial for future improvements in performance and functionality.  Work performed under this grant demonstrates the power of speculation to help secure those improvements.       Last Modified: 09/28/2015       Submitted by: Michael L Scott]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
