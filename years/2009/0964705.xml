<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI:  Medium:  Collaborative Research:  Optimizing Policies for Service Organizations in Complex Structured Domains</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2010</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>581681.00</AwardTotalIntnAmount>
<AwardAmount>588401</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Todd Leen</SignBlockName>
<PO_EMAI>tleen@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project studies an important class of complex structured planning &lt;br/&gt;domains called ``service domains'' using simulators and probabilistic&lt;br/&gt;action models. Examples of service domains include optimizing emergency &lt;br/&gt;response in a typical city, scheduling doctors and nurses in a &lt;br/&gt;hospital, administering tasks in a typical office, optimally delivering &lt;br/&gt;products to shops from distribution centers. These domains &lt;br/&gt;share many characteristics such as relational structure, parallel actions, &lt;br/&gt;multi-time-scale decision making, exogenous events, and the need for &lt;br/&gt;human interpretable solutions that make them highly challenging.&lt;br/&gt;The project develops scalable and principled planning algorithms for &lt;br/&gt;service domains through a variety of techniques including a novel&lt;br/&gt;hierarchical framework of multi-time-scale optimization, new&lt;br/&gt;model-free simulation-based planning algorithms, and model-based &lt;br/&gt;planning via composition of first-order decision diagrams. These &lt;br/&gt;techniques are applied to the real-world problem of optimizing &lt;br/&gt;the fire and emergency response in cities through a collaboration&lt;br/&gt;with the fire department of Corvallis, Oregon. &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The results of the project include new algorithms and frameworks to solve &lt;br/&gt;service domains, prototype implementations of the algorithms&lt;br/&gt;in the emeregency response domain, and new testbeds of service domains &lt;br/&gt;for research. The broader impact of the work includes more cost-effective &lt;br/&gt;emergency response systems, and development of new research-oriented courses, &lt;br/&gt;tutorials and special workshops on the next generation decision support &lt;br/&gt;systems for service domains.</AbstractNarration>
<MinAmdLetterDate>07/08/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/15/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0964705</AwardID>
<Investigator>
<FirstName>Prasad</FirstName>
<LastName>Tadepalli</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Prasad H Tadepalli</PI_FULL_NAME>
<EmailAddress>tadepall@eecs.orst.edu</EmailAddress>
<PI_PHON>5417375552</PI_PHON>
<NSF_ID>000101828</NSF_ID>
<StartDate>07/08/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alan</FirstName>
<LastName>Fern</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alan Fern</PI_FULL_NAME>
<EmailAddress>afern@eecs.oregonstate.edu</EmailAddress>
<PI_PHON>5417373437</PI_PHON>
<NSF_ID>000088242</NSF_ID>
<StartDate>07/08/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Oregon State University</Name>
<CityName>Corvallis</CityName>
<ZipCode>973318507</ZipCode>
<PhoneNumber>5417374933</PhoneNumber>
<StreetAddress>OREGON STATE UNIVERSITY</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<StateCode>OR</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OR04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>053599908</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OREGON STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>053599908</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Oregon State University]]></Name>
<CityName>Corvallis</CityName>
<StateCode>OR</StateCode>
<ZipCode>973318507</ZipCode>
<StreetAddress><![CDATA[OREGON STATE UNIVERSITY]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OR04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~150122</FUND_OBLG>
<FUND_OBLG>2011~141315</FUND_OBLG>
<FUND_OBLG>2012~169627</FUND_OBLG>
<FUND_OBLG>2013~127337</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Automated Planning</strong> seeks to develop algorithms for taking optimal actions to achieve desired long-term objectives. Example applications of planning <br />includes optimizing emergency response in a typical city, conservation <br />planning, scheduling doctors and nurses in a hospital, administering tasks <br />in a typical office, and controlling the inventories of distribution centers.<br />These domains are problematic for state of the art planners due to the <br />presence of highly parallel actions and the high degree of uncertainty in the <br />action outcomes. All these domains are formalized as Markov Decision Processes (MDPs) with large state and action spaces. The current project developed&nbsp; several planning algorithms based on the following approaches. <br /><br />1. <strong>Symbolic Dynamic Programming (SDP).</strong> This class of algorithms are <br />distinguished by employing symbolic reasoning to derive symbolic descriptions of value functions and policies. The current project extended previous work on symbolic value iteration and policy iteration algorithms in <br />the following directions.</p> <p>a. Symbolic value iteration was extended to <em>factored actions</em> by enhancing<br />the representations of policies and value functions to include action <br />variables. A new version of the symbolic value iteration algorithm that <br />smoothly trades off space for time, and a novel approximate algorithm that <br />significantly reduces planning time with little loss in solution quality were <br />also developed and were shown to achieve state of the art performance.</p> <p>b. A new algorithm called <em>optimistic policy iteration</em> was developed, which is <br />a hybrid between the value iteration and the policy iteration and guarantees <br />space-bounded generalization for factored state and action Markov decision <br />processes. <br /><br />c. Multiple versions of <em>symbolic real-time dynamic programming</em> algorithms <br />were also developed that take advantage of the initial state to generalize<br />incrementally. All algorithms were shown to perform competitively on the <br />benchmark planning domains and often significantly outperformed the prior <br />state of the art. <br /><br />d. A <em>relational</em> version of <em>symbolic value iteration</em> algorithm was developed, <br />which exploited first order decision diagrams and novel reduction operators <br />to solve relational MDPs with additive rewards and exogenous events. The <br />algorithm is guaranteed to achieve a lower bound on the optimal performance under some technical conditions. A practical variant was also developed that takes advantage of a set of sampled states to derive an approximate solution.</p> <p>2. <strong>Integer Linear Programming</strong>. It is known that deterministic planning <br />problems can be reduced to integer linear programming (ILP) for which fast <br />algorithms are available. In this work we formulate probabilistic planning in<br />the ILP framework by integrating it with two innovations. The first idea, hindsight approximation, replaces the probabilistic actions with deterministic actions by prior sampling of their outcomes. Although hindsight approximation is known to be optimistic in general, it has been found to work well in practice. The second idea is to facilitate efficient encoding of parallel actions by introducing decision variables that correspond to action factors. The overall idea is to reduce the probabilistic planning to solving a sample of deterministic planning problems with a shared first action via ILP. The resulting approach has been shown to be very effective and competitive with state of the art planners in many benchmark planning domains despite the optimistic hindsight approximation. It has also been successfully demonstrated in a bird conservation application.</p> <p>3. <strong>Factored Monte-...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Automated Planning seeks to develop algorithms for taking optimal actions to achieve desired long-term objectives. Example applications of planning  includes optimizing emergency response in a typical city, conservation  planning, scheduling doctors and nurses in a hospital, administering tasks  in a typical office, and controlling the inventories of distribution centers. These domains are problematic for state of the art planners due to the  presence of highly parallel actions and the high degree of uncertainty in the  action outcomes. All these domains are formalized as Markov Decision Processes (MDPs) with large state and action spaces. The current project developed  several planning algorithms based on the following approaches.   1. Symbolic Dynamic Programming (SDP). This class of algorithms are  distinguished by employing symbolic reasoning to derive symbolic descriptions of value functions and policies. The current project extended previous work on symbolic value iteration and policy iteration algorithms in  the following directions.  a. Symbolic value iteration was extended to factored actions by enhancing the representations of policies and value functions to include action  variables. A new version of the symbolic value iteration algorithm that  smoothly trades off space for time, and a novel approximate algorithm that  significantly reduces planning time with little loss in solution quality were  also developed and were shown to achieve state of the art performance.  b. A new algorithm called optimistic policy iteration was developed, which is  a hybrid between the value iteration and the policy iteration and guarantees  space-bounded generalization for factored state and action Markov decision  processes.   c. Multiple versions of symbolic real-time dynamic programming algorithms  were also developed that take advantage of the initial state to generalize incrementally. All algorithms were shown to perform competitively on the  benchmark planning domains and often significantly outperformed the prior  state of the art.   d. A relational version of symbolic value iteration algorithm was developed,  which exploited first order decision diagrams and novel reduction operators  to solve relational MDPs with additive rewards and exogenous events. The  algorithm is guaranteed to achieve a lower bound on the optimal performance under some technical conditions. A practical variant was also developed that takes advantage of a set of sampled states to derive an approximate solution.  2. Integer Linear Programming. It is known that deterministic planning  problems can be reduced to integer linear programming (ILP) for which fast  algorithms are available. In this work we formulate probabilistic planning in the ILP framework by integrating it with two innovations. The first idea, hindsight approximation, replaces the probabilistic actions with deterministic actions by prior sampling of their outcomes. Although hindsight approximation is known to be optimistic in general, it has been found to work well in practice. The second idea is to facilitate efficient encoding of parallel actions by introducing decision variables that correspond to action factors. The overall idea is to reduce the probabilistic planning to solving a sample of deterministic planning problems with a shared first action via ILP. The resulting approach has been shown to be very effective and competitive with state of the art planners in many benchmark planning domains despite the optimistic hindsight approximation. It has also been successfully demonstrated in a bird conservation application.  3. Factored Monte-Carlo Tree Search. Monte-Carlo Tree Search (MCTS) is a popular method of choice in probabilistic planning. However, MCTS is found to degrade very rapidly when the problem size is increased. The main reason for this is that MCTS searches in the space of atomic states, and fails to take advantage of their internal structure. The current project explored a...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
