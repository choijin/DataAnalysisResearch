<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CDI-Type I: Collaborative Research: A Bio-Inspired Approach to Recognition of Human Movements and their Styles</AwardTitle>
<AwardEffectiveDate>01/01/2010</AwardEffectiveDate>
<AwardExpirationDate>12/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>246666.00</AwardTotalIntnAmount>
<AwardAmount>246666</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Zhi Tian</SignBlockName>
</ProgramOfficer>
<AbstractNarration>This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).&lt;br/&gt;&lt;br/&gt;The objective of this research is to develop bio-inspired algorithms for recognizing human movements and movement styles in videos of human activities.  The approach is based on a new representation of static three-dimensional (3D) shape structure in the ventral visual pathway consisting of configurations of 3D structural fragments.  This project uses neural experiments to validate an analogous four-dimensional (4D) representation of moving 3D shapes based on 4D (space and time) structure-in-motion (SiM) fragments.  These SiM fragment models are used to develop algorithms for automatically extracting SiM fragments from videos of human activities.  Hybrid system identification and clustering techniques are used to learn a dictionary of human movements used for recognition.  This dictionary, in turn, influences the design of the neural experiments.  The algorithms are evaluated using a real-time tele-immersion system.&lt;br/&gt;&lt;br/&gt;The intellectual merit of this project is to substantially reduce the gap between human and machine perception of human movements through the interaction between computational and experimental analyses from neuroscience, computer vision, machine learning, and dynamical systems.  This project also has the potential to generate advances in machine learning and dynamical systems, by extending classification, clustering and system identification methods to time-series data generated by collections of hybrid dynamical models.&lt;br/&gt;&lt;br/&gt;Potential broader impacts include applications in surveillance, security, assisted home living, infant care, tele-immersion, and athlete motion analysis.  The project also sponsors a competition where small robots are constructed by middle and high-school students from Baltimore, and the EL Alliance program for retaining underrepresented minorities at graduate schools.</AbstractNarration>
<MinAmdLetterDate>09/01/2009</MinAmdLetterDate>
<MaxAmdLetterDate>02/22/2012</MaxAmdLetterDate>
<ARRAAmount>246666</ARRAAmount>
<AwardID>0941382</AwardID>
<Investigator>
<FirstName>Ruzena</FirstName>
<LastName>Bajcsy</LastName>
<EmailAddress>bajcsy@eecs.berkeley.edu</EmailAddress>
<StartDate>09/01/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
</Institution>
<FoaInformation>
<Code>0206000</Code>
<Name>Telecommunications</Name>
</FoaInformation>
<ProgramElement>
<Code>7750</Code>
<Text>CDI TYPE I</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>6890</Code>
<Text>RECOVERY ACT ACTION</Text>
</ProgramReference>
<ProgramReference>
<Code>7721</Code>
<Text>FROM DATA TO KNOWLEDGE</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
