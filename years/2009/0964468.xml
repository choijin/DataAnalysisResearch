<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Medium:  Synthesis and Perception of Speaker Identity</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/15/2010</AwardEffectiveDate>
<AwardExpirationDate>04/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>914849.00</AwardTotalIntnAmount>
<AwardAmount>914849</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This proposal addresses the problem of synthesizing speaker identity when only a small training sample is available.  To achieve the goal of synthesis of speaker identity from a small training corpus the project will address problems including trainable abstract parameterizations of the prosodic patterns that characterize a speaker and voice conversion methods.   The project falls into the general category of building Text-to-Speech (TTS) synthesis system in order to generate speech that sounds like that of a specific individual (Speaker Identity Synthesis, or SIS).  Systems of this kind have numerous applications, including the creation of personalized voices for individuals with neurodegenerative disorders who anticipate becoming users of Speech Generating Devices (Sods) in the future and many other applications in the consumer products and entertainment industry.   Consumer products such as navigation systems and mobile phones are rapidly being developed that make use of linguistic information about generated utterance. The project will also provide new tools and data for human perception of speaker identity.  The tools developed in the process and the associated perceptual studies are also relevant for assessment of speaker recognition systems, and the project provides a new generation of concise, trainable characterizations of a speaker?s prosodic patterns that can be incorporated in these systems.  The proposed study will elucidate the trade-offs and algorithm issues of the proposed SIS systems and it is likely that the proposed work will have a strong intellectual impact in the field of speech synthesis.</AbstractNarration>
<MinAmdLetterDate>05/11/2010</MinAmdLetterDate>
<MaxAmdLetterDate>05/11/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0964468</AwardID>
<Investigator>
<FirstName>Jan</FirstName>
<LastName>van Santen</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jan P van Santen</PI_FULL_NAME>
<EmailAddress>vansantj@ohsu.edu</EmailAddress>
<PI_PHON>5033463765</PI_PHON>
<NSF_ID>000319411</NSF_ID>
<StartDate>05/11/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Kain</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander Kain</PI_FULL_NAME>
<EmailAddress>kaina@ohsu.edu</EmailAddress>
<PI_PHON>5037481539</PI_PHON>
<NSF_ID>000296505</NSF_ID>
<StartDate>05/11/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Esther</FirstName>
<LastName>Klabbers</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Esther A Klabbers</PI_FULL_NAME>
<EmailAddress>klabbers@cslu.ogi.edu</EmailAddress>
<PI_PHON>5037483005</PI_PHON>
<NSF_ID>000434840</NSF_ID>
<StartDate>05/11/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Oregon Health &amp; Science University</Name>
<CityName>Portland</CityName>
<ZipCode>972393098</ZipCode>
<PhoneNumber>5034947784</PhoneNumber>
<StreetAddress>3181 S W Sam Jackson Park Rd</StreetAddress>
<StreetAddress2><![CDATA[Mail Code L106OPAM]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<StateCode>OR</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OR03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>096997515</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OREGON HEALTH &amp; SCIENCE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>096997515</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Oregon Health &amp; Science University]]></Name>
<CityName>Portland</CityName>
<StateCode>OR</StateCode>
<ZipCode>972393098</ZipCode>
<StreetAddress><![CDATA[3181 S W Sam Jackson Park Rd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OR03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~914849</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><!--?xml version="1.0" encoding="UTF-8"?--></p> <div class="standard">Today's commercial Text-to-Speech (TTS) systems do not offer the capability of changing their output voices to sound like that of a specific target speaker. One important application of such a capability is the creation of personalized voices for individuals with neurodegenerative disorders who anticipate becoming users of speech generating devices in lieu of natural speech communication.<br />To sound compelling, a personalizable TTS system must capture a substantial portion of the unique features of the target speaker. These features reflect speaker characteristics at multiple levels, ranging from the biomechanical properties of the physical speech production apparatus (e.g. vocal cord size, nasal cavity volume, vocal tract length) to prosodic style (e.g. reduced phrase-final lengthening).<br />Two broad approaches present themselves as potential solutions. The first approach is to build a TTS system from the ground up, based on recordings of the target speaker. However, this effort is typically very time- and resource-intensive, and the speaker may need to talk for several hours. The second approach is to use Voice Conversion (VC) technology to convert the voice of an existing TTS system. VC typically involves training a spectral transformation on parallel recordings of a source speaker and a target speaker. The advantage of this approach is that it requires few recordings. However, prosodic modifications are traditionally implemented using very simple solutions, such as matching speaking rate and the mean and variance of the pitch contour. In addition, despite significant progress in this respect, the quality of the converted speech is still significantly lower than that of the best synthesis systems.<br />We addressed the existing shortcomings in the following ways:(1) We have improved the fidelity of the underlying spectral mapping function by incorporating the latest techniques from machine learning, specifically semi-supervised learning with deep neural networks.(2) We have created a new intonation model that can capture the most important aspects of the target speaker&rsquo;s pitch.(3) For the purposes of evaluation and training of our algorithms, we recorded a database of 10 voices.<br />We evaluated the performance of the final system using both objective and subjective evaluations. The subjective evaluations included listening tests that measured the quality of the final speech, the naturalness of the pitch contour, as well as the accuracy of mimicking the target speaker. We showed that our approaches significantly outperformed existing systems.</div> <p>&nbsp;</p><br> <p>            Last Modified: 06/09/2015<br>      Modified by: Alexander&nbsp;Kain</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Today's commercial Text-to-Speech (TTS) systems do not offer the capability of changing their output voices to sound like that of a specific target speaker. One important application of such a capability is the creation of personalized voices for individuals with neurodegenerative disorders who anticipate becoming users of speech generating devices in lieu of natural speech communication. To sound compelling, a personalizable TTS system must capture a substantial portion of the unique features of the target speaker. These features reflect speaker characteristics at multiple levels, ranging from the biomechanical properties of the physical speech production apparatus (e.g. vocal cord size, nasal cavity volume, vocal tract length) to prosodic style (e.g. reduced phrase-final lengthening). Two broad approaches present themselves as potential solutions. The first approach is to build a TTS system from the ground up, based on recordings of the target speaker. However, this effort is typically very time- and resource-intensive, and the speaker may need to talk for several hours. The second approach is to use Voice Conversion (VC) technology to convert the voice of an existing TTS system. VC typically involves training a spectral transformation on parallel recordings of a source speaker and a target speaker. The advantage of this approach is that it requires few recordings. However, prosodic modifications are traditionally implemented using very simple solutions, such as matching speaking rate and the mean and variance of the pitch contour. In addition, despite significant progress in this respect, the quality of the converted speech is still significantly lower than that of the best synthesis systems. We addressed the existing shortcomings in the following ways:(1) We have improved the fidelity of the underlying spectral mapping function by incorporating the latest techniques from machine learning, specifically semi-supervised learning with deep neural networks.(2) We have created a new intonation model that can capture the most important aspects of the target speakerÆs pitch.(3) For the purposes of evaluation and training of our algorithms, we recorded a database of 10 voices. We evaluated the performance of the final system using both objective and subjective evaluations. The subjective evaluations included listening tests that measured the quality of the final speech, the naturalness of the pitch contour, as well as the accuracy of mimicking the target speaker. We showed that our approaches significantly outperformed existing systems.          Last Modified: 06/09/2015       Submitted by: Alexander Kain]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
