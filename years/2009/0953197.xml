<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Towards Scalable Datacenter Services with Strong Robustness Guarantees</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2010</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>529306.00</AwardTotalIntnAmount>
<AwardAmount>529306</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Modern datacenters have reached hundreds of thousands of servers, and modern distributed services can be spread over multiple or even dozens of datacenters worldwide.  For scalability, availability, and performance, these services have increasingly embraced a weakened model of data consistency.  This trade-off proved highly successful for applications such as web crawling, search, and content distribution.  On the other hand, the recent trend to move ever-more dynamic applications to the "cloud" portends a shift in service requirements---in which losing data or applying operations out-of-order may not be acceptable---as does far-flung demand for concurrent access to data and services.&lt;br/&gt;&lt;br/&gt;To meet these changing needs, this research project reconsiders the challenge of building storage and replicated systems with strong robustness guarantees and at scale.  Recognizing that large-scale, complex systems are typically built-up from smaller groups---which either provide different functionality or partition some larger state-space into smaller, more manageable parts---it leverages a group compositional approach to tackle the problem.  First, the research develops novel protocols for smaller groups of nodes that offer strong properties at minimal overhead.  Second, it proposes a coordination service and a suite of management algorithms that adaptively organizes these groups and composes them together.  These algorithms explore problems of dynamic load balancing, topological control, and security.  The research includes substantial systems building, including the incorporation of its core services into several other distributed systems (e.g., a scalable virtual world and a istributed name resolution service).&lt;br/&gt;&lt;br/&gt;The remarkably rich and varied Internet services run out of datacenters form the core of people's online experiences today.  The algorithms and software systems developed by this research may lower the barrier to developing datacenter services that are scalable, reliable, secure, efficient, and easy to manage. And by reducing the technical difficulty of building robust, large-scale applications---by providing them with a firm foundation---this research may enable further innovation in Internet services.</AbstractNarration>
<MinAmdLetterDate>02/17/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/25/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0953197</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Freedman</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael J Freedman</PI_FULL_NAME>
<EmailAddress>mfreed@cs.princeton.edu</EmailAddress>
<PI_PHON>6092589179</PI_PHON>
<NSF_ID>000500977</NSF_ID>
<StartDate>02/17/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[Off. of Research &amp; Proj. Adm]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>4090</Code>
<Text>ADVANCED NET INFRA &amp; RSCH</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~82112</FUND_OBLG>
<FUND_OBLG>2011~83587</FUND_OBLG>
<FUND_OBLG>2012~117612</FUND_OBLG>
<FUND_OBLG>2013~121135</FUND_OBLG>
<FUND_OBLG>2014~124860</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1"><span class="s1">While the distributed systems literature has long focused on providing strong availability and consistency guarantees in the face of failures, the previously-unprecedented scale and scope of Web services and the adoption of cloud computing have created a paradigm shift in requirements.&nbsp; Systems have gone from tens of machines on-premise to thousands or more in specialized datacenters.&nbsp; Services should be dynamically scalable, always on, and everywhere available.</span>&nbsp;</p> <p class="p1"><span class="s1">To meet these needs, this multi-year research project focused on both designing system architectures and algorithmic techniques, as well as building and evaluating prototype systems, that address key challenges in this new environment.&nbsp; In many cases, our investigations challenged long-held beliefs: Do we need to give up on consistency for highly available cloud storage across the wide area?&nbsp; Is resource fairness really the best way to share cloud resources across tenants?&nbsp; Do we need to trust cloud services that we regularly use?&nbsp; And so on.</span></p> <p class="p1"><span class="s1">In particular, this project focused on the following four areas for scalable cloud services:</span></p> <p class="p1"><span class="s1">1. <strong>Consistency: </strong>Our research achieved stronger consistency in scalable storage systems, particularly when data is distantly replicated.&nbsp; Towards this end, we developed strongly-consistent protocols that scaled throughput linearly for read-only workloads.&nbsp; We also introduced the first protocol for scalable causal consistency across geographically-replicated data, while preserving high availability and low latency for all operations, demonstrating that the options implied by the CAP Theorem are not nearly as severe as commonly preceived.</span></p> <p class="p2"><span class="s1">&nbsp;</span>2. <strong>Predictable Behavior: </strong>Our research introduced new approaches for system-wide resource provisioning for shared storage and computation in multi-tenant datacenters.&nbsp; First, we developed some of the first distributed storage systems that rigorously enforce resource guarantees across an entire shared storage cluster, while ensuring both high utilization (in a work-conserving manner) and robust behavior across dynamic workload changes.&nbsp; It does so through careful adaptive cluster-wide scheduling and optimization, as well as fine-grained resource accounting down through the SSD-backed storage stack.&nbsp; Second, we challenged the notion that multi-tenant cluster scheduling should focus on resource fairness rather than application utility. In settings where answers are inherently approximal (such as model training in machine learning), we showed one can find better answers faster by automatically calculating such utilities and using them as a metric for determining resource allocation, rather than physical resource fairness.</p> <p class="p2"><span class="s1">&nbsp;</span><span class="s1">3. <strong>Performance: </strong>Our research pushed the performance limits for distributed storage systems, by rethinking how existing architectural designs can be leveraged.&nbsp; For example,&nbsp;</span>we demonstrated how to repurpose latent replication necessary for fault-tolerance to also serve in providing alternative indexing methods for higher performance, and how to carefully re-use the programmability of emerging network switches to enable content-aware routing for hybrid cache and scale-out storage architectures.&nbsp; Both still did so while enabling data consistency.</p> <p class="p1"><span class="s1">4. <strong>Security:</strong> Finally, our research showed how reduce the requisit trust in cloud providers for security or safety. We first demonstrated how to scale techniques from Byzantine fault tolerance (BFT) to large-scale applications, by developing algorithms to safely compose many small BFT groups into a large-scale system.&nbsp; But we then turned to address an underlying problem with BFT protocols in that they assume servers fail independently, which is ill-matched to the software monoculture and single administrative control of datacenter services.&nbsp; We address this in two threads of work.&nbsp; First, we minimized the impact that software vulnerabilities can have on database-backed web services, by automatically splitting (previously shared-memory-space) applications into sandboxed processes per distinct view, while requiring little to no assistance by developers.&nbsp; Second, we demonstrated how to build web services that use a centralized cloud provider without trusting it with the privacy or integrity of users&rsquo; data, even for rich applications like concurrent document editing or scalable social networking.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 05/11/2018<br>      Modified by: Michael&nbsp;J&nbsp;Freedman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[While the distributed systems literature has long focused on providing strong availability and consistency guarantees in the face of failures, the previously-unprecedented scale and scope of Web services and the adoption of cloud computing have created a paradigm shift in requirements.  Systems have gone from tens of machines on-premise to thousands or more in specialized datacenters.  Services should be dynamically scalable, always on, and everywhere available.  To meet these needs, this multi-year research project focused on both designing system architectures and algorithmic techniques, as well as building and evaluating prototype systems, that address key challenges in this new environment.  In many cases, our investigations challenged long-held beliefs: Do we need to give up on consistency for highly available cloud storage across the wide area?  Is resource fairness really the best way to share cloud resources across tenants?  Do we need to trust cloud services that we regularly use?  And so on. In particular, this project focused on the following four areas for scalable cloud services: 1. Consistency: Our research achieved stronger consistency in scalable storage systems, particularly when data is distantly replicated.  Towards this end, we developed strongly-consistent protocols that scaled throughput linearly for read-only workloads.  We also introduced the first protocol for scalable causal consistency across geographically-replicated data, while preserving high availability and low latency for all operations, demonstrating that the options implied by the CAP Theorem are not nearly as severe as commonly preceived.  2. Predictable Behavior: Our research introduced new approaches for system-wide resource provisioning for shared storage and computation in multi-tenant datacenters.  First, we developed some of the first distributed storage systems that rigorously enforce resource guarantees across an entire shared storage cluster, while ensuring both high utilization (in a work-conserving manner) and robust behavior across dynamic workload changes.  It does so through careful adaptive cluster-wide scheduling and optimization, as well as fine-grained resource accounting down through the SSD-backed storage stack.  Second, we challenged the notion that multi-tenant cluster scheduling should focus on resource fairness rather than application utility. In settings where answers are inherently approximal (such as model training in machine learning), we showed one can find better answers faster by automatically calculating such utilities and using them as a metric for determining resource allocation, rather than physical resource fairness.  3. Performance: Our research pushed the performance limits for distributed storage systems, by rethinking how existing architectural designs can be leveraged.  For example, we demonstrated how to repurpose latent replication necessary for fault-tolerance to also serve in providing alternative indexing methods for higher performance, and how to carefully re-use the programmability of emerging network switches to enable content-aware routing for hybrid cache and scale-out storage architectures.  Both still did so while enabling data consistency. 4. Security: Finally, our research showed how reduce the requisit trust in cloud providers for security or safety. We first demonstrated how to scale techniques from Byzantine fault tolerance (BFT) to large-scale applications, by developing algorithms to safely compose many small BFT groups into a large-scale system.  But we then turned to address an underlying problem with BFT protocols in that they assume servers fail independently, which is ill-matched to the software monoculture and single administrative control of datacenter services.  We address this in two threads of work.  First, we minimized the impact that software vulnerabilities can have on database-backed web services, by automatically splitting (previously shared-memory-space) applications into sandboxed processes per distinct view, while requiring little to no assistance by developers.  Second, we demonstrated how to build web services that use a centralized cloud provider without trusting it with the privacy or integrity of users? data, even for rich applications like concurrent document editing or scalable social networking.          Last Modified: 05/11/2018       Submitted by: Michael J Freedman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
