<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Dynamic Staging Architecture for Accelerating I/O Pipelines</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2010</AwardEffectiveDate>
<AwardExpirationDate>04/30/2013</AwardExpirationDate>
<AwardTotalIntnAmount>90000.00</AwardTotalIntnAmount>
<AwardAmount>90000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Petascale applications are producing terabytes of data at a great rate. Storage systems in large-scale machines are significantly stressed as I/O rates are not growing as fast to cope with data production. A variety of HPC activities such as writing output and checkpoint data are all stymied by the I/O bandwidth bottleneck. Further to this, the post-processing and subsequent analysis/visualization of computational results is increasingly time consuming due to the widening gap between the storage/processing capacities of supercomputers and users' local clusters. &lt;br/&gt;&lt;br/&gt;This research focuses on building a novel in-job dynamic data staging architecture and in bringing it to bear on the looming petascale I/O crisis. To this end, the following objectives are investigated: (i) the concerted use of node-local memory and emerging hardware such as Solid State Disks (SSDs), from a dedicated set of nodes, as a means to alleviate the I/O bandwidth bottleneck, (ii) the multiplexing of traditional user post-processing pipelines and secondary computations with asynchronous I/O on the staging ground to perform scalable I/O and data analytics, (iii) bypassing memory to access the staging area, and (iv) enabling QoS both in the staging ground and in the communication channel connecting it to compute client and persistent storage. &lt;br/&gt;&lt;br/&gt;This study will have a wide-ranging impact on future provisioning of extreme-scale machines and will provide formative guidelines to this end. The result of this research will be a set of integrated techniques that can fundamentally change the current parallel I/O model and accelerate petascale I/O pipelines. Further, this research will help analyze the utility of SSDs in day-to-day supercomputing I/O and inform the wider HPC community of its viability.</AbstractNarration>
<MinAmdLetterDate>05/28/2010</MinAmdLetterDate>
<MaxAmdLetterDate>05/28/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0937842</AwardID>
<Investigator>
<FirstName>Dhabaleswar</FirstName>
<LastName>Panda</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dhabaleswar K Panda</PI_FULL_NAME>
<EmailAddress>panda@cse.ohio-state.edu</EmailAddress>
<PI_PHON>6142925199</PI_PHON>
<NSF_ID>000487085</NSF_ID>
<StartDate>05/28/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888735</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1960 Kenny Road]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832127323</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001964634</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio State University]]></Name>
<CityName>Columbus</CityName>
<StateCode>OH</StateCode>
<ZipCode>432101016</ZipCode>
<StreetAddress><![CDATA[Office of Sponsored Programs]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7952</Code>
<Text>HECURA</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~90000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The overall goals of this research are to design and build a novel  `in-job data staging architecture' from a dedicated set of nodes using a  combination of node-local memory and emerging hardware such as Solid  State Disks (SSDs). This staging ground will be used to perform scalable  I/O and data analytics while a much larger number of nodes are running  the parallel application. The OSU part of this collaborative research  focuses on the following: 1) Designing appropriate RDMA-based mechanisms  to the staging area consisting of SSDs, 2) Novel approaches  (user/kernel/hybrid) schemes with memory-bypass to access SSDs and 3)  providing InfiniBand-level QoS-aware accesses to the staging area.</p> <p>Research has been done in all these three areas. RDMA-based designs for fast checkpointing and migration schemes have been designed with SSDs. The designs have also been combined with InfiniBand QoS framework to provide performance isolation across communication and I/O traffic. Hybrid schemes have been proposed to access SSDs with InfiniBand and RDMA. New designs for file systems have been proposed for supporting SCR (a popular chekpointing library) to support applications-level checkpointing. The new scheme has been demonstrated to scale up to 3 million MPI tasks. The checkpointing and migration schemes with RDMA and QoS support have been integrated into the popular open-source MVAPICH2 MPI library.</p> <p>The research results have been presented at internationational conferences and workshops. Results have also been disseminated through Keynote Talks, Invited Tutorials and Invited talks, presented by the PI. The designs have been integrated and made available with MVAPICH2 software library which is being used by more than 2,000 organizations in 70 countries. Many production systems, including XSEDE systems, use this software library to extract performance and scalbility from modern InfiniBand clusters. The proposed designs have also influenced other MPI libraries to have similar features and support.</p><br> <p>            Last Modified: 08/05/2013<br>      Modified by: Dhabaleswar&nbsp;K&nbsp;Panda</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The overall goals of this research are to design and build a novel  `in-job data staging architecture' from a dedicated set of nodes using a  combination of node-local memory and emerging hardware such as Solid  State Disks (SSDs). This staging ground will be used to perform scalable  I/O and data analytics while a much larger number of nodes are running  the parallel application. The OSU part of this collaborative research  focuses on the following: 1) Designing appropriate RDMA-based mechanisms  to the staging area consisting of SSDs, 2) Novel approaches  (user/kernel/hybrid) schemes with memory-bypass to access SSDs and 3)  providing InfiniBand-level QoS-aware accesses to the staging area.  Research has been done in all these three areas. RDMA-based designs for fast checkpointing and migration schemes have been designed with SSDs. The designs have also been combined with InfiniBand QoS framework to provide performance isolation across communication and I/O traffic. Hybrid schemes have been proposed to access SSDs with InfiniBand and RDMA. New designs for file systems have been proposed for supporting SCR (a popular chekpointing library) to support applications-level checkpointing. The new scheme has been demonstrated to scale up to 3 million MPI tasks. The checkpointing and migration schemes with RDMA and QoS support have been integrated into the popular open-source MVAPICH2 MPI library.  The research results have been presented at internationational conferences and workshops. Results have also been disseminated through Keynote Talks, Invited Tutorials and Invited talks, presented by the PI. The designs have been integrated and made available with MVAPICH2 software library which is being used by more than 2,000 organizations in 70 countries. Many production systems, including XSEDE systems, use this software library to extract performance and scalbility from modern InfiniBand clusters. The proposed designs have also influenced other MPI libraries to have similar features and support.       Last Modified: 08/05/2013       Submitted by: Dhabaleswar K Panda]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
