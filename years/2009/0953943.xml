<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: A Multi-Disciplinary Approach to the Next Generation of Collaborative Technologies</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2010</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>400139.00</AwardTotalIntnAmount>
<AwardAmount>501918</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research aims to enable the development of the next generation of collaborative technologies and to study their effects on human collaboration. It does so by improving our theoretical understanding of how various features of shared visual context affect communication, coordination and collaboration. The research develops and makes available the code for a new dual eye tracking methodology that can be used to study coordination and collaboration. It also develops a new set of metrics that can be more generally used to understand coordinated eye movements as they relate to dialogue and conversation. The results of this work will add to knowledge in a number of disciplines, including human-computer interaction, language technologies, computer science, computational linguistics, communication studies, cognitive science, and social and cognitive psychology.&lt;br/&gt;&lt;br/&gt;As recent efforts in telemedicine, distance education, and remote training and repair attest, there exists enormous potential for technologies to support these activities at a distance, ultimately resulting in widespread benefits such as equal access to quality education and medical care. The education activities in this research include: (1) developing a new Ph.D. course that teaches a theory-driven design approach; (2) developing a series of course modules that use the dual eye tracking methodology to teach behavioral coding, statistics and machine learning; (3) developing a publicly available multimodal corpus with a set of tutorials, and (4) developing a series of international workshops on dual eye tracking. Finally, the research will simultaneously advance discovery and understanding while training both graduate and undergraduate students in interdisciplinary research methods and will contribute to the development of traditionally underrepresented individuals in the fields of computer and information sciences.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>12/02/2009</MinAmdLetterDate>
<MaxAmdLetterDate>04/15/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0953943</AwardID>
<Investigator>
<FirstName>Darren</FirstName>
<LastName>Gergle</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Darren R Gergle</PI_FULL_NAME>
<EmailAddress>dgergle@northwestern.edu</EmailAddress>
<PI_PHON>8474671221</PI_PHON>
<NSF_ID>000327409</NSF_ID>
<StartDate>12/02/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606114579</ZipCode>
<StreetAddress><![CDATA[750 N. Lake Shore Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~97210</FUND_OBLG>
<FUND_OBLG>2011~108420</FUND_OBLG>
<FUND_OBLG>2012~210861</FUND_OBLG>
<FUND_OBLG>2014~85427</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Popular accounts of technological innovation pledge collaborative systems that, like an expert teammate or personal assistant, will anticipate a partner&rsquo;s needs and actions as they take place in a wide variety of natural contexts. Yet, one of the major roadblocks to developing such systems stems from a lack of understanding the ways in which humans interact naturally in contextualized physical environments. Consider a team of surgeons that work on a time-critical operation and their actions are coordinated through large-scale visual displays, or a college physics professor that watches students run a tabletop physics experiment and she clarifies the experiment based solely on seeing their actions and thus recognizing that they misunderstood her lecture. In each of these settings, inferences about a partner&rsquo;s goals, focus of attention, level of comprehension, shared knowledge, and the state of the joint task all derive from shared visual context and temporal coordination, or lack thereof. In order to create collaborative technologies that are able to anticipate and react appropriately to users&rsquo; need and actions, it is crucial to develop a detailed computational understanding of these basic human collaborative processes.</p> <p><em>Intellectual Merit:</em> The overarching goal of this research was to support the development of the next generation of collaborative technologies capable of dealing with natural human processes of coordination both when the humans are co-present and separated by space or time. We worked toward this goal in a number of ways: First, we developed a series of experimental studies of human-to-human communication in both naturalistic and controlled environments. These studies showed, for example, that when pairs are interacting in a mobile environment they use their body position to help disambiguate and evoke the object they want to talk about. This finding suggests that natural language technologies may benefit from richer models of the spatial context in which collaborative pairs interact. Second, we developed and disseminated a novel, real-time, dual person eye-tracking methodology that automatically recognizes a person&rsquo;s gaze to physical objects in a surrounding environment. This methodology can be widely used by researchers to understand how language use varies across different types of physical spaces, and it provides tools and new metrics for assessing collaborative performance in such environments. Third, we formalized our findings into explicit computational models that can be applied in automated or semi-automated technologies. For example, we developed machine-learning and rule-based techniques that can, in limited scenarios, automatically identify which physical objects people are referring to, even when using vague expressions like &ldquo;that one,&rdquo; and we developed new technologies that selectively display a partner&rsquo;s eye-gaze onto a shared workspace. These approaches and tools can be used to better support distance collaboration and remote physical tasks such as directed repairs of physical infrastructure, remote guidance and navigation, or distributed learning centered on tangible or physical models.</p> <p><em>Broader Impacts:</em> The work was also incorporated into a series of workshops on dual eye-tracking as an emerging methodology for studying collaborative work. Three national and international workshops were held, and papers from 50+ authors were presented. The research methodologies have also been integrated into research methods and seminar classes, and the work was published and presented at multiple conferences, universities and technology firms. The project supported, in part, the training of four PhD students, one master&rsquo;s student, and a large team of undergraduate researchers.</p><br> <p>            Last Modified: 05/20/2016<br>      Modified by: Darren&nbsp;R&nbsp;Gergle...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Popular accounts of technological innovation pledge collaborative systems that, like an expert teammate or personal assistant, will anticipate a partnerÆs needs and actions as they take place in a wide variety of natural contexts. Yet, one of the major roadblocks to developing such systems stems from a lack of understanding the ways in which humans interact naturally in contextualized physical environments. Consider a team of surgeons that work on a time-critical operation and their actions are coordinated through large-scale visual displays, or a college physics professor that watches students run a tabletop physics experiment and she clarifies the experiment based solely on seeing their actions and thus recognizing that they misunderstood her lecture. In each of these settings, inferences about a partnerÆs goals, focus of attention, level of comprehension, shared knowledge, and the state of the joint task all derive from shared visual context and temporal coordination, or lack thereof. In order to create collaborative technologies that are able to anticipate and react appropriately to usersÆ need and actions, it is crucial to develop a detailed computational understanding of these basic human collaborative processes.  Intellectual Merit: The overarching goal of this research was to support the development of the next generation of collaborative technologies capable of dealing with natural human processes of coordination both when the humans are co-present and separated by space or time. We worked toward this goal in a number of ways: First, we developed a series of experimental studies of human-to-human communication in both naturalistic and controlled environments. These studies showed, for example, that when pairs are interacting in a mobile environment they use their body position to help disambiguate and evoke the object they want to talk about. This finding suggests that natural language technologies may benefit from richer models of the spatial context in which collaborative pairs interact. Second, we developed and disseminated a novel, real-time, dual person eye-tracking methodology that automatically recognizes a personÆs gaze to physical objects in a surrounding environment. This methodology can be widely used by researchers to understand how language use varies across different types of physical spaces, and it provides tools and new metrics for assessing collaborative performance in such environments. Third, we formalized our findings into explicit computational models that can be applied in automated or semi-automated technologies. For example, we developed machine-learning and rule-based techniques that can, in limited scenarios, automatically identify which physical objects people are referring to, even when using vague expressions like "that one," and we developed new technologies that selectively display a partnerÆs eye-gaze onto a shared workspace. These approaches and tools can be used to better support distance collaboration and remote physical tasks such as directed repairs of physical infrastructure, remote guidance and navigation, or distributed learning centered on tangible or physical models.  Broader Impacts: The work was also incorporated into a series of workshops on dual eye-tracking as an emerging methodology for studying collaborative work. Three national and international workshops were held, and papers from 50+ authors were presented. The research methodologies have also been integrated into research methods and seminar classes, and the work was published and presented at multiple conferences, universities and technology firms. The project supported, in part, the training of four PhD students, one masterÆs student, and a large team of undergraduate researchers.       Last Modified: 05/20/2016       Submitted by: Darren R Gergle]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
