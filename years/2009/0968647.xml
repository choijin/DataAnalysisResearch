<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER:  Automatic Indexing of Polyphonic Music by Cascade Classifiers</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2010</AwardEffectiveDate>
<AwardExpirationDate>04/30/2011</AwardExpirationDate>
<AwardTotalIntnAmount>35934.00</AwardTotalIntnAmount>
<AwardAmount>35934</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>IIS - 0968647  &lt;br/&gt;Automatic Indexing of Polyphonic Music by Cascade Classifiers&lt;br/&gt;Ras, Zbigniew W.    &lt;br/&gt;University of North Carolina at Charlotte&lt;br/&gt;&lt;br/&gt;Abstract&lt;br/&gt;&lt;br/&gt;The goal of this EAGER project is to support exploratory work on a new class of cascade classifiers and hybrid classifiers for automatic indexing of polyphonic music according to instruments and types of instruments. Testing new classifiers for automatic indexing of polyphonic music, specifically those for the automatic classification of instrumental sound from recordings of orchestral music is difficult and involves a high degree of risk and uncertainty as to the outcome. If successful, the results may prove to be transformative and have significant impact on music information analysis.  The work will employ resources in the MIRAI database developed in an earlier NSF supported project.  The main MIRAI database contains about 1,000,000 musical instrument sounds, each represented as a vector of approximately 1,000 features.  Each instrument sound is identified and matched to a corresponding instrument.</AbstractNarration>
<MinAmdLetterDate>04/27/2010</MinAmdLetterDate>
<MaxAmdLetterDate>04/27/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0968647</AwardID>
<Investigator>
<FirstName>Zbigniew</FirstName>
<LastName>Ras</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zbigniew W Ras</PI_FULL_NAME>
<EmailAddress>ras@uncc.edu</EmailAddress>
<PI_PHON>7046878574</PI_PHON>
<NSF_ID>000119800</NSF_ID>
<StartDate>04/27/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Charlotte</Name>
<CityName>CHARLOTTE</CityName>
<ZipCode>282230001</ZipCode>
<PhoneNumber>7046871888</PhoneNumber>
<StreetAddress>9201 University City Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>066300096</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHARLOTTE, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Charlotte]]></Name>
<CityName>CHARLOTTE</CityName>
<StateCode>NC</StateCode>
<ZipCode>282230001</ZipCode>
<StreetAddress><![CDATA[9201 University City Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~35934</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div><span style="font-size: x-small;"><span style="font-size: x-small;"> <p> <div> <div><span lang="EN"> <p>Huge repositories of audio recordings available from the Internet and private sets offer plethora of options for potential listeners. The listeners might be interested in finding particular titles, but they can also wish to find pieces they are unable to name. For example, the user might be in mood to listen to something joyful, romantic, or nostalgic; he or she may want to find a tune sung to the computer's microphone; also, the user might be in mood to listen to jazz with solo trumpet, or classic music with sweet violin sound. More advanced person (a musician) might need scores for the piece of music found in the Internet, to play it by himself or herself.</p> <p>All these issues are of interest for researchers working in MIR domain, since meta-information enclosed in audio files lacks such data -- usually recordings are labeled by title and performer, maybe category and playing time. However, automatic categorization of music pieces is still one of more often performed tasks, since the user may need more information than it is already provided, i.e. more detailed or different categorization. Automatic extraction of melody or possibly the full score is another aim of MIR. Pitch-tracking techniques yield quite good results for monophonic data, but extraction of polyphonic data is much more complicated. When multiple instruments play, information about timbre may help to separate melodic lines for automatic transcription of music (spatial information might also be used here). In this proposal, we mainly focused on automatic recognition of timbre, i.e. of instrument, playing in polyphonic and polytimbral (multi-instrumental) audio recordings.</p> </span><span style="font-size: x-small;"></span><span lang="EN"><span style="font-size: x-small;"></span></span></div> <span lang="EN"> <p>We introduced and tested&nbsp;the hierarchically structured cascade classification system to estimate multiple timbre information from the polyphonic sound by classification based on acoustic features and short-term power spectrum matching. This cascade classification system makes a first estimate on the higher level decision attribute, which stands for the musical instrument family. Then, the further estimation is done within that specific family range. Our experiments showed better performance of a cascade system than traditional hierarchical system (the same type of classifier is used at all nodes of the tree)&nbsp;or traditional flat classification methods which directly estimate the instruments without higher level of family information analysis.</p> </span></div> </p> <span lang="EN"> <p>We introduced and tested&nbsp;the hierarchically structured cascade classification system to estimate multiple timbre information from the polyphonic sound by classification based on acoustic features and short-term power spectrum matching. For each window frame, the cascade process starts by the classification at the root of hierarchical tree and it is followed by the classification at other lower levels of the tree. The system selects the appropriate classifier and feature set to perform classification at each possible level from the top to the bottom. The confidence of classification at each level has to be either equal or above some user specified confidence threshold. After classification process reaches the bottom level, which is the instrument level, we have the final instrument estimations for the window frame, and the overall confidence for each instrument estimation is calculated by multiplying the confidence obtained at each node. After all the individual frames are estimated by the classification system, a smoothing process is performed by calculating the average confidence of each possible instrument within the indexing window.</p> <p>Our experiments showed better performance of a ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      Huge repositories of audio recordings available from the Internet and private sets offer plethora of options for potential listeners. The listeners might be interested in finding particular titles, but they can also wish to find pieces they are unable to name. For example, the user might be in mood to listen to something joyful, romantic, or nostalgic; he or she may want to find a tune sung to the computer's microphone; also, the user might be in mood to listen to jazz with solo trumpet, or classic music with sweet violin sound. More advanced person (a musician) might need scores for the piece of music found in the Internet, to play it by himself or herself.  All these issues are of interest for researchers working in MIR domain, since meta-information enclosed in audio files lacks such data -- usually recordings are labeled by title and performer, maybe category and playing time. However, automatic categorization of music pieces is still one of more often performed tasks, since the user may need more information than it is already provided, i.e. more detailed or different categorization. Automatic extraction of melody or possibly the full score is another aim of MIR. Pitch-tracking techniques yield quite good results for monophonic data, but extraction of polyphonic data is much more complicated. When multiple instruments play, information about timbre may help to separate melodic lines for automatic transcription of music (spatial information might also be used here). In this proposal, we mainly focused on automatic recognition of timbre, i.e. of instrument, playing in polyphonic and polytimbral (multi-instrumental) audio recordings.    We introduced and tested the hierarchically structured cascade classification system to estimate multiple timbre information from the polyphonic sound by classification based on acoustic features and short-term power spectrum matching. This cascade classification system makes a first estimate on the higher level decision attribute, which stands for the musical instrument family. Then, the further estimation is done within that specific family range. Our experiments showed better performance of a cascade system than traditional hierarchical system (the same type of classifier is used at all nodes of the tree) or traditional flat classification methods which directly estimate the instruments without higher level of family information analysis.     We introduced and tested the hierarchically structured cascade classification system to estimate multiple timbre information from the polyphonic sound by classification based on acoustic features and short-term power spectrum matching. For each window frame, the cascade process starts by the classification at the root of hierarchical tree and it is followed by the classification at other lower levels of the tree. The system selects the appropriate classifier and feature set to perform classification at each possible level from the top to the bottom. The confidence of classification at each level has to be either equal or above some user specified confidence threshold. After classification process reaches the bottom level, which is the instrument level, we have the final instrument estimations for the window frame, and the overall confidence for each instrument estimation is calculated by multiplying the confidence obtained at each node. After all the individual frames are estimated by the classification system, a smoothing process is performed by calculating the average confidence of each possible instrument within the indexing window.  Our experiments showed better performance of a cascade system than traditional hierarchical system (the same type of classifier is used at all nodes of the tree) or traditional flat classification methods which directly estimate the instruments without higher level of family information analysis.           Last Modified: 07/24/2011       Submitted by: Zbigniew W Ras]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
