<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Automating the Large-Scale Measurement of Insect Behavior</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>209014.00</AwardTotalIntnAmount>
<AwardAmount>209014</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>08080200</Code>
<Directorate>
<Abbreviation>BIO</Abbreviation>
<LongName>Direct For Biological Sciences</LongName>
</Directorate>
<Division>
<Abbreviation>DBI</Abbreviation>
<LongName>Div Of Biological Infrastructure</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter McCartney</SignBlockName>
<PO_EMAI>pmccartn@nsf.gov</PO_EMAI>
<PO_PHON>7032928470</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Georgia Institute of Technology and Arizona State University are awarded grants to develop an integrated approach to automating measurements of insect behavior from video records. The study of insect behavior plays a fundamental role in biology, but progress is limited by the rate at which data can be gathered. Researchers have relied largely on direct observation or time-consuming manual annotation of video records. This project will create an automated solution that combines theory, algorithms, software modules, and databases of behavior measurements. These tools will be widely applicable to studies of animal behavior, but development will focus on the particularly rich and challenging problems offered by ants, where multiple interacting animals must be simultaneously tracked. Current multi-tracking technologies are limited in their ability to deal with the huge degree of target interaction in this context, including significant periods of occlusion of one target by another. This project will generate a novel approach that applies the graph-cut optimization method to video object segmentation. This method will be able to identify which portions of the video correspond to distinct targets even when they overlap. Accurate target segmentation will also facilitate more accurate adaptation to changes in appearance due to lighting or other environmental effects. The project will also develop novel behavior recognition methods that infer behavior from target configuration and appearance. Unlike traditional methods this approach will not rely on the state of the tracker and thus will avoid the compounding of recognition errors by tracking errors.&lt;br/&gt;&lt;br/&gt;Two cross-cutting themes inform this project. The first is a focus on algorithms and methods compatible with modular software tools, thus allowing biologists to develop a customized solution to a wide range of sensing tasks. The second theme is the utilization of state-of-the-art ultra-high resolution imaging sensors to obtain more information about ant behavior and identity than is currently possible. These capabilities will enable insect biologists to frame and answer research questions that exceed the limited data collection capabilities of current methods. Algorithms and software modules will be widely disseminated, to maximize their power to transform biology in a more general setting. For more information visit the project website at http://www.kinetrack.org/</AbstractNarration>
<MinAmdLetterDate>09/08/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/04/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.074</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0959514</AwardID>
<Investigator>
<FirstName>Stephen</FirstName>
<LastName>Pratt</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stephen C Pratt</PI_FULL_NAME>
<EmailAddress>stephen.pratt@asu.edu</EmailAddress>
<PI_PHON>4807279425</PI_PHON>
<NSF_ID>000482353</NSF_ID>
<StartDate>09/08/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Arizona State University</Name>
<CityName>TEMPE</CityName>
<ZipCode>852816011</ZipCode>
<PhoneNumber>4809655479</PhoneNumber>
<StreetAddress>ORSPA</StreetAddress>
<StreetAddress2><![CDATA[660 South Mill Avenue, Suite 310]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>943360412</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ARIZONA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>806345658</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Arizona State University]]></Name>
<CityName>TEMPE</CityName>
<StateCode>AZ</StateCode>
<ZipCode>852816011</ZipCode>
<StreetAddress><![CDATA[ORSPA]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1165</Code>
<Text>ADVANCES IN BIO INFORMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<ProgramReference>
<Code>9183</Code>
<Text>GENERAL FOUNDATIONS OF BIOTECHNOLOGY</Text>
</ProgramReference>
<ProgramReference>
<Code>9184</Code>
<Text>BIOTECHNOLOGY - INFRASTRUCTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>BIOT</Code>
<Text>BIOTECHNOLOGY</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~60396</FUND_OBLG>
<FUND_OBLG>2011~68932</FUND_OBLG>
<FUND_OBLG>2012~79686</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project developed software tools for automatic tracking of the position and orientation of multiple interacting animals in video recordings. These tools can greatly accelerate the pace of research on the complex behavior of highly social animals like ants and bees, which has traditionally been limited by reliance on manual annotation of video records. We are applying the software to understand basic questions about how complex collective behavior emerges in societies that lack any well-informed leaders. For example, we can use it to track the thousands of individual behavioral acts that allow a colony of ants to collectively choose the best nest or food site when no individual ant is aware of more than a subset of the options. We can also use it to infer how ants learn to navigate to food or nest sites of interest, by automatically tracking the paths taken by ants as they learn a route and share information about it. Historically, such studies have been hampered by the need for human researchers to directly review and hand annotate video records, identifying each individual, tracking its position, and labeling its behavior. The new software can do part of this work automatically, greatly reducing the time needed to complete an experiment. In addition to enhancing efficiency, the software also makes possible new kinds of experiments. For example, many societies have dominance hierarchies that determine how resources are distributed among group members. The ways in which these hierarchies emerge from social interactions is generally unclear, partly because the process may require many thousands of interactions over days, weeks, or even months. The problem is especially acute when certain rare behaviors may have a large impact on the final outcome. The only way to gather the data needed to infer the mechanism of hierarchy formation is to exhaustively review every interaction. For long timeframes, this approach is essentially impossible when using hand annotation. Our new software makes the strategy a practical one. The program is publicly available for download, and the project is open source, so that other researchers can modify and improve the software, or combine it with other tools.</p><br> <p>            Last Modified: 12/19/2014<br>      Modified by: Stephen&nbsp;C&nbsp;Pratt</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project developed software tools for automatic tracking of the position and orientation of multiple interacting animals in video recordings. These tools can greatly accelerate the pace of research on the complex behavior of highly social animals like ants and bees, which has traditionally been limited by reliance on manual annotation of video records. We are applying the software to understand basic questions about how complex collective behavior emerges in societies that lack any well-informed leaders. For example, we can use it to track the thousands of individual behavioral acts that allow a colony of ants to collectively choose the best nest or food site when no individual ant is aware of more than a subset of the options. We can also use it to infer how ants learn to navigate to food or nest sites of interest, by automatically tracking the paths taken by ants as they learn a route and share information about it. Historically, such studies have been hampered by the need for human researchers to directly review and hand annotate video records, identifying each individual, tracking its position, and labeling its behavior. The new software can do part of this work automatically, greatly reducing the time needed to complete an experiment. In addition to enhancing efficiency, the software also makes possible new kinds of experiments. For example, many societies have dominance hierarchies that determine how resources are distributed among group members. The ways in which these hierarchies emerge from social interactions is generally unclear, partly because the process may require many thousands of interactions over days, weeks, or even months. The problem is especially acute when certain rare behaviors may have a large impact on the final outcome. The only way to gather the data needed to infer the mechanism of hierarchy formation is to exhaustively review every interaction. For long timeframes, this approach is essentially impossible when using hand annotation. Our new software makes the strategy a practical one. The program is publicly available for download, and the project is open source, so that other researchers can modify and improve the software, or combine it with other tools.       Last Modified: 12/19/2014       Submitted by: Stephen C Pratt]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
