<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Place and Response Mechanisms in Human Spatial Learning</AwardTitle>
<AwardEffectiveDate>09/15/2009</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardAmount>370062</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Akaysha Tang</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Humans use spatial memory to successfully navigate in familiar environments on a daily basis, such as finding car keys or getting to the office.  However, perhaps everyone has the experience of ending up at the wrong destination at some point in their lifetime. What has gone wrong in our memory system when this happens?  Our understanding of navigation from memory in the past has relied on research with animals. For example, research has shown two distinct learning strategies that may explain both the flexible and habitual nature of navigation from memory: fast, flexible place learning in the hippocampus of the brain, but slow, rigid learning of specific patterns of response in the striatum. How such research findings from the animal population can apply to human spatial memory is so far unclear. With support from the National Science Foundation, the investigator will use behavioral and brain imaging techniques (e.g., functional magnetic resonance imaging) to study place learning and response learning and to bridge the domains of animal spatial learning and human spatial cognition. The goals of the project are to test whether and how humans engage place and response learning mechanisms, link those mechanisms to predicted neural correlates, and establish their functional significance. The investigator has designed a series of behavioral and neuroscience experiments to achieve these goals.  &lt;br/&gt;&lt;br/&gt;This work represents a transformative step in the study of human learning and memory. Linking findings from non-human animals to studies of human behavior and brain mechanisms has the potential to significantly advance our understanding of human spatial cogntion by building connections among psychology, neurobiology, and genetics. The work also provides an essential bridge to broader issues of human memory and individual differences in learning styles by considering how and under what conditions humans might learn differently. Such results could lead to better strategies for enhancing teaching and learning in a wide range of applications. This grant will readily support educational outreach by providing training opportunities at the undergraduate, graduate, and post-doctoral levels, and by making relevant materials and demos accessible to the public.</AbstractNarration>
<MinAmdLetterDate>09/14/2009</MinAmdLetterDate>
<MaxAmdLetterDate>09/14/2009</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0920221</AwardID>
<Investigator>
<FirstName>Amy</FirstName>
<LastName>Shelton</LastName>
<EmailAddress>ashelton@jhu.edu</EmailAddress>
<StartDate>09/14/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
</Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>1699</Code>
<Text>COGNEURO</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1699</Code>
<Text>COGNEURO</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
