<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: High Dimensional Variable Selection and Risk Properties</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>High dimensional variable selection plays a pivotal role in contemporary statistical modeling, learning and scientific discoveries. Long-standing theoretical questions in the literature include how high dimensionality regularization methods with general penalties can handle, what the role of penalty functions is, and how to characterize the optimality of variable selection procedures. The investigator proposes to study four interrelated research topics. First, the investigator studies penalized likelihood methods with general penalties, which are widely applied for simultaneously selecting important variables and estimating their effects in high dimensional statistical inference, where the dimensionality can be much larger than sample size. Second, various contexts of high dimensional variable selection beyond penalized likelihood methods including penalized empirical risk and hunting for interactions are investigated. Third, the investigator proposes new principles for model selection when models are possibly misspecified and studies the robustness of various regularization methods for high dimensional variable selection under model misspecification. Fourth, the risk properties and optimality of various high dimensional regularization methods in the contexts of penalized least squares and penalized likelihood are further investigated.&lt;br/&gt;&lt;br/&gt;The analysis of vast data sets now commonly arises in diverse fields of sciences, engineering and humanities ranging from genomics and health sciences to economics, finance and machine learning. High dimensional data analysis poses numerous challenges to statistical theory, methods and implementations that are not present in smaller scale studies. A major goal of this proposal is to make theoretical and methodological contributions to the important and challenging topic of high dimensional variable selection and statistical inference. These new developments provide unified and systematic understandings of various regularization methods in high dimensions, and allow scientists to analyze high dimensional data with increased efficiency, expediency and interpretability. The proposed work is incorporated into new courses on the state-of-the-art high dimensional statistical learning, and will benefit the training and learning of undergraduates, graduate students, and underrepresented minorities. The proposed work on variable selection in high dimensions will not only help better identify factors that are important to, for example, public health and market risk, but also benefit a broad range of scientists and researchers in various fields.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>04/07/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/08/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0955316</AwardID>
<Investigator>
<FirstName>Jinchi</FirstName>
<LastName>Lv</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jinchi Lv</PI_FULL_NAME>
<EmailAddress>jinchilv@usc.edu</EmailAddress>
<PI_PHON>2137406603</PI_PHON>
<NSF_ID>000082144</NSF_ID>
<StartDate>04/07/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890001</ZipCode>
<StreetAddress><![CDATA[University Park]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~88960</FUND_OBLG>
<FUND_OBLG>2011~77760</FUND_OBLG>
<FUND_OBLG>2012~77760</FUND_OBLG>
<FUND_OBLG>2013~77760</FUND_OBLG>
<FUND_OBLG>2014~77760</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>High-dimensional variable selection plays a pivotal role in contemporary statistical modeling, learning, and scientific discoveries. Effectively identifying important variables in high or ultra-high dimensional feature spaces is very challenging in theory, methods, and implementations. Long-standing theoretical questions in the literature include how high dimensionality regularization methods with general penalties can handle, what the role of penalty functions is, and how to characterize the optimality of variable selection procedures. To address these challenges, this project has studied various regularization methods for high-dimensional variable selection, developed a variety of new high-dimensional variable selection techniques, and investigated their finite-sample performances and applications. In particular, four interrelated research topics have been investigated systematically and extensively. First, the PI has studied penalized likelihood methods with general penalties, which are applied widely for simultaneous selection of important variables and estimation of their effects in high-dimensional statistical inference, where the number of predictors can be much larger than the number of observations. Second, the PI has investigated various contexts of high-dimensional variable selection beyond penalized likelihood methods including penalized empirical risk and hunting for interactions. Third, the PI has proposed new principles for model selection when models are possibly misspecified and studied the robustness of regularization methods for high-dimensional variable selection under model misspecification. Fourth, the PI has further investigated the risk properties as well as the connections and differences of various high-dimensional regularization methods in the contexts of penalized least squares and penalized likelihood.</p> <p>The analysis of massive data sets now arises commonly in diverse fields of sciences, engineering, and humanities ranging from genomics, neuroscience, and health sciences to economics, finance, and machine learning. High-dimensional data analysis poses numerous challenges to statistical theory, methods, and implementations that are not present in smaller scale studies. This project has made a variety of methodological and theoretical contributions to the important and challenging topic of high-dimensional variable selection and statistical inference. These new developments provide unified and systematic understandings of various regularization methods in ultra-high dimensions, and allow scientists and practitioners to analyze high-dimensional data with increased efficiency, expediency, and interpretability using the newly developed tools. Selected research results from this project have been incorporated into a new course on the state-of-the-art high-dimensional statistical learning, and benefited the training and learning of undergraduates, graduate students, and underrepresented minorities. The work on this project will not only help better identify factors that are important to, for example, public health and market risk, but also benefit a broad range of scientists and researchers in various fields.</p><br> <p>            Last Modified: 09/28/2015<br>      Modified by: Jinchi&nbsp;Lv</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ High-dimensional variable selection plays a pivotal role in contemporary statistical modeling, learning, and scientific discoveries. Effectively identifying important variables in high or ultra-high dimensional feature spaces is very challenging in theory, methods, and implementations. Long-standing theoretical questions in the literature include how high dimensionality regularization methods with general penalties can handle, what the role of penalty functions is, and how to characterize the optimality of variable selection procedures. To address these challenges, this project has studied various regularization methods for high-dimensional variable selection, developed a variety of new high-dimensional variable selection techniques, and investigated their finite-sample performances and applications. In particular, four interrelated research topics have been investigated systematically and extensively. First, the PI has studied penalized likelihood methods with general penalties, which are applied widely for simultaneous selection of important variables and estimation of their effects in high-dimensional statistical inference, where the number of predictors can be much larger than the number of observations. Second, the PI has investigated various contexts of high-dimensional variable selection beyond penalized likelihood methods including penalized empirical risk and hunting for interactions. Third, the PI has proposed new principles for model selection when models are possibly misspecified and studied the robustness of regularization methods for high-dimensional variable selection under model misspecification. Fourth, the PI has further investigated the risk properties as well as the connections and differences of various high-dimensional regularization methods in the contexts of penalized least squares and penalized likelihood.  The analysis of massive data sets now arises commonly in diverse fields of sciences, engineering, and humanities ranging from genomics, neuroscience, and health sciences to economics, finance, and machine learning. High-dimensional data analysis poses numerous challenges to statistical theory, methods, and implementations that are not present in smaller scale studies. This project has made a variety of methodological and theoretical contributions to the important and challenging topic of high-dimensional variable selection and statistical inference. These new developments provide unified and systematic understandings of various regularization methods in ultra-high dimensions, and allow scientists and practitioners to analyze high-dimensional data with increased efficiency, expediency, and interpretability using the newly developed tools. Selected research results from this project have been incorporated into a new course on the state-of-the-art high-dimensional statistical learning, and benefited the training and learning of undergraduates, graduate students, and underrepresented minorities. The work on this project will not only help better identify factors that are important to, for example, public health and market risk, but also benefit a broad range of scientists and researchers in various fields.       Last Modified: 09/28/2015       Submitted by: Jinchi Lv]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
