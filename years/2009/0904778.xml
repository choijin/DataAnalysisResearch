<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Medium:  A Machine Learning Approach to Computational Understanding of Skill Criteria in Surgical Training</AwardTitle>
<AwardEffectiveDate>07/15/2009</AwardEffectiveDate>
<AwardExpirationDate>09/30/2013</AwardExpirationDate>
<AwardTotalIntnAmount>874484.00</AwardTotalIntnAmount>
<AwardAmount>874484</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
</ProgramOfficer>
<AbstractNarration>This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).&lt;br/&gt;&lt;br/&gt;A recent study by the Agency for Healthcare Research and Quality (AHRQ) documented over 32,000 mostly surgery-related deaths, costing nine billion dollars and accounting for 2.4 million extra days in hospital in 2000. At the same time, economic pressures influence medical schools to reduce the cost in training surgeons. The success of simulation-based surgical education and training will not only shorten the time involving a faculty surgeon in various stages of training (hence reducing cost), but also will improve the quality of training. Reviewing the state-of-the-art research, there are two research challenges: (1) to automatically rate the proficiency of a resident surgeon in simulation-based training, and (2) to associate skill ratings with correction procedures. This award will explore a machine-learning-based approach to computational understanding of surgical skills based on temporal inference of visual and motion-capture data from surgical simulation. The approach employs latent space analysis that exploits intrinsic correlations among multiple data sources of a surgical action. This learning approach is enabled by the simulation and data acquisition design that ensures clinical meaningfulness of the acquired data. &lt;br/&gt;&lt;br/&gt;Intellectual Merit: &lt;br/&gt;The intellectual merit of the proposed work lies in the exploration of the answers to two key research questions: (1) how to develop a computational understanding of surgical-skill-defining criteria (high-level knowledge) from low-level, raw sensory observations; and (2) how to efficiently perform temporal inference from correlated data sources of disparate nature with unknown structures. Expertise related to motion is a well-observed phenomenon but the task of inferring motion expertise patterns from captured data especially video is largely unexplored. This award presents a controlled methodology to extract expertise patterns from raw sensory data. This interdisciplinary team with collaboration from medical professionals is well positioned to address these fundamental research issues.&lt;br/&gt;&lt;br/&gt;Broad Impact: &lt;br/&gt;The broad impact of the proposed work is threefold. First, the ability of the proposed system to create descriptive, mathematical models from recorded video and other data in surgical training opens up the possibility of new paradigms that can significantly improve current practice in surgeon education and training. Second, the proposed approaches may enable development of lower-cost surgical training systems. Third, the novel formulation of and solution to the fundamental problem of expertise inference from disparate temporal measurements with intrinsic correlation can find many other applications such as sports (e.g., baseball and golf training), rehabilitation, and surveillance. The impact of the interdisciplinary project on education manifests in its direct contribution to better training and education of surgeons and to nurturing a new generation of students who possesses strong interdisciplinary background and skills. &lt;br/&gt;&lt;br/&gt;Key Words: Surgical Simulation, Surgical Education and Training, Complementary Feature Selection, Visual and Temporal Inference, Learning in Latent Space.</AbstractNarration>
<MinAmdLetterDate>07/10/2009</MinAmdLetterDate>
<MaxAmdLetterDate>02/22/2012</MaxAmdLetterDate>
<ARRAAmount>874484</ARRAAmount>
<AwardID>0904778</AwardID>
<Investigator>
<FirstName>Huan</FirstName>
<LastName>Liu</LastName>
<EmailAddress>hliu@asu.edu</EmailAddress>
<StartDate>07/10/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Baoxin</FirstName>
<LastName>Li</LastName>
<EmailAddress>Baoxin.Li@asu.edu</EmailAddress>
<StartDate>07/10/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kanav</FirstName>
<LastName>Kahol</LastName>
<EmailAddress>Kanav.Kahol@asu.edu</EmailAddress>
<StartDate>07/10/2009</StartDate>
<EndDate>09/15/2011</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Arizona State University</Name>
<CityName>TEMPE</CityName>
<ZipCode>852816011</ZipCode>
<PhoneNumber>4809655479</PhoneNumber>
<StreetAddress>ORSPA</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
</Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>6890</Code>
<Text>RECOVERY ACT ACTION</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
