<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>IGERT: Complex Scene Perception</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2010</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>2697643.00</AwardTotalIntnAmount>
<AwardAmount>2697643</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>11010103</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DGE</Abbreviation>
<LongName>Division Of Graduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Laura Regassa</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This Integrative Graduate Education and Research Training (IGERT) award to the University of Pennsylvania supports the development of a new training paradigm for perception scientists and engineers, and is designed to provide them with a unique grasp of the computational and psychophysical underpinnings of the phenomena of perception. It will create a new role model of a well-rounded perceptual scientist with a firm grasp of both computational and experimental analytic skills. The existence of such a cadre of U.S. researchers will contribute to the country's global competitiveness in the growing machine perception and robotics industry.&lt;br/&gt;&lt;br/&gt;Research and training activities are organized around five thematic areas related to complex scene perception: (1) Spatial perception and navigation; (2) Perception of material and terrain properties; (3) Neural responses to natural signals, saliency and attention; (4) Object Recognition in context and visual memory; and (5) Agile Perception. Interdisciplinary research will enable new insights into the astounding performance of human and animal perception as well as the design of new algorithms that will make robots perceive and act in complex scenes.&lt;br/&gt;&lt;br/&gt;IGERT trainees will commit in advance of acceptance to a five-year graduate training program, comprising the following components: (1) Core disciplinary training; (2) one-year cross-disciplinary training in a chosen second discipline; (3) participation in two foundational and one integrational IGERT courses; (4) attendance of an interdisciplinary IGERT seminar; (5) co-advising throughout the 5 graduate years by an interdisciplinary faculty team ; and (6) completion of the Ph.D. dissertation.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating U.S. Ph.D. scientists and engineers with the interdisciplinary background, deep knowledge in a chosen discipline, and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing innovative new models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries.</AbstractNarration>
<MinAmdLetterDate>07/07/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/27/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0966142</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Brainard</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David Brainard</PI_FULL_NAME>
<EmailAddress>brainard@psych.upenn.edu</EmailAddress>
<PI_PHON>2155737579</PI_PHON>
<NSF_ID>000117728</NSF_ID>
<StartDate>07/07/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Camillo</FirstName>
<LastName>Taylor</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Camillo J Taylor</PI_FULL_NAME>
<EmailAddress>cjtaylor@central.cis.upenn.edu</EmailAddress>
<PI_PHON>2158980376</PI_PHON>
<NSF_ID>000097685</NSF_ID>
<StartDate>07/07/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kostas</FirstName>
<LastName>Daniilidis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kostas Daniilidis</PI_FULL_NAME>
<EmailAddress>kostas@cis.upenn.edu</EmailAddress>
<PI_PHON>2158988549</PI_PHON>
<NSF_ID>000207772</NSF_ID>
<StartDate>07/07/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Lee</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel D Lee</PI_FULL_NAME>
<EmailAddress>ddl46@cornell.edu</EmailAddress>
<PI_PHON>6463709862</PI_PHON>
<NSF_ID>000322679</NSF_ID>
<StartDate>07/07/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Isabel</FirstName>
<LastName>Muzzio</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Isabel A Muzzio</PI_FULL_NAME>
<EmailAddress>isabel.muzzio@utsa.edu</EmailAddress>
<PI_PHON>2014584810</PI_PHON>
<NSF_ID>000528171</NSF_ID>
<StartDate>07/07/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046205</ZipCode>
<StreetAddress><![CDATA[Research Services]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1335</Code>
<Text>IGERT FULL PROPOSALS</Text>
</ProgramElement>
<ProgramReference>
<Code>110E</Code>
<Text>EDUCATION RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>1335</Code>
<Text>IGERT FULL PROPOSALS</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0410</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0412</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0413</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~1075878</FUND_OBLG>
<FUND_OBLG>2012~626964</FUND_OBLG>
<FUND_OBLG>2013~994801</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this IGERT was to create a new training paradigm for perception scientists and engineers with a unique grasp of the computational and psychophysical underpinnings of the phenomena of perception. Complex scene perception involves the study of the brain and the design of algorithms for understanding visual scenes encountered in daily life. Twenty-four (24) trainees were educated cross disciplines with engineering classes attended by psychologists and neuroscientists and psychophysics classes attended by engineers. For six years, this interdisciplinary group was meeting weekly in a journal club learning how to cross boundaries in the study of perception both in research conduct and presentation. Thesis topics were concentrated around spatial perception and navigation, perception of materials, object recognition in context and visual memory, and neural responses to natural signals. An example is the study of spatial perception in animals during reorientation, a situation encountered in many robotics applications as well.&nbsp;When a navigator&rsquo;s internal sense of direction is disrupted, she must rely on external cues to regain her bearings, a process termed spatial reorientation. Extensive research has demonstrated that the geometric shape of the environment exerts powerful control over reorientation behavior, but the neural and cognitive mechanisms underlying this phenomenon are not well understood. Whereas some theories claim that geometry controls behavior through an allocentric mechanism potentially tied to the hippocampus, others postulate that disoriented navigators reach their goals by using an egocentric view-matching strategy. To resolve this debate,&nbsp; &nbsp;hippocampal representations were characterized during reorientation. It was found that the alignment of the recovered hippocampal map was determined by the geometry of the chamber, but not by non-geometric cues, even when these cues could be used to disambiguate geometric ambiguities. Hippocampal activity was recorded as disoriented mice performed a classical goal-directed spatial memory task in a rectangular chamber, and again it was found that the recovered hippocampal map aligned solely to the chamber geometry. A strong correspondence between the hippocampal map alignment and the animal&rsquo;s behavior made possible to predict the search location of the animal from neural responses on a trial-by-trial basis. Together, these results demonstrate that spatial reorientation involves the alignment of the hippocampal map to local geometry. In a robot navigation project, the estimation of motion direction of a robot was studied (figure).&nbsp;This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of the non-Gaussian noise in two-frame optic flow estimates. Nonetheless, a good estimate of motion under these conditions is important for robotic navigation, especially in the case of Unmanned Aerial Vehicles.&nbsp;Situations like monocular sensors on small platforms pose additional problems: computational resources are often very limited and estimates must be made in real time under unusual viewing conditions (e.g. with a vertically flipped camera, no visible ground plane, and a single pass through a scene). These contexts present many sources of noise. Real-time flow estimation produces unreliable data, and the associated noise is often pervasive and non-Gaussian, which makes estimation difficult and explicit outlier rejection problematic. Even in the noise-free case, the estimation of camera motion is plagued with many suboptimal interpretations (illusions) caused by the hilly structure of the cost function. Additionally, forward motion, which is very common in real-world navigation, is known to be particularly hard for monocular visual odometry. The novel contribution was the reformulation of the perspective camera motion equation using a lifted kernel for joint parameter and weight estimation.&nbsp;</p><br> <p>            Last Modified: 10/30/2017<br>      Modified by: Kostas&nbsp;Daniilidis</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/0966142/0966142_10018070_1509346664275_flowUAV--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/0966142/0966142_10018070_1509346664275_flowUAV--rgov-800width.jpg" title="Optical Flow Perceived by an Unmanned Aerial Vehicle"><img src="/por/images/Reports/POR/2017/0966142/0966142_10018070_1509346664275_flowUAV--rgov-66x44.jpg" alt="Optical Flow Perceived by an Unmanned Aerial Vehicle"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The optical flow vector field as perceived by a flying camera. Vectors are emanating from the focus of expansion that indicates the direction of linear velocities and the length of the vectors can reveal the time to collision to the corresponding point in a ray through a pixel.</div> <div class="imageCredit">Kostas Daniilidis</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Kostas&nbsp;Daniilidis</div> <div class="imageTitle">Optical Flow Perceived by an Unmanned Aerial Vehicle</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this IGERT was to create a new training paradigm for perception scientists and engineers with a unique grasp of the computational and psychophysical underpinnings of the phenomena of perception. Complex scene perception involves the study of the brain and the design of algorithms for understanding visual scenes encountered in daily life. Twenty-four (24) trainees were educated cross disciplines with engineering classes attended by psychologists and neuroscientists and psychophysics classes attended by engineers. For six years, this interdisciplinary group was meeting weekly in a journal club learning how to cross boundaries in the study of perception both in research conduct and presentation. Thesis topics were concentrated around spatial perception and navigation, perception of materials, object recognition in context and visual memory, and neural responses to natural signals. An example is the study of spatial perception in animals during reorientation, a situation encountered in many robotics applications as well. When a navigator?s internal sense of direction is disrupted, she must rely on external cues to regain her bearings, a process termed spatial reorientation. Extensive research has demonstrated that the geometric shape of the environment exerts powerful control over reorientation behavior, but the neural and cognitive mechanisms underlying this phenomenon are not well understood. Whereas some theories claim that geometry controls behavior through an allocentric mechanism potentially tied to the hippocampus, others postulate that disoriented navigators reach their goals by using an egocentric view-matching strategy. To resolve this debate,   hippocampal representations were characterized during reorientation. It was found that the alignment of the recovered hippocampal map was determined by the geometry of the chamber, but not by non-geometric cues, even when these cues could be used to disambiguate geometric ambiguities. Hippocampal activity was recorded as disoriented mice performed a classical goal-directed spatial memory task in a rectangular chamber, and again it was found that the recovered hippocampal map aligned solely to the chamber geometry. A strong correspondence between the hippocampal map alignment and the animal?s behavior made possible to predict the search location of the animal from neural responses on a trial-by-trial basis. Together, these results demonstrate that spatial reorientation involves the alignment of the hippocampal map to local geometry. In a robot navigation project, the estimation of motion direction of a robot was studied (figure). This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of the non-Gaussian noise in two-frame optic flow estimates. Nonetheless, a good estimate of motion under these conditions is important for robotic navigation, especially in the case of Unmanned Aerial Vehicles. Situations like monocular sensors on small platforms pose additional problems: computational resources are often very limited and estimates must be made in real time under unusual viewing conditions (e.g. with a vertically flipped camera, no visible ground plane, and a single pass through a scene). These contexts present many sources of noise. Real-time flow estimation produces unreliable data, and the associated noise is often pervasive and non-Gaussian, which makes estimation difficult and explicit outlier rejection problematic. Even in the noise-free case, the estimation of camera motion is plagued with many suboptimal interpretations (illusions) caused by the hilly structure of the cost function. Additionally, forward motion, which is very common in real-world navigation, is known to be particularly hard for monocular visual odometry. The novel contribution was the reformulation of the perspective camera motion equation using a lifted kernel for joint parameter and weight estimation.        Last Modified: 10/30/2017       Submitted by: Kostas Daniilidis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
