<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small: RUI: Parameterization and Collection of Demonstrative Gestures for Interactive Virtual Humans</AwardTitle>
<AwardEffectiveDate>07/15/2009</AwardEffectiveDate>
<AwardExpirationDate>06/30/2013</AwardExpirationDate>
<AwardTotalIntnAmount>499071.00</AwardTotalIntnAmount>
<AwardAmount>499071</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
</ProgramOfficer>
<AbstractNarration>The research goal of this project is to develop new techniques for producing realistic and parameterized humanlike gestures based on motion data acquired from real performances. This work proposes novel computational models and interactive interfaces, and focuses on whole-body demonstrative gestures for interactive training and assistance applications with autonomous virtual humans. &lt;br/&gt;&lt;br/&gt;Although gesture modeling has made substantial advances in recent years, less attention has been given to parameterized demonstrative gestures which can be modified to refer to arbitrary locations in the environment. This particular class of gestures is critical for a number of applications. Typical examples of such gestures include pointing to and demonstrating how to operate particular devices or objects. To ensure the system's effectiveness, this project includes cognitive studies for guiding the development of the computational gesture model. To ensure the achievement of realistic results, motion capture data obtained from real performers executing gestures for demonstration tasks within real scenarios will be employed. The proposed framework will also account for gestures captured interactively from a low-cost wearable set of motion sensors, enabling the interactive customization of gestures needed for programming interactive virtual human demonstrators for a broad range of applications. &lt;br/&gt;&lt;br/&gt;This project will significantly advance research on gesture modeling with two key contributions: (1) a novel computational model which integrates blending of realistic full-body gestures from motion capture with motion modification techniques for achieving precise arbitrary placement of the hands at the gesture stroke time, and (2) a new user interface for gesture modeling based on direct demonstrations which will truly enable a seamless human-centered user interface for programming autonomous characters. The approach constitutes a substantial step toward achieving autonomous virtual assistants which can meaningfully and effectively demonstrate tasks and procedures. The interactive interface component of this project has the transformative potential to enable gesture programming to become accessible to the non-specialized user, and therefore to enable virtual humans to become widely employed as a powerful communication medium. &lt;br/&gt;&lt;br/&gt;This project has the potential to impact the basic and broad research problem of modeling human movement and cognition, which is a central topic in information technology. This project will in addition benefit other researchers by producing a unique type of demonstrative gestures database which will be made available from a public project webpage. It will also provide unique educational opportunities for students and contribute to interdisciplinary educational programs based on new courses being developed or improved around the topics of the research.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>07/13/2009</MinAmdLetterDate>
<MaxAmdLetterDate>07/13/2009</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0915665</AwardID>
<Investigator>
<FirstName>Teenie</FirstName>
<LastName>Matlock</LastName>
<EmailAddress>tmatlock@ucmerced.edu</EmailAddress>
<StartDate>07/13/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Marcelo</FirstName>
<LastName>Kallmann</LastName>
<EmailAddress>mkallmann@ucmerced.edu</EmailAddress>
<StartDate>07/13/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California - Merced</Name>
<CityName>Merced</CityName>
<ZipCode>953435001</ZipCode>
<PhoneNumber>2092012039</PhoneNumber>
<StreetAddress>5200 North Lake Road</StreetAddress>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
</Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9229</Code>
<Text>RES IN UNDERGRAD INST-RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
