<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SoCS:  An International Social Network for Early Childhood Education</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>749998.00</AwardTotalIntnAmount>
<AwardAmount>749998</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The NSF-funded research project led by Javier Movellan at the University of California, San Diego will develop a social network (?RubiNet?) for early childhood education. The network will help integrate the activities of children, teachers, parents, and researchers with the goal of improving early childhood education at a national and international level.  RubiNet will use low-cost sociable robots to connect children and teachers in different classrooms, from different socioeconomic backgrounds, cultures, and ethnicities. Some of the classrooms will be in different countries.  Movellan and his colleagues will investigate the potential of RubiNet as a tool for rapid design and evaluation of low cost early intervention programs.  The potential for RubiNet to improve academic skills in young children will be assessed.  The researchers will also investigate whether this form of interaction engenders more familiarity and positivity in children's views of those who are different from themselves.&lt;br/&gt; &lt;br/&gt;An unprecedented number of children in the US start public school with major deficits in basic skills, including social skills, language, and mathematics.  Children that experience early failure in school are more likely to later become inattentive, disruptive, or disengaged.  These students tend to drop out of school early, and are more likely to depend on public assistance programs for survival. Empirical research suggests that the key to avoiding this vicious cycle is to intervene during the pre-kindergarten years instead of waiting until failures occur in kindergarten or later. Unfortunately, early intervention programs are typically very costly. Thus, another key goal of RubiNet is to significantly lower the cost of intervention in early education.  In addition, RubiNet will be of great potential benefit to researchers who study social interaction in young children.</AbstractNarration>
<MinAmdLetterDate>09/15/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/15/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0968573</AwardID>
<Investigator>
<FirstName>Javier</FirstName>
<LastName>Movellan</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Javier R Movellan</PI_FULL_NAME>
<EmailAddress>movellan@mplab.ucsd.edu</EmailAddress>
<PI_PHON>8588225241</PI_PHON>
<NSF_ID>000451488</NSF_ID>
<StartDate>09/15/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Marian</FirstName>
<LastName>Bartlett</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Marian S Bartlett</PI_FULL_NAME>
<EmailAddress>mbartlett@ucsd.edu</EmailAddress>
<PI_PHON>8588225241</PI_PHON>
<NSF_ID>000091292</NSF_ID>
<StartDate>09/15/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Morana</FirstName>
<LastName>Alac</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Morana Alac</PI_FULL_NAME>
<EmailAddress>alac@ucsd.edu</EmailAddress>
<PI_PHON>8585347229</PI_PHON>
<NSF_ID>000071346</NSF_ID>
<StartDate>09/15/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[Office of Contract &amp; Grant A]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7953</Code>
<Text>SOCIAL-COMPUTATIONAL SYSTEMS</Text>
</ProgramElement>
<ProgramReference>
<Code>7752</Code>
<Text>CDI NON SOLICITED RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>7953</Code>
<Text>SOCIAL-COMPUTATIONAL SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~749998</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-e657f49a-0fb6-981c-ab95-657e7e3f583a"> <span id="docs-internal-guid-e657f49a-0fba-1959-63bd-543570c83634"> </span></span></p> <p dir="ltr"><span>This award covered the most significant period in the life of the RUBI Project. &nbsp;Two additional RUBI prototypes, RUBI-5 and RUBI-6 were designed, built and deployed during the period covered by the award. With each prototype significant developments were made in adding </span><span>perceptual primitives</span><span>, interactive capabilities, and, most significantly, helped establish RUBI as not only an education delivery platform and an Experiment Runner (ER), but also as a full blown research platform. The current RUBI not only records user activity and their emotions (</span><span>Facial Expression Recognition</span><span>) but also identifies and monitors individuals (</span><span>Person Recognition</span><span>) and rudimentary association dynamics, therefore better positioned to capture their socio-emotional development. &nbsp;Current RUBI also recognizes a large set of physical toys (</span><span>Active Object Recognition</span><span>) &nbsp;in a give-and-take game, that complements the vocabulary building and object naming games presented digitally on her &lsquo;belly screen&rsquo;. &nbsp;These capabilities will allow RUBI to participate in critical developmental research into language acquisition and other cognitive skills (joint attention, gaze following, object naming) and potentially provide critical diagnostics and intervention in atypical development such as autism spectrum.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>RUBI is finally coming of age. The vision of </span><span>an international social network</span><span> is now much closer to fruition. We have established two modes of dispersal for RUBI prototypes under the umbrella of </span><span>RUBI-PAL</span><span> (</span><span>Perception, Action and Learning</span><span>): (1)</span><span> RUBI Prototype-on-Loan</span><span>. A research lab that is interested in a RUBI prototype to deploy in a study as an intervention, etc., is required to send a lab member (graduate student or postdoc) to Machine Perception Lab for 2-3 week training on all aspects of usage and repair. The host lab can pursue independent research with RUBI and/or collaborate with MPLab researchers. A case in point, RUBI-5 is currently in Patricia Khul&rsquo;s laboratory in Seattle. (2) </span><span>Parallel RUBI-PAL pipeline.</span><span> Here we form &nbsp;an open-ended collaboration with a research group that is interested in the RUBI-PAL framework as a base for development of additional RUBI prototypes with similar or complementary areas of focus in one or more of the PAL (Perception, Action, Learning) factors. July 2015 saw the kick off of RUBI-Oz in the University of Queensland in Australia, and since August 2015 preliminary steps are unfolding towards a RUBI-Is in Israel&rsquo;s ABC Robotics Center in Ben Gurion University. </span></p> <p>&nbsp;</p> <p dir="ltr"><span>Achieving this level of competence and dispersal depended on critical phases of development during the research cycle covered in this award.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>RUBI-5: Person Identification / recognition</span></p> <p dir="ltr"><span>Automatic person identification helps tracking the performance of individual kids during training to deliver customized contents for education. With this goal in mind, we developed an automatic face recognition system for RUBI-5 to recognize the toddlers and adults from single images. During an interaction, RUBI continuously captures images and feeds them to the recognition pipeline. The pipeline extracts visual features from scaled versions of the input and classifies the features. Using this system, RUBI analyzed the data from home-alone experiment and extracted useful information about the social preference of toddlers while playing games in front of RUBI. </span></p> <p>&nbsp;</p> <p dir="ltr"><span>The lesson learned from RUBI-4: research cycle too slow - by the time finished analyzing video data the kids were nearly 4 years old. &nbsp;How to make the research relevant to the classroom? Person recognition allowed us to conduct Home Alone study - (Malmir et al, 2013) and show its relevance as a monitoring tool for socio-emotional development (Movellan et al., 2014)</span></p> <p>&nbsp;</p> <p dir="ltr"><span>RUBI-6: Active Object Recognition</span></p> <p><span>In order to compare the effects of on-screen images with actual objects in object naming, we developed an active object recognition system for RUBI-6 based on deep convolutional neural networks. Being able to interact with the objects, RUBI can examine objects from different views to improve its performance. This is using a neural network that predicts the label for the current object in the image. The network also predicts an action on the object that is expected to improve the network&rsquo;s label prediction performance.</span></p> <p dir="ltr">&nbsp;</p> <div><span><br /></span></div> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/04/2016<br>      Modified by: Javier&nbsp;R&nbsp;Movellan</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/0968573/0968573_10041828_1451964196053_Figure_RUBI_AutonomyComplexityChart--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/0968573/0968573_10041828_1451964196053_Figure_RUBI_AutonomyComplexityChart--rgov-800width.jpg" title="RUBI series - Autonomy vs Complexity"><img src="/por/images/Reports/POR/2016/0968573/0968573_10041828_1451964196053_Figure_RUBI_AutonomyComplexityChart--rgov-66x44.jpg" alt="RUBI series - Autonomy vs Complexity"></a> <div class="imageCaptionContainer"> <div class="imageCaption">the chart shows the trade off between autonomy and complexity across the RUBI series</div> <div class="imageCredit">Javier Movellan</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Javier&nbsp;R&nbsp;Movellan</div> <div class="imageTitle">RUBI series - Autonomy vs Complexity</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/0968573/0968573_10041828_1451964416125_Figure_RUBIgramNormalized--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/0968573/0968573_10041828_1451964416125_Figure_RUBIgramNormalized--rgov-800width.jpg" title="RUBIgram - pairwise associations"><img src="/por/images/Reports/POR/2016/0968573/0968573_10041828_1451964416125_Figure_RUBIgramNormalized--rgov-66x44.jpg" alt="RUBIgram - pairwise associations"></a> <div class="imageCaptionContainer"> <div class="imageCaption">RUBI can document pairwise associations of users, even after accounting for chance proximity.</div> <div class="imageCredit">Mohsen Malmir</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Javier&nbsp;R&nbsp;Movellan</div> <div class="imageTitle">RUBIgram - pairwise associations</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/0968573/0968573_10041828_1451964499894_Figure_PersonRecognitionPipeline--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/0968573/0968573_10041828_1451964499894_Figure_PersonRecognitionPipeline--rgov-800width.jpg" title="Person Recognition Pipeline"><img src="/por/images/Reports/POR/2016/0968573/0968573_10041828_1451964499894_Figure_PersonRecognitionPipeline--rgov-66x44.jpg" alt="Person Recognition Pipeline"></a> <div class="imageCaptionContainer"> <div class="imageCaption">RUBI-5 person recognition</div> <div class="imageCredit">Mohsen Malmir</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Javier&nbsp;R&nbsp;Movellan</div> <div class="imageTitle">Person Recognition Pipeline</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/0968573/0968573_10041828_1451964613186_Figure_AOR_DeepArchitecture--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/0968573/0968573_10041828_1451964613186_Figure_AOR_DeepArchitecture--rgov-800width.jpg" title="Deep Architecture of Active Object Recognition with RUBI-6"><img src="/por/images/Reports/POR/2016/0968573/0968573_10041828_1451964613186_Figure_AOR_DeepArchitecture--rgov-66x44.jpg" alt="Deep Architecture of Active Object Recognition with RUBI-6"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The deep architecture proposed for active object recognition with RUBI-6</div> <div class="imageCredit">Mohsen Malmir</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Javier&nbsp;R&nbsp;Movellan</div> <div class="imageTitle">Deep Architecture of Active Object Recognition with RUBI-6</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    This award covered the most significant period in the life of the RUBI Project.  Two additional RUBI prototypes, RUBI-5 and RUBI-6 were designed, built and deployed during the period covered by the award. With each prototype significant developments were made in adding perceptual primitives, interactive capabilities, and, most significantly, helped establish RUBI as not only an education delivery platform and an Experiment Runner (ER), but also as a full blown research platform. The current RUBI not only records user activity and their emotions (Facial Expression Recognition) but also identifies and monitors individuals (Person Recognition) and rudimentary association dynamics, therefore better positioned to capture their socio-emotional development.  Current RUBI also recognizes a large set of physical toys (Active Object Recognition)  in a give-and-take game, that complements the vocabulary building and object naming games presented digitally on her ?belly screen?.  These capabilities will allow RUBI to participate in critical developmental research into language acquisition and other cognitive skills (joint attention, gaze following, object naming) and potentially provide critical diagnostics and intervention in atypical development such as autism spectrum.    RUBI is finally coming of age. The vision of an international social network is now much closer to fruition. We have established two modes of dispersal for RUBI prototypes under the umbrella of RUBI-PAL (Perception, Action and Learning): (1) RUBI Prototype-on-Loan. A research lab that is interested in a RUBI prototype to deploy in a study as an intervention, etc., is required to send a lab member (graduate student or postdoc) to Machine Perception Lab for 2-3 week training on all aspects of usage and repair. The host lab can pursue independent research with RUBI and/or collaborate with MPLab researchers. A case in point, RUBI-5 is currently in Patricia Khul?s laboratory in Seattle. (2) Parallel RUBI-PAL pipeline. Here we form  an open-ended collaboration with a research group that is interested in the RUBI-PAL framework as a base for development of additional RUBI prototypes with similar or complementary areas of focus in one or more of the PAL (Perception, Action, Learning) factors. July 2015 saw the kick off of RUBI-Oz in the University of Queensland in Australia, and since August 2015 preliminary steps are unfolding towards a RUBI-Is in Israel?s ABC Robotics Center in Ben Gurion University.     Achieving this level of competence and dispersal depended on critical phases of development during the research cycle covered in this award.    RUBI-5: Person Identification / recognition Automatic person identification helps tracking the performance of individual kids during training to deliver customized contents for education. With this goal in mind, we developed an automatic face recognition system for RUBI-5 to recognize the toddlers and adults from single images. During an interaction, RUBI continuously captures images and feeds them to the recognition pipeline. The pipeline extracts visual features from scaled versions of the input and classifies the features. Using this system, RUBI analyzed the data from home-alone experiment and extracted useful information about the social preference of toddlers while playing games in front of RUBI.     The lesson learned from RUBI-4: research cycle too slow - by the time finished analyzing video data the kids were nearly 4 years old.  How to make the research relevant to the classroom? Person recognition allowed us to conduct Home Alone study - (Malmir et al, 2013) and show its relevance as a monitoring tool for socio-emotional development (Movellan et al., 2014)    RUBI-6: Active Object Recognition  In order to compare the effects of on-screen images with actual objects in object naming, we developed an active object recognition system for RUBI-6 based on deep convolutional neural networks. Being able to interact with the objects, RUBI can examine objects from different views to improve its performance. This is using a neural network that predicts the label for the current object in the image. The network also predicts an action on the object that is expected to improve the network?s label prediction performance.                    Last Modified: 01/04/2016       Submitted by: Javier R Movellan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
