<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small: Eye Movement in Stereoscopic Displays, Implications for Visualization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2009</AwardEffectiveDate>
<AwardExpirationDate>06/30/2013</AwardExpirationDate>
<AwardTotalIntnAmount>498563.00</AwardTotalIntnAmount>
<AwardAmount>587448</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In this project the PI will investigate human eye movement within stereoscopic displays, with the goal of improving the perceptual effectiveness of these displays for visualization.  While numerous studies have looked at eye motion for conventional displays, there has been very little work on stereo displays.  The challenge and the opportunity for eye tracking in stereoscopic displays is that both eyes can be tracked, yielding data on gaze position on the screen and eye vergence, which affords an estimate of where the subject is fixating in three dimensional space (not just in screen space).  Such data would shed light on the strategies users employ to explore and interpret information in these displays, which in turn may enable display designers to increase the effectiveness of their displays in transmitting information.  This research will include technology development, perceptual experimentation, and visualization system development phases.  The PI will initially devise an experimental methodology for binocular eye tracking devices, develop protocols and software, and run preliminary experiments to collect a baseline set of stereo tracking data which establish what tracking precision is possible and provide a basic understanding of how humans perform eye movements on stereo displays.  Later, the focus will be on experiments exploring issues in the design of perceptually optimal visualizations on stereoscopic displays; eye tracking will illuminate how users scan such displays, providing insight into the visual strategies used to navigate, interpret, and extract information.  Finally, the PI will employ what he has learned to design, construct, and evaluate an eye-tracking enhanced stereo 3D visualization system.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  The potential impacts of this project are as broad as the possible application of stereoscopic displays.  For example, the visualization of geographic data often requires the display of layered surfaces; studies have shown that when using a stereoscopic display such visualizations can be designed so that the user's perception of the shapes of two overlapping surface layers is well preserved, but we lack empirical stereo eye tracking data at present to inform and guide this design.   In the medical domain, stereoscopic displays have been shown to significantly improve endoscopic performance, in terms of laparoscopic precision, of both novice and experienced surgeons, but surgeons generally need to maintain a fixed viewing position in order to maintain stereoscopic integrity, a problem which could be alleviated by measuring positional and vergence eye movements and using them to dynamically adjust the location of the autostereoscopic "sweet spot."  This work will contribute to building a perceptual science underlying stereoscopic display technology, which will be critical to its continuing perfection and application in domains such as science, education and training, healthcare, industry, and consumer products.</AbstractNarration>
<MinAmdLetterDate>07/13/2009</MinAmdLetterDate>
<MaxAmdLetterDate>08/08/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0915085</AwardID>
<Investigator>
<FirstName>Donald</FirstName>
<LastName>House</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Donald H House</PI_FULL_NAME>
<EmailAddress>dhouse@cs.clemson.edu</EmailAddress>
<PI_PHON>8646562844</PI_PHON>
<NSF_ID>000165469</NSF_ID>
<StartDate>07/13/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Duchowski</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew T Duchowski</PI_FULL_NAME>
<EmailAddress>duchowski@clemson.edu</EmailAddress>
<PI_PHON>8646567677</PI_PHON>
<NSF_ID>000492350</NSF_ID>
<StartDate>07/13/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Clemson University</Name>
<CityName>CLEMSON</CityName>
<ZipCode>296345701</ZipCode>
<PhoneNumber>8646562424</PhoneNumber>
<StreetAddress>230 Kappa Street</StreetAddress>
<StreetAddress2><![CDATA[Suite 200]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<StateCode>SC</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>SC03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042629816</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CLEMSON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042629816</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Clemson University]]></Name>
<CityName>CLEMSON</CityName>
<StateCode>SC</StateCode>
<ZipCode>296345701</ZipCode>
<StreetAddress><![CDATA[230 Kappa Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>SC03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~211173</FUND_OBLG>
<FUND_OBLG>2010~287390</FUND_OBLG>
<FUND_OBLG>2011~9044</FUND_OBLG>
<FUND_OBLG>2012~79841</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>There is almost no information in the scientific literature about eye movements when viewing stereoscopic displays, and no fundamental understanding of how to design visualizations for such displays to optimize their information carrying capacity. &nbsp;We do know that viewing stereoscopic displays can be uncomfortable, especially at close viewing distances. &nbsp;Our project was concerned with measuring how the eyes moved in response to stereoscopic (3D) images and whether we could reliably measure this vergence response with eye-tracking equipment. &nbsp;Our ultimate objective was to design computer graphics means of stereoscopic display that would alleviate viewing discomfort.</p> <p>Although we encountered technical difficulties with eye-tracking equipment early on, we we were able to overcome these and establish that we could in fact estimate the 3D depth of what a viewer was looking at from the difference of the left and right gaze points as measured on the 2D screen by the eye tracker. &nbsp;This measurement was observed to be quite noisy and required application of a suitable filter to smooth out. &nbsp;More importantly, however, we noticed that vergence was biased toward the depth of the screen plane. &nbsp;That is, the depth of objects presented ahead of the screen would be overestimated while the depth of objects presented behind the screen would be underestimated.</p> <p>Our measurements of vergence error point to a bias of the human visual system toward the screen which sits at a fixed depth relative to the viewer. &nbsp;Hence, the error observed could be due to the brain's estimation of accommodative (focus) distance. &nbsp;Our objective measurement could be evidence of the so-called accommodation-vergence conflict thought to be responsible for visual discomfort.</p> <p>In a nutshell, when looking at a stereo (3D) object that pops out of (or into) the screen, the human visual system expects to match the object's focal distance to where it appears in &nbsp;depth. &nbsp;When projected on a desktop display, however, the object is shown on a display that is at a fixed distance to the viewer producing conflict between what is seen and what is expected.</p> <p>In an effort to produce a better match between stereo depth and focal distance, we are exploring the effect of a computer graphics technique which simulates the depth-of-field effect familiar to photographers. &nbsp;Something in focus close up will render something far away out of focus. &nbsp;Matching the depth-of-field focusing effect in real-time to the point of measured vergence, we believe may alleviate the accommodation-vergence conflict associated with stereo (3D) displays.</p> <p>With recently acquired state-of-the-art custom-built equipment we are also just now beginning to measure whether similar objective vergence errors occur in the real (physical) space. &nbsp;Initial indications suggest that they do not. &nbsp;However, further research is required to determine whether this is consistent for larger populations or whether any individual differences exist, and if so, what those may be.</p><br> <p>            Last Modified: 08/07/2013<br>      Modified by: Andrew&nbsp;T&nbsp;Duchowski</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2013/0915085/0915085_10028426_1375894666182_eye-charts--rgov-214x142.jpg" original="/por/images/Reports/POR/2013/0915085/0915085_10028426_1375894666182_eye-charts--rgov-800width...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ There is almost no information in the scientific literature about eye movements when viewing stereoscopic displays, and no fundamental understanding of how to design visualizations for such displays to optimize their information carrying capacity.  We do know that viewing stereoscopic displays can be uncomfortable, especially at close viewing distances.  Our project was concerned with measuring how the eyes moved in response to stereoscopic (3D) images and whether we could reliably measure this vergence response with eye-tracking equipment.  Our ultimate objective was to design computer graphics means of stereoscopic display that would alleviate viewing discomfort.  Although we encountered technical difficulties with eye-tracking equipment early on, we we were able to overcome these and establish that we could in fact estimate the 3D depth of what a viewer was looking at from the difference of the left and right gaze points as measured on the 2D screen by the eye tracker.  This measurement was observed to be quite noisy and required application of a suitable filter to smooth out.  More importantly, however, we noticed that vergence was biased toward the depth of the screen plane.  That is, the depth of objects presented ahead of the screen would be overestimated while the depth of objects presented behind the screen would be underestimated.  Our measurements of vergence error point to a bias of the human visual system toward the screen which sits at a fixed depth relative to the viewer.  Hence, the error observed could be due to the brain's estimation of accommodative (focus) distance.  Our objective measurement could be evidence of the so-called accommodation-vergence conflict thought to be responsible for visual discomfort.  In a nutshell, when looking at a stereo (3D) object that pops out of (or into) the screen, the human visual system expects to match the object's focal distance to where it appears in  depth.  When projected on a desktop display, however, the object is shown on a display that is at a fixed distance to the viewer producing conflict between what is seen and what is expected.  In an effort to produce a better match between stereo depth and focal distance, we are exploring the effect of a computer graphics technique which simulates the depth-of-field effect familiar to photographers.  Something in focus close up will render something far away out of focus.  Matching the depth-of-field focusing effect in real-time to the point of measured vergence, we believe may alleviate the accommodation-vergence conflict associated with stereo (3D) displays.  With recently acquired state-of-the-art custom-built equipment we are also just now beginning to measure whether similar objective vergence errors occur in the real (physical) space.  Initial indications suggest that they do not.  However, further research is required to determine whether this is consistent for larger populations or whether any individual differences exist, and if so, what those may be.       Last Modified: 08/07/2013       Submitted by: Andrew T Duchowski]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
