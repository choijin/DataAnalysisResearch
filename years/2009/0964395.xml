<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Medium: Scale, Isolation, and Performance in Data Center Networks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2010</AwardEffectiveDate>
<AwardExpirationDate>02/28/2015</AwardExpirationDate>
<AwardTotalIntnAmount>750000.00</AwardTotalIntnAmount>
<AwardAmount>750000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Modern data centers host operations as varied as flight planning, drug discovery, and Internet search running on thousands of machines. These services are often limited by the speed of the underlying network to coordinate parallel data access.  In current data centers, network I/O remains a primary bottleneck and a significant fraction of capital expenditure ($10B/year).  Compounding the problem are operational issues caused by interference between services, down times due to failures, and violations of performance requirements.&lt;br/&gt;This project will develop a hardware/software architecture with the following capabilities: i) non-blocking bandwidth to hundreds of thousands of hosts; ii) ``slicing'' across services with minimum bandwidth guarantees; iii) detecting fine-grained performance violations; iv) tolerating a range of failure scenarios; v) supporting end host virtualization and migration.  Our goal is to enable modular deployment and management of networking infrastructure to keep pace with the burgeoning computation and storage explosion in data centers. This work will result in a prototype fully functional virtualizable data center network fabric to support higher-level services.&lt;br/&gt;Broader impacts include: i) outreach to under-represented minorities through the UCSD COSMOS program; ii) a public release of the data center communication workloads, protocols, and algorithms we develop; iii) working with our industrial partners and advisory board to address key performance and reliability issues in a critical portion of the national computation infrastructure.  A significant outcome will be students trained in data center networking and cloud computing.</AbstractNarration>
<MinAmdLetterDate>03/11/2010</MinAmdLetterDate>
<MaxAmdLetterDate>06/06/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0964395</AwardID>
<Investigator>
<FirstName>George</FirstName>
<LastName>Varghese</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>George Varghese</PI_FULL_NAME>
<EmailAddress>varghese@cs.ucla.edu</EmailAddress>
<PI_PHON>3108257649</PI_PHON>
<NSF_ID>000250977</NSF_ID>
<StartDate>03/11/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Amin</FirstName>
<LastName>Vahdat</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amin M Vahdat</PI_FULL_NAME>
<EmailAddress>vahdat@cs.ucsd.edu</EmailAddress>
<PI_PHON>8585344614</PI_PHON>
<NSF_ID>000438703</NSF_ID>
<StartDate>03/11/2010</StartDate>
<EndDate>06/06/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>George</FirstName>
<LastName>Porter</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>George Porter</PI_FULL_NAME>
<EmailAddress>gmporter@cs.ucsd.edu</EmailAddress>
<PI_PHON>8588226818</PI_PHON>
<NSF_ID>000553493</NSF_ID>
<StartDate>06/06/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[Office of Contract &amp; Grant A]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7354</Code>
<Text>COMPUTER SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~250000</FUND_OBLG>
<FUND_OBLG>2011~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1">The major goal of this project is to improve the scale, efficiency (both in terms of cost and energy), and availability of data center computations.&nbsp; Data centers are the engines of the Internet, with the vast majority of computation and communication taking place in a relatively small number of very dense data centers spread across the planet.</p> <p class="p1">This research addresses a number of critical questions in efficient data center environments:</p> <ul> <li>structuring applications for balance, such that an appropriate amount of CPU, memory, storage, and networking are allocated for individual applications, ensuring high resource utilization and eliminating the need for allocation of otherwise idle resources that contribute to energy and cost inefficiency.</li> <li>designing next-generation low-latency circuit switches capable of delivering higher-levels of performance for substantially less energy and cost than corresponding packet switches.&nbsp; While there are substantial architectural shifts required to take advantage of circuit switching technology, they also present a compelling opportunity for next-generation data center design.</li> </ul> <p class="p1">In the first area, primarily led by graduate students Rasmussen and Conley, we have designed and built Themis, which is an IO-efficient MapReduce implementation.&nbsp; Many MapReduce jobs are I/O-bound, and an efficient MapReduce system must aim to minimize the number of I/O operations it performs.&nbsp; Fundamentally, every MapReduce system must perform at least two I/O operations per record when the amount of data exceeds the amount of memory in the cluster.&nbsp; We refer to a system that meets this lower-bound as having the "2-IO" property, and Themis is (to our knowledge) the first such system.&nbsp; We have specifically tailored Themis for public cloud environments such as Amazon Web Services and Google Compute Engine.&nbsp; Using these platforms, we have achieved three world records in highly efficient data sorting, according to the Jim Gray sorting competition (http://www.sortbenchmark.org).</p> <p class="p1">In the second area, we are working towards a fundamental redesign of data center networks based on circuit-switching, instead of more traditional packet-switching.&nbsp; A key problem in developing such networks is integrating server endhosts.&nbsp; As part of this project, we have designed endhosts capable of supporting the requirements of reconfigurable data center network proposals.&nbsp; Graduate student Tewari has been leading our efforts at building an endhost network stack which is backwards compatible with existing applications, yet able to support the fine-grained (i.e., at microsecond timescales) packet transmission requirements of recent networking proposals for the data center (e.g., Mordia[Sigcomm13] and REACToR[NSDI14]).&nbsp; Such proposals rely on either RF or optical circuits, traversing either free-space (in the case of RF) or optical fibers and circuit switches (in the case of optics).&nbsp; A key limiting factor to deploying these solutions has been the inability of endhosts to synchronize their network transmissions with the needs of the network.</p> <p class="p1">A key focus of this grant has been on graduate student training and mentorship.&nbsp; The main research in this proposal has been principally carried out by graduate students. In particular, this is a systems building effort and the students have received invaluable opportunities to build distributed systems at a scale that is not even typically attempted in industry. All of the students who have participated in the effort have received experience on both cutting-edge software and hardware platforms, but also on public cloud platforms.</p> <p class="p1">The results of this grant have been published in a number of high profile publications and through external talks, and the TritonSort/Them...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[The major goal of this project is to improve the scale, efficiency (both in terms of cost and energy), and availability of data center computations.  Data centers are the engines of the Internet, with the vast majority of computation and communication taking place in a relatively small number of very dense data centers spread across the planet. This research addresses a number of critical questions in efficient data center environments:  structuring applications for balance, such that an appropriate amount of CPU, memory, storage, and networking are allocated for individual applications, ensuring high resource utilization and eliminating the need for allocation of otherwise idle resources that contribute to energy and cost inefficiency. designing next-generation low-latency circuit switches capable of delivering higher-levels of performance for substantially less energy and cost than corresponding packet switches.  While there are substantial architectural shifts required to take advantage of circuit switching technology, they also present a compelling opportunity for next-generation data center design.  In the first area, primarily led by graduate students Rasmussen and Conley, we have designed and built Themis, which is an IO-efficient MapReduce implementation.  Many MapReduce jobs are I/O-bound, and an efficient MapReduce system must aim to minimize the number of I/O operations it performs.  Fundamentally, every MapReduce system must perform at least two I/O operations per record when the amount of data exceeds the amount of memory in the cluster.  We refer to a system that meets this lower-bound as having the "2-IO" property, and Themis is (to our knowledge) the first such system.  We have specifically tailored Themis for public cloud environments such as Amazon Web Services and Google Compute Engine.  Using these platforms, we have achieved three world records in highly efficient data sorting, according to the Jim Gray sorting competition (http://www.sortbenchmark.org). In the second area, we are working towards a fundamental redesign of data center networks based on circuit-switching, instead of more traditional packet-switching.  A key problem in developing such networks is integrating server endhosts.  As part of this project, we have designed endhosts capable of supporting the requirements of reconfigurable data center network proposals.  Graduate student Tewari has been leading our efforts at building an endhost network stack which is backwards compatible with existing applications, yet able to support the fine-grained (i.e., at microsecond timescales) packet transmission requirements of recent networking proposals for the data center (e.g., Mordia[Sigcomm13] and REACToR[NSDI14]).  Such proposals rely on either RF or optical circuits, traversing either free-space (in the case of RF) or optical fibers and circuit switches (in the case of optics).  A key limiting factor to deploying these solutions has been the inability of endhosts to synchronize their network transmissions with the needs of the network. A key focus of this grant has been on graduate student training and mentorship.  The main research in this proposal has been principally carried out by graduate students. In particular, this is a systems building effort and the students have received invaluable opportunities to build distributed systems at a scale that is not even typically attempted in industry. All of the students who have participated in the effort have received experience on both cutting-edge software and hardware platforms, but also on public cloud platforms. The results of this grant have been published in a number of high profile publications and through external talks, and the TritonSort/Themis software is being released to the public in source code form according to the guidelines of the Regents of the University of California.       Last Modified: 06/19/2015       Submitted by: George Porter]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
