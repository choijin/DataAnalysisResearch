<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: SMALL: Category-Driven Affordance Prediction For Autonomous Robots</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2009</AwardEffectiveDate>
<AwardExpirationDate>02/28/2014</AwardExpirationDate>
<AwardTotalIntnAmount>449063.00</AwardTotalIntnAmount>
<AwardAmount>449063</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>jeffrey trinkle</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This research program is developing theory and algorithms that will enable a robot to learn through training and experimentation how to predict object and environmental affordances from sensor data.  These affordances determine which actions a robot may perform when interacting with a given object, and thus define the capabilities of the robot at any given time.  For example, a doorway affords the possibility to leave one room and enter another, and a handle attached to an object affords the ability to grasp it.  The approach being developed leverages a graphical model approach to learn visual categories ? to learn the world contains entities such as doors and handles ? that provide a powerful intermediate representation for affordance prediction and learning.    This is in contrast to the classical direct perception approach in which the agent learns a direct mapping from image features to affordances.    The models and theory are being validated on two robot platforms and tasks: an outdoor mobile robot performing navigation and pursuit/evasion tasks, and an indoor robot manipulator performing assembly/disassembly tasks.&lt;br/&gt;&lt;br/&gt;The importance and broader impact of this research lies in empowering robots to actively and effectively learn about its environment given little human training.  Because pre-programmed sensing capabilities are typically brittle ? not accounting for the variability of the world in which the robot is actually operating ? and because extensive human training and supervision is too labor intensive, such learning paradigms are essential for the development of robots that operate effectively in the human world. &lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>07/10/2009</MinAmdLetterDate>
<MaxAmdLetterDate>07/29/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0916687</AwardID>
<Investigator>
<FirstName>Aaron</FirstName>
<LastName>Bobick</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Aaron F Bobick</PI_FULL_NAME>
<EmailAddress>afb@wustl.edu</EmailAddress>
<PI_PHON>3149356350</PI_PHON>
<NSF_ID>000122613</NSF_ID>
<StartDate>07/10/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>James</FirstName>
<LastName>Rehg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James Rehg</PI_FULL_NAME>
<EmailAddress>rehg@cc.gatech.edu</EmailAddress>
<PI_PHON>4048949105</PI_PHON>
<NSF_ID>000257071</NSF_ID>
<StartDate>07/10/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 NORTH AVE NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~290138</FUND_OBLG>
<FUND_OBLG>2011~158925</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The outcomes of this work fall broadly into two areas.&nbsp; The first is in developing the ability for a computer vision system to understand <em>egocentric </em>imagery - imagery that is take from the point of view of a human agent.&nbsp;&nbsp; The second is in allowing a robot to understand and leverage the <em>affordances </em>of an object - the affordances being the types of actions that can be applied to an object and the resulting out comes form the application of those actions.</p> <p>In the egocentric work, we first developed a novel segmentation method based upon temporally consistent boundary partitioning.&nbsp; This work was essential to generate primitive regions that can be considered as objects relevant to the human activity.&nbsp; This method&nbsp; can be used in both n interactive mode and a purely automatic method; in the latter case we use the system for producing&nbsp; candidate regions for egocentric activity analysis. &nbsp; There we learn a hierarchical model of an activity by exploiting the  consistent appearance of objects, hands, and actions that results from  the egocentric context.</p> <p>The other main focus of the work was the learning of object affordance by a robot.&nbsp; One key development is an architecture that decouples the behavior used to attempt to achieve a particular manipulation goal, the real time controller user to control the behavior primitive, and the perceptual primitive used to encode the state of the object during behavior execution.&nbsp;&nbsp; Through systematic experimentation the robot can learn how to best manipulate a set of objects.</p> <p>The second significant result of this thrust is knowledge transfer between objects.&nbsp; The robot uses a shape description that captures both the global structure of an object and the local context of where it is applying the action.&nbsp; By manipulating a variety of objects, the robot learns a mapping from shape description to effective manipulation strategy.&nbsp; This allows the robot to apply information learned from manipulating a given set of objects to a novel object.&nbsp; This type of learned dynamics can also be applied to a model predictive control strategy though our results there are only preliminary.&nbsp;</p> <p>future work will leverage learned affordance models in a planning framework that permits the robot to plan how to achieve and overall task.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/01/2014<br>      Modified by: Aaron&nbsp;F&nbsp;Bobick</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2014/0916687/0916687_10115117_1401665893747_large_brush_goal_vector_170_7--rgov-214x142.jpg" original="/por/images/Reports/POR/2014/0916687/0916687_10115117_1401665893747_large_brush_goal_vector_170_7--rgov-800width.jpg" title="Leraning object affordances"><img src="/por/images/Reports/POR/2014/0916687/0916687_10115117_1401665893747_large_brush_goal_vector_170_7--rgov-66x44.jpg" alt="Leraning object affordances"></a> <div class="imageCaptionContainer"> <div class="imageCaption">An example of the robot pushing an object based upon the learned behavior of the object with respect to different robot actions. The robot can leverage this learned knowledge to plan how to manipulate the objects in its world.</div> <div class="imageCredit">Tucker Hermans and Aaron Bobick</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Aaron&nbsp;F&nbsp;...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The outcomes of this work fall broadly into two areas.  The first is in developing the ability for a computer vision system to understand egocentric imagery - imagery that is take from the point of view of a human agent.   The second is in allowing a robot to understand and leverage the affordances of an object - the affordances being the types of actions that can be applied to an object and the resulting out comes form the application of those actions.  In the egocentric work, we first developed a novel segmentation method based upon temporally consistent boundary partitioning.  This work was essential to generate primitive regions that can be considered as objects relevant to the human activity.  This method  can be used in both n interactive mode and a purely automatic method; in the latter case we use the system for producing  candidate regions for egocentric activity analysis.   There we learn a hierarchical model of an activity by exploiting the  consistent appearance of objects, hands, and actions that results from  the egocentric context.  The other main focus of the work was the learning of object affordance by a robot.  One key development is an architecture that decouples the behavior used to attempt to achieve a particular manipulation goal, the real time controller user to control the behavior primitive, and the perceptual primitive used to encode the state of the object during behavior execution.   Through systematic experimentation the robot can learn how to best manipulate a set of objects.  The second significant result of this thrust is knowledge transfer between objects.  The robot uses a shape description that captures both the global structure of an object and the local context of where it is applying the action.  By manipulating a variety of objects, the robot learns a mapping from shape description to effective manipulation strategy.  This allows the robot to apply information learned from manipulating a given set of objects to a novel object.  This type of learned dynamics can also be applied to a model predictive control strategy though our results there are only preliminary.   future work will leverage learned affordance models in a planning framework that permits the robot to plan how to achieve and overall task.           Last Modified: 06/01/2014       Submitted by: Aaron F Bobick]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
