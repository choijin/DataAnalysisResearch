<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>DC: Large: Collaborative Research: Mining a Million Scanned Books: Linguistic and Structure Analysis, Fast Expanded Search, and Improved OCR</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2009</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>2113456.00</AwardTotalIntnAmount>
<AwardAmount>2146756</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Center for Intelligent Information Retrieval at UMass Amherst, the Perseus Digital Library Project at Tufts, and the Internet Archive are investigating large-scale information extraction and retrieval technologies for digitized book collections. To provide effective analysis and search for scholars and the general public, and to handle the diversity and scale of these collections, this project focuses on improvements in seven interlocking technologies: improved OCR accuracy through word spotting, creating probabilistic models using joint distributions of features, and building topic-specific language models across documents; structural metadata extraction, to mine headers, chapters, tables of contents, and indices; linguistic analysis and information extraction, to perform syntactic analysis and entity extraction on noisy OCR output; inferred document relational structure, to mine citations, quotations, translations, and paraphrases; latent topic modeling  through time, to improve language modeling for OCR and retrieval, and to track the spread of ideas across periods and genres; query expansion for relevance models, to improve relevance in information retrieval by offline pre-processing of document comparisons; and interfaces for exploratory data analysis, to provide users of the document collection with efficient tools to update complex models of important entities, events, topics, and linguistic features. When applied across large corpora, these technologies reinforce each other: improved topic modeling enables more targeted language models for OCR; extracting structural metadata improves citation analysis; and entity extraction improves topic modeling and query expansion.The testbed for this project is the growing corpus of over one million open-access books from the Internet Archive.</AbstractNarration>
<MinAmdLetterDate>09/24/2009</MinAmdLetterDate>
<MaxAmdLetterDate>08/28/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0910884</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Allan</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>James M Allan</PI_FULL_NAME>
<EmailAddress>allan@cs.umass.edu</EmailAddress>
<PI_PHON>4135453240</PI_PHON>
<NSF_ID>000209290</NSF_ID>
<StartDate>09/24/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Raghavan</FirstName>
<LastName>Manmatha</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Raghavan Manmatha</PI_FULL_NAME>
<EmailAddress>manmatha@cs.umass.edu</EmailAddress>
<PI_PHON>4135453623</PI_PHON>
<NSF_ID>000473590</NSF_ID>
<StartDate>09/24/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Smith</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David A Smith</PI_FULL_NAME>
<EmailAddress>dasmith@ccs.neu.edu</EmailAddress>
<PI_PHON>6173732462</PI_PHON>
<NSF_ID>000510149</NSF_ID>
<StartDate>09/24/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName>Hadley</CityName>
<StateCode>MA</StateCode>
<ZipCode>010359450</ZipCode>
<StreetAddress><![CDATA[Research Administration Building]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramElement>
<Code>7793</Code>
<Text>DATA-INTENSIVE COMPUTING</Text>
</ProgramElement>
<ProgramReference>
<Code>7793</Code>
<Text>DATA-INTENSIVE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~518976</FUND_OBLG>
<FUND_OBLG>2010~704077</FUND_OBLG>
<FUND_OBLG>2011~824720</FUND_OBLG>
<FUND_OBLG>2012~98983</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project was a collaborative effort between the University of Massachusetts Amherst, Tufts University, and the Internet Archive. It developed and evaluated techniques for processing the large amounts of data contained in massive collections of scanned books. One of the challenges with collections of this type is cleaning up the data so that it can be used by people and computer systems. To address that challenge, we developed an efficient approach for recognizing the language of a book in the presence of substantial text recognition (OCR) errors. When we applied the technique to 3.6 million scanned books from the Internet Archive, we found that more than 30,000 were assigned the wrong language, often resulting in lower quality text recognition. We also developed a number of approaches for finding duplicated material in the collection&mdash;not just complete copies of books, but smaller works that are reproduced in other collections. These techniques were largely based on using the sequence of words that occur only once in a given book. Books that overlap heavily in their sequence of unique words are (partial) copies of each other. Using unique words also makes for efficient processing. For example, we analyzed 2.2 million books to find 230 copies of <em>Hamlet</em> included in collected works of Shakespeare, in collections of Elizabethan drama, in anthologies of British literature, and so on. An extended version of this approach allowed us to recognize translations with reasonable accuracy. A different version of this text-reuse analysis allowed us to identify common quotations from canonical works prepared by project partners at Tufts&rsquo; Perseus Digital Library.</p> <p>Another line of work supported by this project investigated approaches to understand and improve state-of-the-art retrieval capabilities on large-scale collections. We showed that recasting a widely-used query expansion technique (adding synonyms and related words to a query) can be done efficiently with pre-processing of the data. We developed methods to extend facet-based query refinement (found in, e.g., shopping sites to restrict results by manufacturer, price range, or color) to the unstructured web setting. We demonstrated that sequences of long queries, often created as part of automated search processes, can be handled efficiently by remembering and incorporating components from past queries, assembling the already-run pieces carefully to handle new queries. We developed new methods for rapidly searching large collections for highly similar sets of documents represented as points in continuous space. We showed that interaction features such as clicks or anchor text used to rank documents (e.g., web pages, medical informatics material) can be transferred from those similar documents with success, allowing less popular material (fewer clicks and fewer links) to rise in the ranking rather than being buried by more frequently accessed but sometimes less relevant items.</p> <p>A final area of work that this project enabled addressed the sorts of pre-processing of data that is necessary to enable efficient search or rich presentation of documents. We developed a number of approaches for recognizing mentions of people, places, and things in text and linking them to an external explanatory database such as Wikipedia &ndash; for example, a news story mentioning &ldquo;President Bush&rdquo; might be linked to the Wikipedia article for the correct Bush. We designed approaches that use expensive linguistic processing and machine learning as well as approaches that are almost as accurate but that use substantially faster processing. We also explored new techniques for learning to automatically annotate images with keywords and phrases that describe what is in them.</p> <p>All of the research carried out within this project investigated ways to process massive collections of text, often in the presence of scanning and text recognition errors, in order to make the collection available for searching, browsing, and data mining. When possible, evaluation data has been made freely available on the web for use by other researchers.</p> <p>This project resulted in 51 refereed conference publications, supported the training of 21 graduate students, resulted in six PhD dissertations, and laid the foundation for technology transition work supported by non-government funding.</p><br> <p>            Last Modified: 01/02/2017<br>      Modified by: James&nbsp;M&nbsp;Allan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project was a collaborative effort between the University of Massachusetts Amherst, Tufts University, and the Internet Archive. It developed and evaluated techniques for processing the large amounts of data contained in massive collections of scanned books. One of the challenges with collections of this type is cleaning up the data so that it can be used by people and computer systems. To address that challenge, we developed an efficient approach for recognizing the language of a book in the presence of substantial text recognition (OCR) errors. When we applied the technique to 3.6 million scanned books from the Internet Archive, we found that more than 30,000 were assigned the wrong language, often resulting in lower quality text recognition. We also developed a number of approaches for finding duplicated material in the collection&mdash;not just complete copies of books, but smaller works that are reproduced in other collections. These techniques were largely based on using the sequence of words that occur only once in a given book. Books that overlap heavily in their sequence of unique words are (partial) copies of each other. Using unique words also makes for efficient processing. For example, we analyzed 2.2 million books to find 230 copies of Hamlet included in collected works of Shakespeare, in collections of Elizabethan drama, in anthologies of British literature, and so on. An extended version of this approach allowed us to recognize translations with reasonable accuracy. A different version of this text-reuse analysis allowed us to identify common quotations from canonical works prepared by project partners at Tufts? Perseus Digital Library.  Another line of work supported by this project investigated approaches to understand and improve state-of-the-art retrieval capabilities on large-scale collections. We showed that recasting a widely-used query expansion technique (adding synonyms and related words to a query) can be done efficiently with pre-processing of the data. We developed methods to extend facet-based query refinement (found in, e.g., shopping sites to restrict results by manufacturer, price range, or color) to the unstructured web setting. We demonstrated that sequences of long queries, often created as part of automated search processes, can be handled efficiently by remembering and incorporating components from past queries, assembling the already-run pieces carefully to handle new queries. We developed new methods for rapidly searching large collections for highly similar sets of documents represented as points in continuous space. We showed that interaction features such as clicks or anchor text used to rank documents (e.g., web pages, medical informatics material) can be transferred from those similar documents with success, allowing less popular material (fewer clicks and fewer links) to rise in the ranking rather than being buried by more frequently accessed but sometimes less relevant items.  A final area of work that this project enabled addressed the sorts of pre-processing of data that is necessary to enable efficient search or rich presentation of documents. We developed a number of approaches for recognizing mentions of people, places, and things in text and linking them to an external explanatory database such as Wikipedia &ndash; for example, a news story mentioning "President Bush" might be linked to the Wikipedia article for the correct Bush. We designed approaches that use expensive linguistic processing and machine learning as well as approaches that are almost as accurate but that use substantially faster processing. We also explored new techniques for learning to automatically annotate images with keywords and phrases that describe what is in them.  All of the research carried out within this project investigated ways to process massive collections of text, often in the presence of scanning and text recognition errors, in order to make the collection available for searching, browsing, and data mining. When possible, evaluation data has been made freely available on the web for use by other researchers.  This project resulted in 51 refereed conference publications, supported the training of 21 graduate students, resulted in six PhD dissertations, and laid the foundation for technology transition work supported by non-government funding.       Last Modified: 01/02/2017       Submitted by: James M Allan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
