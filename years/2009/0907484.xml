<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Density-Preserving Maps</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2009</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>180000.00</AwardTotalIntnAmount>
<AwardAmount>180000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will investigate two aspects of high-dimensional statistics.  First, it develops a new alternative paradigm for nonlinear dimension reduction (often called manifold learning) in which, instead of preserving local distances in the original space as done by existing approaches, the approach preserves the densities in the original space.  The motivation is twofold: Using results from Riemannian geometry, the investigators have shown that is not possible in general to preserve distances, and that it is always possible to preserve densities; in addition, because perhaps the common scientific use of nonlinear dimension reduction methods is to visualize clusters and outliers, which are arguably best formally described in terms of densities, it can be argued that this approach directly preserves the actual information of interest.  This is achieved by means of novel formulations resulting in least-squares problems, as shown in preliminary work, or convex optimization problems to be developed.  Second, the project develops theory and methodology for nonparametrically estimating the densities of points lying on a submanifold, which is needed as the first step in the overall approach.  This includes asymptotic results which are dependent on the dimension of the submanifold rather than that of the ambient space, as current exist.  This provides contrast to the popular conclusion that nonparametric estimation in high dimensional spaces is simply intractable.  Theoretical, methodological, and experimental development will be performed.&lt;br/&gt;&lt;br/&gt;Very high-dimensional data, such as text documents, images, or astronomical spectra as typically encoded, have become increasingly important and prevalent, while statistical theory and methods have only recently attacked such problems with full vigor.  Such data are critical for homeland security, medicine, remote sensing of the environment, e-commerce, and a host of other domains.  The intellectual merit of the work is the introduction of a new way of formulating and analyzing two fundamental statistical operations on such data, called dimension reduction and density estimation.  Each of these could open the door to new avenues in the much-needed area of very high-dimensional statistics.  The broader impact of the work is the transformative ability of analysts to reliably identify outliers and clusters in high-dimensional data -- for example such a tool could help astronomers identify new types of astrophysical objects.  The work will be distributed as part of a well-distributed state-of-the-art toolbox of statistical methods to maximize impact across many areas of data analysis.</AbstractNarration>
<MinAmdLetterDate>09/06/2009</MinAmdLetterDate>
<MaxAmdLetterDate>08/11/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0907484</AwardID>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Gray</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander Gray</PI_FULL_NAME>
<EmailAddress>agray@cc.gatech.edu</EmailAddress>
<PI_PHON>4049330666</PI_PHON>
<NSF_ID>000065875</NSF_ID>
<StartDate>09/06/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 NORTH AVE NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~58236</FUND_OBLG>
<FUND_OBLG>2010~59952</FUND_OBLG>
<FUND_OBLG>2011~61812</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project led to a sequence of work on density and conditional density estimators which are designed to deal with increasingly complicated and challenging datasets, for instance, datasets of astronomical scale, and datasets with heterogeneous smoothness. In particular, the project led to the discovery of CAKE: A Convex Adaptive Kernel Density Estimation, which is a generalization of kernel density estimation that replaces single bandwidth selection by a convex aggregation of kernels at all scales, where the convex aggregation is allowed to vary from one training point to another, treating the fundamental problem of heterogeneous smoothness in a novel way. Learning the CAKE estimator given a training set reduces to solving a single convex quadratic programming problem. &nbsp;The project formally studied the statistical convergence of CAKE and showed that this estimator is optimal in a minimax sense.&nbsp;<br /><br /><br />The project also introduced for the time the dependence distance, a new notion of the intrinsic distance between points, derived as a point-wise extension of local density and statistical dependence measures between variables. The project also introduced a dimension reduction procedure for preserving this distance, which we call the dependence map. The project explored its theoretical justification, connection to other methods, and showed promising empirical behavior on real data sets.<br /><br /><br />Besides new density estimators and density preserving dimensionality reduction, the project also led to efficient algorithms for performing density estimation in astronomical data. Particularly, the algorithms are released for public access in open source machine learning package, MLPACK (<a href="http://mlpack.org/">http://mlpack.org/</a>). The optimized software showed an order-of-magnitude speedup over current state-of-the-art. This enables a much larger computation on the galaxy distribution than was previously possible, which has attractive sizable attention from super-computing community.&nbsp;</p><br> <p>            Last Modified: 07/03/2013<br>      Modified by: Alexander&nbsp;Gray</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project led to a sequence of work on density and conditional density estimators which are designed to deal with increasingly complicated and challenging datasets, for instance, datasets of astronomical scale, and datasets with heterogeneous smoothness. In particular, the project led to the discovery of CAKE: A Convex Adaptive Kernel Density Estimation, which is a generalization of kernel density estimation that replaces single bandwidth selection by a convex aggregation of kernels at all scales, where the convex aggregation is allowed to vary from one training point to another, treating the fundamental problem of heterogeneous smoothness in a novel way. Learning the CAKE estimator given a training set reduces to solving a single convex quadratic programming problem.  The project formally studied the statistical convergence of CAKE and showed that this estimator is optimal in a minimax sense.    The project also introduced for the time the dependence distance, a new notion of the intrinsic distance between points, derived as a point-wise extension of local density and statistical dependence measures between variables. The project also introduced a dimension reduction procedure for preserving this distance, which we call the dependence map. The project explored its theoretical justification, connection to other methods, and showed promising empirical behavior on real data sets.   Besides new density estimators and density preserving dimensionality reduction, the project also led to efficient algorithms for performing density estimation in astronomical data. Particularly, the algorithms are released for public access in open source machine learning package, MLPACK (http://mlpack.org/). The optimized software showed an order-of-magnitude speedup over current state-of-the-art. This enables a much larger computation on the galaxy distribution than was previously possible, which has attractive sizable attention from super-computing community.        Last Modified: 07/03/2013       Submitted by: Alexander Gray]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
