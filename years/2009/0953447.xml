<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: A Self-Tuning Cache Architecture for Multi-Core Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2010</AwardEffectiveDate>
<AwardExpirationDate>04/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>539622.00</AwardTotalIntnAmount>
<AwardAmount>647546</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Due to the increase in design complexity as a result of the paradigm shift from single-core to multi-core technology, designers require advanced optimization techniques to meet design constraints (e.g. power, energy, performance, area) within an ever shrinking time-to-market.  Autonomous self-tuning architectures and methodologies enable a system to dynamically adapt to input stimuli to meet design constraints with little or no designer effort. However, self-tuning systems are exceedingly difficult to design, requiring custom algorithms, flexible reconfigurable architectures, non-intrusive self-evaluation techniques, etc. This research explores and architects novel, lightweight, and efficient reconfigurable architectures, algorithms, and methodologies in the context of a self-tuning cache hierarchy for energy efficient embedded systems. This research will provide fundamental knowledge of how core inter-dependencies both complicate and reveal new optimization opportunities and self-tuning methodologies applicable to other system components (e.g. communication topologies and protocols, voltage scaling, task-to-core mapping, etc.). The outcomes will spawn future research endeavors and enhance academic curricula as well as impact society through more rapidly advancing state-of-the-art technology made possible by reducing design time burdens, enabling application designers to focus more heavily on enhanced functionality rather than system optimization. A research group composed largely of underrepresented minorities and women will undertake the project's endeavors. In addition, the project will include an in-depth evaluation of real-life experiences gathered from successful women engineers to provide tangible motivation, which will be disseminated through scholarly publications, summer research experience programs for high school students, as well as seminars for K-12 students, teachers, and parents.</AbstractNarration>
<MinAmdLetterDate>02/17/2010</MinAmdLetterDate>
<MaxAmdLetterDate>03/02/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0953447</AwardID>
<Investigator>
<FirstName>Ann</FirstName>
<LastName>Ramirez</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ann Ramirez</PI_FULL_NAME>
<EmailAddress>ann@ece.ufl.edu</EmailAddress>
<PI_PHON>3523925356</PI_PHON>
<NSF_ID>000502544</NSF_ID>
<StartDate>02/17/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Florida</Name>
<CityName>GAINESVILLE</CityName>
<ZipCode>326112002</ZipCode>
<PhoneNumber>3523923516</PhoneNumber>
<StreetAddress>1 UNIVERSITY OF FLORIDA</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>969663814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Florida]]></Name>
<CityName>GAINESVILLE</CityName>
<StateCode>FL</StateCode>
<ZipCode>326112002</ZipCode>
<StreetAddress><![CDATA[1 UNIVERSITY OF FLORIDA]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~85533</FUND_OBLG>
<FUND_OBLG>2011~83372</FUND_OBLG>
<FUND_OBLG>2012~122728</FUND_OBLG>
<FUND_OBLG>2013~247989</FUND_OBLG>
<FUND_OBLG>2016~107924</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This CAREER award initially focused solely on self-tuning cache architectures for reduced energy consumption in multi-core architectures.&nbsp;The cache hierarchy consumes 50% or more of a processors total power.&nbsp;Since different applications have different cache requirements due to differences in locality and working set size, the cache parameters (e.g., size, line size, associativity, etc.) can be specialized to an application?s specific requirements. Thus the cache is a good candidate for optimizing system energy consumption.</p> <p>This worked developed several cache tuning mechanisms for dynamically determining the best cache configuration during runtime. Heuristics were developed that quickly explored the design space, however, physically executing each evaluated configuration introduces overhead in the form of increased energy and reduced performance and is not feasible for large and complex design spaces. To reduce/eliminate this overhead, predictive methods were developed to predict the best configuration and directly tune the cache to that configuration for each different phase of execution. This prediction was also extended to determine the best reorder buffer size and instruction queue length.</p> <p>To expand the applicability of cache tuning to modern complex multi-core architectures, this work explored several mechanisms for scheduling and tuning heterogeneous multi-core architectures where in each core has a subset of the entire design space. The scheduler evaluated incoming applications and determines which core is likely to offer the best configuration, the application is scheduled to that core, and the core is in heuristically tuned to the best configuration. Complex scheduling methods were developed that considered the implications of scheduling an application to a non-best or to stall the application until the best core was available if it was energy-advantageous to do so. Later focus has been turned to leveraging machine learning to make cache configuration decisions as well as a close-loop feedback mechanism.</p> <p>Since modern smart devices are operated by myriad users with widely varying device operating requirements (e.g., best performance vs. battery lifetime), cache tuning mechanisms were developed that took into consideration the negative performance and energy impacts incurred during tuning, and tuning heuristics with varying degrees of aggressiveness. Currently, methods are being explored that consider the individual user requirements, allowing the user to give feedback about what level of performance or battery lifetime is expected.</p> <p>In summary, all mechanisms developed were able to reduce the energy consumption of the cache hierarchy, showing that these solutions are ideal for tuning configurable devices.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/31/2018<br>      Modified by: Ann&nbsp;Gordon-Ross</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This CAREER award initially focused solely on self-tuning cache architectures for reduced energy consumption in multi-core architectures. The cache hierarchy consumes 50% or more of a processors total power. Since different applications have different cache requirements due to differences in locality and working set size, the cache parameters (e.g., size, line size, associativity, etc.) can be specialized to an application?s specific requirements. Thus the cache is a good candidate for optimizing system energy consumption.  This worked developed several cache tuning mechanisms for dynamically determining the best cache configuration during runtime. Heuristics were developed that quickly explored the design space, however, physically executing each evaluated configuration introduces overhead in the form of increased energy and reduced performance and is not feasible for large and complex design spaces. To reduce/eliminate this overhead, predictive methods were developed to predict the best configuration and directly tune the cache to that configuration for each different phase of execution. This prediction was also extended to determine the best reorder buffer size and instruction queue length.  To expand the applicability of cache tuning to modern complex multi-core architectures, this work explored several mechanisms for scheduling and tuning heterogeneous multi-core architectures where in each core has a subset of the entire design space. The scheduler evaluated incoming applications and determines which core is likely to offer the best configuration, the application is scheduled to that core, and the core is in heuristically tuned to the best configuration. Complex scheduling methods were developed that considered the implications of scheduling an application to a non-best or to stall the application until the best core was available if it was energy-advantageous to do so. Later focus has been turned to leveraging machine learning to make cache configuration decisions as well as a close-loop feedback mechanism.  Since modern smart devices are operated by myriad users with widely varying device operating requirements (e.g., best performance vs. battery lifetime), cache tuning mechanisms were developed that took into consideration the negative performance and energy impacts incurred during tuning, and tuning heuristics with varying degrees of aggressiveness. Currently, methods are being explored that consider the individual user requirements, allowing the user to give feedback about what level of performance or battery lifetime is expected.  In summary, all mechanisms developed were able to reduce the energy consumption of the cache hierarchy, showing that these solutions are ideal for tuning configurable devices.          Last Modified: 10/31/2018       Submitted by: Ann Gordon-Ross]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
