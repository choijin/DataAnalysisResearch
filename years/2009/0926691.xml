<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Topology-Aware MPI Communication and Scheduling for Petascale Systems</AwardTitle>
<AwardEffectiveDate>10/01/2009</AwardEffectiveDate>
<AwardExpirationDate>09/30/2013</AwardExpirationDate>
<AwardTotalIntnAmount>920000.00</AwardTotalIntnAmount>
<AwardAmount>920000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Daniel Katz</SignBlockName>
</ProgramOfficer>
<AbstractNarration>This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).&lt;br/&gt;&lt;br/&gt;Modern networks (like InfiniBand and 10GigE) have capability to provide topology, routing and also network status information at run-time. This leads to the following  broad challenge: Can the next generation petascale systems provide topology-aware MPI communication, mapping and scheduling which can improve performance and scalability for a range of applications? This challenge leads to the following research questions: 1) What are the topology- aware communication and scheduling requirements of petascale applications? 2) How to design a network topology and state management framework with static and dynamic network information? 3) How to design topology-aware point-to-point and collective communication schemes (such as broadcast, all-to-all, all-reduce) in an MPI library? 4) How to design topology-aware task mapping and scheduling schemes? and 5) How to define and design a flexible topology information interface?  A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and computational scientists from the Texas Advanced Computing Center (TACC) and The Univ. of Calif., San Diego, San Diego Supercomputer Center (SDSC), is proposed to address the above challenges. The research will be driven by a set of applications (PSDNS, UCSDH3D, AWM-Olsen and MPCUGLES) from established NSF computational science researchers running large scale simulations on the Ranger system and other NSF HEC systems.  The transformative impact of the proposed research is to develop topology-aware MPI software and a framework for using derived topology information for scheduling integration in order to maximize petascale application performance. &lt;br/&gt;&lt;br/&gt;&lt;br/&gt; The proposed research is a collaborative and synergistic activity between computer scientists and computational scientists and thus, will have significant impact in deriving guidelines for designing, deploying and using next generation petascale systems. The proposed research directions and their solutions will be used in curriculum of the investigators to train graduate and undergraduate students. The established national-scale training and outreach programs at TACC and SDSC will be used to disseminate the results of this research to HEC users and developers. Research results will also be disseminated to the multiple collaborating organizations of the investigators (national laboratories and industry) to enable impact on their software products and applications. The modified MVAPICH2 library (currently being used by more than 840 organizations) and SGE scheduler plug-in will be available to the HEC community in an open-source manner. Case-studies from this research will be presented at the MPI Forum (OSU is a member of this forum) to in&amp;#64258;uence the design of the upcoming MPI-3 standard and other MPI libraries.</AbstractNarration>
<MinAmdLetterDate>08/13/2009</MinAmdLetterDate>
<MaxAmdLetterDate>08/13/2009</MaxAmdLetterDate>
<ARRAAmount>920000</ARRAAmount>
<AwardID>0926691</AwardID>
<Investigator>
<FirstName>Dhabaleswar</FirstName>
<LastName>Panda</LastName>
<EmailAddress>panda@cse.ohio-state.edu</EmailAddress>
<StartDate>08/13/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University Research Foundation -DO NOT USE</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888734</PhoneNumber>
<StreetAddress>1960 KENNY RD</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
</Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>7684</Code>
<Text>CESER-Cyberinfrastructure for</Text>
</ProgramElement>
<ProgramReference>
<Code>6890</Code>
<Text>RECOVERY ACT ACTION</Text>
</ProgramReference>
<ProgramReference>
<Code>7684</Code>
<Text>STRATEGIC TECHNOLOGIES FOR CI</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
