<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Computational Mechanisms for Storing Motor Memories in Noisy Neural Circuits: How Activity Patterns Evolve during Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2010</AwardEffectiveDate>
<AwardExpirationDate>01/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>716675.00</AwardTotalIntnAmount>
<AwardAmount>716675</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>From throwing a baseball to playing the piano to typing on keyboards, human beings are constantly learning new sensorimotor skills. During learning, synaptic connections in the brain must be modified to form a motor memory. Further, this modification seems both permanent and robust: a sensorimotor skill, once learned, tends to persist throughout the course of a lifetime regardless of its salience (recall the old adage of never forgetting how to ride a bike). Despite the importance of motor memories, their distinctive features, and their ubiquity in vertebrate behavior, little is known about the computational principles and mechanisms that subserve the acquisition of sensorimotor skills. This US-Canadian collaborative project takes an interdisciplinary approach aimed at elucidating neural mechanisms of motor memory formation and unifying -- under a common theoretical principle -- the findings of single-neuron recording studies with established behavioral results. The theory that is proposed makes the following testable prediction: as the level of behavioral expertise in a specific task increases, the neural representation for that skill becomes more selective. By selective, it is meant that a neuron significantly recruited during the performance of the skill tends, with practice, to specialize by firing only when that skill is performed (and not when related skills are performed).&lt;br/&gt;&lt;br/&gt;Central to the theory is a geometric interpretation of "biologically plausible" sensorimotor neural networks, in which neurons are modeled as noisy signal processors and synaptic change is modeled as a noisy morphological process. Because of the high noise levels, it is shown that the system must be "hyperplastic" -- that is, the learning rate must be unusually high in order to compensate for the noise and operate at an acceptable performance level. Geometrically, the solution for a skill can be represented as a manifold in the weight space of the network. To learn multiple skills, a network configuration must be attained such that the solution manifolds intersect. To learn multiple skills without noise leading to destructive interference, the network must arrive at a point where the intersecting solution manifolds are orthogonal. With this principle of orthogonality, the neurophysiological predictions described above can be explicitly formulated. These predictions will be tested with an experimental method -- involving floating microelectrode arrays and antidromic stimulation -- that enables the identifiably same neuron to be recorded from for multiple days/weeks, while a behaving animal learns a task. Finally, psychophysical predictions of the theory will also be tested.&lt;br/&gt;&lt;br/&gt;This project is jointly funded by Collaborative Research in Computational Neuroscience and the OISE Americas program.  A companion project is being funded by the Canadian Institutes of Health Research.</AbstractNarration>
<MinAmdLetterDate>07/30/2010</MinAmdLetterDate>
<MaxAmdLetterDate>07/17/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0904594</AwardID>
<Investigator>
<FirstName>Neville</FirstName>
<LastName>Hogan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Neville Hogan</PI_FULL_NAME>
<EmailAddress>neville@mit.edu</EmailAddress>
<PI_PHON>6172532277</PI_PHON>
<NSF_ID>000215267</NSF_ID>
<StartDate>07/30/2010</StartDate>
<EndDate>02/07/2012</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Emilio</FirstName>
<LastName>Bizzi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Emilio Bizzi</PI_FULL_NAME>
<EmailAddress>ebizzi@mit.edu</EmailAddress>
<PI_PHON>6172535687</PI_PHON>
<NSF_ID>000394483</NSF_ID>
<StartDate>07/30/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Ajemian</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Ajemian</PI_FULL_NAME>
<EmailAddress>ajemian@mit.edu</EmailAddress>
<PI_PHON>6172538174</PI_PHON>
<NSF_ID>000610747</NSF_ID>
<StartDate>02/07/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 MASSACHUSETTS AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7298</Code>
<Text>International Research Collab</Text>
</ProgramElement>
<ProgramElement>
<Code>7327</Code>
<Text>CRCNS-Computation Neuroscience</Text>
</ProgramElement>
<ProgramReference>
<Code>5977</Code>
<Text>AMERICAS PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>7327</Code>
<Text>CRCNS</Text>
</ProgramReference>
<ProgramReference>
<Code>7561</Code>
<Text>CANADA</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~485584</FUND_OBLG>
<FUND_OBLG>2012~231091</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>How do humans remember things?&nbsp; This question has long been a topic of both public and scientific fascination.&nbsp; Whether it is remembering how to hit a topspin backhand or remembering what you did to celebrate your 21st birthday, the very notion that information regarding experience somehow enters your head through physical processes and stays there in some form -- in many instances for the rest of your life -- seems almost miraculous.&nbsp; How is this information stored in the constitutive molecules and cells of our neural circuits?&nbsp; What does the memory look like inside your brain?</p> <p>Over the last several decades, significant advances have been made in uncovering the molecular underpinnings of memory.&nbsp; Many of the key mechanisms have been elaborated for how experience triggers complex molecular cascades within neurons and neural circuits leading to the formation of new and/or modified synapses -- the connections between different neurons -- that embody the memory.&nbsp; It is now known that memory is indeed encoded in the brain by particular patterns of synaptic connectivity that link neurons in specific ways, so that when a trigger for the memory arises, the full memory is recreated.</p> <p>While much is now known about memory, there are still several important questions that remain unanswered.&nbsp; One of these is:</p> <p>How is it that the memories in our brains can last a lifetime when the molecular constituents of these memories -- synaptic proteins -- are turning over every few days?&nbsp;</p> <p>This question at its heart invokes some of the key differences between computers and brains.&nbsp; When information is stored on a computer hard drive or disk, the information (a series of bits or "1"s and "0"s) is burned into the medium once at the time of initial storage.&nbsp; After that, the address register where the memory is contained is left unperturbed (unless the memory is explicitly updated).&nbsp; Nothing changes from an informational standpoint.&nbsp; The same cannot be said for the synaptic proteins that compose memory.&nbsp; All proteins in a cell (except for its DNA) have very short lifetimes on the order of days, because proteins are generally thermodynamically unstable in the warm aqueous medium of the cell where an unpredictable mix of chemical reactions is constantly occurring due to the noisy thermal motion of the molecules. So in the face of non-stop synaptic turnover, how can anything be imprinted permanently with synapses?&nbsp; For some of the other differences between computers and brains which suggest the need for different strategies in managing information, see Fig. 1.&nbsp;</p> <p>Our work initially focused on how motor memories for motor skills are stored.&nbsp; Examples of motor memories are knowing how to ride a bike or how to hit a topspin backhand in tennis.&nbsp; To understand this process of motor memory formation, we developed a new type of biologically plausible neural network -- specifically, a noisy neural network which included the high levels of noise known to be present at many different levels in biological systems.&nbsp; To compensate for the high levels of noise, we incorporated an assumption of "hyperplasticity" in our network.&nbsp; This means that the learning rate is unusually high, so that learning can keep up with noise-related changes to the circuit.&nbsp; Compared to conventional machine learning algorithms (such as the currently popular deep learning nets), the noise levels and learning rate we used were at least an order of magnitude higher.</p> <p>Surprisingly, the high noise levels and high learning rates somehow balanced each other out to give rise to a qualitatively different type of neural network.&nbsp; Critically, the weights in this new type of network never remained constant even when behavioral plateau was attained.&nbsp; Thus, the network was perpetually reconfiguring its ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ How do humans remember things?  This question has long been a topic of both public and scientific fascination.  Whether it is remembering how to hit a topspin backhand or remembering what you did to celebrate your 21st birthday, the very notion that information regarding experience somehow enters your head through physical processes and stays there in some form -- in many instances for the rest of your life -- seems almost miraculous.  How is this information stored in the constitutive molecules and cells of our neural circuits?  What does the memory look like inside your brain?  Over the last several decades, significant advances have been made in uncovering the molecular underpinnings of memory.  Many of the key mechanisms have been elaborated for how experience triggers complex molecular cascades within neurons and neural circuits leading to the formation of new and/or modified synapses -- the connections between different neurons -- that embody the memory.  It is now known that memory is indeed encoded in the brain by particular patterns of synaptic connectivity that link neurons in specific ways, so that when a trigger for the memory arises, the full memory is recreated.  While much is now known about memory, there are still several important questions that remain unanswered.  One of these is:  How is it that the memories in our brains can last a lifetime when the molecular constituents of these memories -- synaptic proteins -- are turning over every few days?   This question at its heart invokes some of the key differences between computers and brains.  When information is stored on a computer hard drive or disk, the information (a series of bits or "1"s and "0"s) is burned into the medium once at the time of initial storage.  After that, the address register where the memory is contained is left unperturbed (unless the memory is explicitly updated).  Nothing changes from an informational standpoint.  The same cannot be said for the synaptic proteins that compose memory.  All proteins in a cell (except for its DNA) have very short lifetimes on the order of days, because proteins are generally thermodynamically unstable in the warm aqueous medium of the cell where an unpredictable mix of chemical reactions is constantly occurring due to the noisy thermal motion of the molecules. So in the face of non-stop synaptic turnover, how can anything be imprinted permanently with synapses?  For some of the other differences between computers and brains which suggest the need for different strategies in managing information, see Fig. 1.   Our work initially focused on how motor memories for motor skills are stored.  Examples of motor memories are knowing how to ride a bike or how to hit a topspin backhand in tennis.  To understand this process of motor memory formation, we developed a new type of biologically plausible neural network -- specifically, a noisy neural network which included the high levels of noise known to be present at many different levels in biological systems.  To compensate for the high levels of noise, we incorporated an assumption of "hyperplasticity" in our network.  This means that the learning rate is unusually high, so that learning can keep up with noise-related changes to the circuit.  Compared to conventional machine learning algorithms (such as the currently popular deep learning nets), the noise levels and learning rate we used were at least an order of magnitude higher.  Surprisingly, the high noise levels and high learning rates somehow balanced each other out to give rise to a qualitatively different type of neural network.  Critically, the weights in this new type of network never remained constant even when behavioral plateau was attained.  Thus, the network was perpetually reconfiguring its internal state.  How can this be?  Suppose you make a skilled movement.  Because of the inherent noise in your motor control system, some noise will corrupt your control signal and, thus, your output.  Howeve...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
