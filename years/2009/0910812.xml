<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>FutureGrid:  An Experimental, High-Performance Grid Test-bed</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2009</AwardEffectiveDate>
<AwardExpirationDate>09/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>10100000.00</AwardTotalIntnAmount>
<AwardAmount>10133500</AwardAmount>
<AwardInstrument>
<Value>Cooperative Agreement</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Robert Chadduck</SignBlockName>
<PO_EMAI>rchadduc@nsf.gov</PO_EMAI>
<PO_PHON>7032922247</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project provides a capability that makes it possible for researchers to tackle complex research challenges in computer science related to the use and security of grids and clouds.  These include topics ranging from authentication, authorization, scheduling, virtualization, middleware design, interface design and cybersecurity, to the optimization of grid-enabled and cloud-enabled computational schemes for researchers in astronomy, chemistry, biology, engineering, atmospheric science and epidemiology.  The project team will provide a significant new experimental computing grid and cloud test-bed, named FutureGrid, to the research community, together with user support for third-party researchers conducting experiments on FutureGrid.&lt;br/&gt; &lt;br/&gt;The test-bed will make it possible for researchers to conduct experiments by submitting an experiment ?plan? that is then executed via a sophisticated workflow engine, preserving the provenance and state information necessary to allow reproducibility.  &lt;br/&gt;&lt;br/&gt;The test-bed includes a geographically distributed set of heterogeneous computing systems, a data management system that will hold both metadata and a growing library of software images, and a dedicated network allowing isolatable, secure experiments. The test-bed will support virtual machine-based environments, as well as native operating systems for experiments aimed at minimizing overhead and maximizing performance.  The project partners will integrate existing open-source software packages to create an easy-to-use software environment that supports the instantiation, execution and recording of grid and cloud computing experiments.  &lt;br/&gt;&lt;br/&gt;One of the goals of the project is to understand the behavior and utility of cloud computing approaches.  Researchers will be able to measure the overhead of cloud technology by requesting linked experiments on both virtual and bare-metal systems. FutureGrid will enable US scientists to develop and test new approaches to parallel, grid and cloud computing, and compare and collaborate with international efforts in this area.  The FutureGrid project will provide an experimental platform that accommodates batch, grid and cloud computing, allowing researchers to attack a range of research questions associated with optimizing, integrating and scheduling the different service models.  The FutureGrid also provides a test-bed for middleware development and, because of its private network, allows middleware researchers to do controlled experiments under different network conditions and to test approaches to middleware that include direct interaction with the network control layer.  Another component of the project is the development of benchmarks appropriate for grid computing, including workflow-based benchmarks derived from applications in astronomy, bioinformatics, seismology and physics.&lt;br/&gt;&lt;br/&gt;The FutureGrid will form part of NSF's TeraGrid high-performance cyberinfrastructure.  It will increase the capability of the TeraGrid to support innovative computer science research requiring access to lower levels of the grid software stack, the networking software stack, and to virtualization and workflow orchestration tools.  Full integration into the TeraGrid is anticipated by 1st October 2011.    &lt;br/&gt;&lt;br/&gt;Education and broader outreach activities include the dissemination of curricular materials on the use of FutureGrid, pre-packaged FutureGrid virtual machines configured for particular course modules, and educational modules based on virtual appliance networks and social networking technologies that will focus on education in networking, parallel computing, virtualization and distributed computing.  The project will advance education and training in distributed computing at academic institutions with less diverse computational resources.  It will do this through the development of instructional resources that include preconfigured environments that provide students with sandboxed virtual clusters.  These can be used for teaching courses in parallel, cloud, and grid computing.  Such resources will also provide academic institutions with a simple opportunity to experiment with cloud technology to see if such technology can enhance their campus resources. The FutureGrid project leverages the fruits of several software development projects funded by the National Science Foundation and the Department of Energy.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/04/2009</MinAmdLetterDate>
<MaxAmdLetterDate>03/25/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>CoopAgrmnt</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0910812</AwardID>
<Investigator>
<FirstName>Jose</FirstName>
<LastName>Fortes</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jose A Fortes</PI_FULL_NAME>
<EmailAddress>fortes@ufl.edu</EmailAddress>
<PI_PHON>3523929265</PI_PHON>
<NSF_ID>000415025</NSF_ID>
<StartDate>09/04/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Grimshaw</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew S Grimshaw</PI_FULL_NAME>
<EmailAddress>andrew.grimshaw@lancium.com</EmailAddress>
<PI_PHON>4342270315</PI_PHON>
<NSF_ID>000293052</NSF_ID>
<StartDate>09/30/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Geoffrey</FirstName>
<LastName>Fox</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Geoffrey C Fox</PI_FULL_NAME>
<EmailAddress>gcf@indiana.edu</EmailAddress>
<PI_PHON>8128567977</PI_PHON>
<NSF_ID>000231257</NSF_ID>
<StartDate>09/04/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Katarzyna</FirstName>
<LastName>Keahey</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Katarzyna Keahey</PI_FULL_NAME>
<EmailAddress>keahey@mcs.anl.gov</EmailAddress>
<PI_PHON>6302521673</PI_PHON>
<NSF_ID>000286883</NSF_ID>
<StartDate>09/04/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Warren</FirstName>
<LastName>Smith</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Warren W Smith</PI_FULL_NAME>
<EmailAddress>Warren.Smith@ll.mit.edu</EmailAddress>
<PI_PHON>7819811509</PI_PHON>
<NSF_ID>000221973</NSF_ID>
<StartDate>09/04/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474013654</ZipCode>
<StreetAddress><![CDATA[509 E 3RD ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramElement>
<Code>7619</Code>
<Text>Innovative HPC</Text>
</ProgramElement>
<ProgramReference>
<Code>7476</Code>
<Text>ETF</Text>
</ProgramReference>
<ProgramReference>
<Code>7619</Code>
<Text>EQUIPMENT ACQUISITIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~4025000</FUND_OBLG>
<FUND_OBLG>2010~6093500</FUND_OBLG>
<FUND_OBLG>2012~15000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>FutureGrid Summary</strong></p> <p>FutureGrid was a national-scale Grid, Cloud and HPC computing test-bed service of modest size that includes a number of computational resources at five distributed locations. FutureGrid experience and architecture wa built around software defined systems at all levels of the stack shown in figure 1 - encompassing VM and bare-metal infrastructure, networks and application, systems and platform software &ndash; with a unifying goal of providing Computing Testbeds as a Service. FutureGrid systems totaled 4704 cores divided into distributed general purpose clusters at Chicago, Florida, IU and Texas; a Cray XT5m at IU and four small specialized clusters supporting SSD (at SDSC), Large Disk Large memory (at IU) and general purpose GPU&rsquo;s (IU). FutureGrid&rsquo;s system model grew in sophistication and ultimately supported software-defined systems &ndash; encompassing virtualized and bare-metal infrastructure, networks, application, systems and platform software &ndash; with a unifying goal of providing Computing Testbeds as a Service (CTaaS). FutureGrid's software system Cloudmesh aggregates resources not only from FutureGrid, but also from OpenCirrus, Amazon, Microsoft Azure, and HP Cloud and GENI resources. Cloudmesh was originally developed in order to simplify the execution of multiple concurrent experiments on a federated cloud infrastructure and in addition to virtual resources, FutureGrid exposed bare-metal provisioning to users. The importance of DevOps tools like Cloudmesh and close integration of Software and Systems administration staff were important lessons from FutureGrid. During its operation, FutureGrid supported 417 projects with 2601 registered users.</p> <p>The six figures show the Computing Testbed as a Service architecture; the distributed network that was FutureGrid superimposed on a map of the USA; the Cloud, Infrastructure, Grid, HPC and Testbed services offered by FutureGrid; a mosaic of FutureGrid machines; four snapshots of FutureGrid monitoring capabilities; and a word cloud constructed from FutureGrid project titles.</p> <p>We found it possible to classify the FutureGrid projects into four major areas: Computer Science and Middleware (56%); Domain Science (21%); Training Education and Outreach (14%); and Computer Systems Evaluation (9%). The numbers in parentheses indicate percentages of total projects and illustrate the importance of computer science projects in FutureGrid&rsquo;s portfolio.&nbsp;Looking at 200 FutureGrid projects in a two year window 10/11-11/13, there were 136 research projects (others were in education, technology evaluation and interoperability) of which 109 had a major CS component and 44 an application component with 17 of these jointly classified.</p> <p>98 of the 200 projects only needed access to virtual machines (VM&rsquo;s) and 54 requested both VM&rsquo;s and physical nodes. Of the 48 projects not requesting VM&rsquo;s, 8 studied cloud technology like Hadoop. In total, 160 projects (80%) were cloud related. 16 projects involved GPU access and 30% of all projects used MapReduce.</p> <p>We identified an initial list of 25 broad cloud computing research areas needing FutureGrid-type capabilities, by pooling our experience with topics studied in these FutureGrid projects (whose numbers are shown in parentheses): Core Virtualization (17); Networking (3); Wireless (0); P2P (2); Cyber-Physical CPS and Mobile Systems (5); Real-Time (0); Storage (2); Distributed Clouds &amp; Systems (8); Resource management (9); Security and Privacy (10); Fault-Tolerance (5); Cyberinfrastructure (11) Programming Models (12); Libraries (5); Data Systems (10); Streaming Data (2); Artificial Intelligence (7); Network/Web Science (3); Software Engineering (2); Education (42, 90% of which on computer science); Open Source Software Testbed (0); Interoperability (3); Energy &amp; Sustainability (0)...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ FutureGrid Summary  FutureGrid was a national-scale Grid, Cloud and HPC computing test-bed service of modest size that includes a number of computational resources at five distributed locations. FutureGrid experience and architecture wa built around software defined systems at all levels of the stack shown in figure 1 - encompassing VM and bare-metal infrastructure, networks and application, systems and platform software &ndash; with a unifying goal of providing Computing Testbeds as a Service. FutureGrid systems totaled 4704 cores divided into distributed general purpose clusters at Chicago, Florida, IU and Texas; a Cray XT5m at IU and four small specialized clusters supporting SSD (at SDSC), Large Disk Large memory (at IU) and general purpose GPUÆs (IU). FutureGridÆs system model grew in sophistication and ultimately supported software-defined systems &ndash; encompassing virtualized and bare-metal infrastructure, networks, application, systems and platform software &ndash; with a unifying goal of providing Computing Testbeds as a Service (CTaaS). FutureGrid's software system Cloudmesh aggregates resources not only from FutureGrid, but also from OpenCirrus, Amazon, Microsoft Azure, and HP Cloud and GENI resources. Cloudmesh was originally developed in order to simplify the execution of multiple concurrent experiments on a federated cloud infrastructure and in addition to virtual resources, FutureGrid exposed bare-metal provisioning to users. The importance of DevOps tools like Cloudmesh and close integration of Software and Systems administration staff were important lessons from FutureGrid. During its operation, FutureGrid supported 417 projects with 2601 registered users.  The six figures show the Computing Testbed as a Service architecture; the distributed network that was FutureGrid superimposed on a map of the USA; the Cloud, Infrastructure, Grid, HPC and Testbed services offered by FutureGrid; a mosaic of FutureGrid machines; four snapshots of FutureGrid monitoring capabilities; and a word cloud constructed from FutureGrid project titles.  We found it possible to classify the FutureGrid projects into four major areas: Computer Science and Middleware (56%); Domain Science (21%); Training Education and Outreach (14%); and Computer Systems Evaluation (9%). The numbers in parentheses indicate percentages of total projects and illustrate the importance of computer science projects in FutureGridÆs portfolio. Looking at 200 FutureGrid projects in a two year window 10/11-11/13, there were 136 research projects (others were in education, technology evaluation and interoperability) of which 109 had a major CS component and 44 an application component with 17 of these jointly classified.  98 of the 200 projects only needed access to virtual machines (VMÆs) and 54 requested both VMÆs and physical nodes. Of the 48 projects not requesting VMÆs, 8 studied cloud technology like Hadoop. In total, 160 projects (80%) were cloud related. 16 projects involved GPU access and 30% of all projects used MapReduce.  We identified an initial list of 25 broad cloud computing research areas needing FutureGrid-type capabilities, by pooling our experience with topics studied in these FutureGrid projects (whose numbers are shown in parentheses): Core Virtualization (17); Networking (3); Wireless (0); P2P (2); Cyber-Physical CPS and Mobile Systems (5); Real-Time (0); Storage (2); Distributed Clouds &amp; Systems (8); Resource management (9); Security and Privacy (10); Fault-Tolerance (5); Cyberinfrastructure (11) Programming Models (12); Libraries (5); Data Systems (10); Streaming Data (2); Artificial Intelligence (7); Network/Web Science (3); Software Engineering (2); Education (42, 90% of which on computer science); Open Source Software Testbed (0); Interoperability (3); Energy &amp; Sustainability (0); Domain Science (44); and Technology Evaluation (19).  Application (domain science) projects in the sample of 200 projects, included 18 from bioinfor...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
