<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Visual Information Delivery Robot for Visually Impaired Children</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2010</AwardEffectiveDate>
<AwardExpirationDate>12/31/2010</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>200000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Glenn H. Larsen</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This Small Business Innovation Research (SBIR) Phase I project will evaluate the feasibility of a robotic system to detect and track a child's finger so that the scene that the child wishes to see can be displayed at high magnification on a monitor. There are two issues of intellectual merit that will be addressed during the Phase I work. The first issue is analyzing the pointing pattern of children. Since each person has his or her own pattern for raising an arm and pointing with a finger, the system requires a machine learning algorithm to adjust the decision from the system. This adaptive algorithm forms one of the key innovations that will be evaluated during the Phase I work. The second issue will be an adaptive zooming and scene segmentation algorithm. &lt;br/&gt;&lt;br/&gt;There are two primary commercial applications for the proposed visual information delivery robot. The first involves the education of visually impaired children. Most of research and products currently available for visually impaired children are focused on learning while they are sitting on the chair in front of the computer monitor. However, the proposed system captures the scene that the child points to in any location, thus bringing a dynamic tool to education. It is anticipated that such tools will have a significant commercial potential in schools for the visually impaired. The second commercial application is in assisting visually impaired adults with enhanced dynamic information when they are in a wheelchair. The commercial and societal impact of the proposed project is that it will enable visually impaired children and adults to enhance their quality of life by adding a dynamic tool for visualizing near and far off objects. The algorithms developed during the Phase I research will also aid researchers in industrial automation with advanced robots.</AbstractNarration>
<MinAmdLetterDate>11/04/2009</MinAmdLetterDate>
<MaxAmdLetterDate>06/07/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0945236</AwardID>
<Investigator>
<FirstName>Yudaya</FirstName>
<LastName>Sivathanu</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yudaya R Sivathanu</PI_FULL_NAME>
<EmailAddress>sivathan@enurga.com</EmailAddress>
<PI_PHON>7654973269</PI_PHON>
<NSF_ID>000320734</NSF_ID>
<StartDate>12/01/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Hyukseong</FirstName>
<LastName>Kwon</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hyukseong Kwon</PI_FULL_NAME>
<EmailAddress>hyukseon@purdue.edu</EmailAddress>
<PI_PHON>3174637288</PI_PHON>
<NSF_ID>000530439</NSF_ID>
<StartDate>11/04/2009</StartDate>
<EndDate>12/01/2009</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>EN'URGA INC</Name>
<CityName>WEST LAFAYETTE</CityName>
<ZipCode>479061359</ZipCode>
<PhoneNumber>7654973269</PhoneNumber>
<StreetAddress>1201 CUMBERLAND AVE, Suite R</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>944533009</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>EN'URGA INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[EN'URGA INC]]></Name>
<CityName>WEST LAFAYETTE</CityName>
<StateCode>IN</StateCode>
<ZipCode>479061359</ZipCode>
<StreetAddress><![CDATA[1201 CUMBERLAND AVE, Suite R]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0308000</Code>
<Name>Industrial Technology</Name>
</FoaInformation>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>1658</Code>
<Text>SOFTWARE</Text>
</ProgramReference>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~200000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Assistive Technologies have proven to be a significant help to the visually impaired children in their educational goals.  Most of the products (and research) for the visually impaired are focused on learning while they are sitting on the chair in front of the computer monitor.  A challenging goal is to extend such assistive technology products to a dynamic learning environment so as to provide greater benefits to visually impaired children.   For the Phase I work, En&rsquo;Urga Inc. developed a visual information delivery system for visually impaired children.  Briefly, the system works as follows:  With the initial signal (by touching a key on the keyboard or by a simple voice recognition), a stereo camera system mounted on a mobile platform detects the pointed finger of a child, then the system estimates where the scene/object that the child is pointing to is located, and finally the system displays the scene or object zooming in to show an enlarged view on the display for the visually-impaired child.</p> <p>Based on the Phase I results, the feasibility of utilizing a mobile camera system to detect a hand, estimate the direction in which the index finger of the hand was pointing, and displaying the scene on a laptop computer was completely demonstrated.  For the Phase II work, En&rsquo;Urga Inc. will mount the system on a headphone.  In practice, the visually impaired child or adult, wearing the headphone, can point to different objects in their surroundings and have a magnified view of the same displayed on a tablet PC or an Iphone.  Some examples are a visually impaired person wanting to read the title of a book on the top shelf of the library, or a street sign while walking.  In a classroom setting, it could be the need to observe an experiment being demonstrated by an instructor near the front of the class while the student is seated a distance.</p> <p>There are two primary commercial applications for the visual information delivery robot.  The first is in the education of visually impaired children.  Our system captures the scene that the child points while he or she is moving around, enabling education in a dynamic environment.  The second commercial application is in assisting visually impaired adults with enhanced information when they are in a wheelchair.  The commercial and societal benefit of the proposed project is that it will enable visually impaired children and adults to enhance their quality of life by adding a dynamic tool for visualizing near and far off objects.</p><br> <p>            Last Modified: 03/01/2011<br>      Modified by: Yudaya&nbsp;R&nbsp;Sivathanu</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2011/0945236/0945236_10029352_1299007950491_outcome1--rgov-214x142.jpg" original="/por/images/Reports/POR/2011/0945236/0945236_10029352_1299007950491_outcome1--rgov-800width.jpg" title="Prototype visual information delivery system"><img src="/por/images/Reports/POR/2011/0945236/0945236_10029352_1299007950491_outcome1--rgov-66x44.jpg" alt="Prototype visual information delivery system"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A learning aid for visually impaired children</div> <div class="imageCredit">En'Urga Inc.</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Yudaya&nbsp;R&nbsp;Sivathanu</div> <div class="imageTitle">Prototype visual information delivery system</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Assistive Technologies have proven to be a significant help to the visually impaired children in their educational goals.  Most of the products (and research) for the visually impaired are focused on learning while they are sitting on the chair in front of the computer monitor.  A challenging goal is to extend such assistive technology products to a dynamic learning environment so as to provide greater benefits to visually impaired children.   For the Phase I work, EnÆUrga Inc. developed a visual information delivery system for visually impaired children.  Briefly, the system works as follows:  With the initial signal (by touching a key on the keyboard or by a simple voice recognition), a stereo camera system mounted on a mobile platform detects the pointed finger of a child, then the system estimates where the scene/object that the child is pointing to is located, and finally the system displays the scene or object zooming in to show an enlarged view on the display for the visually-impaired child.  Based on the Phase I results, the feasibility of utilizing a mobile camera system to detect a hand, estimate the direction in which the index finger of the hand was pointing, and displaying the scene on a laptop computer was completely demonstrated.  For the Phase II work, EnÆUrga Inc. will mount the system on a headphone.  In practice, the visually impaired child or adult, wearing the headphone, can point to different objects in their surroundings and have a magnified view of the same displayed on a tablet PC or an Iphone.  Some examples are a visually impaired person wanting to read the title of a book on the top shelf of the library, or a street sign while walking.  In a classroom setting, it could be the need to observe an experiment being demonstrated by an instructor near the front of the class while the student is seated a distance.  There are two primary commercial applications for the visual information delivery robot.  The first is in the education of visually impaired children.  Our system captures the scene that the child points while he or she is moving around, enabling education in a dynamic environment.  The second commercial application is in assisting visually impaired adults with enhanced information when they are in a wheelchair.  The commercial and societal benefit of the proposed project is that it will enable visually impaired children and adults to enhance their quality of life by adding a dynamic tool for visualizing near and far off objects.       Last Modified: 03/01/2011       Submitted by: Yudaya R Sivathanu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
