<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Supercomputing on a Cluster of Workstations via Scalable Locality and Scalable Parallelism</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2009</AwardEffectiveDate>
<AwardExpirationDate>02/28/2013</AwardExpirationDate>
<AwardTotalIntnAmount>123465.00</AwardTotalIntnAmount>
<AwardAmount>129661</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Modern scientific research often includes a substantial computational component, which may use a supercomputer and special software to automatically tune ("optimize") the application for the computer. A cluster of standard workstations can offer similar net processing power at a fraction of the cost, but automatic optimization of some important numerical applications for these systems remains an elusive challenge.&lt;br/&gt;The quest for good performance of parallel applications on clusters of workstations has traditionally employed software techniques that are quite different from those applied to the programming of parallel supercomputers. In particular, static automatic parallelization has been employed on supercomputers (especially for dense matrix codes on shared memory systems) but has not been successful on clusters. The lack of success with static parallelization is due in part to the inability of classic parallelization techniques to expose sufficient memory locality.&lt;br/&gt;&lt;br/&gt;The PI proposes to develop compiler techniques that will allow dense matrix problems to run efficiently on clusters of workstations by dramatically increasing locality while respecting the parallelism constraints of the code. The PI plans to  investigate techniques for automatically producing high performance for dense matrix codes executing on clusters of workstations by "tiling" time-skewed loop nests such that they can execute efficiently on a cluster of multicore workstations. This research will enable automatic program optimization for numerical applications. The proposed activity could advance the state of performance models for tiling for clusters.</AbstractNarration>
<MinAmdLetterDate>09/02/2009</MinAmdLetterDate>
<MaxAmdLetterDate>04/11/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0943455</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Wonnacott</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David G Wonnacott</PI_FULL_NAME>
<EmailAddress>davew@cs.haverford.edu</EmailAddress>
<PI_PHON>6108964973</PI_PHON>
<NSF_ID>000235940</NSF_ID>
<StartDate>09/02/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Haverford College</Name>
<CityName>Haverford</CityName>
<ZipCode>190411336</ZipCode>
<PhoneNumber>6108961000</PhoneNumber>
<StreetAddress>370 Lancaster Avenue</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002502615</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORPORATION OF HAVERFORD COLLEGE, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002502615</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Haverford College]]></Name>
<CityName>Haverford</CityName>
<StateCode>PA</StateCode>
<ZipCode>190411336</ZipCode>
<StreetAddress><![CDATA[370 Lancaster Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9229</Code>
<Text>RES IN UNDERGRAD INST-RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~123465</FUND_OBLG>
<FUND_OBLG>2012~6196</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Project Outcomes: Supercomputing on a Cluster of Workstations via Scalable Locality and Scalable Parallelism<br /><br /><strong>Intellectual Merit</strong></p> <p><br />Wonnacott's investigation of the use of distributed shared memory ("DSM") software to allow the use of shared-memory program optimization techniques has demonstrated that this combination of tools can produce scalable performance on a cluster of workstations. This result is described in detail in the undergraduate senior thesis of Tim Douglas, and in Haverford Computer Science Technical Reports (HC-CS-TR) 2009-01 and 2011-02. It was presented publicly by Mohamed Abdalkader (Haverford College class of 2014) at the CCGrid 2012 poster session. Correctness issues with DSM software, together with increasing availability of MPI code generation for tools that previously supported only shared memory, have caused Wonnacott to move ongoing work away from DSM. However, his results indicate that the combination of shared-memory optimization and DSM should be considered when special-purpose code generators are not available.</p> <p><br />Figures 1 and 2 of this report are taken from the CCGrid presentation of Abdalkader, Burnette, Douglas, and Wonnacott. They illustrate several key results of this work.</p> <ul> <li>The "time tiling" transformation that is used to improve performance on shared-memory systems is critically important to distributed-memory execution of the benchmarks that were explored. In both figures, the time-tiled execution (shown with x's and +'s on the figures) are dramatically above the results without time tiling (shown with circles and squares). This difference goes beyond a simple extrapolation of the value of time tiling for a single-node system (shown as solid lines), as the non-time-tiled results experience dramatic reductions of total performance when nodes are added to the cluster, and time-tiled results do not.</li> <li>When experiments were run over gigaBit Ethernet, performance of time-tiled codes increased fairly consistently with the number of nodes involved in the computation, achieving a significant fraction of the potential speedup. In this work, data sets were chosen to avoid cache-based "superlinear scaling", and potential speedup was thus estimated by multiplying the fastest single-node result by the number of nodes (producing the top solid line on each graph).</li> <li>When experiments were run over 100Mbps Ethernet, the time-tiled code avoided the dramatic drops in performance that can be seen even at relatively small number of nodes for the non-time-tiled codes. Attempts to explore the use of larger tile sizes (requiring larger total data sets) to increase the time-tiled performance led Wonnacott to appreciate the dramatic impacts of different asymptotic scaling behaviors of different variants of the time tiling transformation. This led to a paper, with Dr. Michelle Mills Strout of Colorado State University, describing this difference (presented at the IMPACT 2013 workshops).</li> </ul> <p>The IMPACT 2013 presentation by Wonnacott and Strout presents the most significant theoretical result of this project, of interest to the static optimization community:</p> <ul> <li>The asymptotic degree of effective parallelism depends on not just the computation being optimized, but also on the specific variant of the time tiling optimization that is applied. For the version of time tiling that was described by Wonnacott in his presentation at IPDPS 2000, full performance required that the data set, but not the number of time steps, be scaled up with the number of processors. Newer variants of the time tiling transformation, such as those that were orginially used in PluTo and AlphaZ, require that the number of iterations and the data set size both increase with the degree of parallelism. This factor could become significant as time tiling is used for larger scale parallelism.</li> ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Project Outcomes: Supercomputing on a Cluster of Workstations via Scalable Locality and Scalable Parallelism  Intellectual Merit   Wonnacott's investigation of the use of distributed shared memory ("DSM") software to allow the use of shared-memory program optimization techniques has demonstrated that this combination of tools can produce scalable performance on a cluster of workstations. This result is described in detail in the undergraduate senior thesis of Tim Douglas, and in Haverford Computer Science Technical Reports (HC-CS-TR) 2009-01 and 2011-02. It was presented publicly by Mohamed Abdalkader (Haverford College class of 2014) at the CCGrid 2012 poster session. Correctness issues with DSM software, together with increasing availability of MPI code generation for tools that previously supported only shared memory, have caused Wonnacott to move ongoing work away from DSM. However, his results indicate that the combination of shared-memory optimization and DSM should be considered when special-purpose code generators are not available.   Figures 1 and 2 of this report are taken from the CCGrid presentation of Abdalkader, Burnette, Douglas, and Wonnacott. They illustrate several key results of this work.  The "time tiling" transformation that is used to improve performance on shared-memory systems is critically important to distributed-memory execution of the benchmarks that were explored. In both figures, the time-tiled execution (shown with x's and +'s on the figures) are dramatically above the results without time tiling (shown with circles and squares). This difference goes beyond a simple extrapolation of the value of time tiling for a single-node system (shown as solid lines), as the non-time-tiled results experience dramatic reductions of total performance when nodes are added to the cluster, and time-tiled results do not. When experiments were run over gigaBit Ethernet, performance of time-tiled codes increased fairly consistently with the number of nodes involved in the computation, achieving a significant fraction of the potential speedup. In this work, data sets were chosen to avoid cache-based "superlinear scaling", and potential speedup was thus estimated by multiplying the fastest single-node result by the number of nodes (producing the top solid line on each graph). When experiments were run over 100Mbps Ethernet, the time-tiled code avoided the dramatic drops in performance that can be seen even at relatively small number of nodes for the non-time-tiled codes. Attempts to explore the use of larger tile sizes (requiring larger total data sets) to increase the time-tiled performance led Wonnacott to appreciate the dramatic impacts of different asymptotic scaling behaviors of different variants of the time tiling transformation. This led to a paper, with Dr. Michelle Mills Strout of Colorado State University, describing this difference (presented at the IMPACT 2013 workshops).   The IMPACT 2013 presentation by Wonnacott and Strout presents the most significant theoretical result of this project, of interest to the static optimization community:  The asymptotic degree of effective parallelism depends on not just the computation being optimized, but also on the specific variant of the time tiling optimization that is applied. For the version of time tiling that was described by Wonnacott in his presentation at IPDPS 2000, full performance required that the data set, but not the number of time steps, be scaled up with the number of processors. Newer variants of the time tiling transformation, such as those that were orginially used in PluTo and AlphaZ, require that the number of iterations and the data set size both increase with the degree of parallelism. This factor could become significant as time tiling is used for larger scale parallelism.    Broader Impacts   This project has informed the static optimization and cluster computing communities (via the publications cited above), provided feedback for develop...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
