<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>II-NEW: Towards an Infrastructure for Research on Multimodal Language Processing in Situated Human Robot Dialogue</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2010</AwardEffectiveDate>
<AwardExpirationDate>02/28/2014</AwardExpirationDate>
<AwardTotalIntnAmount>217275.00</AwardTotalIntnAmount>
<AwardAmount>229275</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A new generation of robots is emerging which aims to interact with humans on a daily basis to provide service, care, and companionship.  To support natural interaction between people and this type of robot, robust language processing enabling situated human robot dialogue will become increasingly important.  Compared to traditional spoken dialogue systems and multimodal conversational interfaces, situated human robot dialogue is drastically different due to two unique characteristics.  The first characteristic is situatedness.  A robot is situated in a physical world that is cohabited with human partners.  The spatial relations between the robot, the human, and the environment, and the dynamic nature of the surroundings, have a massive influence on how the robot accomplishes its task and interacts with the human.  The second characteristic is embodiment.  A robot and its human partner both have physical bodies in the environment.  In embodied communication, speakers make extensive use of non-verbal modalities (e.g., eye gaze and gestures) to engage in conversation and make reference to the shared environment.  These two characteristics make automated interpretation of human language in situated human robot dialogue extremely challenging.  This is funding to provide the PI and her team with an enhanced infrastructure that will enable them to address these challenges.  The new resources will include a physical environment and a virtual environment to enable human-centered investigation, tools to capture human multimodal language behaviors that include human speech, eye gaze, and gesture during situated human robot dialogue, and systems to support Wizard-of-Oz experiments, data collection, and data analysis.   The integration of a physical world and a virtual world is an innovation of the new infrastructure.  Limitations of sensor and effector technology often make it difficult or expensive to change robot configurations and implement desired behaviors.  The virtual world paradigm allows efficient and high fidelity simulation of the physical world and robots, as well as studies on multimodal language behavior under many different conditions which are otherwise difficult to obtain in the physical world.  The new infrastructure will enable a human-centered approach by facilitating a wide variety of controlled experiments.  It will enable new empirical findings and provide a testbed for advanced techniques for multimodal language processing that are psycholinguistically plausible.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  The new infrastructure will provide tremendous research and collaborative opportunities for the PI and her colleagues at Michigan State University.  It will have profound implications in enabling the next generation of social and cognitive robots.  This infrastructure will also provide new and exciting training and education opportunities for students at MSU through research mentoring and curriculum development.  It will bring new educational experiences to K-12 students and encourage broader participation in engineering through several outreach programs at MSU.</AbstractNarration>
<MinAmdLetterDate>02/23/2010</MinAmdLetterDate>
<MaxAmdLetterDate>12/05/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0957039</AwardID>
<Investigator>
<FirstName>Joyce</FirstName>
<LastName>Chai</LastName>
<PI_MID_INIT>Y</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joyce Y Chai</PI_FULL_NAME>
<EmailAddress>chaijy@umich.edu</EmailAddress>
<PI_PHON>7347648505</PI_PHON>
<NSF_ID>000477137</NSF_ID>
<StartDate>02/23/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Michigan State University</Name>
<CityName>East Lansing</CityName>
<ZipCode>488242600</ZipCode>
<PhoneNumber>5173555040</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[426 Administration Bldg, Rm2]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>193247145</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MICHIGAN STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>053343976</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Michigan State University]]></Name>
<CityName>East Lansing</CityName>
<StateCode>MI</StateCode>
<ZipCode>488242600</ZipCode>
<StreetAddress><![CDATA[Office of Sponsored Programs]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~229275</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>A new generation of robots have emerged in recent years to serve as assistants and companions to human partners. To support natural interaction between humans and this type of cognitive robots, technology enabling situated human-robot dialogue has become increasingly important.&nbsp; This project has acquired resources towards a new infrastructure to support research on situated human-robot dialogue. &nbsp;More specifically, through this project, several robots (e.g., PeopleBot, NAO, and a robotic arm) have been acquired together with equipment to track human multimodal language behaviors (e.g., mobile eye tracker, a Kinect, and a stereo camera). This project has also developed software tools for virtual world simulation and created systems for Wizard-of-Oz experiments. The resulting infrastructure has provided many opportunities to conduct research on situated human-robot dialogue. For example, it has been used to develop and evaluate algorithms for collaborative referential grounding in human-robot dialogue. In addition, this project has also provided exciting training and education opportunities for K-12 students and students at MSU. For example, the NAO robot was demonstrated to kindergarten children and the PeopleBot was brought to the classroom to teach concepts in Artificial Intelligence. In the long run, this infrastructure will enable new empirical findings and provide a testbed for advanced techniques for situated multimodal language processing in human-robot dialogue.</p><br> <p>            Last Modified: 05/30/2014<br>      Modified by: Joyce&nbsp;Y&nbsp;Chai</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ A new generation of robots have emerged in recent years to serve as assistants and companions to human partners. To support natural interaction between humans and this type of cognitive robots, technology enabling situated human-robot dialogue has become increasingly important.  This project has acquired resources towards a new infrastructure to support research on situated human-robot dialogue.  More specifically, through this project, several robots (e.g., PeopleBot, NAO, and a robotic arm) have been acquired together with equipment to track human multimodal language behaviors (e.g., mobile eye tracker, a Kinect, and a stereo camera). This project has also developed software tools for virtual world simulation and created systems for Wizard-of-Oz experiments. The resulting infrastructure has provided many opportunities to conduct research on situated human-robot dialogue. For example, it has been used to develop and evaluate algorithms for collaborative referential grounding in human-robot dialogue. In addition, this project has also provided exciting training and education opportunities for K-12 students and students at MSU. For example, the NAO robot was demonstrated to kindergarten children and the PeopleBot was brought to the classroom to teach concepts in Artificial Intelligence. In the long run, this infrastructure will enable new empirical findings and provide a testbed for advanced techniques for situated multimodal language processing in human-robot dialogue.       Last Modified: 05/30/2014       Submitted by: Joyce Y Chai]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
