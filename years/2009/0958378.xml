<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Assessing Assessments: A Historical and Philosophical Study of Scientific Assessments for Environmental Policy in the Late 20th Century</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2010</AwardEffectiveDate>
<AwardExpirationDate>07/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>387085.00</AwardTotalIntnAmount>
<AwardAmount>387085</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Frederick Kronz</SignBlockName>
<PO_EMAI>fkronz@nsf.gov</PO_EMAI>
<PO_PHON>7032927283</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Assessing Assessments: A Historical and Philosophical Study of Scientific &lt;br/&gt;Assessments for Environmental Policy in the late 20th Century&lt;br/&gt;&lt;br/&gt;Intellectual merit&lt;br/&gt;&lt;br/&gt;This project investigates the process by which scientists attempt to assess and summarize--accurately, fairly, objectively--the scientific knowledge that society needs to make informed judgments about complex issues. How do scientists evaluate the reliability of their colleagues? research, understand its limits and degrees of uncertainty, and come to consensus (or not)?  How do scientists respond to the subtle or overt pressures that arise when they know the world is watching the outcome of the process? What factors internal to the process may lead to systematic bias, error, or distortion? &lt;br/&gt;Periodically, scientists come together to present scientific information at the request of policy-makers and for the benefit of the general public, particularly in the area of environmental science relevant to policy.  Recent examples include the National Acid Precipitation Assessment Program, the Ozone Trends Panel, and the Intergovernmental Panel on Climate Change, organized to summarize and assess the state of the art with respect to acid rain, global ozone depletion, and anthropogenic climate change, respectively.  Such assessments consume large amounts of time, money, and manpower, and they can play a major role in policy debates. Therefore, it is important to understand how they work (or fail to work). Yet, few scholars have studied them.  &lt;br/&gt;&lt;br/&gt;Broader Impact&lt;br/&gt;&lt;br/&gt;Understanding the internal dynamics of the assessment process puts us in a better position to judge the quality of any particular assessment, as well as to suggest potential means of improvement, and, ultimately, to provide an improved foundation for science-based policy. The presumption that assessments should influence policy assumes that the quality of these assessments is high, but that assumption has rarely been closely examined.  &lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>05/05/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/19/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0958378</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Oppenheimer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Oppenheimer</PI_FULL_NAME>
<EmailAddress>omichael@princeton.edu</EmailAddress>
<PI_PHON>6092583090</PI_PHON>
<NSF_ID>000498729</NSF_ID>
<StartDate>05/05/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[Off. of Research &amp; Proj. Adm]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7603</Code>
<Text>STS-Sci, Tech &amp; Society</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1353</Code>
<Text>Hist &amp; Philosophy of SET</Text>
</ProgramReference>
<ProgramReference>
<Code>7956</Code>
<Text>SBE Interdisciplinary Research</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~559510</FUND_OBLG>
<FUND_OBLG>2011~119301</FUND_OBLG>
<FUND_OBLG>2012~53124</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <p>Science assessment has largely been a black box to date. Yet this approach to developing a relevant scientific basis for public policy has become not only pervasive but a critical influence on research agendas. The process has been studied from a political science perspective, i.e., what are the characteristics of an assessment process which is successful in terms of external influence.&nbsp;However, there has been no study of how evaluation,negotiation,deliberation, and decision on consensus about scientific evidence in actually carried out within <em>assessments</em>.&nbsp;</p> <p><br />We organized an unusual multidisciplinary team to carry out the research, including a physical scientist, an historian, and a philosopher at the senior level, and two ethnographers and two historians at the junior level. The methods included interviews of participating scientists, archival research on documents related to or directly produced by assessments, and direct observation in the case of National Research Council (NRC) panels.</p> <p><br />We focused mainly upon three assessment processes: the US National Acid Deposition Assessment Program, carried on over more than a decade, which both supported and assessed research; a series of ozone assessments, particularly those under the auspices of the World Meteorological Organization (WMO) beginning in 1985; and a set of assessments of the stability of the ice sheets in the context of predicting sea level rise, especially the assessments of the Intergovernmental Panel on Climate Change which began in 1988.&nbsp; Studies of a fourth set of assessments, those of the NRC, are still underway.</p> <p><br />A fundamentally unique aspect of this project was the decision to try to study assessments via ethnographic observations and our initial implementation of this approach at the NRC.&nbsp;As indicated above, this has not previously been done for assessment panels to our knowledge.&nbsp;The importance of this approach is not only to transcend the limitations of interviews and archival material, but also to give us direct and synchronous access to the institutional rules, regulations,structures, and individuals who create the conditions under which the assessment proceeds.&nbsp;</p> <p><br /><strong>Our key findings were these:</strong></p> <p><br />1. Particularly based on the study of NAPAP and supported by our recent observations of NRC panels, we concluded that institutional factors are critical for determining the scope and sometimes the conclusions of an assessment.&nbsp;</p> <p><br />2. Based particularly on the study of the WMO ozone assessment and an early NRC assessment in 1983, we concluded that participants sometimes develop new knowledge ad hoc to fill gaps in the existing literature in order to meet the needs of the policy-maker clients.</p> <p><br />3. On the whole, particularly in the case of IPCC, assessments tend to be cautious in their findings.&nbsp; Whether this is an inevitable result of consensus processes or a deeper inclination of scientists needs to be studied more carefully, hopefully through our ongoing ethnographic work.&nbsp; However, the evidence base illustrates that in the case of IPCC, projections of climate outcomes have generally been on the mark or indicated a lower level of change than actual outcomes years later.</p> <p><br /><br /><br /><br /></p><br> <p>            Last Modified: 10/29/2013<br>      Modified by: Michael&nbsp;Oppenheimer</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    Science assessment has largely been a black box to date. Yet this approach to developing a relevant scientific basis for public policy has become not only pervasive but a critical influence on research agendas. The process has been studied from a political science perspective, i.e., what are the characteristics of an assessment process which is successful in terms of external influence. However, there has been no study of how evaluation,negotiation,deliberation, and decision on consensus about scientific evidence in actually carried out within assessments.    We organized an unusual multidisciplinary team to carry out the research, including a physical scientist, an historian, and a philosopher at the senior level, and two ethnographers and two historians at the junior level. The methods included interviews of participating scientists, archival research on documents related to or directly produced by assessments, and direct observation in the case of National Research Council (NRC) panels.   We focused mainly upon three assessment processes: the US National Acid Deposition Assessment Program, carried on over more than a decade, which both supported and assessed research; a series of ozone assessments, particularly those under the auspices of the World Meteorological Organization (WMO) beginning in 1985; and a set of assessments of the stability of the ice sheets in the context of predicting sea level rise, especially the assessments of the Intergovernmental Panel on Climate Change which began in 1988.  Studies of a fourth set of assessments, those of the NRC, are still underway.   A fundamentally unique aspect of this project was the decision to try to study assessments via ethnographic observations and our initial implementation of this approach at the NRC. As indicated above, this has not previously been done for assessment panels to our knowledge. The importance of this approach is not only to transcend the limitations of interviews and archival material, but also to give us direct and synchronous access to the institutional rules, regulations,structures, and individuals who create the conditions under which the assessment proceeds.    Our key findings were these:   1. Particularly based on the study of NAPAP and supported by our recent observations of NRC panels, we concluded that institutional factors are critical for determining the scope and sometimes the conclusions of an assessment.    2. Based particularly on the study of the WMO ozone assessment and an early NRC assessment in 1983, we concluded that participants sometimes develop new knowledge ad hoc to fill gaps in the existing literature in order to meet the needs of the policy-maker clients.   3. On the whole, particularly in the case of IPCC, assessments tend to be cautious in their findings.  Whether this is an inevitable result of consensus processes or a deeper inclination of scientists needs to be studied more carefully, hopefully through our ongoing ethnographic work.  However, the evidence base illustrates that in the case of IPCC, projections of climate outcomes have generally been on the mark or indicated a lower level of change than actual outcomes years later.             Last Modified: 10/29/2013       Submitted by: Michael Oppenheimer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
