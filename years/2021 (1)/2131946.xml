<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Holistic Design of High-performance and Energy-efficient Accelerators for Graph Neural Networks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2021</AwardEffectiveDate>
<AwardExpirationDate>09/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuanyuan Yang</SignBlockName>
<PO_EMAI>yyang@nsf.gov</PO_EMAI>
<PO_PHON>7032928067</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Graph Neural Networks (GNNs) have emerged as one of the most powerful techniques for next-generation learning systems, and are gaining attention in many high-impact domains such as graph mining (graph machine, graph clustering), biology (drug discovery, disease classification), traffic networks (traffic prediction), recommendation systems (user-item prediction, social recommendation), e-commerce analysis, stock market prediction, natural language processing (text classification, neural machine translation), image processing (image classification, object detection, semantic segmentation), and autonomous systems, among many others. The explosive growth of these applications has created an enormous demand for customized accelerator design to satisfy the computational requirements of GNNs, since many of these applications require high-throughput and energy-efficient GNN inference. Conventional deep neural network (DNN) accelerators cannot efficiently process GNNs due to the combination of irregular memory accesses, dynamic parallelism imposed by the graph structure, and the dense computation in learning algorithms. This project addresses these challenges with a holistic design framework spanning architecture study, Network-on-Chip (NoC) design, machine-learning algorithms development, and algorithm-architecture co-optimization with the aim of designing energy-efficient and high-performance accelerator architectures for GNNs. The cross-cutting nature of this project will offer valuable insights and solutions to many critical problems in GNN accelerator design. The research will also play a major role in education by integrating discovery with teaching and training. The outcomes of this project will be widely disseminated to researchers, engineers, and educators through technical publications and presentations. &lt;br/&gt;&lt;br/&gt;The goal of this project is to develop GNN accelerators with much-improved performance and energy efficiency for a wide variety of graph-based machine learning applications. To achieve this goal, this project proposes: (1) the design of a morphable GNN accelerator architecture and a reconfigurable NoC to satisfy the computational demands of various GNNs, (2) the development of a GNN accelerator/algorithm co-optimization exploration framework to maximize both inference accuracy and performance (latency, area, energy, etc.) for given graph-based machine learning tasks, (3) the development of an extensive modeling and simulation framework for GNN accelerators that will be used to validate the proposed design approach, and (4) the implementation of a small-scale prototype of the proposed accelerator using Field Programmable Gate Arrays (FPGAs) and its application to real-world problems. This timely research will greatly advance the state-of-the-art of GNN acceleration, benefit both the computing and machine-learning communities, and provide strong implications on advancements in society and the US computing industry-at-large.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/27/2021</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2131946</AwardID>
<Investigator>
<FirstName>Ahmed</FirstName>
<LastName>Louri</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ahmed Louri</PI_FULL_NAME>
<EmailAddress>louri@email.gwu.edu</EmailAddress>
<PI_PHON>2029946083</PI_PHON>
<NSF_ID>000465038</NSF_ID>
<StartDate>08/27/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>George Washington University</Name>
<CityName>Washington</CityName>
<ZipCode>200520086</ZipCode>
<PhoneNumber>2029940728</PhoneNumber>
<StreetAddress>1922 F Street NW</StreetAddress>
<StreetAddress2><![CDATA[4th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043990498</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGE WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>043990498</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[George Washington University]]></Name>
<CityName>Washington DC</CityName>
<StateCode>DC</StateCode>
<ZipCode>200520042</ZipCode>
<StreetAddress><![CDATA[800 22nd Street NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~500000</FUND_OBLG>
</Award>
</rootTag>
