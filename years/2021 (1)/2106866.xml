<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: HCC: Medium: TouchBots for Surface Haptics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2021</AwardEffectiveDate>
<AwardExpirationDate>09/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>399999.00</AwardTotalIntnAmount>
<AwardAmount>399999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Balakrishnan Prabhakaran</SignBlockName>
<PO_EMAI>bprabhak@nsf.gov</PO_EMAI>
<PO_PHON>7032924847</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will lead to a new class human-machine interfaces that provide tactile feedback to fingertips. Fingertips are remarkable multimodal sensors capable of detecting pressure and vibration, local defor-mation such as braille cells, hardness/softness, warmth/coolness, and endless types of textures.  There are strong reasons to exploit these sensory capabilities in interfaces: making touch screens more accessi-ble for the vision impaired, making it easier to use touch interfaces in automobiles, providing touch feedback in augmented and virtual reality, and supporting remote touch in social, medical and retail applications.  The wealth of commercially important use cases has led to a fast-growing market for touch feedback (“haptic”) technologies, yet none of the existing approaches engages more than a small fraction of the fingertips’ capabilities.  The new interfaces to be developed – TouchBots – will provide a greatly expanded suite of haptic feedback modalities.  A TouchBot is a miniaturized module that is placed be-tween a fingertip and a touch surface, such as a touchscreen or trackpad.  It includes two main subsys-tems, one that guides the finger along a programmable path, and another that provides a sense of the shape and texture of objects along that path.  Together, these subsystems will enable TouchBots to offer many new haptic interactions such as touch-typing interfaces without keyboards, realistic surface tex-tures for virtual reality, and fully programmable braille and tactile graphics anywhere there is a touch surface. These interactions have the potential to make touch screen devices accessible to the vision im-paired. Recognizing that a new generation of technological innovators will be needed to commercialize the fruits of this research, close ties will be forged to graduate-level curricula in innovation, leading to impact-minded individuals who are well-positioned to provide leadership in the growing haptics indus-try.&lt;br/&gt;&lt;br/&gt;The capabilities of a TouchBot stem from the manner in which it interacts with the underlying surface. The proposed TouchBot design include: (a) kinesthetic subsystem that provides feedback via passively rolling, but actively steered, wheels; (b) a cutaneous module that provides feedback via an array of tiny "pucks"; (c) selective brake mechanism for each of these pucks using electroadhesion. This approach, known as “cobotic,” has been thoroughly developed for macro-scale devices, and will be adapted here to the meso-scale by careful integration of steering actuation, wheel design, and intent sensing.  Touch-bot will be extremely compact, with a low-power, and high-performance system for motion guidance. The ability of the TouchBot to provide local shape and texture will be provided by an array of tiny “pucks,” using the selective breaking mechanism. This research will leverage prior efforts in areas as diverse as surface haptics, wall-climbing robots and data storage read-write heads to realize an electro-adhesive device that is high-force and high-bandwidth and that can be used to control a high-density tactile array. By bringing these technologies together into fingertip-scale devices, it will be possible to demonstrate a wide range of surface haptic interactions including buttons, toggles, raised line drawings, braille characters, tactile graphics, and textures. Participatory design methods will be used to identify potential uses of TouchBot in the areas such as automotive and user interaction design. Experience proto-type and rapid iteration will be used to create stimuli that evoke rich feedback.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/09/2021</MinAmdLetterDate>
<MaxAmdLetterDate>07/09/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2106866</AwardID>
<Investigator>
<FirstName>Mary</FirstName>
<LastName>Hipwell</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mary C Hipwell</PI_FULL_NAME>
<EmailAddress>cynthia.hipwell@tamu.edu</EmailAddress>
<PI_PHON>9794582564</PI_PHON>
<NSF_ID>000780414</NSF_ID>
<StartDate>07/09/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas A&amp;M Engineering Experiment Station</Name>
<CityName>College Station</CityName>
<ZipCode>778454645</ZipCode>
<PhoneNumber>9798626777</PhoneNumber>
<StreetAddress>400 Harvey Mitchell Pkwy S</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>847205572</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEXAS A&amp;M ENGINEERING EXPERIMENT STATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042915991</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Texas A&M Engineering Experiment Station]]></Name>
<CityName>College Station</CityName>
<StateCode>TX</StateCode>
<ZipCode>778430001</ZipCode>
<StreetAddress><![CDATA[600 Discovery Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~399999</FUND_OBLG>
</Award>
</rootTag>
