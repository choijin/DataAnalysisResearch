<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CHS: Small: Collaborative Research: Validating and Communiciating Model-Based Approaches for Data Visualization Ability Assessment</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>238848.00</AwardTotalIntnAmount>
<AwardAmount>121869</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Andruid Kerne</SignBlockName>
<PO_EMAI>akerne@nsf.gov</PO_EMAI>
<PO_PHON>7032928574</PO_PHON>
</ProgramOfficer>
<AbstractNarration>People are encountering graphs, charts, and other visual representations of data now more than ever before. Yet creators of these visualizations currently must reason with sparse and conflicting evidence on how well people can read the visualizations they publish.  Current guidelines do not take into account the possibility that different people have different strengths and weaknesses when interpreting visual data. This project will use studies of visualization effectiveness to inform our understanding of the abilities and biases of viewers, both individually and collectively.  To do this, the project team will use a combination of experiments, statistical modeling, and interview studies to both challenge long-standing assumptions about visualization effectiveness, and to lay a foundation for future experiments that account for differences in visualization reading ability. The work will also support a broader educational goal of using robust statistical modeling techniques in experimentation, through course modules that can be integrated into existing data visualization courses, and through outreach activities that allow individuals to see how well they perform visualization tasks compared to others who have taken the experiments.&lt;br/&gt;&lt;br/&gt;This work seeks to answer three primary research questions. The first is to determine the extent to which individuals differ in their ability to perform basic tasks with data visualizations, through large-scale crowdsourced experiments that use transparent statistical methodologies to establish individual differences in data visualization performance. The second question evaluates the relationship between low-level visualization performance and higher-level assessments such as visualization literacy and cognitive abilities, recruiting both expert and novice populations to evaluate the extent to which these hypothesized measures of visualization literacy correlate with each other. The third question determines how alternative ways of presenting visualization experiment results shape the design recommendations researchers and designers draw from them, through a comparative evaluation of longstanding ways of presenting visualization experiment results, and by designing new ways of presenting results that may lead to more mature interpretation of experiment results by broader visualization community. The work will provide new perspectives on visualization literacy by augmenting chart reading experiments with novel measures of visualization ability, and by studying how creators currently make use of existing visualization design guidelines in their design process.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>05/11/2021</MinAmdLetterDate>
<MaxAmdLetterDate>05/11/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2120750</AwardID>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Kay</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew Kay</PI_FULL_NAME>
<EmailAddress>mjskay@northwestern.edu</EmailAddress>
<PI_PHON>3126232125</PI_PHON>
<NSF_ID>000734010</NSF_ID>
<StartDate>05/11/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606114579</ZipCode>
<StreetAddress><![CDATA[750 N. Lake Shore Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~121869</FUND_OBLG>
</Award>
</rootTag>
