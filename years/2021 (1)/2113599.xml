<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Slow Kill for Big Data Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2021</AwardEffectiveDate>
<AwardExpirationDate>08/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>170000.00</AwardTotalIntnAmount>
<AwardAmount>170000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Pena Edsel</SignBlockName>
<PO_EMAI>epena@nsf.gov</PO_EMAI>
<PO_PHON>7032928080</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Big-data applications typically involve large numbers of samples and features and are often contaminated with outliers, posing challenges for variable selection and parameter estimation.  Fitting a sparse model with a prescribed cardinality is a common request in practice, but it is associated with solving a highly nonconvex and discrete problem. Using multiple starting points in such nonconvex optimization is common, but is often computationally prohibitive on big data; new cost-effective techniques are needed to alleviate the starting point requirement and ensure the best statistical accuracy. Moreover, how to adjust an arbitrarily given loss function to guard against gross outliers and achieve a high break-down point poses a major challenge for modern-day data analysis. The project will study innovative and efficient statistical methods and perform rigorous theoretical analysis to answer these questions. In this project, education is tightly coupled with research, consisting of course development, student mentoring, outreach, and recruiting underrepresented students.&lt;br/&gt;&lt;br/&gt;The project will propose a novel slow-kill technique for large-scale variable selection, motivated by a scalable optimization algorithm with iteration-varying threshold and simultaneous L2-regularization. The three main elements of progressive quantile control, growing learning rate and adaptive L2-shrinkage in slow kill have solid theoretical support, and its ability to reduce the problem size during the iteration, as opposed to boosting and forward pathwise algorithms, makes it attractive for big data. The interplay between statistics and optimization in the project will reveal tight error rates and fast convergence under some regularity conditions, without the need to pursue a globally optimal solution. Furthermore, a framework of outlier-resistant estimation will be introduced to robustify a given method beyond the standard likelihood setup. It has a close connection to the method of trimming but includes explicit outlyingness parameters for all samples, which in turn facilitates computation and theory. With slow kill, the number of data resamplings will be substantially reduced, and the obtained resistant estimators can enjoy minimax rate optimality in both low and high dimensions. Overall, the proposed research will create a new-generation high dimensional tool for robust sparse learning that can accommodate coherent designs and gross outliers in big data applications, to deepen and broaden existing methods and theory in statistics and optimization.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/11/2021</MinAmdLetterDate>
<MaxAmdLetterDate>06/11/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2113599</AwardID>
<Investigator>
<FirstName>Yiyuan</FirstName>
<LastName>She</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yiyuan She</PI_FULL_NAME>
<EmailAddress>yshe@stat.fsu.edu</EmailAddress>
<PI_PHON>8506443218</PI_PHON>
<NSF_ID>000549795</NSF_ID>
<StartDate>06/11/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Florida State University</Name>
<CityName>Tallahassee</CityName>
<ZipCode>323064166</ZipCode>
<PhoneNumber>8506445260</PhoneNumber>
<StreetAddress>874 Traditions Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790877419</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>FLORIDA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Florida State University]]></Name>
<CityName>Tallahassee</CityName>
<StateCode>FL</StateCode>
<ZipCode>323064330</ZipCode>
<StreetAddress><![CDATA[117 N Woodward Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Fund>
<Code>01002122DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2021~170000</FUND_OBLG>
</Award>
</rootTag>
