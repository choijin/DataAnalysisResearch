<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Small: Self-Adaptive Optimization Algorithms with Fast Convergence via  Geometry-Adapted Hyper-Parameter Scheduling</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2021</AwardEffectiveDate>
<AwardExpirationDate>06/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>411248.00</AwardTotalIntnAmount>
<AwardAmount>411248</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Scott Acton</SignBlockName>
<PO_EMAI>sacton@nsf.gov</PO_EMAI>
<PO_PHON>7032922124</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Machine-learning and artificial-intelligence techniques have been widely applied in modern society to enhance quality of lifr. In these applications, machine-learning models such as neural networks are trained on a large dataset using various optimization algorithms, which iteratively adjust the model parameters and converge to a good model. In particular, the convergence of these optimization algorithms often relies on choosing a good set of hyper-parameters. For example, one important algorithm hyper-parameter is the step size, which controls the scale of the update applied to the model parameters in every iteration, and it must be carefully chosen to avoid slow convergence and possible divergence.  In practice, these algorithm hyper-parameters either are guided by optimization theory or are set through manual fine-tuning. While theory-guided algorithm hyper-parameters often rely on certain unknown geometrical information of the model and are often too conservative, resulting in result in slow convergence, manually fine-tuned algorithm hyper-parameters critically depend on the specific application and algorithm, and often introduce much computation overhead. This project aims to address these issues by developing a principled, computation-light and effective hyper-parameter scheduling scheme for different types of optimization algorithms to achieve fast and stable convergence. The developed adapted hyper-parameter scheduling scheme is intended to facilitate machine-learning practitioners tuning the algorithm hyper-parameters and dynamically adapt them to the ongoing optimization process. This has further positive impact on implementation of large-scale machine learning applications such as autonomous driving, training adversary-robust models, robust decision making in finance and control, etc. &lt;br/&gt;&lt;br/&gt;In this project, the researchers are developing a principled and efficient algorithm hyper-parameter scheduling framework that jointly adapts different algorithm hyper-parameters to the local geometry of the nonconvex objective function for a variety of popular optimization algorithms, and corroborate them with strong theoretical convergence guarantees in nonconvex machine learning. Specifically, the researchers are developing such geometry-adapted hyper-parameter scheduling scheme for deterministic optimization algorithms, including first-order gradient-based algorithms, accelerated gradient algorithms and second-order Newton-type algorithms. The researchers are developing new analysis tools that advance the understanding of the relation between hyper-parameters and the dynamic optimization process. Iteration and computation complexities of these algorithms is being established in nonconvex optimization. Based on this development, the researchers are extending the adapted hyper-parameter scheduling scheme to stochastic optimization algorithms, which use mini-batch random sampling and therefore necessitate a joint scheduling of step-size and batch size. Analysis of sample complexity and high probability convergence guarantee is being established for these algorithms. Furthermore, these developments are guiding the design of adapted hyper-parameter scheduling scheme for gradient-based minimax optimization algorithms.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>05/28/2021</MinAmdLetterDate>
<MaxAmdLetterDate>05/28/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2106216</AwardID>
<Investigator>
<FirstName>Yi</FirstName>
<LastName>Zhou</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yi Zhou</PI_FULL_NAME>
<EmailAddress>yi.zhou@utah.edu</EmailAddress>
<PI_PHON>3157516542</PI_PHON>
<NSF_ID>000805869</NSF_ID>
<StartDate>05/28/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Utah</Name>
<CityName>SALT LAKE CITY</CityName>
<ZipCode>841128930</ZipCode>
<PhoneNumber>8015816903</PhoneNumber>
<StreetAddress>75 S 2000 E</StreetAddress>
<StreetAddress2><![CDATA[Second Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009095365</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF UTAH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009095365</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName>Salt lake city</CityName>
<StateCode>UT</StateCode>
<ZipCode>841128930</ZipCode>
<StreetAddress><![CDATA[75 S 2000 E]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~411248</FUND_OBLG>
</Award>
</rootTag>
