<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: CCRI:NEW: Research Infrastructure for Real-Time Computer Vision and Decision Making via Mobile Robots</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2021</AwardEffectiveDate>
<AwardExpirationDate>09/30/2024</AwardExpirationDate>
<AwardTotalIntnAmount>229565.00</AwardTotalIntnAmount>
<AwardAmount>229565</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project will create a research infrastructure for computer vision and real-time control of autonomous mobile robots (both aerial and ground). The infrastructure includes four integrated components: (1) A Purdue laboratory decorated as miniature cities. (2) Simulators that reflect the physical laboratory. (3) Programmable aerial robots with the same interface as the simulators. (4) Sample solutions for research on artificial intelligence, computer vision, and robot control for evaluation and comparison. This infrastructure will be available to the research community in multiple ways: (1) Users can evaluate their solutions with the simulators in a safe virtual environment. (2) Users can upload their control programs and this team will launch the robots inside Purdue's laboratory. Users can observe the robots remotely using the high-speed cameras already deployed in the laboratory. (3) Users can bring their own robots to the laboratory and conduct experiments. (4) This project will create competitions for researchers to demonstrate their solutions using autonomous mobile robots in simulated emergency and rescue scenarios. The competitions will use miniature buildings and people for the robots to recognize and count objects (such as number of people, vehicles, and houses), assess situations (such as the number of collapsed bridges), while avoiding obstacles.&lt;br/&gt;&lt;br/&gt;This infrastructure will be available for investigating a wide range of research topics, including (1) real-time computer vision and control. The decorated laboratory will allow researchers to evaluate their solutions for real-time vision and control methods using active computer vision, navigation, and semantic segmentation in a three-dimensional environment. (2) simulation of robot fleets. Users can evaluate and improve their methods in a safe virtual environment before deployment. (3) This infrastructure will integrate virtual and physical environments so that solutions running in the simulators can be ported directly to the physical robots for experiments. (4) collision avoidance, multi-robot coordination, emergency response, computer security, and efficient machine learning on embedded systems. (5) agriculture, city planning, emergency response, and inspection of civil structures. This project will build STEM talents because autonomous robots and visual data are naturally appealing to the general public. With the simulators, students at all levels can participate without the cost of purchasing physical robots. This research infrastructure will reduce the barriers to innovations. This infrastructure will also encourage innovations in machine learning that are efficient in energy and can be ported to resource constrained embedded systems such as aerial robots. The project will engage a broader audience including K-12 students as well because of the many applications described above.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/10/2021</MinAmdLetterDate>
<MaxAmdLetterDate>08/10/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2120333</AwardID>
<Investigator>
<FirstName>Yiran</FirstName>
<LastName>Chen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yiran Chen</PI_FULL_NAME>
<EmailAddress>yiran.chen@duke.edu</EmailAddress>
<PI_PHON>6127039849</PI_PHON>
<NSF_ID>000575362</NSF_ID>
<StartDate>08/10/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277054010</ZipCode>
<StreetAddress><![CDATA[2200 W. Main St, Suite 710]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~229565</FUND_OBLG>
</Award>
</rootTag>
