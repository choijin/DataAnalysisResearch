<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC-Small: Understanding and Supporting Online Question-Answering Sites</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
<AwardExpirationDate>02/28/2013</AwardExpirationDate>
<AwardTotalIntnAmount>445519.00</AwardTotalIntnAmount>
<AwardAmount>483531</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Crowston</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Online Question-Answering Sites (Q&amp;A sites, for short) are a new and rapidly growing piece of the community-created content landscape. From Knowledge iN in South Korea to Microsoft's Live QnA in the U.S., to Yahoo! Answers in 26 countries world-wide, users are flocking to sites where they can post questions and get answers. Yahoo! Answers alone attracts about 18 million unique visitors monthly and has accumulated over 400 million answers since its launch in 2005. Not only do these sites meet individual needs for information, the content they generate is an important source for online search and knowledge discovery. However, a casual browse through any one site reveals significant noise in the signal. Too many questions receive sarcastic or even insulting answers. Too often, it seems, users re-ask a question rather than finding value in the answers already posed.&lt;br/&gt;&lt;br/&gt;Because the dramatic growth of Q&amp;A sites is so recent, there has been little opportunity to empirically investigate them as a new information resource and as a new type of social space. This project examines five Q&amp;A sites using a variety of observational and experimental methods to gain an understanding of how users interact in and with these sites while also developing tools to help users better meet their goals. Specifically, this project will: 1.) identify structures or properties in questions posed on Q&amp;A sites that affect response characteristics such as quantity, quality, and timeliness; and explore the use of templates, critics, and bots to help question-askers obtain better responses; 2.) identify structures or properties in response threads that suggest high risk of failure; and explore the use of bots to intervene and "rescue" derailed Q&amp;A threads; 3.) understand the lifecycle of Q&amp;A site participants, including their social interaction on the sites; and develop tools to help support user integration into online Q&amp;A communities.&lt;br/&gt;&lt;br/&gt;Given their importance as an information resource and social phenomenon, understanding online Q&amp;A sites has intellectual merit in its own right, in addition: this work advances an important line of research on online and computer-mediated communications that has helped rhetoric and communications experts contribute to the design of more effective online communication tools. It will also be the first work to study online Q&amp;A from both a social and an information resource perspective, giving new insight into the nature of online voluntary knowledge creation.&lt;br/&gt;&lt;br/&gt;Broader Impacts: Online question-answering sites have become an important source of information and advice for individuals and businesses, as well as an important source of content for web search engines. By understanding these sites and developing tools to support their continued successful operation, we will help Q&amp;A community designers understand approaches that can both promote beneficial social experiences for users and the construction of valuable community-contributed repositories of knowledge.</AbstractNarration>
<MinAmdLetterDate>08/28/2008</MinAmdLetterDate>
<MaxAmdLetterDate>06/15/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0812148</AwardID>
<Investigator>
<FirstName>Joseph</FirstName>
<LastName>Konstan</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joseph A Konstan</PI_FULL_NAME>
<EmailAddress>konstan@cs.umn.edu</EmailAddress>
<PI_PHON>6126251831</PI_PHON>
<NSF_ID>000155041</NSF_ID>
<StartDate>08/28/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>John</FirstName>
<LastName>Logie</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John H Logie</PI_FULL_NAME>
<EmailAddress>logie@umn.edu</EmailAddress>
<PI_PHON>6126245599</PI_PHON>
<NSF_ID>000320694</NSF_ID>
<StartDate>08/28/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<StateCode>MN</StateCode>
<ZipCode>554552070</ZipCode>
<StreetAddress><![CDATA[200 OAK ST SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~445519</FUND_OBLG>
<FUND_OBLG>2009~16000</FUND_OBLG>
<FUND_OBLG>2010~22012</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project was inspired by the wide variety of outcomes in existing online social question-answering communities -- communities ranging from broad interest ones such as Yahoo! Answers and Ask Metafilter to narrower topical communities such as StackOverflow (for programming) and TurboTax Live Community (for tax preparation).&nbsp; While we, and others, had done some initial research that showed experimentally that these communities are capable of producing high quality answers--even to challenging question, we did not know enough about how they work, and particularly about what motivates the most valuable community members to contribute substantial time and effort to helping others through question-answering and in the process to creating an archive of searchable information that others later use to find information.</p> <p>Our approach was two-pronged.&nbsp; Through a collaboration between computer scientists and rhetoricians we were able to study the communication and behaviors of online question asking while also developing computational tools to identify promising contributors, to identify interesting question-asking situations, and the like.&nbsp;</p> <p>Among the results of this interdisciplinary collaboration were the following outcomes:</p> <p>*&nbsp; Development and validation of the first comprehensive taxonomy of question types asked online.&nbsp; This six-category taxonomy is both interesting in its own right and useful for processing questions.&nbsp; We've empirically shown that different categories of questions inspire different degrees of response, produce different levels of reusable content, and fit with or are out of place in different communities.&nbsp; We also have developed classifiers that classify parts of this taxonomy.&nbsp;</p> <p>*&nbsp; Development of a suite of data mining and machine learning tools for identifying promising participants in a question-answering community--specifically, people who have high potential to become top-level question answerers.&nbsp; We experimented with a variety of different techniques for identifying such people (ratings of initial answers, frequency of answering, etc.), but eventually developed a new measure that turned out to outperform the rest--high performers are particularly good at directing their effort to where it is useful (i.e., answering questions that need answers, rather than piling another answer onto the most-answered ones).&nbsp;</p> <p>*&nbsp; A lifecycle study of high contributors, bringing together a mix of survey research, interviews, and artifact analysis to understand how top-contributors behave and why they behave that way.&nbsp; Among the interesting findings is the fact that top contributors are far from uniform--there are different roles that people settle into, each of them useful and based on different skills and interests.</p> <p>*&nbsp; Along the way, we developed a number of useful computational tools.&nbsp; We developed classifiers to idenify the likely lifespan of a question (i.e., how long answers will be good).&nbsp; Such a classifier can distinguish a question such as "How many Japanese civilians were killed in WWII?", which has a long lifespan, from "Who are the Twins playing this evening?", which has a short one. We also experimented with different techniques for entering items from restricted sets (e.g., movies) to find ones that are most successful without distracting users from their underlying question or answer.</p> <p>Along the way, we are proud to have conducted research in a manner designed to educate as many students as possible.&nbsp; We trained five doctoral students and twelve undergraduates in research, exposing them to the excitement and challenges of interdisciplinary research.&nbsp; All the doctoral students and many of the undergraduates succeeded to the point of publishing peer-reviewed research papers, and many had the experience of making oral pre...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project was inspired by the wide variety of outcomes in existing online social question-answering communities -- communities ranging from broad interest ones such as Yahoo! Answers and Ask Metafilter to narrower topical communities such as StackOverflow (for programming) and TurboTax Live Community (for tax preparation).  While we, and others, had done some initial research that showed experimentally that these communities are capable of producing high quality answers--even to challenging question, we did not know enough about how they work, and particularly about what motivates the most valuable community members to contribute substantial time and effort to helping others through question-answering and in the process to creating an archive of searchable information that others later use to find information.  Our approach was two-pronged.  Through a collaboration between computer scientists and rhetoricians we were able to study the communication and behaviors of online question asking while also developing computational tools to identify promising contributors, to identify interesting question-asking situations, and the like.   Among the results of this interdisciplinary collaboration were the following outcomes:  *  Development and validation of the first comprehensive taxonomy of question types asked online.  This six-category taxonomy is both interesting in its own right and useful for processing questions.  We've empirically shown that different categories of questions inspire different degrees of response, produce different levels of reusable content, and fit with or are out of place in different communities.  We also have developed classifiers that classify parts of this taxonomy.   *  Development of a suite of data mining and machine learning tools for identifying promising participants in a question-answering community--specifically, people who have high potential to become top-level question answerers.  We experimented with a variety of different techniques for identifying such people (ratings of initial answers, frequency of answering, etc.), but eventually developed a new measure that turned out to outperform the rest--high performers are particularly good at directing their effort to where it is useful (i.e., answering questions that need answers, rather than piling another answer onto the most-answered ones).   *  A lifecycle study of high contributors, bringing together a mix of survey research, interviews, and artifact analysis to understand how top-contributors behave and why they behave that way.  Among the interesting findings is the fact that top contributors are far from uniform--there are different roles that people settle into, each of them useful and based on different skills and interests.  *  Along the way, we developed a number of useful computational tools.  We developed classifiers to idenify the likely lifespan of a question (i.e., how long answers will be good).  Such a classifier can distinguish a question such as "How many Japanese civilians were killed in WWII?", which has a long lifespan, from "Who are the Twins playing this evening?", which has a short one. We also experimented with different techniques for entering items from restricted sets (e.g., movies) to find ones that are most successful without distracting users from their underlying question or answer.  Along the way, we are proud to have conducted research in a manner designed to educate as many students as possible.  We trained five doctoral students and twelve undergraduates in research, exposing them to the excitement and challenges of interdisciplinary research.  All the doctoral students and many of the undergraduates succeeded to the point of publishing peer-reviewed research papers, and many had the experience of making oral presentations of their research.  Finally, we engaged in outreach to the broader research community, participating in a variety of panels and workshops and publishing broad-interest articles to hel...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
