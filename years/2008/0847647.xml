<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Random matrices and High-dimensional statistics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2009</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>400001.00</AwardTotalIntnAmount>
<AwardAmount>400001</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research program is focused on the development of data analysis methods and of a theoretical framework for the new paradigm of high-dimensional statistical problems. The theoretical problems are concerned with spectral properties of large dimensional random matrices. More precisely, four of the main objectives of the program are: 1) further develop new covariance estimation methods;  2) further our understanding of the spectral properties of relevant large random matrices; 3) find and contribute to areas of application where this high-dimensional statistics framework is relevant; 4) train graduate students in high-dimensional statistics and make undergraduate students at least aware of possible pitfalls of classical methods and of better alternatives when available. More specifically, statisticians are now often faced with "n by p" data matrices X, for which p, the number of variables recorded per observations, is of the same order of magnitude as n, the number of recorded observations, and p and n are both large. The sample   covariance matrix computed from this data is of great importance to a number of applications, as it underlies widely used methods like principal components analysis. However, the theoretical results which underlie the method, classically developed in the "small p and large n" setting, fail to apply in the "large n and large p" setting just described. Hence, a thorough study of sample covariance matrices in this setting is needed. Eigenvalues of such large dimensional matrices are of particular interest. The investigator plans to launch a multi-pronged effort to get at various kinds of properties of these objects: for instance, he plans to develop theoretical results that will allow inferential work to be done from computation of extreme eigenvalues of sample covariance matrices, develop new methods of estimation of the whole covariance matrix, and also work on the impact of naively plugging-in the sample covariance matrix as a proxy for the population covariance in certain optimization problems which depend on this latter parameter. An effort will be made to try and apply this theoretical work to real-world problems, both to raise awareness in applied communities about the pitfalls associated with high-dimensional covariance matrices, and to shape the models that will be studied to be of most relevance to applied researchers.&lt;br/&gt;&lt;br/&gt;Technological progress allows us to store and use massive amounts of data about many aspects of our daily lives. An interesting problem is to use the data to understand how certain traits depend on each other. In the stock market, we might be interested in how the behavior of one stock affects the behavior of another stock; understanding all these interrelationships leads to having a measure of the risk taken by investing in portfolios that use the corresponding stocks. Statisticians have a number of tools to deal with all these interrelationships. We can discover ways to look at the data so that, even if all interrelationships are small or weak, so each trait "should" not help us learn too much about any other trait, we might still find combinations of the traits that carry enormous amounts of information. We also know what typical values for these combinations are, so we might be able to detect unusual features in the data set by looking at it the right way. Those statistical techniques have very wide applications in various fields of science, ranging from climatology to genetics, image recognition, finance etc... Thousands of research papers are published each year that use these techniques. However, the theory that underlies these statistical techniques was created in an era where massive datasets just did not exist. This research project is focusing on theories and their applications that are better suited to handle our current massive datasets. The applications should allow us to see structure where the classical tools fail to see any and tell us when there is no structure when the classical tools tell us there is. We also have increasing evidence that our standard tools give us often very inaccurate results about our standard measures of risk or amount of information carried in combination of traits. It seems that risks might be underestimated and amount of information might be overestimated. Part of this research program will be dedicated to measuring how inaccurate the classical results are for large datasets, how much practical predictions are affected, and how a more relevant theory can be used for correcting these inaccuracies.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>05/12/2009</MinAmdLetterDate>
<MaxAmdLetterDate>07/30/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0847647</AwardID>
<Investigator>
<FirstName>Noureddine</FirstName>
<LastName>El Karoui</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Noureddine El Karoui</PI_FULL_NAME>
<EmailAddress>nkaroui@stat.berkeley.edu</EmailAddress>
<PI_PHON>5106423332</PI_PHON>
<NSF_ID>000488593</NSF_ID>
<StartDate>05/12/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName>BERKELEY</CityName>
<StateCode>CA</StateCode>
<ZipCode>947101749</ZipCode>
<StreetAddress><![CDATA[Sponsored Projects Office]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>1263</Code>
<Text>PROBABILITY</Text>
</ProgramElement>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~31652</FUND_OBLG>
<FUND_OBLG>2010~87594</FUND_OBLG>
<FUND_OBLG>2011~91388</FUND_OBLG>
<FUND_OBLG>2012~95629</FUND_OBLG>
<FUND_OBLG>2013~93738</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The work supported by this grant was aimed at understanding the performance of statistical methods when one is dealing with datasets that have a lot of observations and a lot of measurements per observation. <br /><br />A typical example would be looking at the daily returns of the components of the S&amp;P 500 index for one year: there would be roughly 250 observations (one per day where the stock market is open) and about 500 measurements per observation (the daily returns of the 500 companies making up the S&amp;P 500 index). In this example, we have n=250 observations and p=500 measurements per observation. <br /><br />The classical theory of statistics, which underlies much of data analysis that is performed in practice, has been developed in the setting where n, the number of observations in the dataset, is much much larger than the number of measurements per observation, p. So the ratio p/n is essentially 0. The aim of the grant was to get an understanding of how widely used statistical methods perform when the ratio p/n is not small. One motivation in particular was that in practice p/n is not always small. So having a theory that reflects that important fact should shed a more accurate light on what is happening practically.<br /><br />The short answer is that in the setting investigated in the grant, our intuition, built on the small p/n case, is completely misleading. And many unexpected phenomena occur in the p/n not small case. <br /><br />The key findings can be summarized as followed:<br /><br />- in financial theory, a well-known approach to choosing an optimal portfolio is to rely on Markowitz's theory of portfolio optimization. Work done under this grant showed that the naive application of Markowitz's theory to financial returns data yield portfolios whose risk is severely underestimated (at least under cetrain assumptions). <br /><br />- in statistics, a key principle is that of maximum-likelihood: one assumes that the data is generated according to a certain random mechanism, that depends on unknown non-random parameters. A natural way to estimate these parameters is to compute the probability that we see a dataset like the one we saw for various values of these unknown parameters and pick as our estimate of the unknown parameters the set of parameters that maximize the probability of seeing the dataset we have at hand. A fundamental result is that when we have p parameters and n observations, with p/n small, this approach to estimating the parameters is not only natural, it is also optimal. I studied with co-authors this question in the setting where the ratio p/n is not small, for one of the most important methods in statistics, namely linear regression. We showed that maximum-likelihood methods are not optimal when p/n is not small and found ways of improving upon them, in certain settings pertaining to how the data is generated. It&nbsp; suggests new methods and approaches to attempt to extract more information from existing datasets and new ways to shape our intuition for these hard statistical problems.<br /><br />- another central tool in statistics is the bootstrap. The estimate of a parameter described in the previous paragraph is important, but what is equally important is to know how much uncertainty is associated with this estimate. In other words, how accurate our estimate is. A very widely used approach to quantify this uncertainty is to use the bootstrap: shuffle the data appropriately many times to create many new datasets, estimate the quantity of interest on each of these new datasets, and use all these estimates to see how accurate our original estimate was. In the setting where the number of quantities to estimate, p, is small compared to the number of observations, n, this method is known to work well for a broad class of problem. Namely, it yields the same uncertainty assessment as we would have gotten had we known the dat...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The work supported by this grant was aimed at understanding the performance of statistical methods when one is dealing with datasets that have a lot of observations and a lot of measurements per observation.   A typical example would be looking at the daily returns of the components of the S&amp;P 500 index for one year: there would be roughly 250 observations (one per day where the stock market is open) and about 500 measurements per observation (the daily returns of the 500 companies making up the S&amp;P 500 index). In this example, we have n=250 observations and p=500 measurements per observation.   The classical theory of statistics, which underlies much of data analysis that is performed in practice, has been developed in the setting where n, the number of observations in the dataset, is much much larger than the number of measurements per observation, p. So the ratio p/n is essentially 0. The aim of the grant was to get an understanding of how widely used statistical methods perform when the ratio p/n is not small. One motivation in particular was that in practice p/n is not always small. So having a theory that reflects that important fact should shed a more accurate light on what is happening practically.  The short answer is that in the setting investigated in the grant, our intuition, built on the small p/n case, is completely misleading. And many unexpected phenomena occur in the p/n not small case.   The key findings can be summarized as followed:  - in financial theory, a well-known approach to choosing an optimal portfolio is to rely on Markowitz's theory of portfolio optimization. Work done under this grant showed that the naive application of Markowitz's theory to financial returns data yield portfolios whose risk is severely underestimated (at least under cetrain assumptions).   - in statistics, a key principle is that of maximum-likelihood: one assumes that the data is generated according to a certain random mechanism, that depends on unknown non-random parameters. A natural way to estimate these parameters is to compute the probability that we see a dataset like the one we saw for various values of these unknown parameters and pick as our estimate of the unknown parameters the set of parameters that maximize the probability of seeing the dataset we have at hand. A fundamental result is that when we have p parameters and n observations, with p/n small, this approach to estimating the parameters is not only natural, it is also optimal. I studied with co-authors this question in the setting where the ratio p/n is not small, for one of the most important methods in statistics, namely linear regression. We showed that maximum-likelihood methods are not optimal when p/n is not small and found ways of improving upon them, in certain settings pertaining to how the data is generated. It  suggests new methods and approaches to attempt to extract more information from existing datasets and new ways to shape our intuition for these hard statistical problems.  - another central tool in statistics is the bootstrap. The estimate of a parameter described in the previous paragraph is important, but what is equally important is to know how much uncertainty is associated with this estimate. In other words, how accurate our estimate is. A very widely used approach to quantify this uncertainty is to use the bootstrap: shuffle the data appropriately many times to create many new datasets, estimate the quantity of interest on each of these new datasets, and use all these estimates to see how accurate our original estimate was. In the setting where the number of quantities to estimate, p, is small compared to the number of observations, n, this method is known to work well for a broad class of problem. Namely, it yields the same uncertainty assessment as we would have gotten had we known the data-generating mechanism. With a co-author, I showed that this was not the case when p/n is not small (for the case of linear regression fo...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
