<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research, II-NEW: An Instrumented Data Center Infrastructure for Research on Cross-Layer Autonomics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2009</AwardEffectiveDate>
<AwardExpirationDate>09/30/2013</AwardExpirationDate>
<AwardTotalIntnAmount>210000.00</AwardTotalIntnAmount>
<AwardAmount>210000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Theodore Baker</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This project's goal is to acquire and develop an instrumented datacenter testbed spanning the three sites of the NSF Center for Autonomic Computing (CAC)-the University of Florida (UF), the University of Arizona (UA) and Rutgers, the State University of New Jersey (RU). Datacenters are a growing component of society's IT infrastructure, including services related to health, banking, commerce, defense, education and entertainment. Annual energy and administration costs of today's datacenters amount to billions of dollars; high energy consumption also translates into excessive heat dissipation, which, in turn, increases cooling costs and increases servers' failure rates. The proposed testbed will enable a fundamental understanding of the operations of data centers and the autonomic control and management of their resources and services. The design of the underlying infrastructure reflects the natural heterogeneity, dynamism and distribution of real-world datacenters, and includes embedded instrumentation at all levels, including the platform, virtualization, middleware and application layers. Its scale and geographical distribution enables studies of challenges faced by datacenter applications, services, middleware and architectures related to both "scale-up" (increases in the capacity of individual servers) and "scale-out" (increases in the number of servers in the system). This testbed will enable fundamental and far-reaching research focused on cross-layer autonomics for managing and optimizing large-scale datacenters. The participant sites will contribute complementary expertise-UA at the resource level, UF at the virtualization layer, and RU in the area of services and applications. The collaboration between the university sites will bring coherence across ongoing separate research efforts and have a transformative impact on the modeling, formulation and solution of datacenter management problems, which have so far been considered mostly in terms of individual layers. &lt;br/&gt;The testbed will also provide a critical infrastructure for education at multiple levels, including providing students with hands-on experience via course projects, enable development of new advanced multi-university and cross-disciplinary courses, as well as multi-site group projects focused on end-to-end autonomics, which will use the proposed testbed. Students from underrepresented groups will be actively involved in the research and their participation will be increased through ongoing collaborations with minority institutions. Even broader community participation will result from an evolving partnership with the recently proposed industry cloud initiatives.</AbstractNarration>
<MinAmdLetterDate>09/21/2009</MinAmdLetterDate>
<MaxAmdLetterDate>08/03/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0855123</AwardID>
<Investigator>
<FirstName>Jose</FirstName>
<LastName>Fortes</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jose A Fortes</PI_FULL_NAME>
<EmailAddress>fortes@ufl.edu</EmailAddress>
<PI_PHON>3523929265</PI_PHON>
<NSF_ID>000415025</NSF_ID>
<StartDate>09/21/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Renato</FirstName>
<LastName>Figueiredo</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Renato J Figueiredo</PI_FULL_NAME>
<EmailAddress>renato@acis.ufl.edu</EmailAddress>
<PI_PHON>3523926430</PI_PHON>
<NSF_ID>000286908</NSF_ID>
<StartDate>09/21/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Florida</Name>
<CityName>GAINESVILLE</CityName>
<ZipCode>326112002</ZipCode>
<PhoneNumber>3523923516</PhoneNumber>
<StreetAddress>1 UNIVERSITY OF FLORIDA</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>969663814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Florida]]></Name>
<CityName>GAINESVILLE</CityName>
<StateCode>FL</StateCode>
<ZipCode>326112002</ZipCode>
<StreetAddress><![CDATA[1 UNIVERSITY OF FLORIDA]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~100000</FUND_OBLG>
<FUND_OBLG>2010~90000</FUND_OBLG>
<FUND_OBLG>2011~20000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1">The goal of this project has been to acquire and develop an instrumented data center testbed for research on self-managing autonomic computing techniques at different layers a distributed data center infrastructure. Data centers are a growing component of society's IT infrastructure, providing services related to health, banking, commerce, defense, education and entertainment. With continuous growth in size and complexity of data centers, the annual energy and administration costs of today's data centers amount to billions of dollars; high energy consumption also translates into excessive heat dissipation, which, in turn, increases cooling costs and increases servers' failure rates. The testbed has enabled research towards fundamental understanding of the operations of data centers and the autonomic control and management of their resources and services.&nbsp;</p> <p class="p1">To this end, the design of the underlying testbed infrastructure reflects the natural heterogeneity, dynamism and distribution of real-world data centers, and includes embedded instrumentation at all levels, including the platform, virtualization, middleware and application layers. Its scale and geographical distribution enables studies of challenges faced by data center applications, services, middleware and architectures related to both "scale-up" (increases in the capacity of individual servers) and "scale-out" (increases in the number of servers in the system). Experiments conducted using the testbed have resulted in contributions to computer science and engineering, including novel approaches to online profiling/clustering applied to application provisioning, self-organizing scalable information systems, multi- objective optimization of VM placement/migration, and dynamic power-aware server configuration.&nbsp;</p> <p class="p1">At the hardware layer, with respect to power management and thermal monitoring &ndash; which are critical problems faced by large-scale data centers &ndash; the testbed enabled research on innovative algorithms to minimize energy consumption while maintaining performance, and the knowledge acquired by analyzing and processing the heterogeneous data collected by the sensing infrastructure has helped demonstrate how heat propagates by conduction and convection in a data center.&nbsp;</p> <p class="p1">At the resource management layer, the testbed enabled research on a novel approach to provide flexible, scalable resource discovery in large-scale distributed data centers &ndash; MatchTree. This approach is based on self-organizing peer-to-peer overlay networks that support: scalable query distribution using multicast trees; flexible match-making of available resources to job requests; and distributed query result aggregation using map/reduce style processing along the multicast tree. This approach has demonstrated a novel way to self-organize virtual cluster pools on demand to support high-throughput computing, through the concurrent execution of long-running jobs across multiple resources, potentially across geographically distributed data center sites.</p> <p class="p1">At the application layer, the testbed enabled research on a novel technique (AppFlow) that models the dynamic behavior of applications and can be used to accurately predict the near future of application behavior, and a methodology for using AppFlow to drive the resource reconfiguration decisions that reduce power while maintaining the overall performance of the data center. A framework for percolation and refinement of power and performance management decisions from upper to lower resource layers within a data center and flow of feedback from the lower to the upper resource layers working in a continuous closed-feedback control loop has also been devised and evaluated.</p> <p class="p1">As outcomes in education, training, and outreach, the project has provided students with opportuniti...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[The goal of this project has been to acquire and develop an instrumented data center testbed for research on self-managing autonomic computing techniques at different layers a distributed data center infrastructure. Data centers are a growing component of society's IT infrastructure, providing services related to health, banking, commerce, defense, education and entertainment. With continuous growth in size and complexity of data centers, the annual energy and administration costs of today's data centers amount to billions of dollars; high energy consumption also translates into excessive heat dissipation, which, in turn, increases cooling costs and increases servers' failure rates. The testbed has enabled research towards fundamental understanding of the operations of data centers and the autonomic control and management of their resources and services.  To this end, the design of the underlying testbed infrastructure reflects the natural heterogeneity, dynamism and distribution of real-world data centers, and includes embedded instrumentation at all levels, including the platform, virtualization, middleware and application layers. Its scale and geographical distribution enables studies of challenges faced by data center applications, services, middleware and architectures related to both "scale-up" (increases in the capacity of individual servers) and "scale-out" (increases in the number of servers in the system). Experiments conducted using the testbed have resulted in contributions to computer science and engineering, including novel approaches to online profiling/clustering applied to application provisioning, self-organizing scalable information systems, multi- objective optimization of VM placement/migration, and dynamic power-aware server configuration.  At the hardware layer, with respect to power management and thermal monitoring &ndash; which are critical problems faced by large-scale data centers &ndash; the testbed enabled research on innovative algorithms to minimize energy consumption while maintaining performance, and the knowledge acquired by analyzing and processing the heterogeneous data collected by the sensing infrastructure has helped demonstrate how heat propagates by conduction and convection in a data center.  At the resource management layer, the testbed enabled research on a novel approach to provide flexible, scalable resource discovery in large-scale distributed data centers &ndash; MatchTree. This approach is based on self-organizing peer-to-peer overlay networks that support: scalable query distribution using multicast trees; flexible match-making of available resources to job requests; and distributed query result aggregation using map/reduce style processing along the multicast tree. This approach has demonstrated a novel way to self-organize virtual cluster pools on demand to support high-throughput computing, through the concurrent execution of long-running jobs across multiple resources, potentially across geographically distributed data center sites. At the application layer, the testbed enabled research on a novel technique (AppFlow) that models the dynamic behavior of applications and can be used to accurately predict the near future of application behavior, and a methodology for using AppFlow to drive the resource reconfiguration decisions that reduce power while maintaining the overall performance of the data center. A framework for percolation and refinement of power and performance management decisions from upper to lower resource layers within a data center and flow of feedback from the lower to the upper resource layers working in a continuous closed-feedback control loop has also been devised and evaluated. As outcomes in education, training, and outreach, the project has provided students with opportunities to develop, deploy, monitor and manage systems at different layers (environment, physical, virtual, and application layers) across distributed data center infrastructures. Resu...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
