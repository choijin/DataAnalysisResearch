<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III-COR-Small: Beyond Feature Selection and Extraction - An Integrated Framework for High-Dimensional  Data of Small Labeled Samples</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>430626.00</AwardTotalIntnAmount>
<AwardAmount>462605</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Vijayalakshmi Atluri</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>A plethora of digital data is being generated at unparalleled speed with an inordinate number of dimensions. Machine learning and data mining are approaches that can assist us in keeping pace with the rapidly advancing data gathering and storage techniques and help us mine nuggets or patterns from high-dimensional data. Semi-supervised learning can be interpreted as supervised learning that uses additional information from unlabeled data, or as unsupervised learning guided by constraints formed from labeled data. This research is addressing two key pressing issues with massive data: high dimensionality and a shortage of labeled data. In particular, this project is: investigating semi-supervised feature selection to remove irrelevant features; studying the combination of feature extraction and model selection to further reduce dimensionality; and developing a novel framework to integrate feature selection and feature extraction based on sparse learning. This study is an explicit attempt to connect and unify feature selection and extraction for hypothesis space reduction. The project is directly facilitating basic machine learning research and practical data mining and advances innovative research beyond feature selection and extraction. The work is engaging students in both teaching and research, and the algorithms, tools and databases will be made publically available for research purposes and for use as teaching resources.</AbstractNarration>
<MinAmdLetterDate>08/20/2008</MinAmdLetterDate>
<MaxAmdLetterDate>05/03/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0812551</AwardID>
<Investigator>
<FirstName>Huan</FirstName>
<LastName>Liu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Huan Liu</PI_FULL_NAME>
<EmailAddress>hliu@asu.edu</EmailAddress>
<PI_PHON>4807277349</PI_PHON>
<NSF_ID>000449054</NSF_ID>
<StartDate>08/20/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jieping</FirstName>
<LastName>Ye</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jieping Ye</PI_FULL_NAME>
<EmailAddress>jpye@umich.edu</EmailAddress>
<PI_PHON>7346155510</PI_PHON>
<NSF_ID>000488391</NSF_ID>
<StartDate>08/20/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Arizona State University</Name>
<CityName>TEMPE</CityName>
<ZipCode>852816011</ZipCode>
<PhoneNumber>4809655479</PhoneNumber>
<StreetAddress>ORSPA</StreetAddress>
<StreetAddress2><![CDATA[660 South Mill Avenue, Suite 310]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>943360412</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ARIZONA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>806345658</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Arizona State University]]></Name>
<CityName>TEMPE</CityName>
<StateCode>AZ</StateCode>
<ZipCode>852816011</ZipCode>
<StreetAddress><![CDATA[ORSPA]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~288527</FUND_OBLG>
<FUND_OBLG>2009~158099</FUND_OBLG>
<FUND_OBLG>2011~15979</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>High-dimensional data is ubiquitous in real-world applications. The shortage of labeled data, resulting from high labeling costs, necessitates the need to explore machine learning approaches beyond classic classification and clustering paradigms. Semi-supervised learning is one such approach that demonstrates its potential in handling data with small labeled samples and reducing the need for expensive labeled data. However, high-dimensional data with small labeled samples permits too large a hypothesis space yet with too few constraints (labeled instances). The combination of the two data characteristics manifests a new research challenge. Employing computational and statistical learning theory, we analyze specific challenges presented by such data, show preliminary studies, delineate the need to integrate feature selection and extraction in a novel framework to reduce hypothesis space, design efficient and novel algorithms, and conduct theoretical and empirical studies to understand complex relationships between high-dimensional data and classification performance.</p> <p>We propose an integrated framework that promotes and facilitates the computational understanding of machine learning and data mining, and goes beyond the state of the art to bridge feature selection and extraction. Though sharing a common interest, the two lines of research have run largely in parallel. Based on our extensive research in individual areas, we join our expertise in feature selection and extraction and leverage the two approaches to the effective reduction of hypothesis space for high-dimensional data with small-labeled samples. The proposed framework presents an explicit attempt to connect and unify feature selection and extraction for hypothesis space reduction, adds to the existing theory and practice on evolving data with increasingly large dimensionality and few labeled instances, and expands the current capability of handling high-dimensional data with small labeled samples The joint framework connects and unifies feature selection and feature extraction on both theoretical and empirical level, giving rise to new research and curriculum opportunities. Learning from high-dimensional data with small-labeled samples is also a problem in many fields outside of machine learning and data mining. For example, the proposed techniques can be used in computer vision for image and video processing, in computational biology for gene expression pattern image analysis, and in analytical chemistry for quality control of raw materials, intermediates, and final products.</p> <p>Representative Outcomes</p> <ul> <li>Two web-based resources: (1) A Feature Selection Repository, <a href="http://featureselection.asu.edu/">http://featureselection.asu.edu/</a>, and (2) A Sparse Learning Software Package, <a href="http://www.public.asu.edu/_jye02/Software/SLEP">www.public.asu.edu/_jye02/Software/SLEP</a></li> <li>Two Workshops and One Tutorial on Feature Selection: (1) FSDM&rsquo;08 at ECML-PKDD&rsquo;08, and (2) FSDM&rsquo;10 at PAKDD&rsquo;10, and (3) Tutorial at SDM&rsquo;10</li> <li>Selected Publications    <ul> <li>Zheng Alan Zhao and Huan Liu. "<a href="http://dmml.asu.edu/sfs">Spectral Feature Selection for Data Mining</a>",&nbsp;<a href="http://www.crcpress.com/product/isbn/9781439862094">Chapman and Hall/CRC Press</a>, 2012.</li> <li>Zheng Zhao, Lei Wang, Huan Liu, and Jieping Ye. "On Similarity Preserving Feature Selection", IEEE Transactions on Knowledge and Data engineering (TKDE), to appear. </li> <li>Jieping Ye and Jun Liu. Sparse Methods for Biomedical Data. SIGKDD Explorations, to appear.</li> </ul> </li> </ul> <p>Project website: http://www.public.asu.edu/~huanliu/projects/NSF08/</p><br> <p>            Last Modified: 09/16/2012<br>      Modified by: Huan&nbsp;Liu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ High-dimensional data is ubiquitous in real-world applications. The shortage of labeled data, resulting from high labeling costs, necessitates the need to explore machine learning approaches beyond classic classification and clustering paradigms. Semi-supervised learning is one such approach that demonstrates its potential in handling data with small labeled samples and reducing the need for expensive labeled data. However, high-dimensional data with small labeled samples permits too large a hypothesis space yet with too few constraints (labeled instances). The combination of the two data characteristics manifests a new research challenge. Employing computational and statistical learning theory, we analyze specific challenges presented by such data, show preliminary studies, delineate the need to integrate feature selection and extraction in a novel framework to reduce hypothesis space, design efficient and novel algorithms, and conduct theoretical and empirical studies to understand complex relationships between high-dimensional data and classification performance.  We propose an integrated framework that promotes and facilitates the computational understanding of machine learning and data mining, and goes beyond the state of the art to bridge feature selection and extraction. Though sharing a common interest, the two lines of research have run largely in parallel. Based on our extensive research in individual areas, we join our expertise in feature selection and extraction and leverage the two approaches to the effective reduction of hypothesis space for high-dimensional data with small-labeled samples. The proposed framework presents an explicit attempt to connect and unify feature selection and extraction for hypothesis space reduction, adds to the existing theory and practice on evolving data with increasingly large dimensionality and few labeled instances, and expands the current capability of handling high-dimensional data with small labeled samples The joint framework connects and unifies feature selection and feature extraction on both theoretical and empirical level, giving rise to new research and curriculum opportunities. Learning from high-dimensional data with small-labeled samples is also a problem in many fields outside of machine learning and data mining. For example, the proposed techniques can be used in computer vision for image and video processing, in computational biology for gene expression pattern image analysis, and in analytical chemistry for quality control of raw materials, intermediates, and final products.  Representative Outcomes  Two web-based resources: (1) A Feature Selection Repository, http://featureselection.asu.edu/, and (2) A Sparse Learning Software Package, www.public.asu.edu/_jye02/Software/SLEP Two Workshops and One Tutorial on Feature Selection: (1) FSDMÆ08 at ECML-PKDDÆ08, and (2) FSDMÆ10 at PAKDDÆ10, and (3) Tutorial at SDMÆ10 Selected Publications     Zheng Alan Zhao and Huan Liu. "Spectral Feature Selection for Data Mining", Chapman and Hall/CRC Press, 2012. Zheng Zhao, Lei Wang, Huan Liu, and Jieping Ye. "On Similarity Preserving Feature Selection", IEEE Transactions on Knowledge and Data engineering (TKDE), to appear.  Jieping Ye and Jun Liu. Sparse Methods for Biomedical Data. SIGKDD Explorations, to appear.     Project website: http://www.public.asu.edu/~huanliu/projects/NSF08/       Last Modified: 09/16/2012       Submitted by: Huan Liu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
