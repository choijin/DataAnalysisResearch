<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:II-NEW: An Instrumented Data Center Infrastructure for Research on Cross-Layer Autonomics</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2009</AwardEffectiveDate>
<AwardExpirationDate>09/30/2012</AwardExpirationDate>
<AwardTotalIntnAmount>210000.00</AwardTotalIntnAmount>
<AwardAmount>210000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Theodore Baker</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This project's goal is to acquire and develop an instrumented datacenter testbed spanning the three sites of the NSF Center for Autonomic Computing (CAC)-the University of Florida (UF), the University of Arizona (UA) and Rutgers, the State University of New Jersey (RU). Datacenters are a growing component of society's IT infrastructure, including services related to health, banking, commerce, defense, education and entertainment. Annual energy and administration costs of today's datacenters amount to billions of dollars; high energy consumption also translates into excessive heat dissipation, which, in turn, increases cooling costs and increases servers' failure rates. The proposed testbed will enable a fundamental understanding of the operations of data centers and the autonomic control and management of their resources and services. The design of the underlying infrastructure reflects the natural heterogeneity, dynamism and distribution of real-world datacenters, and includes embedded instrumentation at all levels, including the platform, virtualization, middleware and application layers. Its scale and geographical distribution enables studies of challenges faced by datacenter applications, services, middleware and architectures related to both "scale-up" (increases in the capacity of individual servers) and "scale-out" (increases in the number of servers in the system). This testbed will enable fundamental and far-reaching research focused on cross-layer autonomics for managing and optimizing large-scale datacenters. The participant sites will contribute complementary expertise-UA at the resource level, UF at the virtualization layer, and RU in the area of services and applications. The collaboration between the university sites will bring coherence across ongoing separate research efforts and have a transformative impact on the modeling, formulation and solution of datacenter management problems, which have so far been considered mostly in terms of individual layers. &lt;br/&gt;The testbed will also provide a critical infrastructure for education at multiple levels, including providing students with hands-on experience via course projects, enable development of new advanced multi-university and cross-disciplinary courses, as well as multi-site group projects focused on end-to-end autonomics, which will use the proposed testbed. Students from underrepresented groups will be actively involved in the research and their participation will be increased through ongoing collaborations with minority institutions. Even broader community participation will result from an evolving partnership with the recently proposed industry cloud initiatives.</AbstractNarration>
<MinAmdLetterDate>09/21/2009</MinAmdLetterDate>
<MaxAmdLetterDate>08/28/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0855087</AwardID>
<Investigator>
<FirstName>Salim</FirstName>
<LastName>Hariri</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Salim Hariri</PI_FULL_NAME>
<EmailAddress>hariri@ece.arizona.edu</EmailAddress>
<PI_PHON>5206214378</PI_PHON>
<NSF_ID>000476022</NSF_ID>
<StartDate>09/21/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mazin</FirstName>
<LastName>Yousif</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mazin Yousif</PI_FULL_NAME>
<EmailAddress>yousif@ece.arizona.edu</EmailAddress>
<PI_PHON>5038194638</PI_PHON>
<NSF_ID>000208998</NSF_ID>
<StartDate>09/21/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Arizona</Name>
<CityName>Tucson</CityName>
<ZipCode>857194824</ZipCode>
<PhoneNumber>5206266000</PhoneNumber>
<StreetAddress>888 N Euclid Ave</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AZ03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>806345617</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ARIZONA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072459266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Arizona]]></Name>
<CityName>Tucson</CityName>
<StateCode>AZ</StateCode>
<ZipCode>857194824</ZipCode>
<StreetAddress><![CDATA[888 N Euclid Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramElement>
<Code>7640</Code>
<Text>CPATH</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~100000</FUND_OBLG>
<FUND_OBLG>2010~110000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!--   @font-face  {font-family:"Cambria Math";  panose-1:2 4 5 3 5 4 6 3 2 4;  mso-font-charset:0;  mso-generic-font-family:auto;  mso-font-pitch:variable;  mso-font-signature:-536870145 1107305727 0 0 415 0;}   p.MsoNormal, li.MsoNormal, div.MsoNormal  {mso-style-unhide:no;  mso-style-qformat:yes;  mso-style-parent:";  margin-top:0in;  margin-right:0in;  margin-bottom:4.0pt;  margin-left:0in;  text-align:justify;  mso-pagination:widow-orphan;  font-size:9.0pt;  mso-bidi-font-size:10.0pt;  font-family:"Times New Roman";  mso-fareast-font-family:"Times New Roman";} h1  {mso-style-priority:9;  mso-style-unhide:no;  mso-style-qformat:yes;  mso-style-link:"Heading 1 Char";  mso-style-next:Normal;  margin-top:24.0pt;  margin-right:0in;  margin-bottom:0in;  margin-left:0in;  margin-bottom:.0001pt;  text-align:justify;  mso-pagination:widow-orphan lines-together;  page-break-after:avoid;  mso-outline-level:1;  font-size:16.0pt;  font-family:Cambria;  mso-ascii-font-family:Cambria;  mso-ascii-theme-font:major-latin;  mso-fareast-font-family:"?? ????";  mso-fareast-theme-font:major-fareast;  mso-hansi-font-family:Cambria;  mso-hansi-theme-font:major-latin;  mso-bidi-font-family:"Times New Roman";  mso-bidi-theme-font:major-bidi;  color:#345A8A;  mso-themecolor:accent1;  mso-themeshade:181;  mso-font-kerning:0pt;} p.Abstract, li.Abstract, div.Abstract  {mso-style-name:Abstract;  mso-style-unhide:no;  mso-style-parent:"Heading 1";  margin-top:0in;  margin-right:0in;  margin-bottom:6.0pt;  margin-left:0in;  text-align:justify;  mso-pagination:widow-orphan;  page-break-after:avoid;  font-size:9.0pt;  mso-bidi-font-size:10.0pt;  font-family:"Times New Roman";  mso-fareast-font-family:"Times New Roman";  mso-font-kerning:14.0pt;} span.Heading1Char  {mso-style-name:"Heading 1 Char";  mso-style-priority:9;  mso-style-unhide:no;  mso-style-locked:yes;  mso-style-link:"Heading 1";  mso-ansi-font-size:16.0pt;  mso-bidi-font-size:16.0pt;  font-family:Cambria;  mso-ascii-font-family:Cambria;  mso-ascii-theme-font:major-latin;  mso-fareast-font-family:"?? ????";  mso-fareast-theme-font:major-fareast;  mso-hansi-font-family:Cambria;  mso-hansi-theme-font:major-latin;  mso-bidi-font-family:"Times New Roman";  mso-bidi-theme-font:major-bidi;  color:#345A8A;  mso-themecolor:accent1;  mso-themeshade:181;  font-weight:bold;} .MsoChpDefault  {mso-style-type:export-only;  mso-default-props:yes;  font-size:10.0pt;  mso-ansi-font-size:10.0pt;  mso-bidi-font-size:10.0pt;} @page WordSection1  {size:8.5in 11.0in;  margin:1.0in 1.25in 1.0in 1.25in;  mso-header-margin:.5in;  mso-footer-margin:.5in;  mso-paper-source:0;} div.WordSection1  {page:WordSection1;} --> <p class="Abstract">With the rapid growth of data centers and clouds, the power cost and power consumption of their computing and storage resources become critically important to be managed efficiently. Several research studies have shown that data servers typically operate at a low utilization of 10% to 15%, while their power consumption is close to those at peak loads. With this significant fluctuation in the workloads, an elastic delivery of computing services with an efficient power provisioning mechanism becomes an important design goal. Live workload migrations and virtualization are important techniques to optimize power and performance in large-scale data centers. This project presents an application specific autonomic adaptive power and performance management system that utilizes AppFlow-based reasoning to configure dynamically datacenter resources and workload allocations. This system will continuously monitor the workload to determine the current operating point of both workloads and the virtual machines (VMs) running these workloads and then predict the next operating points for these VMs. This enables the system to allocate the appropriate amount of hardware resources that can run efficie...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ With the rapid growth of data centers and clouds, the power cost and power consumption of their computing and storage resources become critically important to be managed efficiently. Several research studies have shown that data servers typically operate at a low utilization of 10% to 15%, while their power consumption is close to those at peak loads. With this significant fluctuation in the workloads, an elastic delivery of computing services with an efficient power provisioning mechanism becomes an important design goal. Live workload migrations and virtualization are important techniques to optimize power and performance in large-scale data centers. This project presents an application specific autonomic adaptive power and performance management system that utilizes AppFlow-based reasoning to configure dynamically datacenter resources and workload allocations. This system will continuously monitor the workload to determine the current operating point of both workloads and the virtual machines (VMs) running these workloads and then predict the next operating points for these VMs. This enables the system to allocate the appropriate amount of hardware resources that can run efficiently the VM workloads with minimum power consumption. We have experimented with and evaluated our approach to manage the VMs running RUBiS bidding application. Our experimental results showed that our approach can reduce the VMsÆ power consumption up to 84% compared to static resource allocation and up to 30% compared to other methods with minimum performance degradation. The growing scales of enterprise computing environments and data centers have made issues related to power consumption, air conditioning, and cooling infrastructures of critical concern in terms of the growing operating costs of both power and cooling. Furthermore, power and cooling rates are increasing by an alarming 8 fold every year and are becoming the dominant part of IT budgets. Clearly, the impact of this project is a crucial issue for enterprise data centers beyond science and technology.       Last Modified: 06/05/2013       Submitted by: Salim Hariri]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
