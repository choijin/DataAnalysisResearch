<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CPA-SEL-T: Collaborative Research: Unified Open Source Transactional Infrastructure</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2008</AwardEffectiveDate>
<AwardExpirationDate>12/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>532000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Reppy</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>As general-purpose computing moves into the age of pervasive parallelism, programmability becomes the key hurdle limiting the effective use of available computing resources.  Transactional memory promises to simplify parallel programming for application programmers. However, research in Transactional Memory is being seriously hampered by the lack of a reusable open source infrastructure. The project will develop the key pieces necessary to overcome this  situation: A transactional memory library built out of highly decomposed pieces will provide reusable and replaceable parts suitable for investigating tradeoffs in software TM implementations.  Standardized interfaces will allow libraries conforming to the interfaces to be used in a variety of environments. TM-aware run-time analysis tools, particularly profilers and debuggers, will provide the necessary tool support for TM implementors and application programmers to understand and improve the performance of software using transactions.  Interesting benchmarks, in a variety of high-level languages, will move forward our understanding of TM performance characteristics.&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/09/2008</MinAmdLetterDate>
<MaxAmdLetterDate>05/05/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0811631</AwardID>
<Investigator>
<FirstName>Jan</FirstName>
<LastName>Vitek</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jan Vitek</PI_FULL_NAME>
<EmailAddress>j.vitek@neu.edu</EmailAddress>
<PI_PHON>6173732462</PI_PHON>
<NSF_ID>000290862</NSF_ID>
<StartDate>09/09/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Suresh</FirstName>
<LastName>Jagannathan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Suresh Jagannathan</PI_FULL_NAME>
<EmailAddress>suresh@cs.purdue.edu</EmailAddress>
<PI_PHON>7654940971</PI_PHON>
<NSF_ID>000181308</NSF_ID>
<StartDate>09/09/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<StreetAddress2><![CDATA[155 S Grant Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072051394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072051394</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>West Lafayette</CityName>
<StateCode>IN</StateCode>
<ZipCode>479072114</ZipCode>
<StreetAddress><![CDATA[Young Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7352</Code>
<Text>COMPUTING PROCESSES &amp; ARTIFACT</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>7943</Code>
<Text>PROGRAMMING LANGUAGES</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~500000</FUND_OBLG>
<FUND_OBLG>2009~16000</FUND_OBLG>
<FUND_OBLG>2010~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As general-purpose computing moves into the age of pervasive parallelism forced upon us by multicore and then manycore architectures, programmability becomes the key hurdle limiting the effective use of available computing resources. &nbsp;Transactional memory (TM) holds great promise as a means to simplify parallel programming for application programmers, and it has been an extremely popular research topic in the programming languages, computer architecture, and parallel programming communities in the last few years.</p> <p>Prior to this project, the explosion in transactional-memory research had not been matched by the distillation of key concepts and design trade-offs required to build reusable infrastructure supporting a variety of implementation techniques, programming languages, hardware platforms, and analysis tools. The key contribution of this project has been to address this deficiency via the development of interfaces and mechanisms to allow clients to use TM (and TM components) effectively while still allowing innovation in TM techniques. &nbsp;</p> <p>This project has made notable contributions in four focus areas: (1) Hardware/Software interfaces, (2) Language Design, (3) Debugging Standardization, and (4) Application Development.</p> <p><span style="text-decoration: underline;"><strong>Hardware/Software Interfaces.</strong></span></p> <div class="page" title="Page 6"> <div class="layoutArea"> <div class="column"> <p><span>A detailed exploration of mechanisms to exploit hardware TM support in the Azul&nbsp;appliance that was purchased in a companion grant was conducted. A full-fledged port of the multi-MLton compiler and runtime onto the Azul appliance was performed and a new garbage collection algorithm that exploits hardware TM support was devised, implemented, and evaluated.&nbsp;</span>Performance analysis for both Java and ML were performed. &nbsp;A real-time transactional memory (RTTM) infrastructure as a time-predictable synchronization solution for chip-multiprocessors&nbsp;in real-time systems has been developed. This work defined hardware for time-predictable transactions and provided a bound for the&nbsp;maximum transaction retries. The proposed RTTM was evaluated with a simulation of a Java chip-multiprocessor.</p> <p><span style="text-decoration: underline;"><strong>Language Design.</strong></span></p> <div class="page" title="Page 7"> <div class="layoutArea"> <div class="column"> <p><span>An important outcome of this project has been the generation of new techniques to eliminate concurrency- related errors such as data races based on foundational TM principles. &nbsp;Traditional approaches to preventing data races rely on protecting instruction sequences with synchronization operations. Such control-centric approaches are inherently brittle as the burden is on the programmer to ensure that all concurrently accessed memory locations are consistently protected. Data-centric synchronization is an alternative approach that offloads some of the work onto the language implementation by grouping fields of objects into atomic sets to indicate that these fields always must be updated atomically. Each atomic set has associated units of work, code fragments that preserve the consistency of that atomic set. Synchronization operations are added automatically by the compiler. &nbsp;An extension to the Java programming language has been developed that integrates annotations for data-centric concurrency control. The resulting language, called AJ, relies on a type system that enables separate compilation and supports atomic sets that span multiple objects and that also supports full encapsulation for more efficient code generation. &nbsp;Our results indicate that data-centric synchronization is easy to use, enjoys low annotation overhead, and successfully preventing data races.&nbsp;</span></p> </div> </div> </div> <div class="page" title="Page 7"> <div ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As general-purpose computing moves into the age of pervasive parallelism forced upon us by multicore and then manycore architectures, programmability becomes the key hurdle limiting the effective use of available computing resources.  Transactional memory (TM) holds great promise as a means to simplify parallel programming for application programmers, and it has been an extremely popular research topic in the programming languages, computer architecture, and parallel programming communities in the last few years.  Prior to this project, the explosion in transactional-memory research had not been matched by the distillation of key concepts and design trade-offs required to build reusable infrastructure supporting a variety of implementation techniques, programming languages, hardware platforms, and analysis tools. The key contribution of this project has been to address this deficiency via the development of interfaces and mechanisms to allow clients to use TM (and TM components) effectively while still allowing innovation in TM techniques.    This project has made notable contributions in four focus areas: (1) Hardware/Software interfaces, (2) Language Design, (3) Debugging Standardization, and (4) Application Development.  Hardware/Software Interfaces.     A detailed exploration of mechanisms to exploit hardware TM support in the Azul appliance that was purchased in a companion grant was conducted. A full-fledged port of the multi-MLton compiler and runtime onto the Azul appliance was performed and a new garbage collection algorithm that exploits hardware TM support was devised, implemented, and evaluated. Performance analysis for both Java and ML were performed.  A real-time transactional memory (RTTM) infrastructure as a time-predictable synchronization solution for chip-multiprocessors in real-time systems has been developed. This work defined hardware for time-predictable transactions and provided a bound for the maximum transaction retries. The proposed RTTM was evaluated with a simulation of a Java chip-multiprocessor.  Language Design.     An important outcome of this project has been the generation of new techniques to eliminate concurrency- related errors such as data races based on foundational TM principles.  Traditional approaches to preventing data races rely on protecting instruction sequences with synchronization operations. Such control-centric approaches are inherently brittle as the burden is on the programmer to ensure that all concurrently accessed memory locations are consistently protected. Data-centric synchronization is an alternative approach that offloads some of the work onto the language implementation by grouping fields of objects into atomic sets to indicate that these fields always must be updated atomically. Each atomic set has associated units of work, code fragments that preserve the consistency of that atomic set. Synchronization operations are added automatically by the compiler.  An extension to the Java programming language has been developed that integrates annotations for data-centric concurrency control. The resulting language, called AJ, relies on a type system that enables separate compilation and supports atomic sets that span multiple objects and that also supports full encapsulation for more efficient code generation.  Our results indicate that data-centric synchronization is easy to use, enjoys low annotation overhead, and successfully preventing data races.         Dynamic detection of concurrency errors depend fundamentally on the level of abstraction at which one views memory. We have shown that a dynamic concurrency tool (specifically a data-race detector) operating at the binary level can lead to false positives and false negatives in terms of a higher-level  managed language. We have demonstrated how an open API can allow a language run-time system to communicate with the low-level detector to avoid sources of imprecision.       Debugging Standardization     If the transac...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
