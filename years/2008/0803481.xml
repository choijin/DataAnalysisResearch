<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III-Medium: Reading the Web: Utilizing Markov Logic in Open Information Extraction</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>899716.00</AwardTotalIntnAmount>
<AwardAmount>963716</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This project adresses the challenge of automatically extracting high-quality knowledge bases from text corpora.  Previous work, led by Prof. Etzioni, developed KnowItAll (http://www.cs.washington.edu/research/knowitall), an unsupervised, domain-independent, scalable system that learns from the Web in an open-ended fashion.  Another project, led by Prof. Domingos, has formalized and fully implemented a powerful framework called Markov Logic Networks (MLNs) (see http://www.cs.washington.edu/ai/srl.html) that enable inference and learning in large, first-order models.  This project integrates KnowItAll and MLNs to build large-scale ontologies from text corpora: extracting relational tuples, using joint inference to merge and validate the tuples, mapping extracted phrases to a taxonomy, and using probabilistic inference rules to answer queries about the ontology.&lt;br/&gt;&lt;br/&gt;Consider, for example, the query "how many Nobel Laureates where born in Europe?"  In response, Google merely provides documents matching the keywords in the query.  KnowItAll can only identify people who are explicitly identified as Nobel Laureates and Europeans.  This project investigates a system that utilizes both information extraction and probabilistic reasoning to identify candidate answers, not explicitly stated in the text, and their likelihood of being correct.  As a simple example, the system concludes that Einstein was born in Europe based on the sentence "Einstein was born in Ulm, Germany". The query "what foods help prevent osteoporosis?" is answered using a multi-step reasoning chain regarding the ingredients of the food and their ability to prevent the disease.&lt;br/&gt;&lt;br/&gt;The broader impact of this research includes novel methods of building knowledge bases automatically. Such knowledge bases (after some manual tuning, perhaps) could be used to support a wide range of applications from question-answering systems, to knowledge-based systems for medical applications, to background knowledge in support of machine reading of text.  The knowledge bases created by this project will be made freely available to the research community as a Web-site and also as a Web-based API via the project Web site (http://www.cs.washington.edu/research/knowitall/ReadingTheWeb/).&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/20/2008</MinAmdLetterDate>
<MaxAmdLetterDate>03/02/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0803481</AwardID>
<Investigator>
<FirstName>Oren</FirstName>
<LastName>Etzioni</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Oren Etzioni</PI_FULL_NAME>
<EmailAddress>etzioni@cs.washington.edu</EmailAddress>
<PI_PHON>2066853035</PI_PHON>
<NSF_ID>000186598</NSF_ID>
<StartDate>08/20/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Pedro</FirstName>
<LastName>Domingos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Pedro Domingos</PI_FULL_NAME>
<EmailAddress>pedrod@cs.washington.edu</EmailAddress>
<PI_PHON>2065434229</PI_PHON>
<NSF_ID>000489770</NSF_ID>
<StartDate>08/20/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Stephen</FirstName>
<LastName>Soderland</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stephen Soderland</PI_FULL_NAME>
<EmailAddress>soderlan@cs.washington.edu</EmailAddress>
<PI_PHON>2065434043</PI_PHON>
<NSF_ID>000430952</NSF_ID>
<StartDate>08/20/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mausam</FirstName>
<LastName>Mausam</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mausam Mausam</PI_FULL_NAME>
<EmailAddress>mausam@cs.washington.edu</EmailAddress>
<PI_PHON>2065434043</PI_PHON>
<NSF_ID>000080843</NSF_ID>
<StartDate>08/20/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981950001</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Ave NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~587673</FUND_OBLG>
<FUND_OBLG>2009~328043</FUND_OBLG>
<FUND_OBLG>2010~16000</FUND_OBLG>
<FUND_OBLG>2011~16000</FUND_OBLG>
<FUND_OBLG>2012~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This research effort focused on furthering the goal of integrating machine learning with natural language processing to address the fundamental challenge of the knowledge acquisition bottleneck. The artificial intelligence field has struggled to develop methods able to learn, represent, and reason over a massive body of knowledge at scale.</p> <p>&nbsp;</p> <p>The Web has made a vast library of online text readily accessible. More broadly, technology trends (including Moore&rsquo;s Law, the exponential increase in disk capacity, and more) suggest an opportunity to build on previous research in statistical NLP, machine learning, and information extraction and scale it up to the challenge of autonomous learning from text, in the context of the Web.</p> <p>&nbsp;</p> <p>This project resulted in several contributions using probabilistic techniques for Open Information Extraction. Firstly and most importantly we designed several Open IE extractors, including our two latest state-of-the-art extractors ReVerb and OLLIE. They are both significant improvements (in terms of precision and yield) compared to the previous extractor, Textrunner. We have run ReVerb on data from over a billion Webpages and the results are available in a demo at <a href="http://openie.cs.washington.edu">http://openie.cs.washington.edu</a>.</p> <p>&nbsp;</p> <p>We developed USP, the first-ever system for unsupervised semantic parsing, using Markov logic. USP transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluated USP by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms previous open IE systems on both precision and recall on this task. We also developed extensions of USP to automatically extract ontologies from text, scale up to larger corpora, and carry out logical-probabilistic reasoning over the extracted knowledge bases.</p> <p>&nbsp;</p> <p>We have developed HOLMES, a novel inference based question answering system, which applies Markov Logic Networks to the task of constructing several proof trees for inferring a fact in response to a given query. HOLMES is able to answer those questions where the right answer isn&rsquo;t present at a single webpage and requires fact composition. The proof trees require a set of inference rules, which are automatically learned from data using a rule learning system called SHERLOCK.</p> <p>&nbsp;</p> <p>We have learned commonsense information on top of the relational data extracted by Open IE. This commonsense information includes domain and ranges of relations (called selectional preferences in the literature) and meta-properties of relations (e.g., functionality). Our selectional preferences learner uses a novel probabilistic graphical model, which is an extension to Latent Dirichlet Allocation. The learned selectional preferences are available at <a href="http://www.cs.washington.edu/research/ldasp">http://www.cs.washington.edu/research/ldasp</a>. The functionality learner uses a novel combination of extractions and world knowledge resources such as FreeBase.</p> <p>&nbsp;</p> <p>Open IE scales to millions of relations and arguments by representing them as textual strings. However, for some applications we wish to normalize these strings into entities and relational predicates. We developed novel techniques that are able to ontologize Open IE tuples, in particular map the argument strings to a database of entities, such as FreeBase. Our techniques are scalable and operate at Web scales. When entity linking textual corpora at large scale, we find that many strings cannot be linked to FreeBase since Freebase does not yet have those entities...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This research effort focused on furthering the goal of integrating machine learning with natural language processing to address the fundamental challenge of the knowledge acquisition bottleneck. The artificial intelligence field has struggled to develop methods able to learn, represent, and reason over a massive body of knowledge at scale.     The Web has made a vast library of online text readily accessible. More broadly, technology trends (including MooreÆs Law, the exponential increase in disk capacity, and more) suggest an opportunity to build on previous research in statistical NLP, machine learning, and information extraction and scale it up to the challenge of autonomous learning from text, in the context of the Web.     This project resulted in several contributions using probabilistic techniques for Open Information Extraction. Firstly and most importantly we designed several Open IE extractors, including our two latest state-of-the-art extractors ReVerb and OLLIE. They are both significant improvements (in terms of precision and yield) compared to the previous extractor, Textrunner. We have run ReVerb on data from over a billion Webpages and the results are available in a demo at http://openie.cs.washington.edu.     We developed USP, the first-ever system for unsupervised semantic parsing, using Markov logic. USP transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluated USP by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms previous open IE systems on both precision and recall on this task. We also developed extensions of USP to automatically extract ontologies from text, scale up to larger corpora, and carry out logical-probabilistic reasoning over the extracted knowledge bases.     We have developed HOLMES, a novel inference based question answering system, which applies Markov Logic Networks to the task of constructing several proof trees for inferring a fact in response to a given query. HOLMES is able to answer those questions where the right answer isnÆt present at a single webpage and requires fact composition. The proof trees require a set of inference rules, which are automatically learned from data using a rule learning system called SHERLOCK.     We have learned commonsense information on top of the relational data extracted by Open IE. This commonsense information includes domain and ranges of relations (called selectional preferences in the literature) and meta-properties of relations (e.g., functionality). Our selectional preferences learner uses a novel probabilistic graphical model, which is an extension to Latent Dirichlet Allocation. The learned selectional preferences are available at http://www.cs.washington.edu/research/ldasp. The functionality learner uses a novel combination of extractions and world knowledge resources such as FreeBase.     Open IE scales to millions of relations and arguments by representing them as textual strings. However, for some applications we wish to normalize these strings into entities and relational predicates. We developed novel techniques that are able to ontologize Open IE tuples, in particular map the argument strings to a database of entities, such as FreeBase. Our techniques are scalable and operate at Web scales. When entity linking textual corpora at large scale, we find that many strings cannot be linked to FreeBase since Freebase does not yet have those entities. We devised the "challenge of unlinkable entities" and proposed methods to distinguish novel entities from non-entity strings and also learned the types for each new entity.     We have also designed extraction techniques over Twitter data. Twitter is a great information source ...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
