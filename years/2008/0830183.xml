<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>ITR:  Intelligent Deformable Models</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2007</AwardEffectiveDate>
<AwardExpirationDate>09/30/2009</AwardExpirationDate>
<AwardTotalIntnAmount>696472.00</AwardTotalIntnAmount>
<AwardAmount>696472</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Deformable models and level set methods are related techniques that have proven to be phenomenally successful computational tools across a variety of disciplines, ranging from computer vision and image processing, computer graphics and image synthesis, computer-aided design and geometric modeling, as well as in applied mathematics and physics.  The two techniques, each a major investigative avenue in itself, are complementary in several fundamental ways.  The goal of this project is, for the first time, to harness the complementary strengths of these two physics-based methods.  The PIs will do so by unifying them under a biology-based control paradigm derived from the emerging field of artificial life.  The unification will lead to a novel breed of intelligent, deformable organisms capable of performing a wide range of challenging data analysis tasks, such as image segmentation and data reconstruction, in a highly automated fashion.  The intellectual merit of the research is the goal of incorporating within an ultimately symbolic control hierarchy, two fundamentally numeric methods, the first related to the continuum mechanics of solids, the second related to liquids.  Hence, deformable models maintain their topological structure as they evolve, while level set methods are topologically adaptive.  Enabling these methods seamlessly to join forces within a rigorous mathematical and tractable computational foundation is an intellectually challenging problem.  The research team, whose expertise spans the computational and mathematical sciences and includes the inventors of deformable models and level set methods, is uniquely qualified to meet this challenge.  The anticipated outcome is a new, highly automated methodology for analyzing large-scale datasets that are subject to uncertainty and noise.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  This work will streamline the entire visual information processing pipeline.  It will be possible to deploy the novel models to be developed by the PIs in massive datasets to automatically extract geometric boundaries and to discover the unknown topology of structures of interest within the data, with immediate applications to medical image analysis, as well as to modeling and rendering tasks.  The research team will incorporate the newly developed theory and algorithms into the graduate curricula of their respective institutions, and they will also seek collaborations with industry to provide the longer-term financial support for technology transfer of the prototype software.</AbstractNarration>
<MinAmdLetterDate>05/08/2008</MinAmdLetterDate>
<MaxAmdLetterDate>05/08/2008</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0830183</AwardID>
<Investigator>
<FirstName>Demetri</FirstName>
<LastName>Terzopoulos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Demetri Terzopoulos</PI_FULL_NAME>
<EmailAddress>dt@cs.ucla.edu</EmailAddress>
<PI_PHON>3102066946</PI_PHON>
<NSF_ID>000101075</NSF_ID>
<StartDate>05/08/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<CountyName>LOS ANGELES</CountyName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2>Suite 700</StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The Regents of the University of California Los Angeles]]></Name>
<CityName>Los Angeles</CityName>
<CountyName/>
<StateCode>CA</StateCode>
<ZipCode>90095</ZipCode>
<StreetAddress><![CDATA[University of California Los Ang]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>30</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA30</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1687</Code>
<Text>ITR MEDIUM (GROUP) GRANTS</Text>
</ProgramElement>
<ProgramReference>
<Code>1654</Code>
<Text>HUMAN COMPUTER INTERFACE</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0105</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0106</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2005~136471</FUND_OBLG>
<FUND_OBLG>2006~340001</FUND_OBLG>
<FUND_OBLG>2007~220000</FUND_OBLG>
</Award>
</rootTag>
