<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI-Small: Real-Time Planning of a Conductable Orchestra</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>449995.00</AwardTotalIntnAmount>
<AwardAmount>457995</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this research is to allow a (live) conductor to create a personal musical performance by controlling a computer-driven (virtual) orchestra through gesture captured on video.  The immediate focus is to provide an educational tool for conducting students, or others with some serious musical training who are capable of communicating musical intent clearly through the traditional language of gesture used by conductors.  However, the PI expects that less-schooled or novice users will also be able to learn from and find enjoyment with the outcome of this research, which will culminate in a computer system that runs on generic computer hardware, and will be made freely available.  The system will take video of a conductor as input, reducing this input into a two-dimensional conducting trace that describes the movement of the tip of the conductor's baton over time.  The system will perform real-time estimation of the conductor's precise "state" within the composition, using an approach that fuses hidden Markov model methodology with a Kalman filter model for musical timing.  Using this on-line estimate, the system will predict the location of future musical events, thus addressing the inevitable issue of detection latency.  Concurrently, the system will synthesize real-time audio to follow the conducted performance, using a previously recorded performance whose timing is continually warped using phase-vocoding.  The initial focus of this work will be on musical timing rather than dynamics, articulation, etc., as this is the aspect of conducting that is most clearly communicated through motion and usually also that which affords the most expressive potential and sense of "ownership" of the performance.  Educated musicians find surprising agreement when evaluating the accuracy with which a musician or ensemble follows a knowledgeable conductor, suggesting that the conductor's "signal" must be relatively unambiguous.  Making mathematical sense of the relationship between this signal and its meaning constitutes a challenging dimension of this research.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  This work will have lasting impact on conducting pedagogy, by providing a tireless and responsive laboratory for musical experimentation.   The research will also make contributions to instrumental and voice pedagogy, by allowing a musician to focus on the interpretive aspects of a piece without simultaneously addressing the technical challenges.  The problem of planning the orchestra's musical evolution with uncertain and continually evolving knowledge of the conductor's actions is deeply challenging; thus, this work has implications for the general domain of planning under uncertainty.   Perhaps most importantly for society at large, a successful conducting system would bring the pleasure of music-making to a broad and international collection of users who might otherwise have little or no experience creating music.</AbstractNarration>
<MinAmdLetterDate>08/04/2008</MinAmdLetterDate>
<MaxAmdLetterDate>05/12/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0812244</AwardID>
<Investigator>
<FirstName>Christopher</FirstName>
<LastName>Raphael</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christopher S Raphael</PI_FULL_NAME>
<EmailAddress>craphael@indiana.edu</EmailAddress>
<PI_PHON>8128561849</PI_PHON>
<NSF_ID>000453143</NSF_ID>
<StartDate>08/04/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474013654</ZipCode>
<StreetAddress><![CDATA[509 E 3RD ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~449995</FUND_OBLG>
<FUND_OBLG>2010~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our NSF project, Real-Time Planning of a Conductable Orchestra,explored the possibility &nbsp;of controlling a real-time music performancethrough gestures, similar to the way a conductor leads a performanceof live musicians. &nbsp;Our approach uses live video from the conductorand produces audio by resynthesizing an existing recording whileallowing the timing to vary. &nbsp;This work leverages our long-standingwork in &nbsp;accompaniment systems which follows a live musician byanalzying the live player's audio and determining where the notes havebeen played, and will be played. &nbsp;</p> <p><br />Under this grant we developed a rudimentary system that analyzes thevideo from a conductor employing a known beat pattern, recognizing thecondcutor's beat times, and controlling the output audio by predictingthe future beat given what is currently known. &nbsp;While our initiallyproposed apporach works well in the case of audio input, it fallsshort with video from a conductor. &nbsp;For one, a conductor typicallymakes only one gesture per musical beat, while the music oftencontains several notes per beat: thus the density of timinginformation is considerably lower for the conducting problem. &nbsp;Inaddition, conducting gestures often indicate beat times withconsiderably less clarity than one finds with soloist audio; oftenthis is intentional on the part of the conductor, as with legatomusic, in which the conductor seeks to hide individual beats, thusfocusing attention on longer time units such &nbsp;as phrases. &nbsp;</p> <p><br />Our response to this difficulty was to develop more sophisticatedmodels for musical timing than those used with our previousaudio-based accompaniment systems. &nbsp;Our earlier models are inspired byclassical position tracking approaches (e.g. Kalman filter) where themusical score position and tempo are modeled analogously to &nbsp;locationand velocity. &nbsp;Our newer model goes beyond the confines of Kalmanfilter approaches, introducing a hidden layer of variables thatdescribe the local musical state. &nbsp;This switching Kalman filter modelachieve better prediction by inferring the local intent for a shortsection of music (maintaining constant tempo, slowing down, etc.)Furthermore, the newer model capitalizes on the logic that governs theordering of these hidden states. &nbsp;We have explored our switchingKalman filter model in other musical applications, such as theautomatic identification and correction of performance errors and thevisualization of musical intent. &nbsp;</p> <p><br />A related challenge pursued through this grant is the understanding ofmusical expression through &nbsp;score analysis, rather than analysis ofperformance data. &nbsp;This line of research seeks a note-by-note labelingof a simple melody, describing each note's role in a larger prosodiccontext (stress, direction, grouping). &nbsp;As far as we know, this is thefirst attempt to explicitly represent musical expression itself, asopposed to the performance consequences (louder, slower, vibrato,etc.) of the expression. &nbsp;Our representation makes it possible toemploy statistical approaches that can learn from musical copora, andestimate convincing interpretations resulting in actual synthesis ofaudio. &nbsp; This work has commercial applications in computer games thatemploy music, as well as score-writing programs that produce morehuman-sounding performances.</p> <p><br />The core intellectual challenge posed by this project is therepresentation and understanding of musical expression, &nbsp;as it relatesto human-computer performance systems which require expressivesynthesis. &nbsp;Simple state space models that assume only smoothlyvarying tempo fail to relate any higher level musical ideas to theactual performance. &nbsp;While such models can be trained with acutalperformance data to predict reasonbly well, they come to each newmusical piece as a blank slate, with...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our NSF project, Real-Time Planning of a Conductable Orchestra,explored the possibility  of controlling a real-time music performancethrough gestures, similar to the way a conductor leads a performanceof live musicians.  Our approach uses live video from the conductorand produces audio by resynthesizing an existing recording whileallowing the timing to vary.  This work leverages our long-standingwork in  accompaniment systems which follows a live musician byanalzying the live player's audio and determining where the notes havebeen played, and will be played.     Under this grant we developed a rudimentary system that analyzes thevideo from a conductor employing a known beat pattern, recognizing thecondcutor's beat times, and controlling the output audio by predictingthe future beat given what is currently known.  While our initiallyproposed apporach works well in the case of audio input, it fallsshort with video from a conductor.  For one, a conductor typicallymakes only one gesture per musical beat, while the music oftencontains several notes per beat: thus the density of timinginformation is considerably lower for the conducting problem.  Inaddition, conducting gestures often indicate beat times withconsiderably less clarity than one finds with soloist audio; oftenthis is intentional on the part of the conductor, as with legatomusic, in which the conductor seeks to hide individual beats, thusfocusing attention on longer time units such  as phrases.     Our response to this difficulty was to develop more sophisticatedmodels for musical timing than those used with our previousaudio-based accompaniment systems.  Our earlier models are inspired byclassical position tracking approaches (e.g. Kalman filter) where themusical score position and tempo are modeled analogously to  locationand velocity.  Our newer model goes beyond the confines of Kalmanfilter approaches, introducing a hidden layer of variables thatdescribe the local musical state.  This switching Kalman filter modelachieve better prediction by inferring the local intent for a shortsection of music (maintaining constant tempo, slowing down, etc.)Furthermore, the newer model capitalizes on the logic that governs theordering of these hidden states.  We have explored our switchingKalman filter model in other musical applications, such as theautomatic identification and correction of performance errors and thevisualization of musical intent.     A related challenge pursued through this grant is the understanding ofmusical expression through  score analysis, rather than analysis ofperformance data.  This line of research seeks a note-by-note labelingof a simple melody, describing each note's role in a larger prosodiccontext (stress, direction, grouping).  As far as we know, this is thefirst attempt to explicitly represent musical expression itself, asopposed to the performance consequences (louder, slower, vibrato,etc.) of the expression.  Our representation makes it possible toemploy statistical approaches that can learn from musical copora, andestimate convincing interpretations resulting in actual synthesis ofaudio.   This work has commercial applications in computer games thatemploy music, as well as score-writing programs that produce morehuman-sounding performances.   The core intellectual challenge posed by this project is therepresentation and understanding of musical expression,  as it relatesto human-computer performance systems which require expressivesynthesis.  Simple state space models that assume only smoothlyvarying tempo fail to relate any higher level musical ideas to theactual performance.  While such models can be trained with acutalperformance data to predict reasonbly well, they come to each newmusical piece as a blank slate, without any accumulated musicalknowledge.  Our current efforts address this difficulty using greatersophistication in the modeling of musical timing and prosody.  Wedevelop models that explicitly incorporate higher level musical i...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
