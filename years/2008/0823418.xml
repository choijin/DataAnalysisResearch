<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Fallibility and Revision in Science and Society - Standard Research Grant</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2008</AwardEffectiveDate>
<AwardExpirationDate>07/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>222523.00</AwardTotalIntnAmount>
<AwardAmount>222523</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050300</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Frederick Kronz</SignBlockName>
<PO_EMAI>fkronz@nsf.gov</PO_EMAI>
<PO_PHON>7032927283</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Introduction: The focal problem of the proposal is significant and a matter of broad interest: how to bring second order beliefs (beliefs about beliefs) to bear on first order beliefs. Such issues arise, for example, in the context of legal assessments of evidence where jurists are called on to consider general considerations of reliability when adjudicating specific eye witness evidence. &lt;br/&gt;&lt;br/&gt;Intellectual Merit:  Although probabilistic approaches to rational belief have become very popular, there is no standard way of using these models to represent the first-order/second-order interactions such as the one mentioned above. A successful account of these interactions would represent a major achievement in probabilistic understanding of rational belief. The project promises to provide a formula for how second order beliefs affect first order beliefs. &lt;br/&gt;&lt;br/&gt;Broader Impact:  Among the potential broader impacts of the results of this project is a potential for providing a basis for a view of science that is neither dogmatic nor skeptical, which could mitigate the polarization of opinion exemplified in the so-called "science wars". These results might also be used to understand how jurors can take proper account of their own fallibility in doing their job thereby providing the project with obvious social importance. Other broader impacts may result from the publication of a planned book whose audience includes students in courses in criminology and philosophy of science. Insofar as the book will be studied by college students in various majors, its impact will reach well beyond professionals in the investigator's field. The project may also have a significant effect beyond the usual academic circles through the production of pamphlets that are addressed to policymakers, jurors, and the public at large that address the public perception of science (for example, how to reconcile the reliability of science with its fallibility), the dynamics of group decision-making by juries, and the proper way to evaluate forensic evidence, expert testimony, and eyewitness reports. For a project in philosophy, the proposed project has unusually broad impact.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>07/29/2008</MinAmdLetterDate>
<MaxAmdLetterDate>05/26/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0823418</AwardID>
<Investigator>
<FirstName>Sherrilyn</FirstName>
<LastName>Roush</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sherrilyn Roush</PI_FULL_NAME>
<EmailAddress>roush@berkeley.edu</EmailAddress>
<PI_PHON>7134190298</PI_PHON>
<NSF_ID>000217612</NSF_ID>
<StartDate>07/29/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName>BERKELEY</CityName>
<StateCode>CA</StateCode>
<ZipCode>947101749</ZipCode>
<StreetAddress><![CDATA[Sponsored Projects Office]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7603</Code>
<Text>STS-Sci, Tech &amp; Society</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1353</Code>
<Text>Hist &amp; Philosophy of SET</Text>
</ProgramReference>
<ProgramReference>
<Code>7567</Code>
<Text>SOC STUDIES OF SCI, ENG &amp; TECH</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~142087</FUND_OBLG>
<FUND_OBLG>2009~69496</FUND_OBLG>
<FUND_OBLG>2010~10940</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project has made contributions in three areas: general theory of rationality, replies to skeptical arguments about error, and the effect of calibration on equitable treatment of testimony.</p> <p>The question I addressed in the theory of rationality is how to use evidence about reliability, yours or someone else&rsquo;s, to revise your beliefs about the world. If you are sure from witnessing a murder that the defendant is the guilty party, and then learn of the psychological evidence that human beings are generally overconfident in eyewitness testimony, how confident should you now be about the defendant&rsquo;s guilt? This new information was evidence about you, not about the murder. We feel that it is relevant to what you should believe about the murder, but why? Similarly, when you learn that scientists have made mistakes, what effect should that have on your believing the next thing they say? The fact that they&rsquo;ve made a mistake in the past is not relevant to the substance of a new hypothesis.</p> <p>In the standard probabilistic (Bayesian) theory of rationality &ndash; which is used widely in philosophy and the social sciences &ndash; such &ldquo;second-order&rdquo; evidence cannot affect your primary beliefs, because of idealizing assumptions I have generalized away from. &nbsp;I defined a rule, called Re-Cal, that says that you should revise your confidence in p to what your best evidence says is your reliability in p-like matters; this amounts to aiming for the well-known state of <em>calibration</em>, where your confidence matches your reliability. I derived Re-Cal from an already accepted more general principle of rationality, and thereby provided an explanation why the rule should be followed. With a collaborator I proved a convergence theorem that says that if you were to follow this rule then in the long run your confidence in p would match the objective probability of p. &nbsp;The rule is the first explicit and fully general re-calibration rule in the form of a conditionalization; as such it provides the answer for what re-calibration should be for a Bayesian. This formulation of re-calibration avoids standard objections to the idea, and escapes the threats of inconsistency that had led to resistance to even trying to develop an explicit rule in second-order probability.</p> <p>The rule has practical applications since we have recently become inundated from empirical psychology with information about human beings&rsquo; judgments and proclivities to action that doesn&rsquo;t match our expectations about ourselves. We need to learn how to use it in assessing, for example, eyewitnesses. Also, the general public perpetually has difficulty learning of the fallibility of science without falling into a generic mistrust. The answer in both cases is a proportionality that is captured in Re-Cal: one mistake is not evidence for unreliability, but five mistakes out of ten assertions is; adjust confidence to reliability. The rule I defined can be understood intuitively and applied in both contexts.</p> <p>Second, I have evaluated five skeptical arguments discussed among philosophers that have strong negative consequences about the possibility of science and mathematics, and I found in each case that errors have been made about error, the growth of error over steps of reasoning, or the relevance of past science to present science. For example, in the pessimistic induction over the history of science, mistakes are made about the relevance of the failures of past scientists to the legitimacy of our confidence in our own science.</p> <p>Third, I&rsquo;ve found empirical evidence, in two experiments, that gender has a systematic effect on calibration level &ndash; relation of confidence to accuracy &ndash; with women learning and men not learning, to be less overconfident. (The effect is causal, not merely correlational.) The significance of this can be see...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has made contributions in three areas: general theory of rationality, replies to skeptical arguments about error, and the effect of calibration on equitable treatment of testimony.  The question I addressed in the theory of rationality is how to use evidence about reliability, yours or someone elseÆs, to revise your beliefs about the world. If you are sure from witnessing a murder that the defendant is the guilty party, and then learn of the psychological evidence that human beings are generally overconfident in eyewitness testimony, how confident should you now be about the defendantÆs guilt? This new information was evidence about you, not about the murder. We feel that it is relevant to what you should believe about the murder, but why? Similarly, when you learn that scientists have made mistakes, what effect should that have on your believing the next thing they say? The fact that theyÆve made a mistake in the past is not relevant to the substance of a new hypothesis.  In the standard probabilistic (Bayesian) theory of rationality &ndash; which is used widely in philosophy and the social sciences &ndash; such "second-order" evidence cannot affect your primary beliefs, because of idealizing assumptions I have generalized away from.  I defined a rule, called Re-Cal, that says that you should revise your confidence in p to what your best evidence says is your reliability in p-like matters; this amounts to aiming for the well-known state of calibration, where your confidence matches your reliability. I derived Re-Cal from an already accepted more general principle of rationality, and thereby provided an explanation why the rule should be followed. With a collaborator I proved a convergence theorem that says that if you were to follow this rule then in the long run your confidence in p would match the objective probability of p.  The rule is the first explicit and fully general re-calibration rule in the form of a conditionalization; as such it provides the answer for what re-calibration should be for a Bayesian. This formulation of re-calibration avoids standard objections to the idea, and escapes the threats of inconsistency that had led to resistance to even trying to develop an explicit rule in second-order probability.  The rule has practical applications since we have recently become inundated from empirical psychology with information about human beingsÆ judgments and proclivities to action that doesnÆt match our expectations about ourselves. We need to learn how to use it in assessing, for example, eyewitnesses. Also, the general public perpetually has difficulty learning of the fallibility of science without falling into a generic mistrust. The answer in both cases is a proportionality that is captured in Re-Cal: one mistake is not evidence for unreliability, but five mistakes out of ten assertions is; adjust confidence to reliability. The rule I defined can be understood intuitively and applied in both contexts.  Second, I have evaluated five skeptical arguments discussed among philosophers that have strong negative consequences about the possibility of science and mathematics, and I found in each case that errors have been made about error, the growth of error over steps of reasoning, or the relevance of past science to present science. For example, in the pessimistic induction over the history of science, mistakes are made about the relevance of the failures of past scientists to the legitimacy of our confidence in our own science.  Third, IÆve found empirical evidence, in two experiments, that gender has a systematic effect on calibration level &ndash; relation of confidence to accuracy &ndash; with women learning and men not learning, to be less overconfident. (The effect is causal, not merely correlational.) The significance of this can be seen in light of well-known psychological evidence that on average, by default, human beings believe confident people more. This means that in a population where ...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
