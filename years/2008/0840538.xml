<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SGER: Collaborative Research: Contextual Machine Translation</AwardTitle>
<AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
<AwardExpirationDate>02/28/2010</AwardExpirationDate>
<AwardTotalIntnAmount>0.00</AwardTotalIntnAmount>
<AwardAmount>80000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
</ProgramOfficer>
<AbstractNarration>SGER: Collaborative Research: Contextual Machine Translation&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Despite significant progress in machine translation, little work has been done to address large-scale translation of natural multi-party human interactions in real-world contexts. Multi-party interactions typically take place in a context where multimodal events such as hand gestures, head gestures, gaze, body movements, etc. are available and utilized by participants to interpret human language and establish common ground. This observation leads to the hypothesis that modeling multimodal environment and interaction discourse can improve automated language interpretation for the purpose of machine translation.  Based on this hypothesis, this exploratory research investigates the role of multimodality in machine translation of multi-party conversations using a subset of the AMI meeting corpus.  An appropriate level of multimodal representation is identified focusing on user gestures and presentation slides. Correct referents to referring expressions and correct senses to ambiguous words are annotated and used to evaluate whether and how multimodal context improves reference resolution and word sense disambiguation. The enhanced semantic processing utilizing multimodal information is incorporated in statistical machine translation for English-German.  The objective is to conduct proof-of-concept experiments exploring the usefulness of multimodal and discourse information for statistical machine translation in real-world contexts.  The results from this exploratory study will provide insights on algorithms and systems for automatic extraction of multimodal information, language interpretation using multimodal information, and incorporation of this information into statistical machine translation. The annotated data will be made available to the research community to facilitate the development of translation technology that produces more natural, human-like translations in real-world interactive situations.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/05/2008</MinAmdLetterDate>
<MaxAmdLetterDate>07/02/2009</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0840538</AwardID>
<Investigator>
<FirstName>Joyce</FirstName>
<LastName>Chai</LastName>
<EmailAddress>chaijy@umich.edu</EmailAddress>
<StartDate>08/05/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Michigan State University</Name>
<CityName>East Lansing</CityName>
<ZipCode>488242600</ZipCode>
<PhoneNumber>5173555040</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
</Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9237</Code>
<Text>SMALL GRANTS-EXPLORATORY RSRCH</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
</Award>
</rootTag>
