<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>A Machine Learning Approach to Human Visual Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2008</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>371621.00</AwardTotalIntnAmount>
<AwardAmount>371621</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Finbarr Sloane</SignBlockName>
<PO_EMAI>fsloane@nsf.gov</PO_EMAI>
<PO_PHON>7032928465</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The proposed research program consists of experimental and computational studies of human visual learning. The project focuses on the information processing mechanisms mediating the perceptual learning that underlies expertise in a variety of STEM fields, such as biology, astronomy, and geoscience. In particular, the investigators attempt to take advantage of insights from the field of Machine Learning (e.g., its formalisms for conceptualizing the properties of different learning environments, its powerful sets of statistical learning algorithms for each environment, and its numerous mathematical and empirical findings about the advantages and disadvantages of these algorithms). The studies look at learning performance on lower-level and higher-level discrimination tasks in four types of learning environments: supervised, unsupervised, semi-supervised, and reinforcement learning environments. The project also explores visual learning based on correlated perceptual signals in multisensory or multi-cue environments, such as when a person both sees and touches surfaces. The computational studies compare people's learning performances with the statistically optimal performances of "ideal learners", and also with the performances of on-line learning algorithms from the Machine Learning literature. A key hypothesis is that people can visually learn with "unlabeled" data items (i.e., items that are not labeled by an instructor as examples of a particular category of interest) by transferring knowledge gained with "labeled" data items or by transferring knowledge gained from other sensory modalities. The work has important implications for the design of STEM training environments.</AbstractNarration>
<MinAmdLetterDate>09/16/2008</MinAmdLetterDate>
<MaxAmdLetterDate>06/22/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0817250</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Jacobs</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert A Jacobs</PI_FULL_NAME>
<EmailAddress>robbie@bcs.rochester.edu</EmailAddress>
<PI_PHON>5852750753</PI_PHON>
<NSF_ID>000172078</NSF_ID>
<StartDate>09/16/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Rochester</Name>
<CityName>Rochester</CityName>
<ZipCode>146270140</ZipCode>
<PhoneNumber>5852754031</PhoneNumber>
<StreetAddress>518 HYLAN, RC BOX 270140</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY25</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041294109</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ROCHESTER</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041294109</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Rochester]]></Name>
<CityName>Rochester</CityName>
<StateCode>NY</StateCode>
<ZipCode>146270140</ZipCode>
<StreetAddress><![CDATA[518 HYLAN, RC BOX 270140]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7625</Code>
<Text>REAL</Text>
</ProgramElement>
<ProgramReference>
<Code>9177</Code>
<Text>ELEMENTARY/SECONDARY EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0408</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0409</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0411</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~120734</FUND_OBLG>
<FUND_OBLG>2009~123843</FUND_OBLG>
<FUND_OBLG>2011~127044</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>We study how people learn to perceive their environments. In addition to being intrinsically interesting, learning more about perceptual learning has important implications for science education, including how we train novice scientists to behave more like expert scientists in a large range of scientific fields (e.g., consider the perceptual skills of expert radiologists, geoscientists, chemists, biologists, etc.).</p> <p>Our lab conducts experimental and computational research on visual learning and memory, both in visual environments and in multisensory (visual-auditory and visual-haptic) environments. Two research questions that motivate many of our projects are: (1) How can people acquire high-level cognitive knowledge of objects and scenes from low-level perceptual inputs such as visual, auditory, or haptic features?; (2) Why do people show biases and dependencies in their visual short-term memory recollections?</p> <p>Our findings show that people learn to represent their environments in ways that are independent of the individual or multiple sensory modalities used to observe those environments. These modality-independent representations facilitate the transfer of knowledge from one modality to another. We have developed a computational theory explaining the acquistion and use of modality-independent object representations.</p> <p>Our findings also show that people's visual short-term memory recollections are often inaccurate in the sense that they contain both biases and dependencies. These inaccuracies may be a sensible outcome of a memory system that is trying to be as efficient as possible. We have developed a computational model of the organization of visual short-term memory that explains many of our memory inaccuracies.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/03/2013<br>      Modified by: Robert&nbsp;A&nbsp;Jacobs</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ We study how people learn to perceive their environments. In addition to being intrinsically interesting, learning more about perceptual learning has important implications for science education, including how we train novice scientists to behave more like expert scientists in a large range of scientific fields (e.g., consider the perceptual skills of expert radiologists, geoscientists, chemists, biologists, etc.).  Our lab conducts experimental and computational research on visual learning and memory, both in visual environments and in multisensory (visual-auditory and visual-haptic) environments. Two research questions that motivate many of our projects are: (1) How can people acquire high-level cognitive knowledge of objects and scenes from low-level perceptual inputs such as visual, auditory, or haptic features?; (2) Why do people show biases and dependencies in their visual short-term memory recollections?  Our findings show that people learn to represent their environments in ways that are independent of the individual or multiple sensory modalities used to observe those environments. These modality-independent representations facilitate the transfer of knowledge from one modality to another. We have developed a computational theory explaining the acquistion and use of modality-independent object representations.  Our findings also show that people's visual short-term memory recollections are often inaccurate in the sense that they contain both biases and dependencies. These inaccuracies may be a sensible outcome of a memory system that is trying to be as efficient as possible. We have developed a computational model of the organization of visual short-term memory that explains many of our memory inaccuracies.          Last Modified: 09/03/2013       Submitted by: Robert A Jacobs]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
