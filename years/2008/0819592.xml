<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Predictive Cues and Multiple Fixation Search</AwardTitle>
<AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>297000.00</AwardTotalIntnAmount>
<AwardAmount>297000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040500</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Lawrence Gottlob</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Human visual search involves scrutinizing the visual scene by making eye movements to orient the center of the eye, the fovea, to regions of interest.  The high resolution of the fovea allows for processing of the visual world with finer detail. In addition, visual search also involves attending to different points in the visual scene independent of the eye position. Visual search is also often facilitated when there are other objects (cues) that co-occur with the target of interest. When the target object appears at unexpected locations, people often have difficulty finding the target. Researchers have for decades studied how such predictive cues benefit human search performance in a variety of tasks. However, most of these studies were designed to prevent humans from making eye movements to study the mechanisms of covert visual attention in isolation. Little is known about the mechanisms by which cues and context aid search in natural behavior where people are free to move their eyes.  In this proposal, the investigators seek to understand the mechanisms by which humans use cues and context to plan eye movements and covert attention to optimize visual search. They will develop mathematical models implemented on computers to mimic the processes by which the human brain mediates visual search.  The research will allow for a better understanding of the mechanisms by which cues and context aid visual search and also predict the benefits in accuracy of perceptual judgments brought by the use of predictive cues in visual search.  &lt;br/&gt;&lt;br/&gt;Looking for a friend in a crowd, a car in a parking lot, and your house keys in the living room are all examples of visual searches that humans do on a daily basis. The proposed research is a necessary step to understand human visual search in the natural environment. Visual search also include life-critical tasks such as finding a tumor in a medical image or a potentially dangerous object in a baggage x-ray. This research will also have the potential to provide insights into the efficiency in life-critical search tasks.  Finally, the research can potentially contribute to better assessments of the expected deficits of patients with attentional disorders (e.g., hemi-neglect) in naturalistic search behavior. The work will integrate research and educational activities at all levels (high school interns, undergraduate, and graduate) through the development of a website that enables students and researchers worldwide to enter experimental data through an internet browser and test the various computer models. &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/02/2008</MinAmdLetterDate>
<MaxAmdLetterDate>09/02/2008</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0819592</AwardID>
<Investigator>
<FirstName>Miguel</FirstName>
<LastName>Eckstein</LastName>
<EmailAddress>eckstein@psych.ucsb.edu</EmailAddress>
<StartDate>09/02/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Santa Barbara</Name>
<CityName>Santa Barbara</CityName>
<ZipCode>931062050</ZipCode>
<PhoneNumber>8058934188</PhoneNumber>
<StreetAddress>Office of Research</StreetAddress>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
</Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
