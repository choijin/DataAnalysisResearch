<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>MRI: Acquisition of Motion Capture and Eye Tracking Equipment to Enable Innovative Research on Joint Action</AwardTitle>
<AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
<AwardExpirationDate>08/31/2009</AwardExpirationDate>
<AwardAmount>317510</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John E. Yellen</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Joint actions involve two or more people coordinated in a common endeavor "bodies and minds moving together." A conversation is a joint action, as is the shared hefting and moving of a piano, or a blistering rally in professional tennis. Real-time joint action has been notoriously difficult to study because of the complexity of measuring coordinated cognitive and bodily activity across two or more people. Nevertheless, people participate in joint activities on a daily basis, and they often have enormous practical significance, as when a team of air-traffic controllers shepherds passenger planes through air space. &lt;br/&gt;&lt;br/&gt;Recent research suggests a point of entry into this difficult topic: body and eye movements become closely synchronized between participants in joint actions. Thus measurements of synchronization may reveal the nature of the coordination and the subtle changes that sustain coordination in joint action. Success also depends on the new mathematics of synchronization developed across the last half of the 20th century and on very recent developments in wireless sensor technology. Wireless sensor technology allows researchers to take precise measurements of movement without the elaborate tethers created by dozens of connecting wires. The new technology allows subjects to move freely in joint activities while a dense array of real-time measurements are taken.&lt;br/&gt;&lt;br/&gt;With the support of the National Science Foundation, Drs. Michael Riley, Kevin Shockley and Guy Van Orden will purchase state-of-the-art wireless motion-capture and eye-tracking technology to objectively measure coordination of moving bodies and of eye movements during joint activities. Body, limb, and eye movements of up to four people can be tracked simultaneously and recorded for later analysis to quantify coordination. These powerful sensor technologies, combined with new mathematical analysis techniques, define the next generation of scientific instrumentation to investigate joint action. This instrumentation provides unparalleled detail to address basic questions about joint action and provides practical knowledge of joint actions in applied settings requiring teams of operators to collaborate on joint tasks. If cognitive and perceptual-motor coordination that arises during joint action is related to enhanced team performance, then performance can be improved by stimulating coordination.&lt;br/&gt;&lt;br/&gt;The equipment purchased with National Science Foundation support will also enhance undergraduate and graduate training at the University of Cincinnati, where an interdisciplinary network of researchers from psychology, biology, biomedical engineering, electrical engineering, and medicine study basic and applied aspects of coordination in human activity. Topics include joint cognitive activities, learning and development, ergonomics, and other specific topics in motor coordination (balance, gait, movement disorders, and rehabilitation). The newly equipped laboratory will also enhance ongoing efforts to attract students to STEM disciplines, especially from groups who are underrepresented in science but well represented in undergraduate studies at the University of Cincinnati. Many students are attracted to motion-capture technology it is used to create animated films and video games but are unaware of its scientific applications.</AbstractNarration>
<MinAmdLetterDate>09/02/2008</MinAmdLetterDate>
<MaxAmdLetterDate>09/02/2008</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>0821241</AwardID>
<Investigator>
<FirstName>Kevin</FirstName>
<LastName>Shockley</LastName>
<EmailAddress>kevin.shockley@uc.edu</EmailAddress>
<StartDate>09/02/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Riley</LastName>
<EmailAddress>michael.riley@uc.edu</EmailAddress>
<StartDate>09/02/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Guy</FirstName>
<LastName>Van Orden</LastName>
<EmailAddress>guy.van.orden@uc.edu</EmailAddress>
<StartDate>09/02/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Cincinnati Main Campus</Name>
<CityName>Cincinnati</CityName>
<ZipCode>452210222</ZipCode>
<PhoneNumber>5135564358</PhoneNumber>
<StreetAddress>University Hall, Suite 530</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
</Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>1189</Code>
<Text>MAJOR RESEARCH INSTRUMENTATION</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
</Award>
</rootTag>
