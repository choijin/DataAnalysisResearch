<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC-Medium: End-user debugging of machine-learned programs</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2008</AwardEffectiveDate>
<AwardExpirationDate>09/30/2013</AwardExpirationDate>
<AwardTotalIntnAmount>618219.00</AwardTotalIntnAmount>
<AwardAmount>929362</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This is a project to give the end user some ability to debug programs that were written by a machine instead of a person, especially when the users are not expert programmers.  This is the problem faced by users of a new sort of program, namely, one generated by a machine learning system. For example, intelligent user interfaces, categorizers of email and web sites, and recommender systems use machine learning to learn how to behave. This learned set of behaviors is a program. Learned programs do not come into existence until the learning environment has left the hands of the machine learning specialist, because they learn from the user's ongoing data. Thus, if these programs make a mistake, the only one present to debug them is the user.  Giving end users the ability to debug such programs can improve the speed and accuracy of these systems.&lt;br/&gt;&lt;br/&gt;Specifically, the project envisions a fine-grained, iterative, interactive debugging process. First, a user notices an erroneous classification (with the system's help, based on reasoning about its own competence), such as an email message that might be misfiled. Second, the user asks for an explanation. Third, using the system's explanation, the user provides reasoning constraints, declaring, for example, that "today" is not an important word, and that anything from the company president should go into the "company" folder. The learned program reevaluates competence models and redoes its reasoning, giving the user an opportunity to immediately see the result of the change. The loop then begins again. Thus, the goals of this project are the following: 1. To help users identify reasoning problems, and to provide explanations of the behavior of machine-learned programs suitable for end users. 2. To elicit rich feedback from the user, incorporating it into the reasoning of the learned program. 3. To improve the speed and accuracy of machine learning by integrating this rich feedback into learning.&lt;br/&gt;&lt;br/&gt;In addition to the potential speed and accuracy improvement in the machine learner, users may become more productive and make fewer errors. Providing disclosure of the learned programs' reasoning engenders trust, and with it, increased willingness to use the system.  Thus, this project has the potential to make significant advances in the user acceptance of machine learning in a variety of new, real-world applications.  Combining human constraints and guidance with statistical learning could enable highly accurate learning from small data sets, which is critical to creating successful intelligent user interfaces. The project will also result in learning systems whose data sources and input features are easy to change and whose behavior is easy to control.  In combining human-computer interaction principles with machine learning, this project opens opportunities for novel perspectives, especially in the realm of interdisciplinary education.  Graduate students will be trained in this blended research area, and aspects of it will be incorporated in classes in both human-computer interaction and machine learning, and in other educational experiences for undergraduates and high school students. &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/12/2008</MinAmdLetterDate>
<MaxAmdLetterDate>05/06/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0803487</AwardID>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Dietterich</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas G Dietterich</PI_FULL_NAME>
<EmailAddress>tgd@cs.orst.edu</EmailAddress>
<PI_PHON>5417375559</PI_PHON>
<NSF_ID>000190533</NSF_ID>
<StartDate>09/12/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Margaret</FirstName>
<LastName>Burnett</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Margaret M Burnett</PI_FULL_NAME>
<EmailAddress>burnett@eecs.oregonstate.edu</EmailAddress>
<PI_PHON>5417372539</PI_PHON>
<NSF_ID>000308026</NSF_ID>
<StartDate>09/12/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Simone</FirstName>
<LastName>Stumpf</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Simone C Stumpf</PI_FULL_NAME>
<EmailAddress>stumpf@eecs.oregonstate.edu</EmailAddress>
<PI_PHON>5417373437</PI_PHON>
<NSF_ID>000437418</NSF_ID>
<StartDate>09/12/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Weng-Keen</FirstName>
<LastName>Wong</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Weng-Keen Wong</PI_FULL_NAME>
<EmailAddress>wong@eecs.oregonstate.edu</EmailAddress>
<PI_PHON>5417374544</PI_PHON>
<NSF_ID>000211106</NSF_ID>
<StartDate>09/12/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Oregon State University</Name>
<CityName>Corvallis</CityName>
<ZipCode>973318507</ZipCode>
<PhoneNumber>5417374933</PhoneNumber>
<StreetAddress>OREGON STATE UNIVERSITY</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<StateCode>OR</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OR04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>053599908</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OREGON STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>053599908</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Oregon State University]]></Name>
<CityName>Corvallis</CityName>
<StateCode>OR</StateCode>
<ZipCode>973318507</ZipCode>
<StreetAddress><![CDATA[OREGON STATE UNIVERSITY]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Oregon</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OR04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~405221</FUND_OBLG>
<FUND_OBLG>2009~283893</FUND_OBLG>
<FUND_OBLG>2010~11250</FUND_OBLG>
<FUND_OBLG>2011~228998</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our findings and results have shown that it is feasible for end users to interactively debug machine learning systems and intelligent agents.&nbsp; We developed, prototyped, and empirically evaluated several approaches that accomplish this capability, which we describe next in the context of each objective.</p> <p>&nbsp;</p> <p><span style="text-decoration: underline;">Objective 1</span>: To improve user <em>acceptance</em> of machine learning by providing <em>explanations</em> of the behavior of intelligent agents (based on machine learning) that are <em>suitable for end users</em> untrained in computer science.&nbsp;</p> <p>Key Results:</p> <p>We developed a technology called WYSIWYT/ML that allows end users to quickly assess how much trust they should place in a particular intelligent agent. Our empirical investigation showed that WYSIWYT/ML allows ordinary users to identify mistakes of a machine learning assistant by displaying various visualizations about each test instance: in only 10 minutes, ordinary users without computer science backgrounds were able to judge enough instances to adequately cover over 1000 cases. We also compared three different strategies of selecting test instances for helping users quickly assess their intelligent agents. We found that selecting test instances in which the agent was &ldquo;least confident&rdquo; worked overall, but that using algorithms based on &ldquo;unusual instances&rdquo; and &ldquo;least-common features&rdquo; were better able to reveal &ldquo;surprise&rdquo; mistakes.</p> <p>We developed a crowdsourced variant of WYSIWYT/ML to investigate whether using very small &ldquo;crowd&rdquo; of end users (mini-crowdsourcing) to assess a machine-learning assistant is useful from a cost/benefit perspective. Mini-crowds are relevant when an intelligent agent&rsquo;s task is to serve a small group, such as a family or a team at work. Our empirical investigation showed that the mini-crowd supplied many more benefits besides the obvious decrease in workload: a crowd of as few as six found more errors, tested more of its logic, and introduced enough redundancy to reduce crowd mistakes.</p> <p>&nbsp;</p> <p><span style="text-decoration: underline;">Objective 2</span>: To help ordinary users <em>identify reasoning problems</em> in their intelligent agents, and then allow the users to provide <em>rich feedback</em> &nbsp;that can then be <em>incorporated into the reasoning</em> of the agent.</p> <p>Key Results:</p> <p>In identifying reasoning problems, users compare an intelligent agent&rsquo;s reasoning against a mental model they have formed as to how the reasoning is done or should be done. They form such mental models based on their own prior experiences, their assumptions, and what they see the agent do. Our empirical investigations showed that a user&rsquo;s mental model has a strong effect on the ability for the user to learn about the agent and to provide feedback to the agent that is useful enough to the agent to help it succeed better.</p> <p>Building upon that result, we then investigated <em>how</em> an agent should attempt to explain its reasoning to ordinary users. We created a range of variants of explanation fidelity along two dimensions: soundness (which is whether an explanation contains any untruths, usually via oversimplifications) and completeness (which is the extent of the reasoning factors that are actually described in an explanation). We found that using high soundness and high completeness led to improved trust and improved understanding of the agent, and that, surprisingly, users were willing to put in the time and effort to process these higher fidelity explanations in order to improve their agents.</p> <p>&nbsp;</p> <p><span style="text-decoration: underline;">Objective 3</span>: To enable the user to improve the <em>speed and/or accuracy of their intelligent agent&rsquo;s machine learning</em>.</p> <p...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our findings and results have shown that it is feasible for end users to interactively debug machine learning systems and intelligent agents.  We developed, prototyped, and empirically evaluated several approaches that accomplish this capability, which we describe next in the context of each objective.     Objective 1: To improve user acceptance of machine learning by providing explanations of the behavior of intelligent agents (based on machine learning) that are suitable for end users untrained in computer science.   Key Results:  We developed a technology called WYSIWYT/ML that allows end users to quickly assess how much trust they should place in a particular intelligent agent. Our empirical investigation showed that WYSIWYT/ML allows ordinary users to identify mistakes of a machine learning assistant by displaying various visualizations about each test instance: in only 10 minutes, ordinary users without computer science backgrounds were able to judge enough instances to adequately cover over 1000 cases. We also compared three different strategies of selecting test instances for helping users quickly assess their intelligent agents. We found that selecting test instances in which the agent was "least confident" worked overall, but that using algorithms based on "unusual instances" and "least-common features" were better able to reveal "surprise" mistakes.  We developed a crowdsourced variant of WYSIWYT/ML to investigate whether using very small "crowd" of end users (mini-crowdsourcing) to assess a machine-learning assistant is useful from a cost/benefit perspective. Mini-crowds are relevant when an intelligent agentÆs task is to serve a small group, such as a family or a team at work. Our empirical investigation showed that the mini-crowd supplied many more benefits besides the obvious decrease in workload: a crowd of as few as six found more errors, tested more of its logic, and introduced enough redundancy to reduce crowd mistakes.     Objective 2: To help ordinary users identify reasoning problems in their intelligent agents, and then allow the users to provide rich feedback  that can then be incorporated into the reasoning of the agent.  Key Results:  In identifying reasoning problems, users compare an intelligent agentÆs reasoning against a mental model they have formed as to how the reasoning is done or should be done. They form such mental models based on their own prior experiences, their assumptions, and what they see the agent do. Our empirical investigations showed that a userÆs mental model has a strong effect on the ability for the user to learn about the agent and to provide feedback to the agent that is useful enough to the agent to help it succeed better.  Building upon that result, we then investigated how an agent should attempt to explain its reasoning to ordinary users. We created a range of variants of explanation fidelity along two dimensions: soundness (which is whether an explanation contains any untruths, usually via oversimplifications) and completeness (which is the extent of the reasoning factors that are actually described in an explanation). We found that using high soundness and high completeness led to improved trust and improved understanding of the agent, and that, surprisingly, users were willing to put in the time and effort to process these higher fidelity explanations in order to improve their agents.     Objective 3: To enable the user to improve the speed and/or accuracy of their intelligent agentÆs machine learning.  Key Results:  We have developed ways for users to modify explanations themselves (not just example answers) to explain to the agent how it should change its reasoning. Our algorithms and user interface prototypes have empirically shown that users can be efficient and effective at guiding their agents to better, more correct behavior. These users do not have to spend much time doing so, and ordinary users can do so effectively&mdash;no prior background in computer scie...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
