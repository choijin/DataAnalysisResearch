<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:   A Study and implementation of Semantic Constructs for Highly Scalable Leading-edge Scientific Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
<AwardExpirationDate>08/31/2010</AwardExpirationDate>
<AwardTotalIntnAmount>69999.00</AwardTotalIntnAmount>
<AwardAmount>69999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Scaling scientific problems to 10,000,000 processors for the next generation HEC systems is today severely challenged by conventional practices of programming models, languages, and their supporting compilation systems. To achieve this goal one must expose greater degree of parallelism and improve parallel computing efficiency than is otherwise feasible with conventional methods such as MPI. &lt;br/&gt;The goal of this collaborative research project is to dramatically enhance the scalability of challenging physics problems, through the application of an innovative programming model. The strategy is to replace static message-passing course grained processes using global barrier synchronization in a distributed memory space with a model using dynamic message-driven multiple threads using lightweight synchronization objects in a partitioned global address space. Parallelism is to be extracted directly from the large irregular sparse and time varying data structures. Ephemeral user-threads will permit many simultaneous tasks over the data structures, exposing the intrinsic near-fine grain parallelism. System-wide latency will be hidden by overlapping computation with communication through the advanced communication strategy of asynchronous message-driven processing. Consequently this will enable a class of physics problems that cannot currently be done using conventional methods. &lt;br/&gt; &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>07/31/2008</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2008</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0832966</AwardID>
<Investigator>
<FirstName>Eric</FirstName>
<LastName>Hirschmann</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eric W Hirschmann</PI_FULL_NAME>
<EmailAddress>ehirsch@physics.byu.edu</EmailAddress>
<PI_PHON>8014229271</PI_PHON>
<NSF_ID>000233768</NSF_ID>
<StartDate>07/31/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>David</FirstName>
<LastName>Neilsen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David Neilsen</PI_FULL_NAME>
<EmailAddress>david.neilsen@byu.edu</EmailAddress>
<PI_PHON>8014226078</PI_PHON>
<NSF_ID>000423876</NSF_ID>
<StartDate>07/31/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Anderson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew Anderson</PI_FULL_NAME>
<EmailAddress>andersmw@indiana.edu</EmailAddress>
<PI_PHON>8128556512</PI_PHON>
<NSF_ID>000424249</NSF_ID>
<StartDate>07/31/2008</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Brigham Young University</Name>
<CityName>Provo</CityName>
<CountyName>UTAH</CountyName>
<ZipCode>846021231</ZipCode>
<PhoneNumber>8014223360</PhoneNumber>
<StreetAddress>A-285 ASB</StreetAddress>
<StreetAddress2><![CDATA[Campus Drive]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009094012</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BRIGHAM YOUNG UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001940170</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Brigham Young University]]></Name>
<CityName>Provo</CityName>
<CountyName>UTAH</CountyName>
<StateCode>UT</StateCode>
<ZipCode>846021231</ZipCode>
<StreetAddress><![CDATA[A-285 ASB]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7583</Code>
<Text>ITR-HECURA</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~69999</FUND_OBLG>
</Award>
</rootTag>
