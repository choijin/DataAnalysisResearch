<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Beyond Perspective Cameras: Multi-perspective Imaging, Reconstruction, Rendering, and Projection</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2009</AwardEffectiveDate>
<AwardExpirationDate>03/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>400001.00</AwardTotalIntnAmount>
<AwardAmount>400001</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>A perspective camera captures the spatial relationships of objects in a scene as they would appear from a single viewpoint. In contrast, a multi-perspective camera combines what is seen from several viewpoints into a single image and provides a potentially advantageous imaging system for understanding the structure of observed scenes. However, most existing computer vision and graphics algorithms are not directly applicable to multi-perspective cameras. The goal of this project is to develop a complete framework to characterize and design new multi-perspective cameras and displays and to use these systems for computer vision and graphics applications.&lt;br/&gt;&lt;br/&gt;On the vision front, the PI explores two types of real multi-perspective cameras: the first extends existing catadioptric mirrors and the second uses specially-shaped apertures. To categorize these cameras, a ray geometry analysis is applied to identify important ray structures and study their implications on image distortions. The PI also investigates several new 3D reconstruction methods, such as epsilon stereo matching, ray-curvature analysis, and curvature-from-distortions, for a broad class of multi-perspective cameras. On the graphics front, the PI explores practical multi-perspective display architectures by combining a consumer projector with specially-shaped mirrors/lenses. Such displays can offer an unprecedented level of flexibility in terms of aspect ratio, size, field of view, etc. The PI also investigates real-time multi-perspective rasterization and frustum culling algorithms by modifying the traditional rendering pipeline. &lt;br/&gt;&lt;br/&gt;The project contributes to education by involving undergraduate and graduate students in self-contained projects with both theory and application components, and attracting under-represented students through the pre-engineering program between the University of Delaware and the Delaware State University. &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>03/27/2009</MinAmdLetterDate>
<MaxAmdLetterDate>08/20/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0845268</AwardID>
<Investigator>
<FirstName>Jingyi</FirstName>
<LastName>Yu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jingyi Yu</PI_FULL_NAME>
<EmailAddress>yu@cis.udel.edu</EmailAddress>
<PI_PHON>3028310345</PI_PHON>
<NSF_ID>000487324</NSF_ID>
<StartDate>03/27/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Delaware</Name>
<CityName>Newark</CityName>
<ZipCode>197160099</ZipCode>
<PhoneNumber>3028312136</PhoneNumber>
<StreetAddress>210 Hullihen Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Delaware</StateName>
<StateCode>DE</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DE00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>059007500</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF DELAWARE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>059007500</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Delaware]]></Name>
<CityName>Newark</CityName>
<StateCode>DE</StateCode>
<ZipCode>197160099</ZipCode>
<StreetAddress><![CDATA[210 Hullihen Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Delaware</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DE00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>9150</Code>
<Text>EPSCoR Co-Funding</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~236120</FUND_OBLG>
<FUND_OBLG>2011~80551</FUND_OBLG>
<FUND_OBLG>2012~83330</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Camera models are fundamental to the fields of computer vision, computer graphics, and photogrammetry. The classic pinhole and orthographic camera models have long served as the workhorse of 3D imaging applications. Recent studies suggested alternative camera models that can capture rays from spatially varying viewpoints and provide potentially advantageous imaging systems for understanding the structure of observed scenes.</p> <p>In this NSF CAREER Award, the PI has built a complete framework to characterize and design new multiperspective cameras and displays and to use these systems for vision and graphics applications. Results from this project have&nbsp;investigated the core scientific challenges in multi-perspective imaging and projection, in both system designs and algorithm developments, including (1) designing application-specific&nbsp;multi-perspective cameras using mirrors or specially-shaped apertures; (2) 3D reconstruction from a single or multiple multi-perspective camera images; (3) real-time multi-perspective rendering pipeline based on rasterization; and (4) practical multi-perspective display architectures and algorithms.</p> <p>Through the course of this project, the PI has published a large number of papers at the premiere computer vision and computer graphics conferences and journals, has filed multiple patent applications based on the new multi-perspective camera and projector designs, has disseminated new datasets and source codes for respective algorithms, and has deloped new computational photography course and redesigned the computer graphics course at the University of Delaware. Multiple graduate students (including one female and one monirity students) supported by this grant have received Ph.D. and now work as leaders in industry in the field of computational imaging and computer vision.&nbsp;</p> <p>Analogous to the consumer digital cameras and displays in use today, these new multi-perspective cameras and displays developed by the PI through this project have the potential to redefine the landscape for images and videos and may lead to profound changes in the way people communicate, educate, and entertain. In particular, the capability of capturing and displaying what is seen from several viewpoints in a single image may enable new paradigms for scientific visualizations, photography, cinematography, tele-immersion, and education.</p><br> <p>            Last Modified: 06/30/2015<br>      Modified by: Jingyi&nbsp;Yu</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2015/0845268/0845268_10138753_1435679365500_figure1--rgov-214x142.jpg" original="/por/images/Reports/POR/2015/0845268/0845268_10138753_1435679365500_figure1--rgov-800width.jpg" title="multi-perspective processing pipeline"><img src="/por/images/Reports/POR/2015/0845268/0845268_10138753_1435679365500_figure1--rgov-66x44.jpg" alt="multi-perspective processing pipeline"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The Multi-perspective Acquisition, Reconstruction, Rendering, and Visualization Framework</div> <div class="imageCredit">Jingyi Yu</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Jingyi&nbsp;Yu</div> <div class="imageTitle">multi-perspective processing pipeline</div> </div> </li> <li> <a href="/por/images/Reports/POR/2015/0845268/0845268_10138753_1435679403188_figure2--rgov-214x142.jpg" original="/por/images/Reports/POR/2015/084...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Camera models are fundamental to the fields of computer vision, computer graphics, and photogrammetry. The classic pinhole and orthographic camera models have long served as the workhorse of 3D imaging applications. Recent studies suggested alternative camera models that can capture rays from spatially varying viewpoints and provide potentially advantageous imaging systems for understanding the structure of observed scenes.  In this NSF CAREER Award, the PI has built a complete framework to characterize and design new multiperspective cameras and displays and to use these systems for vision and graphics applications. Results from this project have investigated the core scientific challenges in multi-perspective imaging and projection, in both system designs and algorithm developments, including (1) designing application-specific multi-perspective cameras using mirrors or specially-shaped apertures; (2) 3D reconstruction from a single or multiple multi-perspective camera images; (3) real-time multi-perspective rendering pipeline based on rasterization; and (4) practical multi-perspective display architectures and algorithms.  Through the course of this project, the PI has published a large number of papers at the premiere computer vision and computer graphics conferences and journals, has filed multiple patent applications based on the new multi-perspective camera and projector designs, has disseminated new datasets and source codes for respective algorithms, and has deloped new computational photography course and redesigned the computer graphics course at the University of Delaware. Multiple graduate students (including one female and one monirity students) supported by this grant have received Ph.D. and now work as leaders in industry in the field of computational imaging and computer vision.   Analogous to the consumer digital cameras and displays in use today, these new multi-perspective cameras and displays developed by the PI through this project have the potential to redefine the landscape for images and videos and may lead to profound changes in the way people communicate, educate, and entertain. In particular, the capability of capturing and displaying what is seen from several viewpoints in a single image may enable new paradigms for scientific visualizations, photography, cinematography, tele-immersion, and education.       Last Modified: 06/30/2015       Submitted by: Jingyi Yu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
