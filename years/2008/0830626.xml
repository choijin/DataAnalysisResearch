<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Semi-algebraic complexity and models for massive data set processing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2008</AwardEffectiveDate>
<AwardExpirationDate>07/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>414481.00</AwardTotalIntnAmount>
<AwardAmount>497377</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Dmitri Maslov</SignBlockName>
<PO_EMAI>dmaslov@nsf.gov</PO_EMAI>
<PO_PHON>7032928910</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project focuses on the computational complexity of problems in semi-algebraic inference and in massive data set processing, as well as on advancing the analytical techniques of multiparty communication complexity, which has been a useful tool for understanding problems in both of these areas.&lt;br/&gt;&lt;br/&gt;Semi-algebraic inference systems express sets of constraints using polynomial inequalities and use rules of inference to derive new polynomial inequalities from existing ones.  They are widely used in operations research and combinatorial optimization.  Part of this project is to investigate what can be derived efficiently using semi-algebraic inference, including the complexity of proofs of unsatisfiability based on semi-algebraic inference, the quality of the linear and semi-definite programming approximations for NP-hard optimization problems that can derived using known semi-algebraic inference, and the extent to which our understanding of inference over binary domains can be leveraged to yield better algorithms for classes of semi-algebraic inference over the real numbers.&lt;br/&gt;&lt;br/&gt;Networked computers have allowed the accumulation of data sets of unprecedented size.  New distributed methods that process such massive data sets efficiently by giving only approximate answers are having substantial impact in practice.  However, the theoretical models of massive data set processing that have been analyzed to date represent only a very limited class of methods and do not capture techniques already used in practice.  Part of this research is aimed at producing and analyzing theoretical models that will allow us to understand the limitations of these methods for massive data set processing and to make better use of them.&lt;br/&gt;&lt;br/&gt;Multiparty communication complexity measures the amount of information that must be communicated between multiple participants, each having partial information about the inputs to a function, in order to compute its output.  Because of its flexibility it is widely applicable to problems in computational complexity.  The final goal of this project is to add to the rather limited set of techniques that can be used to analyze multiparty communication complexity with the aim goal of improving its applications to semi-algebraic inference and distributed massive data set processing.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>07/24/2008</MinAmdLetterDate>
<MaxAmdLetterDate>07/30/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0830626</AwardID>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Beame</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul W Beame</PI_FULL_NAME>
<EmailAddress>beame@cs.washington.edu</EmailAddress>
<PI_PHON>2065435114</PI_PHON>
<NSF_ID>000169746</NSF_ID>
<StartDate>07/24/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981950001</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Ave NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>2860</Code>
<Text>THEORY OF COMPUTING</Text>
</ProgramElement>
<ProgramElement>
<Code>7927</Code>
<Text>COMPLEXITY &amp; CRYPTOGRAPHY</Text>
</ProgramElement>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~270810</FUND_OBLG>
<FUND_OBLG>2010~226567</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The research supported by this grant developed new methods for analyzing the complexity of semi-algebraic inference problems.&nbsp; These are problems in which constraints are represented by multivariate polynomial inequalities, and there are inference rules to derive new constraints from a given constraint.&nbsp; The research focused on the complexity of determining whether given sets of contraints can be simultaneously satisfied given such methods for inference. The length of the derivation, given some bound on the degree of the polynomials involved, is a natural measure of the complexity of such inference.Specific systems for low degree inference are the best methods currently known for finding approximations to NP-hard optimization problems involving constraint satisfaction.&nbsp; This research developed new general methods for showing that low degree semi-algebraic inference problems require large complexity.&nbsp; These methods are based on an approach for taking problems that are of high complexity for much simpler inference methods and modifying them so that they are hard for semi-algebraic inference.</p> <p><br />This research also analyzed a relatively recent model of how massive data can be processed, the read/write streams data processing model.&nbsp; This is an extension of a much simpler one-pass data stream model that has been very important for high-speed data analysis.&nbsp; The read/write streams model also models the ability of the high-speed processing to store data streams and recombine these parallel streams sequentially at high speed.&nbsp; This analysis showed that for a variety of important problems of estimating statistics on the input data, read/write streams did not provide significant additional power relative to the simpler one-pass data streams.</p> <p>Both the research on semi-algebraic inference and read/write streams relied on relating these problems to multiparty communication complexity.&nbsp; Multiparty communication complexity studies the amount of communication (but not computation) required by cooperating agents to solve problems related to their shared inputs.&nbsp; The research supported by this award also developed a new method for analyzing multiparty communication complexity.&nbsp; One consequence of this new method was a new lower bound on the complexity required for small depth circuits to compute simple functions.<br /><br />The research supported by this award also produced new methods for analyzing the tradeoffs between the time and storage space required to solve a variety of computational and inference problems. It also produced new lower bounds on the number of input queries a quantum computer would require to compute simple functions of its input.</p> <p>This award provided extensive research support and training for two graduate students, one of whom graduated with a PhD in 2011 and the other of whom is expected to graduate with a PhD in the first half of 2013.</p><br> <p>            Last Modified: 10/18/2012<br>      Modified by: Paul&nbsp;W&nbsp;Beame</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The research supported by this grant developed new methods for analyzing the complexity of semi-algebraic inference problems.  These are problems in which constraints are represented by multivariate polynomial inequalities, and there are inference rules to derive new constraints from a given constraint.  The research focused on the complexity of determining whether given sets of contraints can be simultaneously satisfied given such methods for inference. The length of the derivation, given some bound on the degree of the polynomials involved, is a natural measure of the complexity of such inference.Specific systems for low degree inference are the best methods currently known for finding approximations to NP-hard optimization problems involving constraint satisfaction.  This research developed new general methods for showing that low degree semi-algebraic inference problems require large complexity.  These methods are based on an approach for taking problems that are of high complexity for much simpler inference methods and modifying them so that they are hard for semi-algebraic inference.   This research also analyzed a relatively recent model of how massive data can be processed, the read/write streams data processing model.  This is an extension of a much simpler one-pass data stream model that has been very important for high-speed data analysis.  The read/write streams model also models the ability of the high-speed processing to store data streams and recombine these parallel streams sequentially at high speed.  This analysis showed that for a variety of important problems of estimating statistics on the input data, read/write streams did not provide significant additional power relative to the simpler one-pass data streams.  Both the research on semi-algebraic inference and read/write streams relied on relating these problems to multiparty communication complexity.  Multiparty communication complexity studies the amount of communication (but not computation) required by cooperating agents to solve problems related to their shared inputs.  The research supported by this award also developed a new method for analyzing multiparty communication complexity.  One consequence of this new method was a new lower bound on the complexity required for small depth circuits to compute simple functions.  The research supported by this award also produced new methods for analyzing the tradeoffs between the time and storage space required to solve a variety of computational and inference problems. It also produced new lower bounds on the number of input queries a quantum computer would require to compute simple functions of its input.  This award provided extensive research support and training for two graduate students, one of whom graduated with a PhD in 2011 and the other of whom is expected to graduate with a PhD in the first half of 2013.       Last Modified: 10/18/2012       Submitted by: Paul W Beame]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
