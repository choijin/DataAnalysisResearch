<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Word Meaning: Beyond Dictionary Senses</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2009</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>433451.00</AwardTotalIntnAmount>
<AwardAmount>441451</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Most words have more than one meaning. The standard computational model for word meaning is through lists of dictionary senses. However, choosing the right dictionary sense is a highly difficult task for humans as well as machines. This CAREER project follows the hypothesis, based on current models of human concept representation, that word meaning is better described through a graded notion of similarity than through dictionary senses. The hypothesis is tested through a novel meaning annotation framework and computationally through a vector space model of word meaning. The model uses vector characterizations of typical arguments to compute the meaning of an individual occurrence compositionally from the words in its syntactic context. For evaluation, the project focuses on the ability to draw appropriate inferences, in both shallow and deep frameworks, from similarity-based meaning representations. The research effort goes together with educational work that focuses on supporting undergraduate research, stressing hands-on data exploration and interdisciplinary work.&lt;br/&gt;&lt;br/&gt;The characterization of word meaning is a central issue in lexical semantics and in computational linguistics as a whole. This CAREER project will yield a broadly applicable paradigm that describes word meaning without recourse to dictionary senses. It aims both to provide a more cognitively adequate model and to benefit language technology applications, in particular information retrieval, which already relies heavily on vector space models.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/10/2009</MinAmdLetterDate>
<MaxAmdLetterDate>05/22/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0845925</AwardID>
<Investigator>
<FirstName>Katrin</FirstName>
<LastName>Erk</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Katrin Erk</PI_FULL_NAME>
<EmailAddress>katrin.erk@mail.utexas.edu</EmailAddress>
<PI_PHON>5129837913</PI_PHON>
<NSF_ID>000258351</NSF_ID>
<StartDate>08/10/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787595316</ZipCode>
<StreetAddress><![CDATA[3925 W Braker Lane, Ste 3.340]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~122700</FUND_OBLG>
<FUND_OBLG>2010~82980</FUND_OBLG>
<FUND_OBLG>2011~129394</FUND_OBLG>
<FUND_OBLG>2012~25929</FUND_OBLG>
<FUND_OBLG>2013~80448</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Words change their meaning depending on the context in which they appear. "Throwing popsicles at someone" is something different from "throwing questions at someone". The usual way for describing these different meanings is through a list of senses in a dictionary. But on closer look, these sense lists do not work well. The same word can often be used&nbsp; to mean many slightly different things, compare "raw sewage", "raw hamburger", "raw wool", and "raw beauty".</p> <p>For language technology applications like question answering, we need to analyze sentences automatically to determine what the words mean and how they connect. In this project, we have developed word meaning representations that are more flexible than sense lists, and we have developed ways to integrate them into sentence processing.</p> <p>First we took a closer look at the phenomenon by asking people to rate the degree of similarity between pairs of occurrences of a word. The resulting dataset is publicly available. We found that people do use intermediate levels of similarity, but not equally for all words. Some, like "to fire", have easily separable senses ("fire a gun" versus "fire an employee"), others, like "raw" above, do not. So for some words dictionary senses are fine, but for others they are not flexible enough.</p> <p>To represent word meaning in a more flexible way, we used distributional models. They represent a word through the contexts in which it appears in text. Words that occur in similar contexts are similar in meaning. This can be visualized as a space where each word is a point, and similar words are close together. We extended distributional models to word occurrences: Similar occurrences are close together, less similar ones at greater distance. So we represent word meanings without dictionary senses, as points in space at varying distance.&nbsp;</p> <p>To use this representation in practice, we have built systems that determine possible paraphrases from these points in space, such that differences in paraphrases indicate differences in meaning: "raw wool" is unprocessed, and "raw hamburger" is uncooked. This paraphrasing task has been picked up by other groups, and there is now a range of systems for this task.</p> <p>These systems do not just say yes or no, they assign degrees of fit to each paraphrase: For "raw hamburger", "uncooked" is a good paraphrase, "unprocessed" is okay, and "harsh" is not a good fit. To be able to draw conclusions from whole sentences, we designed a system that combines graded information about word meaning (degrees of paraphrase fit) with an exact, logic-based representation of how the words in a sentence are connected. We have shown that this system can be used to draw conclusions from text.</p> <p>We have also performed further analysis of distributional models. People have long wondered what the "meaning similarity" means that these models detect. We have shown that&nbsp; distributional information provides clues about the properties of words. Words that appear in similar contexts have similar properties: "I don't know what a breadfruit is, but from the contexts in which I have seen it appear, it seems to be something like apple, so I guess that it is edible".</p> <p>The project has provided training for 5 graduate students and for 7 undergraduate students, including one graduate and one undergraduate student who turned their work on the project into successful Ph.D. scholarships, and one undergraduate who won a scholarship for her honors thesis related to the project. It has also served as the springboard for the development of two graduate seminars, one undergraduate class, and one intercontinental reading group that meets via teleconference.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/21/2015<br>      Modified by: Katrin&nbsp;Erk</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Words change their meaning depending on the context in which they appear. "Throwing popsicles at someone" is something different from "throwing questions at someone". The usual way for describing these different meanings is through a list of senses in a dictionary. But on closer look, these sense lists do not work well. The same word can often be used  to mean many slightly different things, compare "raw sewage", "raw hamburger", "raw wool", and "raw beauty".  For language technology applications like question answering, we need to analyze sentences automatically to determine what the words mean and how they connect. In this project, we have developed word meaning representations that are more flexible than sense lists, and we have developed ways to integrate them into sentence processing.  First we took a closer look at the phenomenon by asking people to rate the degree of similarity between pairs of occurrences of a word. The resulting dataset is publicly available. We found that people do use intermediate levels of similarity, but not equally for all words. Some, like "to fire", have easily separable senses ("fire a gun" versus "fire an employee"), others, like "raw" above, do not. So for some words dictionary senses are fine, but for others they are not flexible enough.  To represent word meaning in a more flexible way, we used distributional models. They represent a word through the contexts in which it appears in text. Words that occur in similar contexts are similar in meaning. This can be visualized as a space where each word is a point, and similar words are close together. We extended distributional models to word occurrences: Similar occurrences are close together, less similar ones at greater distance. So we represent word meanings without dictionary senses, as points in space at varying distance.   To use this representation in practice, we have built systems that determine possible paraphrases from these points in space, such that differences in paraphrases indicate differences in meaning: "raw wool" is unprocessed, and "raw hamburger" is uncooked. This paraphrasing task has been picked up by other groups, and there is now a range of systems for this task.  These systems do not just say yes or no, they assign degrees of fit to each paraphrase: For "raw hamburger", "uncooked" is a good paraphrase, "unprocessed" is okay, and "harsh" is not a good fit. To be able to draw conclusions from whole sentences, we designed a system that combines graded information about word meaning (degrees of paraphrase fit) with an exact, logic-based representation of how the words in a sentence are connected. We have shown that this system can be used to draw conclusions from text.  We have also performed further analysis of distributional models. People have long wondered what the "meaning similarity" means that these models detect. We have shown that  distributional information provides clues about the properties of words. Words that appear in similar contexts have similar properties: "I don't know what a breadfruit is, but from the contexts in which I have seen it appear, it seems to be something like apple, so I guess that it is edible".  The project has provided training for 5 graduate students and for 7 undergraduate students, including one graduate and one undergraduate student who turned their work on the project into successful Ph.D. scholarships, and one undergraduate who won a scholarship for her honors thesis related to the project. It has also served as the springboard for the development of two graduate seminars, one undergraduate class, and one intercontinental reading group that meets via teleconference.          Last Modified: 08/21/2015       Submitted by: Katrin Erk]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
