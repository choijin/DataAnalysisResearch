<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>A Phase II trial of the Systems Evaluation Protocol for Assessing and Improving STEM Education Evaluation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2008</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>2300753.00</AwardTotalIntnAmount>
<AwardAmount>2528216</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Finbarr Sloane</SignBlockName>
<PO_EMAI>fsloane@nsf.gov</PO_EMAI>
<PO_PHON>7032928465</PO_PHON>
</ProgramOfficer>
<AbstractNarration>DRL- 0814364&lt;br/&gt;PI:  William Trochim&lt;br/&gt;&lt;br/&gt;PROJECT Abstract&lt;br/&gt;&lt;br/&gt;The investigator has created a protocol for developing evaluations of programs using a web-based system.  The project will assess the degree to which the Systems Evaluation Protocol (SEP) is associated with changes in organizational evaluation capacity and performance outcomes.  The investigator will conduct a trial of how well this protocol would work in two NSF funded research settings:  a Materials Science and Engineering Centers and a 4-H Engineering and Technology program.  The project includes a development phase where the investigator will be testing and enhancing the on-line system using a team of evaluators, educators, and software engineers.   It also includes a phase to empirically measure the results of the evaluation protocol on evaluation quality.  The intent of the project is to significantly enhance knowledge of STEM education evaluation and how to implement it with rigor.  The results are intended to inform educators and education researchers about the strengths and weaknesses of these evaluation methods.  The researcher has established an intern program to advance the role of underrepresented populations in evaluation.  &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/21/2008</MinAmdLetterDate>
<MaxAmdLetterDate>09/23/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0814364</AwardID>
<Investigator>
<FirstName>William</FirstName>
<LastName>Trochim</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William M Trochim</PI_FULL_NAME>
<EmailAddress>wmt1@cornell.edu</EmailAddress>
<PI_PHON>6072550887</PI_PHON>
<NSF_ID>000315066</NSF_ID>
<StartDate>08/21/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell Univ - State: AWDS MADE PRIOR MAY 2010]]></Name>
<CityName>Ithica</CityName>
<StateCode>NY</StateCode>
<ZipCode>148502488</ZipCode>
<StreetAddress><![CDATA[373 Pine Tree Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7625</Code>
<Text>REAL</Text>
</ProgramElement>
<ProgramReference>
<Code>9177</Code>
<Text>ELEMENTARY/SECONDARY EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0408</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0410</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~2300753</FUND_OBLG>
<FUND_OBLG>2010~227463</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Over the course of this project, we developed a unique, systems-oriented evaluation capacity building process that was effectively implemented with 62 STEM outreach programs and can be (and has been) used with a diverse array of programs in a variety of disciplines (Urban &amp; Trochim, 2009; Urban, Hargraves, &amp; Trochim, 2014). This project resulted in several tangible outputs that provide a direct benefit to society by enabling program planning and evaluation across a diverse range of disciplines. We developed several resources that are available free of charge to the public and can help anyone to plan an effective program evaluation.</p> <p>&nbsp;</p> <p>One of the major outputs of this effort is the Systems Evaluation Protocol, a written guidebook and set of resources that provide a systematic process for developing evaluation plans while addressing the systems issues inherent in any outreach or educational program. We published (in hard copy and electronically) the Guide to the Systems Evaluation Protocol (2012) which provides in-depth explanations, advice, and exercises that can be used for evaluation capacity building efforts. More recently, we also published the Workbook to the Systems Evaluation Protocol (2015) which includes worksheets, descriptive text, and FAQs that support each of the Protocol tasks.</p> <p>&nbsp;</p> <p>Perhaps the single most notable outcome, unanticipated in our original proposal, was the development of the Netway (<a href="http://www.evaluationnetway.com">www.evaluationnetway.com</a>; 2015) a web-based tool, educational resource, and networking platform for developing program models and evaluation plans. The Netway was originally designed to be used in conjunction with the Systems Evaluation Protocol, but we eventually integrated the Protocol entirely into the Netway. Consequently, anyone who uses the Netway tool to develop evaluation plans and models will have context sensitive access to the Protocol to guide their efforts through every step of the process. All of these resources were developed for and used with our partnering programs for this project. The Netway system, now encompassing hundreds of programs, was made available to the public in the summer of 2015. We expect that this database will itself be an import source of information about current and future programmatic efforts, and will be increasingly useful in research on evaluation going forward.</p> <p>&nbsp;</p> <p>In the process of conducting this research, we developed, managed, facilitated, and evaluated the progress of 62 STEM outreach programs (52 programs completed the evaluation planning phase and 24+ completed the implementation phase). In total, we worked with more than 104 STEM program practitioners. In order to effectively evaluate our work, we developed three measurement rubrics that assess the quality of logic models, pathway models, and evaluation plans (Urban, Burgermaster, Archibald, &amp; Byrne, 2015). These rubrics are generalizable to any evaluation contexts where such elements are developed. We also developed and tested a measure of Evaluation Satisfaction and Attitudes. These measures can play an important role in improving the quality of evaluations.</p> <p>&nbsp;</p> <p>We produced a number of important research publications. One paper provides the foundation for linking program evaluations with broader bodies of research evidence using the mechanism of causal pathway models and the metaphor of the &ldquo;golden spike&rdquo; (Urban &amp; Trochim, 2009). A second paper introduced and defined the idea of evaluative thinking, a construct that emerged as central as this research project evolved (Buckely, Archibald, Hargraves &amp; Trochim, 2015). We wrote a significant paper that extended earlier thinking on evolutionary epistemology in the philosophy of science into the realm of evaluation, providing a central theoretical basis for a new way of ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Over the course of this project, we developed a unique, systems-oriented evaluation capacity building process that was effectively implemented with 62 STEM outreach programs and can be (and has been) used with a diverse array of programs in a variety of disciplines (Urban &amp; Trochim, 2009; Urban, Hargraves, &amp; Trochim, 2014). This project resulted in several tangible outputs that provide a direct benefit to society by enabling program planning and evaluation across a diverse range of disciplines. We developed several resources that are available free of charge to the public and can help anyone to plan an effective program evaluation.     One of the major outputs of this effort is the Systems Evaluation Protocol, a written guidebook and set of resources that provide a systematic process for developing evaluation plans while addressing the systems issues inherent in any outreach or educational program. We published (in hard copy and electronically) the Guide to the Systems Evaluation Protocol (2012) which provides in-depth explanations, advice, and exercises that can be used for evaluation capacity building efforts. More recently, we also published the Workbook to the Systems Evaluation Protocol (2015) which includes worksheets, descriptive text, and FAQs that support each of the Protocol tasks.     Perhaps the single most notable outcome, unanticipated in our original proposal, was the development of the Netway (www.evaluationnetway.com; 2015) a web-based tool, educational resource, and networking platform for developing program models and evaluation plans. The Netway was originally designed to be used in conjunction with the Systems Evaluation Protocol, but we eventually integrated the Protocol entirely into the Netway. Consequently, anyone who uses the Netway tool to develop evaluation plans and models will have context sensitive access to the Protocol to guide their efforts through every step of the process. All of these resources were developed for and used with our partnering programs for this project. The Netway system, now encompassing hundreds of programs, was made available to the public in the summer of 2015. We expect that this database will itself be an import source of information about current and future programmatic efforts, and will be increasingly useful in research on evaluation going forward.     In the process of conducting this research, we developed, managed, facilitated, and evaluated the progress of 62 STEM outreach programs (52 programs completed the evaluation planning phase and 24+ completed the implementation phase). In total, we worked with more than 104 STEM program practitioners. In order to effectively evaluate our work, we developed three measurement rubrics that assess the quality of logic models, pathway models, and evaluation plans (Urban, Burgermaster, Archibald, &amp; Byrne, 2015). These rubrics are generalizable to any evaluation contexts where such elements are developed. We also developed and tested a measure of Evaluation Satisfaction and Attitudes. These measures can play an important role in improving the quality of evaluations.     We produced a number of important research publications. One paper provides the foundation for linking program evaluations with broader bodies of research evidence using the mechanism of causal pathway models and the metaphor of the "golden spike" (Urban &amp; Trochim, 2009). A second paper introduced and defined the idea of evaluative thinking, a construct that emerged as central as this research project evolved (Buckely, Archibald, Hargraves &amp; Trochim, 2015). We wrote a significant paper that extended earlier thinking on evolutionary epistemology in the philosophy of science into the realm of evaluation, providing a central theoretical basis for a new way of thinking about systems in evaluation based on evolutionary and ecological theories.     The results of our research showed that there was a general and significant upward trend toward more...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
