<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III-COR-Small: Bootstrapping Adaptive Personalized Music Search with Game-based Collaborative Tagging</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
<AwardExpirationDate>12/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>444171.00</AwardTotalIntnAmount>
<AwardAmount>476171</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This proposal addresses novel ways of indexing large music databases programmatically. It proposes to use melody, rhythm, and to some extent lyrics to index musical databases. The PI proposes to build a search engine for music that takes sung examples as input for searching relevant songs. The search engine adapts to individual needs through feedback and ongoing personalization to improve the search results. Finding ways to automatically index, label, and access multimedia content (such as music documents) is a research question that is increasing in importance as multimedia databases proliferate and grow.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/27/2008</MinAmdLetterDate>
<MaxAmdLetterDate>05/03/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0812314</AwardID>
<Investigator>
<FirstName>Bryan</FirstName>
<LastName>Pardo</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bryan A Pardo</PI_FULL_NAME>
<EmailAddress>pardo@northwestern.edu</EmailAddress>
<PI_PHON>8474917184</PI_PHON>
<NSF_ID>000275342</NSF_ID>
<StartDate>08/27/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<StreetAddress2><![CDATA[Rubloff 7th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>160079455</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHWESTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005436803</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northwestern University]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606114579</ZipCode>
<StreetAddress><![CDATA[750 N. Lake Shore Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9216</Code>
<Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~444171</FUND_OBLG>
<FUND_OBLG>2009~16000</FUND_OBLG>
<FUND_OBLG>2011~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>PROBLEM STATEMENT: Finding ways to automatically index, label, and access multimedia content (such as music documents) is a research question that is increasing in importance as multimedia databases proliferate and grow. Collections of music recordings, such as the millions of songs in Apple Computer&rsquo;s iTunes repository, form one of the most popular categories of on-line multimedia content. These collections are indexed by such features as title, composer, and performer. Finding the desired recording through this indexing scheme is a problem for users who do not already know the metadata for the desired piece. Often, those who can&rsquo;t recall the name of a song can provide an example of some aspect of the musical content, such as the melody (&ldquo;What is the name of that song that goes like this ?&rdquo;) or the rhythms (&ldquo;Find songs that have this rhythm: &lsquo;boom,&nbsp; ka CHAK, boom, boom ka CHAK&rdquo;). To handle such queries, researchers must build databases that are indexed by the musical content, develop interfaces that let people make queries in a natural manner, and create search methods that quickly return results on large databases.</p> <p><br />RESEARCH OBJECTIVES:&nbsp; In this work, we developed the following key technologies: <br />&bull;&nbsp;&nbsp;&nbsp; Methods to create and vet perceptually relevant search keys for music audio that allow the automatic creation of very large music databases indexed by musical content<br />&bull;&nbsp;&nbsp;&nbsp; Automated methods to quickly model each individual&rsquo;s music perception/production as a distance metric useful for real-world music search<br />&bull;&nbsp;&nbsp;&nbsp; Methods to speed search by organizing large music databases using personalized distance metrics developed from user interaction<br />These advances are embodied in a new query-by-example search engine for music available at tunebot.org. This search engine allows naturally sung examples as input and continuously improves search results in response to user feedback. The system does not require hand-coded search keys, as the search keys are actually crowd-sourced: users of the system sing contributions that are then used as examples the system compares to in future searches. We also developed methods to autonomously vet existing search keys and add new search keys to the database, based on user queries. To speed data collection and encourage collaborative participation from the public, we integrate this learning system with an on-line social music game that encouraged collaborative tagging of audio.</p> <p><br />INTELLECTUAL MERIT: This work has led to new insight into the mappings between human music production/perception and machine-measurable features of music. Effective methods to automatically tag large databases of multimedia content with perceptually relevant search keys are a significant advance. Methods for automatic self-vetting of databases are useful anywhere potentially corrupt database keys are a problem. The social music game developed for this work is an example of a new kind of asynchronous musical interaction that leverages existing technology to allow new kinds of computer-mediated socialization.</p> <p><br />BROADER IMPACT: Effective, fast, personalized multimedia search is an advance for society as a whole, with a potential impact as large as text search engines (such as Google) have had. The search engine, itself, has been visited by 200,000 people since January 2010. The data collected from this can be used to facilitate research in the broader research community involved in multimedia and audio search. We have published results in the relevant journals and conferences, as well as disseminating results to students through the PI&rsquo;s course on machine perception of music. <br /><br /></p><br> <p>            Last Modified: 01/30/2013<br>      Modified by: Bryan&nbsp;A&nbsp;Pardo</p> </div> <div...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ PROBLEM STATEMENT: Finding ways to automatically index, label, and access multimedia content (such as music documents) is a research question that is increasing in importance as multimedia databases proliferate and grow. Collections of music recordings, such as the millions of songs in Apple ComputerÆs iTunes repository, form one of the most popular categories of on-line multimedia content. These collections are indexed by such features as title, composer, and performer. Finding the desired recording through this indexing scheme is a problem for users who do not already know the metadata for the desired piece. Often, those who canÆt recall the name of a song can provide an example of some aspect of the musical content, such as the melody ("What is the name of that song that goes like this ?") or the rhythms ("Find songs that have this rhythm: æboom,  ka CHAK, boom, boom ka CHAK"). To handle such queries, researchers must build databases that are indexed by the musical content, develop interfaces that let people make queries in a natural manner, and create search methods that quickly return results on large databases.   RESEARCH OBJECTIVES:  In this work, we developed the following key technologies:  &bull;    Methods to create and vet perceptually relevant search keys for music audio that allow the automatic creation of very large music databases indexed by musical content &bull;    Automated methods to quickly model each individualÆs music perception/production as a distance metric useful for real-world music search &bull;    Methods to speed search by organizing large music databases using personalized distance metrics developed from user interaction These advances are embodied in a new query-by-example search engine for music available at tunebot.org. This search engine allows naturally sung examples as input and continuously improves search results in response to user feedback. The system does not require hand-coded search keys, as the search keys are actually crowd-sourced: users of the system sing contributions that are then used as examples the system compares to in future searches. We also developed methods to autonomously vet existing search keys and add new search keys to the database, based on user queries. To speed data collection and encourage collaborative participation from the public, we integrate this learning system with an on-line social music game that encouraged collaborative tagging of audio.   INTELLECTUAL MERIT: This work has led to new insight into the mappings between human music production/perception and machine-measurable features of music. Effective methods to automatically tag large databases of multimedia content with perceptually relevant search keys are a significant advance. Methods for automatic self-vetting of databases are useful anywhere potentially corrupt database keys are a problem. The social music game developed for this work is an example of a new kind of asynchronous musical interaction that leverages existing technology to allow new kinds of computer-mediated socialization.   BROADER IMPACT: Effective, fast, personalized multimedia search is an advance for society as a whole, with a potential impact as large as text search engines (such as Google) have had. The search engine, itself, has been visited by 200,000 people since January 2010. The data collected from this can be used to facilitate research in the broader research community involved in multimedia and audio search. We have published results in the relevant journals and conferences, as well as disseminating results to students through the PIÆs course on machine perception of music.          Last Modified: 01/30/2013       Submitted by: Bryan A Pardo]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
