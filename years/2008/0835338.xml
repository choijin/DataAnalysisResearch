<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Technological and Educational Foundations for Understanding and Improving Large-classroom Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2009</AwardEffectiveDate>
<AwardExpirationDate>07/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>317000.00</AwardTotalIntnAmount>
<AwardAmount>332975</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Large-enrollment courses are a practical necessity for introductory courses in science, technology, engineering and mathematics (STEM) at many institutions. Innovative technologies, such as audience response systems, can enhance instruction in large-enrollment classes, but the information that can be conveyed with these existing technologies remains quite limited. The goal of this project is to develop a new role for technology in large STEM classes, one that exploits advances in computer vision technology and the rapid proliferation of digital video. By building a multi-camera array to simultaneously observe all individuals in a large classroom, the investigators will pursue foundational research in both education and computer vision. For computer vision, large classrooms provide a convenient microcosm of social interaction in which individual activities are constrained but not controlled; this project will leverage this property to develop vision-based recognition systems for large group activites. Educationally, this new vision system will be used to systematically study learning in large classrooms---something that has not previously been possible.&lt;br/&gt;&lt;br/&gt;Insights gained from research in these two areas will be used to create a radically new tool for computer-assisted collaborative instruction. This project will develop systems to automatically detect and summarize real-time activity information for course instructors, thereby enhancing their ability to make optimal use of interactive class time. These systems are viewed as prototypes that can ultimately be replicated at other institutions. In addition, the results of this research into the nature of learning in large classrooms will serve as a basis for improving instruction in large-enrollment STEM courses nationwide, regardless of their technological assets. More broadly, the project offers a new paradigm for education research, in which small-scale ecological observations are scaled up by automated visual activity recognition.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>08/11/2009</MinAmdLetterDate>
<MaxAmdLetterDate>08/15/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0835338</AwardID>
<Investigator>
<FirstName>Eric</FirstName>
<LastName>Mazur</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eric Mazur</PI_FULL_NAME>
<EmailAddress>mazur@physics.harvard.edu</EmailAddress>
<PI_PHON>6174958729</PI_PHON>
<NSF_ID>000101021</NSF_ID>
<StartDate>08/11/2009</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Todd</FirstName>
<LastName>Zickler</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Todd Zickler</PI_FULL_NAME>
<EmailAddress>zickler@eecs.harvard.edu</EmailAddress>
<PI_PHON>6174954390</PI_PHON>
<NSF_ID>000118883</NSF_ID>
<StartDate>08/11/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard University]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021385369</ZipCode>
<StreetAddress><![CDATA[1033 MASSACHUSETTS AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>1707</Code>
<Text>ADVANCED LEARNING TECHNOLOGIES</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1707</Code>
<Text>ADVANCED LEARNING TECHNOLOGIES</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~145000</FUND_OBLG>
<FUND_OBLG>2010~86000</FUND_OBLG>
<FUND_OBLG>2011~101975</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Intellectual Merit. </strong></p> <p>We have provided a cornerstone for computer vision-based observation, understanding, and augmentation of large-classroom learning. We built a multi-camera and microphone array to simultaneously observe student-to-student interactions for the duration of a four-month undergraduate physics course, and used this system for foundational research in education and computer vision.</p> <p>For computer vision, large classrooms are a convenient microcosm of social interaction in which individual activities are constrained but not controlled; and we leveraged this property to develop vision-based recognition systems for large group activities. We created a suite of computer vision tools, including a general framework for the video-based analysis of group behavior, new computational representations of group behavior, and new computational processes for localizing and recognizing salient group behaviors that are embedded in a crowd of non-participants.</p> <p>For education,&nbsp;our&nbsp;system&nbsp;was used to&nbsp;study learning in large classrooms by systematically&nbsp;observing interactions of an entire student population&mdash;something that&nbsp;had&nbsp;not previously been possible. Experts in education analyzed the audio and video manually, forming the basis for associating particular kinds of learning interactions with visible student behaviors, and for developing a coding scheme of behaviors that occur in this instructional environment. Based on our analysis of over fifty student discussion video segments, we have learned that in interactive lecture classrooms, students work together to build new physics ideas; that the seating arrangement in a classroom significantly affects how students talk to one another; and that visual cues reveal the kind of conversations students are having, whether or not we can hear what they are saying.</p> <p>Finally, we created an advanced audience response system, called Learning Catalytics, that substantially broadens the scope of interactive teaching. It is a web-based platform that acts as a computational catalyst for enhanced human-to-human interaction in classrooms. Unlike traditional "clickers", which allow the one-way flow of multiple-choice or simple-text answers from students to instructor, this new platform allows information to flow from instructor to students. Moreover, since it is web-based, it operates on common laptops and smart-phones and can be readily extended to allow soliciting, recording, and analyzing very rich forms of student responses, including graphs, drawings, free text, algebraic expressions, photographs, and more.</p> <p><strong>Broader Impacts. </strong></p> <p>This project was carried out by a research team with significant expertise in computer vision, observation-based education research, and collaborative instruction. The results of our research into the nature of learning in large classrooms are a basis for improving instruction in large-enrollment STEM courses nationwide, regardless of their technological assets. Our project offers a new paradigm for education research, in which we aim for small-scale ecological observations to be scaled up by automated visual activity recognition.</p> <p>The project also had broader impacts for research in computer vision. Machine learning techniques have shown tremendous potential for the automatic understanding of human interactions at a small scale, but to date, we have lacked the &ldquo;ground-truth&rdquo; data that is required to train and test related approaches for large groups. The constrained activity in classrooms provided an environment in which models for uncontrolled large group interactions were trained and tested for the first time. Thus, although the focus of our research was in large classrooms, our models are applicable more generally for surveillance, video archival and retrieval, human-computer intera...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual Merit.   We have provided a cornerstone for computer vision-based observation, understanding, and augmentation of large-classroom learning. We built a multi-camera and microphone array to simultaneously observe student-to-student interactions for the duration of a four-month undergraduate physics course, and used this system for foundational research in education and computer vision.  For computer vision, large classrooms are a convenient microcosm of social interaction in which individual activities are constrained but not controlled; and we leveraged this property to develop vision-based recognition systems for large group activities. We created a suite of computer vision tools, including a general framework for the video-based analysis of group behavior, new computational representations of group behavior, and new computational processes for localizing and recognizing salient group behaviors that are embedded in a crowd of non-participants.  For education, our system was used to study learning in large classrooms by systematically observing interactions of an entire student population&mdash;something that had not previously been possible. Experts in education analyzed the audio and video manually, forming the basis for associating particular kinds of learning interactions with visible student behaviors, and for developing a coding scheme of behaviors that occur in this instructional environment. Based on our analysis of over fifty student discussion video segments, we have learned that in interactive lecture classrooms, students work together to build new physics ideas; that the seating arrangement in a classroom significantly affects how students talk to one another; and that visual cues reveal the kind of conversations students are having, whether or not we can hear what they are saying.  Finally, we created an advanced audience response system, called Learning Catalytics, that substantially broadens the scope of interactive teaching. It is a web-based platform that acts as a computational catalyst for enhanced human-to-human interaction in classrooms. Unlike traditional "clickers", which allow the one-way flow of multiple-choice or simple-text answers from students to instructor, this new platform allows information to flow from instructor to students. Moreover, since it is web-based, it operates on common laptops and smart-phones and can be readily extended to allow soliciting, recording, and analyzing very rich forms of student responses, including graphs, drawings, free text, algebraic expressions, photographs, and more.  Broader Impacts.   This project was carried out by a research team with significant expertise in computer vision, observation-based education research, and collaborative instruction. The results of our research into the nature of learning in large classrooms are a basis for improving instruction in large-enrollment STEM courses nationwide, regardless of their technological assets. Our project offers a new paradigm for education research, in which we aim for small-scale ecological observations to be scaled up by automated visual activity recognition.  The project also had broader impacts for research in computer vision. Machine learning techniques have shown tremendous potential for the automatic understanding of human interactions at a small scale, but to date, we have lacked the "ground-truth" data that is required to train and test related approaches for large groups. The constrained activity in classrooms provided an environment in which models for uncontrolled large group interactions were trained and tested for the first time. Thus, although the focus of our research was in large classrooms, our models are applicable more generally for surveillance, video archival and retrieval, human-computer interaction.                         Last Modified: 11/29/2012       Submitted by: Todd Zickler]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
