<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>OMP-D:   Shared-Address-Space Model and Programming System for High-End Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2008</AwardEffectiveDate>
<AwardExpirationDate>09/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>608666.00</AwardTotalIntnAmount>
<AwardAmount>721602</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This research develops shared-address-space (SAS) programming models for distributed computer architectures. Almost all of today's high-end computers (HEC) have a distributed system architecture. Programming such architectures is tedious and contributes to very high software costs. SAS programming is easier and leads to higher software productivity. Achieving the set goals of developing translators that efficiently execute SAS programs on distributed computer systems may have a large impact on the software industry and on the way software engineering for HEC is taught.&lt;br/&gt;&lt;br/&gt;The specific research is motivated by recent successes of a new SAS model and compiler that has demonstrated to perform close to hand-coded message passing programs by successful translation of the OpenMP SAS model to MPI.  &lt;br/&gt;This project brings together expertise in programming models, compilers for parallel computing, compiler infrastructures, and languages for HEC, including HPF and OpenMP.  The new system will allow current OpenMP programs to run on large HEC platforms, and will allow programmers to incrementally port and debug their sequential programs to utilize the OpenMP SAS model while achieving the benefits of larger-scale parallelism than is available on a single node.  A number of novel compiler techniques, such as dynamic affinity optimizations, scalability enhancements, runtime array range detection, multicore integration, and parallel data-flow semantics will be developed and implemented in a prototype OMP-D (OpenMP for Distributed architectures) translator.  The performance will be evaluated on a large set of benchmarks from the NAS, SPEC OMP, and SPEC MPI suites as well as several other HEC applications.</AbstractNarration>
<MinAmdLetterDate>08/01/2008</MinAmdLetterDate>
<MaxAmdLetterDate>08/26/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0833115</AwardID>
<Investigator>
<FirstName>Rudolf</FirstName>
<LastName>Eigenmann</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rudolf Eigenmann</PI_FULL_NAME>
<EmailAddress>eigenman@udel.edu</EmailAddress>
<PI_PHON>3028310678</PI_PHON>
<NSF_ID>000315744</NSF_ID>
<StartDate>08/01/2008</StartDate>
<EndDate>06/06/2013</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Samuel</FirstName>
<LastName>Midkiff</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Samuel P Midkiff</PI_FULL_NAME>
<EmailAddress>smidkiff@purdue.edu</EmailAddress>
<PI_PHON>7654943440</PI_PHON>
<NSF_ID>000344348</NSF_ID>
<StartDate>08/26/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Samuel</FirstName>
<LastName>Midkiff</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Samuel P Midkiff</PI_FULL_NAME>
<EmailAddress>smidkiff@purdue.edu</EmailAddress>
<PI_PHON>7654943440</PI_PHON>
<NSF_ID>000344348</NSF_ID>
<StartDate>08/01/2008</StartDate>
<EndDate>06/06/2013</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<StreetAddress2><![CDATA[155 S Grant Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072051394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072051394</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName>West Lafayette</CityName>
<StateCode>IN</StateCode>
<ZipCode>479072114</ZipCode>
<StreetAddress><![CDATA[Young Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7583</Code>
<Text>ITR-HECURA</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramElement>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~608666</FUND_OBLG>
<FUND_OBLG>2009~16000</FUND_OBLG>
<FUND_OBLG>2013~96936</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="Body">The existing OpenMP standard is a set of extensions to Fortran, C and C++ that allow shared memory computers, such as multicore processors, to be easily programmed.&nbsp; OpenMP suffers from the fact that shared memory computers typically allow 16 or 32-way parallelism, limiting the increase in performance from parallelism to less than 32.&nbsp; The MPI Message Passing Interface allows programs to be run on distributed memory machines with thousands of processors.&nbsp; The primary goal of the OMP-D project was to explore new techniques that can compile OpenMP programs to run on distributed memory machines using MPI.</p> <p class="Body">The intellectual challenge of this work is developing techniques to partition parallel OpenMP loops onto the distributed machine; to determine, from this partitioning, what data is needed on each processor for the loop iterations it executes; to determine what data must be sent from a processor that computed the last value of the data to another processor that needs to read that data.&nbsp; The project used the default OpenMP static distribution scheme to partition the loop iteration space across processors.&nbsp; Our initial work then performed a compile time analysis that identified the read and written data items in a loop nest in the form of regular section expressions, which are then evaluated at runtime.&nbsp; The compiler also identified points where communication between processors needs to take place.&nbsp; It then inserted calls to a runtime system at these points for the runtime system to evaluate the expressions, determine communication and perform the communication.&nbsp;</p> <p class="Body">For programs that are repetitive, i.e., the data being communicated and the involved processors are the same at different invocations of every communication point, the above scheme works well.&nbsp; This is because the compiler can pull the calls to the runtime system out of loops and evaluate them only once, amortizing the cost of finding the communication pattern.&nbsp; If either the communicated data or the communication pattern changes, the overhead of evaluating what needs to be communicated must be done at every communication point.&nbsp; To overcome this extra overhead, we developed the pi abstraction, a way of representing the data accessed by distributed loops and an algebra that allows pi abstractions for different loops to be combined to find data that must be communicated.&nbsp; This allows the determination of what must be communicated to be performed at compile time for many programs, leading to low runtime overheads and good performance. &nbsp;</p> <p class="Body">With these analyses we can achieve 50% or more of handwritten MPI performance on OpenMP programs.&nbsp; This allows students in courses and computational scientists to scale their programs to larger machines by simply recompiling their programs with the new OMP-D system.&nbsp; It also allows computational scientists to develop programs using the simpler OpenMP programming model and then run them on distributed machines.&nbsp; Finally, we have made the original runtime based system available at <a href="http://cetus.ecn.purdue.edu">http://cetus.ecn.purdue.edu</a> to enable additional research.&nbsp; We plan to release the pi abstraction based system in the near future.</p><br> <p>            Last Modified: 03/24/2015<br>      Modified by: Samuel&nbsp;P&nbsp;Midkiff</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[The existing OpenMP standard is a set of extensions to Fortran, C and C++ that allow shared memory computers, such as multicore processors, to be easily programmed.  OpenMP suffers from the fact that shared memory computers typically allow 16 or 32-way parallelism, limiting the increase in performance from parallelism to less than 32.  The MPI Message Passing Interface allows programs to be run on distributed memory machines with thousands of processors.  The primary goal of the OMP-D project was to explore new techniques that can compile OpenMP programs to run on distributed memory machines using MPI. The intellectual challenge of this work is developing techniques to partition parallel OpenMP loops onto the distributed machine; to determine, from this partitioning, what data is needed on each processor for the loop iterations it executes; to determine what data must be sent from a processor that computed the last value of the data to another processor that needs to read that data.  The project used the default OpenMP static distribution scheme to partition the loop iteration space across processors.  Our initial work then performed a compile time analysis that identified the read and written data items in a loop nest in the form of regular section expressions, which are then evaluated at runtime.  The compiler also identified points where communication between processors needs to take place.  It then inserted calls to a runtime system at these points for the runtime system to evaluate the expressions, determine communication and perform the communication.  For programs that are repetitive, i.e., the data being communicated and the involved processors are the same at different invocations of every communication point, the above scheme works well.  This is because the compiler can pull the calls to the runtime system out of loops and evaluate them only once, amortizing the cost of finding the communication pattern.  If either the communicated data or the communication pattern changes, the overhead of evaluating what needs to be communicated must be done at every communication point.  To overcome this extra overhead, we developed the pi abstraction, a way of representing the data accessed by distributed loops and an algebra that allows pi abstractions for different loops to be combined to find data that must be communicated.  This allows the determination of what must be communicated to be performed at compile time for many programs, leading to low runtime overheads and good performance.   With these analyses we can achieve 50% or more of handwritten MPI performance on OpenMP programs.  This allows students in courses and computational scientists to scale their programs to larger machines by simply recompiling their programs with the new OMP-D system.  It also allows computational scientists to develop programs using the simpler OpenMP programming model and then run them on distributed machines.  Finally, we have made the original runtime based system available at http://cetus.ecn.purdue.edu to enable additional research.  We plan to release the pi abstraction based system in the near future.       Last Modified: 03/24/2015       Submitted by: Samuel P Midkiff]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
