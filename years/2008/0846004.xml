<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Deterministic Shared Memory Multiprocessing: Vision, Architecture, and Impact on Programmability</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2009</AwardEffectiveDate>
<AwardExpirationDate>02/29/2016</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>558938</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>tao li</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Software innovation typically relies on performance improvements of&lt;br/&gt;the underlying hardware. However, technology limitations hinder&lt;br/&gt;further significant progress in single-thread performance. Therefore,&lt;br/&gt;the software industry has the immense problem of rethinking its&lt;br/&gt;software development process and techniques to adopt multicore&lt;br/&gt;systems. Popularizing parallel programming is a Grand Research&lt;br/&gt;Challenge for the systems community [CRA]. Being able to leverage the&lt;br/&gt;full potential of multicores would put us back into exponential growth&lt;br/&gt;of usable performance as well as lead to significant power savings.&lt;br/&gt;&lt;br/&gt;One of the main reasons why parallel programming is hard is that&lt;br/&gt;parallel code in current multicore systems can execute&lt;br/&gt;nondeterministically. Each time a multicore executes a parallel&lt;br/&gt;application, it can produce a different output even if supplied with&lt;br/&gt;the same input. This frustrates debugging efforts and limits the&lt;br/&gt;ability to properly test parallel code, becoming a major obstacle to&lt;br/&gt;widespread adoption of parallel programming. &lt;br/&gt;&lt;br/&gt;This project poses broad intellectual questions with far-reaching&lt;br/&gt;implications in modern computer systems: Can nondeterminism be removed&lt;br/&gt;from shared-memory multiprocessor systems without degrading&lt;br/&gt;performance? What are the trade-offs in designing deterministic&lt;br/&gt;multiprocessor systems?  What are the implications and uses of&lt;br/&gt;deterministic behavior in programmability? The PI plans to answer these&lt;br/&gt;questions by devising efficient, general purpose, fully deterministic&lt;br/&gt;shared memory multiprocessor systems and demonstrating that they can&lt;br/&gt;enable significant changes in how parallel programs are written,&lt;br/&gt;tested and deployed. An integral part of the concurrency challenge is&lt;br/&gt;education, so this project also aims to develop a graduate and&lt;br/&gt;undergraduate curriculum that will teach students about concurrency&lt;br/&gt;principles and practical parallel programming.</AbstractNarration>
<MinAmdLetterDate>02/13/2009</MinAmdLetterDate>
<MaxAmdLetterDate>06/05/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0846004</AwardID>
<Investigator>
<FirstName>Luis</FirstName>
<LastName>Ceze</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Luis Ceze</PI_FULL_NAME>
<EmailAddress>luisceze@cs.washington.edu</EmailAddress>
<PI_PHON>2065431896</PI_PHON>
<NSF_ID>000083036</NSF_ID>
<StartDate>02/13/2009</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981950001</ZipCode>
<StreetAddress><![CDATA[4333 Brooklyn Ave NE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2009~179502</FUND_OBLG>
<FUND_OBLG>2010~101282</FUND_OBLG>
<FUND_OBLG>2011~88887</FUND_OBLG>
<FUND_OBLG>2012~92658</FUND_OBLG>
<FUND_OBLG>2013~96609</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="Default">We developed techniques for deterministic multiprocessing. The key idea is to make arbitrary parallel programs execute deterministically. This dramatically simplifies debugging, testing, and replication of multithreaded programs. The non-determinism problem is at the heart of the programmability issues in current multicore systems.&nbsp; The bulk of the research was on computer architecture and compilers. We also worked on on language level techniques, operating system support, distributed systems, testing methodologies, and new static analysis techniques that further improve the utility of deterministic execution. This past year we have explored distributed systems and compilation ideas to exploit data locality, and symbolic execution techniques. We demonstrated that it is possible to explore storage class memories to execute a collection of processes (container) with continuous checkpointing of state at a low performance cost (~10%). Over the course of the entire project, we published over a dozen papers in major venues such as ASPLOS, MICRO, OOPSLA and Usenix ATC.</p> <p class="Default">&nbsp;</p> <p>We have shown, contrary to popular belief in the field of parallel computer architecture, that it is possible to provide deterministic execution of arbitrary programs with little cost. We have also shown that it is possible to conveniently deal with fundamental external non-determinism. This settled some deep issues with how deterministic multiprocessing interacts with the external world. We have also shown that it is possible to deeply integrate determinism across the system stack, from language to compilers to testing systems to OS, hardware and even distributed systems. We then used the techniques developed to make distributed systems scale and to make concurrent programs safer. The project initiated collaborations with faculty in Programming Languages as well as Operating Systems and distributed systems. Some of the technology described was licensed by a startup company.&nbsp; Other groups have been using our released software to do follow-on research. This research has an impact in industry. We have heard interest from startup companies and established companies. Related work led to a startup company that created over a dozen jobs and&nbsp; also brought the benefits of this technology to the IT industry.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/20/2016<br>      Modified by: Luis&nbsp;Ceze</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[We developed techniques for deterministic multiprocessing. The key idea is to make arbitrary parallel programs execute deterministically. This dramatically simplifies debugging, testing, and replication of multithreaded programs. The non-determinism problem is at the heart of the programmability issues in current multicore systems.  The bulk of the research was on computer architecture and compilers. We also worked on on language level techniques, operating system support, distributed systems, testing methodologies, and new static analysis techniques that further improve the utility of deterministic execution. This past year we have explored distributed systems and compilation ideas to exploit data locality, and symbolic execution techniques. We demonstrated that it is possible to explore storage class memories to execute a collection of processes (container) with continuous checkpointing of state at a low performance cost (~10%). Over the course of the entire project, we published over a dozen papers in major venues such as ASPLOS, MICRO, OOPSLA and Usenix ATC.    We have shown, contrary to popular belief in the field of parallel computer architecture, that it is possible to provide deterministic execution of arbitrary programs with little cost. We have also shown that it is possible to conveniently deal with fundamental external non-determinism. This settled some deep issues with how deterministic multiprocessing interacts with the external world. We have also shown that it is possible to deeply integrate determinism across the system stack, from language to compilers to testing systems to OS, hardware and even distributed systems. We then used the techniques developed to make distributed systems scale and to make concurrent programs safer. The project initiated collaborations with faculty in Programming Languages as well as Operating Systems and distributed systems. Some of the technology described was licensed by a startup company.  Other groups have been using our released software to do follow-on research. This research has an impact in industry. We have heard interest from startup companies and established companies. Related work led to a startup company that created over a dozen jobs and  also brought the benefits of this technology to the IT industry.             Last Modified: 05/20/2016       Submitted by: Luis Ceze]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
