<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Large-vocabulary Semantic Image Processing: Theory and Algorithms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
<AwardExpirationDate>12/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>239499.00</AwardTotalIntnAmount>
<AwardAmount>1860696</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Cozzens</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Classical image processing has mostly disregarded semantic image representations, in favor of more mathematically tractable representations based on low?]level signal properties (frequency decompositions, mean squared error, etc.). This is unlike biological solutions to image processing&lt;br/&gt;problems, which rely extensively on understanding of scene content. For example, regions of faces are usually processed more carefully than the bushes in the background. The inability to tune image processing to the semantic relevance of image content frequently leads to the sub?]optimal allocation of&lt;br/&gt;resources, such as bandwidth, error protection, or viewing time, to image areas that are perceptually irrelevant. One of the main obstacles to the deployment of semantic image processing systems has been the difficulty of training content?]understanding systems with large scale vocabularies. This is, in great part, due to the requirement for large amounts of training data and intensive human supervision associated with the classical methods for vocabulary learning. This research aims to establish a foundation for semantic image processing systems that can learn large scale vocabularies from&lt;br/&gt;informally annotated data and no additional human supervision. It builds on recent advances in semantic image labeling, which have made it possible to learn vocabularies from noisy training data, such as that massively (and inexpensively) available on the web. The research studies both theoretical&lt;br/&gt;issues in vocabulary learning, and the design of image processing algorithms that tune their behavior according to the content of the images being processed. Semantic image processing could lead to transformative advances in areas such as image compression, enhancement, encryption, de?]noising, or&lt;br/&gt;segmentation, among others, which are of interest for applications as diverse as medical imaging, image search and retrieval, or security and surveillance.</AbstractNarration>
<MinAmdLetterDate>07/17/2008</MinAmdLetterDate>
<MaxAmdLetterDate>09/13/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0830535</AwardID>
<Investigator>
<FirstName>Nuno</FirstName>
<LastName>Vasconcelos</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nuno M Vasconcelos</PI_FULL_NAME>
<EmailAddress>nuno@ece.ucsd.edu</EmailAddress>
<PI_PHON>8585345550</PI_PHON>
<NSF_ID>000104017</NSF_ID>
<StartDate>07/17/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[Office of Contract &amp; Grant A]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>4720</Code>
<Text>SIGNAL PROCESSING SYS PROGRAM</Text>
</ProgramElement>
<ProgramElement>
<Code>I198</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>J168</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>L565</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>L570</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>L585</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>m201</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>4720</Code>
<Text>SIGNAL PROCESSING SYS PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>7797</Code>
<Text>COMM &amp; INFORMATION FOUNDATIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~239499</FUND_OBLG>
<FUND_OBLG>2009~402062</FUND_OBLG>
<FUND_OBLG>2010~206139</FUND_OBLG>
<FUND_OBLG>2012~778771</FUND_OBLG>
<FUND_OBLG>2013~234225</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project produced a number of contributions in the semantic representation of images. This is a novel image representation, which replaces the space of image pixels by a semantic space, where each dimension represents a visual concept. A vocabulary of visual concepts (objects, object attributes, scene classes, etc.) is first defined and a classifier is learned to detect the different concepts in the image. The image is finally represented by the vector of probabilities of containing the concepts in the vocabulary. This is known as the semantic multinomial (SMN) descriptor of the image. The process is illustrated in Figure 1.</p> <p>The project showed that this&nbsp; representation is superior to the classical representation of the image by pixels, or features derived from those pixels, for a number of image processing operations.</p> <p>Image retrieval: This addresses the search for images within an image database. The standard paradigm is query-by-visual-example (QBVE), where a vector of features is derived from each image and used to measure image similarity. Database images are then ranked by similarity to a query image. The project has introduced the query-by-semantic &ndash;example (QBSE) paradigm, where the feature vector is replaced by the SMN descriptor. This corresponds to measuring similarity between images in the semantic space. Since this space has a higher level of abstraction than that of classical image features (colors, textures, etc.) it enables retrieval systems with higher generalization ability and robustness. &nbsp;This is illustrated in Figure 2, where we compare the images retrieved&nbsp; by QBSE and QBVE, for a common image query. For QBSE, we also show the concepts of highest probability for each image.&nbsp; Note that the classic QBVE approach tends to return images with similar colors and textures and does not work well for complicated scenes. &nbsp;On the other hand, the QBSE approach now proposed returns images that contain similar concepts, e.g. peoples and&nbsp; buildings, to those in the query. This allows the retrieval operation to succeed even when these concepts appear in different sizes, positions, colors, etc. In result, the retrieval operation mimics the similarity judgments of people much more closely than under QBVE.</p> <p>Cross-modal retrieval: One difficult problem in multimedia is to bring together information from multiple modalities. For example, how does one search an image database for the best match to a query text. The semantic representation provides a universal solution to this problem. Since, the representation is based on concept probabilities, not pixels, sounds, or characters, it provides an abstract space where information of multiple modalities can be equally projected. After all, concept classifiers can be built for images, sound, speech, music, or text with nearly equal difficulty. This is illustrated in Figure 3, for an example involving image and text data. Images and texts are initially represented in an image and text space, respectively. A common semantic vocabulary is defined, and a set of concept classifiers built on each of these spaces. This allows the mapping of images and texts&nbsp; into a common semantic space. In this space, it is trivial to measure similarities from data of different modalities, e.g. find the text article that best matches a query image, or vice-versa.</p> <p>Action and event recognition: Video classification requires reasoning in terms of semantic events. Figure 4 illustrates this with an example of video from TV coverage of the Olympic games. At the highest level, the video can be broken down into the events &ldquo;context shot,&rdquo; &ldquo;pole vault,&rdquo; &ldquo;triple jump,&rdquo; and &ldquo;100 m dash.&rdquo; Each of these concepts is itself defined in terms of semantic properties of lower level. For example, the &ldquo;long jump&rdquo; event is visually simila...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project produced a number of contributions in the semantic representation of images. This is a novel image representation, which replaces the space of image pixels by a semantic space, where each dimension represents a visual concept. A vocabulary of visual concepts (objects, object attributes, scene classes, etc.) is first defined and a classifier is learned to detect the different concepts in the image. The image is finally represented by the vector of probabilities of containing the concepts in the vocabulary. This is known as the semantic multinomial (SMN) descriptor of the image. The process is illustrated in Figure 1.  The project showed that this  representation is superior to the classical representation of the image by pixels, or features derived from those pixels, for a number of image processing operations.  Image retrieval: This addresses the search for images within an image database. The standard paradigm is query-by-visual-example (QBVE), where a vector of features is derived from each image and used to measure image similarity. Database images are then ranked by similarity to a query image. The project has introduced the query-by-semantic &ndash;example (QBSE) paradigm, where the feature vector is replaced by the SMN descriptor. This corresponds to measuring similarity between images in the semantic space. Since this space has a higher level of abstraction than that of classical image features (colors, textures, etc.) it enables retrieval systems with higher generalization ability and robustness.  This is illustrated in Figure 2, where we compare the images retrieved  by QBSE and QBVE, for a common image query. For QBSE, we also show the concepts of highest probability for each image.  Note that the classic QBVE approach tends to return images with similar colors and textures and does not work well for complicated scenes.  On the other hand, the QBSE approach now proposed returns images that contain similar concepts, e.g. peoples and  buildings, to those in the query. This allows the retrieval operation to succeed even when these concepts appear in different sizes, positions, colors, etc. In result, the retrieval operation mimics the similarity judgments of people much more closely than under QBVE.  Cross-modal retrieval: One difficult problem in multimedia is to bring together information from multiple modalities. For example, how does one search an image database for the best match to a query text. The semantic representation provides a universal solution to this problem. Since, the representation is based on concept probabilities, not pixels, sounds, or characters, it provides an abstract space where information of multiple modalities can be equally projected. After all, concept classifiers can be built for images, sound, speech, music, or text with nearly equal difficulty. This is illustrated in Figure 3, for an example involving image and text data. Images and texts are initially represented in an image and text space, respectively. A common semantic vocabulary is defined, and a set of concept classifiers built on each of these spaces. This allows the mapping of images and texts  into a common semantic space. In this space, it is trivial to measure similarities from data of different modalities, e.g. find the text article that best matches a query image, or vice-versa.  Action and event recognition: Video classification requires reasoning in terms of semantic events. Figure 4 illustrates this with an example of video from TV coverage of the Olympic games. At the highest level, the video can be broken down into the events "context shot," "pole vault," "triple jump," and "100 m dash." Each of these concepts is itself defined in terms of semantic properties of lower level. For example, the "long jump" event is visually similar to a "triple jump" event. In fact, the only way to distinguish the two is that while a "long jump" is a sequence of the simpler actions "run" and "jump," a "triple jump" is a sequ...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
