<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Interior-point algorithms for conic optimization with sparse matrix cone constraints</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>303100.00</AwardTotalIntnAmount>
<AwardAmount>303100</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Junping Wang</SignBlockName>
<PO_EMAI>jwang@nsf.gov</PO_EMAI>
<PO_PHON>7032924488</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Conic optimization is an extension of linear programming in which the &lt;br/&gt;componentwise vector inequalities are replaced by inequalities with &lt;br/&gt;respect to nonpolyhedral convex cones.  The conic optimization model is  &lt;br/&gt;widely used in the recent literature on convex optimization and provides&lt;br/&gt;an elegant framework for extending interior-point algorithms from linear &lt;br/&gt;programming to convex optimization.  It is also the basis of popular &lt;br/&gt;modeling systems for convex optimization.  &lt;br/&gt;The research on algorithms for conic optimization has mainly focused on  &lt;br/&gt;three types of inequalities, associated with the nonnegative orthant, &lt;br/&gt;the second-order cone, and the positive semidefinite cone.  &lt;br/&gt;This restriction is motivated by symmetry properties that can be exploited &lt;br/&gt;to formulate symmetric primal-dual interior-point algorithms.&lt;br/&gt;However, large gaps in linear algebra complexity exist between the &lt;br/&gt;three types of conic constraints, and this can lead to inefficiencies when &lt;br/&gt;convex optimization problems are converted to the standard conic format.  &lt;br/&gt;This study considers approaches to improve the efficiency of conic optimization &lt;br/&gt;solvers by considering a larger class of conic constraints, &lt;br/&gt;defined by chordal sparse matrix cones, i.e., cones of positive &lt;br/&gt;semidefinite matrices with a given chordal sparsity pattern, &lt;br/&gt;and the associated dual cones of chordal sparse matrices that &lt;br/&gt;have a positive semidefinite completion.  These cones include as special &lt;br/&gt;cases the three standard cones, but also several interesting non-self-dual &lt;br/&gt;cones.  Moreover non-chordal sparsity patterns can often be efficiently &lt;br/&gt;embedded in chordal patterns and, as a consequence, sparse semidefinite &lt;br/&gt;programs can be solved as non-symmetric cone programs involving &lt;br/&gt;lower-dimensional cones than the positive semidefinite cone used in  &lt;br/&gt;semidefinite programming methods.  The choice for chordal matrix cones is  &lt;br/&gt;further motivated by the existence of fast algorithms for evaluating the &lt;br/&gt;associated barrier functions and their derivatives.&lt;br/&gt;The investigator and his collaborators study nonsymmetric &lt;br/&gt;interior-point algorithms for sparse matrix cones, building on techniques &lt;br/&gt;developed for large-scale sparse matrix computations, in particular, &lt;br/&gt;multifrontal and supernodal factorization algorithms and parallel sparse &lt;br/&gt;matrix algorithms.&lt;br/&gt;&lt;br/&gt;A wide variety of practical problems in engineering and science can be  &lt;br/&gt;formulated as nonlinear convex optimization problems, and solved using &lt;br/&gt;algorithms developed over the last few decades.  &lt;br/&gt;The success of these techniques has created a demand for robust and &lt;br/&gt;efficient algorithms for very large convex optimization problems, &lt;br/&gt;especially for applications in machine learning, computer vision, &lt;br/&gt;electronic design automation, sensor networks, and combinatorial &lt;br/&gt;optimization.  The problem sizes that arise in these fields often &lt;br/&gt;exceed the capabilities of general-purpose solvers.  &lt;br/&gt;The work of the prinicipal investigator with his collaborators considers approaches to improve the scalability of interior-point&lt;br/&gt;algorithms, an important class of convex optimization algorithms.&lt;br/&gt;Freely available high-quality software implementations of the techniques developed in the&lt;br/&gt;project are a product of the research.</AbstractNarration>
<MinAmdLetterDate>06/06/2011</MinAmdLetterDate>
<MaxAmdLetterDate>06/06/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1115963</AwardID>
<Investigator>
<FirstName>Lieven</FirstName>
<LastName>Vandenberghe</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lieven Vandenberghe</PI_FULL_NAME>
<EmailAddress>vandenbe@ee.ucla.edu</EmailAddress>
<PI_PHON>3102061259</PI_PHON>
<NSF_ID>000488711</NSF_ID>
<StartDate>06/06/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName>LOS ANGELES</CityName>
<StateCode>CA</StateCode>
<ZipCode>900951406</ZipCode>
<StreetAddress><![CDATA[10889 Wilshire Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~303100</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Semidefinite optimization is an important class of convex optimization and has been studied for applications in control, signal processing, statistics, combinatorial optimization, and other disciplines.&nbsp;&nbsp; It is also used extensively in modeling systems for convex optimization, which are based on converting general convex optimization problems to equivalent semidefinite optimization problems (SDPs).&nbsp;&nbsp; Many of these applications of semidefinite optimization involve large problems that are difficult to solve by existing general-purpose implementations of interior-point algorithms.&nbsp; This project has contributed to algorithms for solving large sparse semidefinite optimization, by combining advances in convex optimization algorithms with techniques from sparse matrix theory and graph theory.<br /><br />As a first contribution, we developed fast algorithms for evaluating logarithmic barrier functions for cones of positive semidefinite matrices matrices with a given sparsity pattern, and the derivatives of the barrier functions.&nbsp; Efficient algorithms for these barrier computations are important as building blocks of interior-point methods for sparse semidefinite optimization.&nbsp; The techniques used in the algorithms extend multifrontal and supernodal algorithms for sparse Cholesky factorization.<br /><br />Second, we developed decomposition methods that make it possible to solve large sparse SDPs iteratively, by solving a sequence of quadratic semidefinite optimization problems that are less expensive to solve by interior-point methods than the full problem.&nbsp; For many problems, the quadratic subproblems are separable and reduce to independent smaller problems that can be solved in parallel.&nbsp; Related decomposition methods were also&nbsp; developed for certain classes of sparse matrix nearness problems.&nbsp; Here the problem is to fit to a given matrix a matrix with a specified sparsity pattern and with the additional property of being positive semidefinite, having a positive semidefinite completion, or having a Euclidean distance matrix completion. Problems of this type arise in statistics and signal processing, for example, in the problem of fitting a covariance matrix to&nbsp; estimates of a subset of its entries.&nbsp; In the decomposition approach these problems are reduced to solving a sequence of smaller dense eigenvalue problems.<br /><br />In a third component of the project we developed primal-dual strategies for decomposing general convex optimization problems, by applying splitting algorithms to the primal-dual optimality conditions.&nbsp; This can be viewed as an extension of dual decomposition techniques (for optimization problems that are separable after removal of coupling constraints) and primal decomposition (for problems that are separable except for a few coupling variables) to more general types&nbsp; of problem structure.&nbsp; As specific applications we considered problems of image deblurring via regularized convex optimization.</p> <p>The project has resulted in several publications, supplemented with software available from the PI's website.&nbsp; Some of the results have been incorporated into a graduate course on large-scale optimization at UCLA, and in lectures by the PI in short courses and summer schools.</p><br> <p>            Last Modified: 11/23/2015<br>      Modified by: Lieven&nbsp;Vandenberghe</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Semidefinite optimization is an important class of convex optimization and has been studied for applications in control, signal processing, statistics, combinatorial optimization, and other disciplines.   It is also used extensively in modeling systems for convex optimization, which are based on converting general convex optimization problems to equivalent semidefinite optimization problems (SDPs).   Many of these applications of semidefinite optimization involve large problems that are difficult to solve by existing general-purpose implementations of interior-point algorithms.  This project has contributed to algorithms for solving large sparse semidefinite optimization, by combining advances in convex optimization algorithms with techniques from sparse matrix theory and graph theory.  As a first contribution, we developed fast algorithms for evaluating logarithmic barrier functions for cones of positive semidefinite matrices matrices with a given sparsity pattern, and the derivatives of the barrier functions.  Efficient algorithms for these barrier computations are important as building blocks of interior-point methods for sparse semidefinite optimization.  The techniques used in the algorithms extend multifrontal and supernodal algorithms for sparse Cholesky factorization.  Second, we developed decomposition methods that make it possible to solve large sparse SDPs iteratively, by solving a sequence of quadratic semidefinite optimization problems that are less expensive to solve by interior-point methods than the full problem.  For many problems, the quadratic subproblems are separable and reduce to independent smaller problems that can be solved in parallel.  Related decomposition methods were also  developed for certain classes of sparse matrix nearness problems.  Here the problem is to fit to a given matrix a matrix with a specified sparsity pattern and with the additional property of being positive semidefinite, having a positive semidefinite completion, or having a Euclidean distance matrix completion. Problems of this type arise in statistics and signal processing, for example, in the problem of fitting a covariance matrix to  estimates of a subset of its entries.  In the decomposition approach these problems are reduced to solving a sequence of smaller dense eigenvalue problems.  In a third component of the project we developed primal-dual strategies for decomposing general convex optimization problems, by applying splitting algorithms to the primal-dual optimality conditions.  This can be viewed as an extension of dual decomposition techniques (for optimization problems that are separable after removal of coupling constraints) and primal decomposition (for problems that are separable except for a few coupling variables) to more general types  of problem structure.  As specific applications we considered problems of image deblurring via regularized convex optimization.  The project has resulted in several publications, supplemented with software available from the PI's website.  Some of the results have been incorporated into a graduate course on large-scale optimization at UCLA, and in lectures by the PI in short courses and summer schools.       Last Modified: 11/23/2015       Submitted by: Lieven Vandenberghe]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
