<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Advances and Applications in Submodularity for Machine Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2012</AwardEffectiveDate>
<AwardExpirationDate>06/30/2018</AwardExpirationDate>
<AwardTotalIntnAmount>814509.00</AwardTotalIntnAmount>
<AwardAmount>814509</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Weng-keen Wong</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Submodularity is an intuitive diminishing returns property, stating that adding an element to a smaller set helps more than adding it to a larger set. Submodularity allows one to efficiently find provably optimal or near-optimal solutions to discrete problems.  Submodular minimization has found use, e.g., in graphical model inference and clustering, whereas maximization has been applied, e.g., to variable/feature selection and active learning.  Submodularity, however, is still only beginning to show applicability in machine learning and its applications.  Moreover, work on submodular optimization in the combinatorics and operations research literature has been primarily unaware of unique problems arising in machine learning. Therefore, existing standard algorithms do not exploit certain structures or variants of the submodular problems arising in machine learning. Studying novel machine learning problems involving submodular objectives can thus lead to advances in the pure combinatorics literature.  We propose to pursue activities that bring together research in machine learning and combinatorial optimization to solve problems which neither of the communities can solve alone.&lt;br/&gt;&lt;br/&gt;In particular, we propose to use insights from machine learning to enable scaling up typical submodular optimization problem sizes (by focusing on problem instances arising in learning). We also propose to further chart the territory that submodularity plays in machine learning. In this grant, we will introduce new submodular structures specifically related to submodularity. We will introduce submodular learning problems for machine learning. We will introduce new submodular optimization problems with constraints. And lastly, we will apply these submodular instances to real-world applications in computer vision, speech recognition, and natural language processing.</AbstractNarration>
<MinAmdLetterDate>04/05/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/30/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1162606</AwardID>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Bilmes</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeffrey A Bilmes</PI_FULL_NAME>
<EmailAddress>bilmes@uw.edu</EmailAddress>
<PI_PHON>2062215236</PI_PHON>
<NSF_ID>000168952</NSF_ID>
<StartDate>04/05/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952500</ZipCode>
<StreetAddress><![CDATA[Electrical Engr, Box  352500]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~173783</FUND_OBLG>
<FUND_OBLG>2013~260462</FUND_OBLG>
<FUND_OBLG>2014~267576</FUND_OBLG>
<FUND_OBLG>2015~112688</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Submodularity is an intuitive diminishing returns property, stating that adding an element to a set helps more than adding it to a larger set (see the figure). The main goals of this project has been to study and advance the use of submodular functions and their use in machine learning and data science. The goals of the project can be described by the following four main threads:</p> <p>1) Develop new submodular structures, which are mathematical constructs that use submodularity useful for machine learning (either theoretically or practically or ideally both).</p> <p>2) Study new methods to learn submodular functions.</p> <p>3) Study submodular function optimization under submodular constraints.</p> <p>4) Apply submodular functions to applications, such as summarization tasks of various forms, clustering applications, and feature selection tasks, and applications in data science fields, such as natural language processing or computational biology.</p> <p>The results of this grant has made major advances in these directions, and has directly resulted many publications including three best paper awards at major machine learning conferences, including NeurIPS (Neural Information Processing Systems, formally known as NIPS), and ICML (International Conference on Machine Learning).</p> <p>One important application submodularity, amongst many others, is data summarization, where we wish to take a large and difficult to manage data set and reduce its size to something more manageable but without incurring information loss. As an example, imagine a large collection of photos, and we wish to select a representative subset that we might wish to show to a family member.&nbsp; The results of this research has directly and significantly advanced the technology behind all forms of data summarization.</p><br> <p>            Last Modified: 03/20/2019<br>      Modified by: Jeffrey&nbsp;A&nbsp;Bilmes</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1162606/1162606_10162247_1553124487318_urns--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1162606/1162606_10162247_1553124487318_urns--rgov-800width.jpg" title="diminishing returns very simple example"><img src="/por/images/Reports/POR/2019/1162606/1162606_10162247_1553124487318_urns--rgov-66x44.jpg" alt="diminishing returns very simple example"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The urn's value is the number of different ball colors, not the number of balls. Left: the value increases when we add a blue ball since a blue ball is not present.  Right: the value does not increase when we add a blue ball as there is already a blue ball present. This is diminishing returns.</div> <div class="imageCredit">Jeff A. Bilmes</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Jeffrey&nbsp;A&nbsp;Bilmes</div> <div class="imageTitle">diminishing returns very simple example</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Submodularity is an intuitive diminishing returns property, stating that adding an element to a set helps more than adding it to a larger set (see the figure). The main goals of this project has been to study and advance the use of submodular functions and their use in machine learning and data science. The goals of the project can be described by the following four main threads:  1) Develop new submodular structures, which are mathematical constructs that use submodularity useful for machine learning (either theoretically or practically or ideally both).  2) Study new methods to learn submodular functions.  3) Study submodular function optimization under submodular constraints.  4) Apply submodular functions to applications, such as summarization tasks of various forms, clustering applications, and feature selection tasks, and applications in data science fields, such as natural language processing or computational biology.  The results of this grant has made major advances in these directions, and has directly resulted many publications including three best paper awards at major machine learning conferences, including NeurIPS (Neural Information Processing Systems, formally known as NIPS), and ICML (International Conference on Machine Learning).  One important application submodularity, amongst many others, is data summarization, where we wish to take a large and difficult to manage data set and reduce its size to something more manageable but without incurring information loss. As an example, imagine a large collection of photos, and we wish to select a representative subset that we might wish to show to a family member.  The results of this research has directly and significantly advanced the technology behind all forms of data summarization.       Last Modified: 03/20/2019       Submitted by: Jeffrey A Bilmes]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
