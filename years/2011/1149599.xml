<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Information Misperceptions in the Internet Era</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2012</AwardEffectiveDate>
<AwardExpirationDate>02/28/2018</AwardExpirationDate>
<AwardTotalIntnAmount>536771.00</AwardTotalIntnAmount>
<AwardAmount>536771</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Bainbridge</SignBlockName>
<PO_EMAI>wbainbri@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Scholars have observed that online news and political talk have the potential to promote belief in false or misleading factual claims, frequently attributing this to distinctive characteristics of the Internet such as the absence of gatekeepers, the freedom to screen out disagreeable evidence, and personalization systems that shield news consumers from uncomfortable truths.  However, these mechanisms are largely speculative, and do not align well with existing data.  This project moves toward a theoretically grounded and empirically tested understanding of political misperceptions in the Internet era. &lt;br/&gt;&lt;br/&gt;Hypotheses focus both on broad effects associated with use of online news media, and on the mechanisms through which these effects occur. Specifically, a series of media-effects hypotheses address the prevalence and consequences (1) of partisan bias in media exposure decisions, (2) of the use of social media as a source of political news, and (3) of automated (and often invisible) personalization technologies that shape what online news consumers see. A second more nuanced set of predictions concerns how these technologies could lead individuals to be more likely to accept as true inaccurate political information. These hypotheses link the attributes of the technologies in question with established theoretical work on processing fluency, affect, and biased assimilation. &lt;br/&gt;&lt;br/&gt;To test the hypotheses, the study will pair multi-wave surveys of representative samples of Americans with a series of controlled experiments designed to evaluate the specific mechanisms theorized. A three-wave survey will be conducted in year one, and a two-wave survey in year five, corresponding with U.S. Presidential elections. Measuring respondents' use of various online political news sources and services in early waves, and assessing respondents' store of political knowledge and misperceptions in later waves will provide clear evidence concerning the consequences of using these Internet technologies. The first survey will also include an embedded experiment in order to test the influence of processing fluency in the field. A series of interrelated experiments will be conducted in the intervening years. Year two will focus on testing the influence of metacognitive experiences, such as processing fluency, on the acceptance of false information and factual corrections. In year three the emphasis will be on affect, testing the influence of emotional responses to political claims on individuals' assessments. Year four experiments will focus on biased assimilation as it informs credibility effects, and on the relative importance of institutional trust and individual judgment on participants' beliefs. &lt;br/&gt;&lt;br/&gt;The datasets will be shared with students and other scholars, further enhancing the impact of the research endeavor. A new undergraduate course, intended to help students become more informed consumers of political information, will be based on the theoretical and empirical work coming out of this research.  Public outreach, communicating key lessons concerning the sources and remedies of political misperceptions to journalists and the broader public, as well as publications in scientific venues, will raise awareness of effective strategies for assessing political information.</AbstractNarration>
<MinAmdLetterDate>02/27/2012</MinAmdLetterDate>
<MaxAmdLetterDate>02/08/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1149599</AwardID>
<Investigator>
<FirstName>R.</FirstName>
<LastName>Garrett</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>R. K Garrett</PI_FULL_NAME>
<EmailAddress>garrett.258@osu.edu</EmailAddress>
<PI_PHON>6142477414</PI_PHON>
<NSF_ID>000595334</NSF_ID>
<StartDate>02/27/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888735</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1960 Kenny Road]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832127323</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001964634</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio State University]]></Name>
<CityName>Columbus</CityName>
<StateCode>OH</StateCode>
<ZipCode>432101016</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~198038</FUND_OBLG>
<FUND_OBLG>2013~60161</FUND_OBLG>
<FUND_OBLG>2014~63768</FUND_OBLG>
<FUND_OBLG>2015~64181</FUND_OBLG>
<FUND_OBLG>2016~150623</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project extended our understanding of how Internet technologies interact with psychological processes to promote or correct misperceptions. It included a series of studies aimed at examining how beliefs are formed and updated, and at testing predictions about the effects of online media on misperceptions, with the ultimate goal of proposing strategies to fight misinformation.</p> <p>Our first insights concern psychological factors that contribute to misperceptions. Politically biased beliefs are commonplace, but there are other factors that contribute to misperceptions. &nbsp;We found that individuals&rsquo; beliefs about how they &ldquo;know&rdquo; what&rsquo;s true and what&rsquo;s not, which we refer to as their epistemic beliefs, are also influential. For example, the more people trust their intuition to tell them what is true, the more likely they are to embrace conspiracy theories. &nbsp;In contrast, people who place more value on their ability to provide evidence for their beliefs are less likely to engage in conspiracist thinking and to endorse falsehoods about divisive political issues. This may seem like commonsense, but Americans&rsquo; epistemic beliefs vary widely: similar numbers of people embrace and reject intuition as a way of knowing.</p> <p>Emotions also shape misperceptions. &nbsp;Individuals&rsquo; emotional state alters how they respond to new information. Those who are angry tend to rely heavily on what their political party leaders say when deciding what they believe, while those who feel anxious are more swayed by information about the evidence. People also treat emotions themselves as &ldquo;information&rdquo; that they can use when deciding what to believe. Claims eliciting emotions matching how the individual already feels tend to be seen as more plausible. &nbsp;For example, an angry person is more inclined to believe a rumor that someone took advantage of him than a person who is not angry.</p> <p>This project also sheds light on the role of online technologies, including social media. Results suggest that social media use was associated with a small increase in endorsement of candidate falsehoods during the 2012 U.S. Presidential election. This isn&rsquo;t just a matter of social media being exploited to spread misinformation. Online social interactions that have nothing to do with misinformation can also promote misperceptions. &nbsp;For example, we have evidence that individuals who receive fewer &ldquo;likes&rdquo; on their posts than their peers tend to feel more socially excluded, and this in turn makes them more likely to embrace falsehoods favored by members of their political party.</p> <p>Results also suggest that partisan online news sites, which clearly favor the interests of one party over another, are a powerful contributor to political misperceptions. But how this happens may be surprising. There is ample evidence that &ldquo;echo chambers&rdquo; are a myth. The vast majority of Americans regularly encounter ideas that differ from their own. &nbsp;That means that partisan news outlets can&rsquo;t count on audience members being ignorant of inconvenient evidence. Instead, we find that, regardless of what people know about the evidence, the more they rely on partisan outlets, the more they believe claims favored by those outlets. &nbsp;An increase in negative feelings toward political opponents, and increasing confidence that other citizens share their point of view, may help explain this.</p> <p>The final goal was to test strategies for keeping the effects of online misinformation in check. Flagging misinformation at the source is problematic. Although providing corrective information alongside a falsehood is intuitively appealing, we found that it only helped those already inclined to be accurate.&nbsp; Flags made little difference to those predisposed to believe the misinformation. &nbsp;User ratings have a similar limitation. When individuals see a well rated article that contradicts their beliefs, they tend to question the motives of the raters and to distrust the ratings. Warning people of their own biases doesn&rsquo;t work either. For example, reminding people that bias is commonplace, and that they should try to guard against it can lead to more bias.&nbsp;The only approach we found to have any beneficial effect was pairing a falsehood shared on social media with a warning that the <em>source</em> of the message has a history of promoting inaccurate information.</p> <p>The insights generated by this project have important social implications. Individuals' ability to reach reasoned conclusions rests on their capacity to accurately perceive the world. There is considerable concern about the effects of communication technology on the accuracy of these perceptions. Disinformation shared on social media has received a great deal of attention recently, but this project suggests that as of today, exposure to partisan news sites does considerably more to promote Americans&rsquo; misperceptions than social media use. The research also underscores the fact that ignorance of accurate information is rarely the reason people endorse falsehoods.&nbsp; Instead false beliefs reflect individuals&rsquo; tendency to weigh empirical evidence and expert opinion against a variety of other considerations, including their intuition, emotion, and the positions endorsed by members of their preferred political party.</p><br> <p>            Last Modified: 05/15/2018<br>      Modified by: R.&nbsp;K&nbsp;Garrett</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project extended our understanding of how Internet technologies interact with psychological processes to promote or correct misperceptions. It included a series of studies aimed at examining how beliefs are formed and updated, and at testing predictions about the effects of online media on misperceptions, with the ultimate goal of proposing strategies to fight misinformation.  Our first insights concern psychological factors that contribute to misperceptions. Politically biased beliefs are commonplace, but there are other factors that contribute to misperceptions.  We found that individuals? beliefs about how they "know" what?s true and what?s not, which we refer to as their epistemic beliefs, are also influential. For example, the more people trust their intuition to tell them what is true, the more likely they are to embrace conspiracy theories.  In contrast, people who place more value on their ability to provide evidence for their beliefs are less likely to engage in conspiracist thinking and to endorse falsehoods about divisive political issues. This may seem like commonsense, but Americans? epistemic beliefs vary widely: similar numbers of people embrace and reject intuition as a way of knowing.  Emotions also shape misperceptions.  Individuals? emotional state alters how they respond to new information. Those who are angry tend to rely heavily on what their political party leaders say when deciding what they believe, while those who feel anxious are more swayed by information about the evidence. People also treat emotions themselves as "information" that they can use when deciding what to believe. Claims eliciting emotions matching how the individual already feels tend to be seen as more plausible.  For example, an angry person is more inclined to believe a rumor that someone took advantage of him than a person who is not angry.  This project also sheds light on the role of online technologies, including social media. Results suggest that social media use was associated with a small increase in endorsement of candidate falsehoods during the 2012 U.S. Presidential election. This isn?t just a matter of social media being exploited to spread misinformation. Online social interactions that have nothing to do with misinformation can also promote misperceptions.  For example, we have evidence that individuals who receive fewer "likes" on their posts than their peers tend to feel more socially excluded, and this in turn makes them more likely to embrace falsehoods favored by members of their political party.  Results also suggest that partisan online news sites, which clearly favor the interests of one party over another, are a powerful contributor to political misperceptions. But how this happens may be surprising. There is ample evidence that "echo chambers" are a myth. The vast majority of Americans regularly encounter ideas that differ from their own.  That means that partisan news outlets can?t count on audience members being ignorant of inconvenient evidence. Instead, we find that, regardless of what people know about the evidence, the more they rely on partisan outlets, the more they believe claims favored by those outlets.  An increase in negative feelings toward political opponents, and increasing confidence that other citizens share their point of view, may help explain this.  The final goal was to test strategies for keeping the effects of online misinformation in check. Flagging misinformation at the source is problematic. Although providing corrective information alongside a falsehood is intuitively appealing, we found that it only helped those already inclined to be accurate.  Flags made little difference to those predisposed to believe the misinformation.  User ratings have a similar limitation. When individuals see a well rated article that contradicts their beliefs, they tend to question the motives of the raters and to distrust the ratings. Warning people of their own biases doesn?t work either. For example, reminding people that bias is commonplace, and that they should try to guard against it can lead to more bias. The only approach we found to have any beneficial effect was pairing a falsehood shared on social media with a warning that the source of the message has a history of promoting inaccurate information.  The insights generated by this project have important social implications. Individuals' ability to reach reasoned conclusions rests on their capacity to accurately perceive the world. There is considerable concern about the effects of communication technology on the accuracy of these perceptions. Disinformation shared on social media has received a great deal of attention recently, but this project suggests that as of today, exposure to partisan news sites does considerably more to promote Americans? misperceptions than social media use. The research also underscores the fact that ignorance of accurate information is rarely the reason people endorse falsehoods.  Instead false beliefs reflect individuals? tendency to weigh empirical evidence and expert opinion against a variety of other considerations, including their intuition, emotion, and the positions endorsed by members of their preferred political party.       Last Modified: 05/15/2018       Submitted by: R. K Garrett]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
