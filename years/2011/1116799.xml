<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small: Modeling Acoustic and Articulatory Features for Hybrid Synthesis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2011</AwardEffectiveDate>
<AwardExpirationDate>09/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>180902.00</AwardTotalIntnAmount>
<AwardAmount>212902</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In recent decades synthetic speech has become a ubiquitous and increasingly seamless aspect of human-machine interfaces.  Although cars, microwaves, phones, and kiosks all "talk" in human-like ways, the naturalness and personality of these voices fall short of human expression.  While this may not matter for many text-to-speech (TTS) applications, over two million Americans with severe speech-motor impairments require assistive communication aids with TTS output.  Concatenative TTS synthesizers yield highly intelligible voices, yet many assistive devices rely on small footprint, formant synthesis that sounds robotic and has poor intelligibility.  Moreover, the choice of voices on conventional devices is limited and does not reflect the user; it is not uncommon for a child to use the same voice her whole life and for her peers to share that same voice even when using different devices.  This lack of attention to the individuality of synthetic voices has consequences on adoption of assistive technology as an extension of the user, and may adversely impact societal attitudes toward the user group.&lt;br/&gt;&lt;br/&gt;In her prior work the PI began to address these issues by adapting a concatenative synthesizer constructed from acoustic recordings of a healthy talker using vocal source characteristics obtained from a target talker with speech impairment.  The adapted voice was highly intelligible and conveyed the target user's identity, yet it also retained substantial elements of the healthy talker's identity due to the influence of vocal tract filter characteristics.  This suggests that personalized speech synthesis may be more successful utilizing an alternative approach, in which acoustic and articulatory data from healthy talkers are combined with both source and filter characteristics from target talkers to generate an individualized voice.  In this project, the PI will develop hybrid statistical parametric synthesis techniques to model vocal tract and source characteristics of impaired talkers, with the goal of generating highly intelligible and personalized synthetic speech.  The PI envisages a future where source and filter parameters of a Hidden Markov Model (HMM) based synthesizer can be adapted to model a child user's vocal tract and modified over time to "grow" with his maturing vocal system, fostering a stronger personal connection between the user and the communication device.&lt;br/&gt;&lt;br/&gt;Broader Impacts:   This project strives to make communication accessible and socially fulfilling by designing an enabling technology that blurs the line between system and user.  The human voice is not merely a signal; it has an individualized and personal quality that impacts how others perceive us and how we interact with those around us.  The ultimate goal of this work is to afford users of TTS the same ownership and individuality as the natural voice.  Project outcomes will have broad impact both on users of assistive aids and able-bodied users of TTS technologies.  The research may also lead to a novel and innovative means of assessing the nature and articulatory locus of speech impairment, by comparing model parameters to impaired productions.  The interdisciplinary nature of this research will promote teaching, training and learning in computer science and in speech and hearing sciences.</AbstractNarration>
<MinAmdLetterDate>09/08/2011</MinAmdLetterDate>
<MaxAmdLetterDate>06/05/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1116799</AwardID>
<Investigator>
<FirstName>Rupal</FirstName>
<LastName>Patel</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Rupal Patel</PI_FULL_NAME>
<EmailAddress>rupal@vocaliD.ai</EmailAddress>
<PI_PHON>3393680416</PI_PHON>
<NSF_ID>000671566</NSF_ID>
<StartDate>09/08/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northeastern University</Name>
<CityName>BOSTON</CityName>
<ZipCode>021155005</ZipCode>
<PhoneNumber>6173733004</PhoneNumber>
<StreetAddress>360 HUNTINGTON AVE</StreetAddress>
<StreetAddress2><![CDATA[177-500]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001423631</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NORTHEASTERN UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001423631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Northeastern University]]></Name>
<CityName>BOSTON</CityName>
<StateCode>MA</StateCode>
<ZipCode>021155005</ZipCode>
<StreetAddress><![CDATA[360 HUNTINGTON AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~180902</FUND_OBLG>
<FUND_OBLG>2013~16000</FUND_OBLG>
<FUND_OBLG>2014~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>We are surrounded by a world in which we interact with computers using speech. For individuals who rely on assistive communication technologies, synthetic speech offers them a voice. Unfortunately, the voice options are limited and do not reflect the user; it is not uncommon for several children in a classroom or adults in a workplace to user the same voice. Individuals may also use the same voice their whole life despite having grown and changed devices over a dozen times. This work addressed this lack of individuation of prosthetic voice technology.</p> <p>The intellectual merit of this work relate to scientific advances in speech production and speech technologies and include new algorithm development and deployment. To develop personalized voices for individuals with limited vocal output, we began by adapting a concatenative synthesizer built using a database of recordings from a healthy donor and samples of residual vocalizations from the end user. Our empirical findings and recent developments in Hidden Markov Model (HMM) based synthesis suggested that alternative approaches may improve personalized synthesis and also provide unique insights into the nature of a target talker&rsquo;s speech production deficits. The current grant extended our earlier work by integrating acoustic and articulatory features to 1) extract vocal <span style="text-decoration: underline;">source</span> characteristics that convey speaker identity, 2) extract vocal tract <span style="text-decoration: underline;">filter</span> characteristics to further preserve speaker identity and 3) statistical modeling of natural, intelligible speech using aggregates of acoustic and articulatory dynamics across speakers. We found that modeling acoustic and articulatory similarities across speakers resulted in fluent natural-sounding synthetic speech. We then made advances toward integrating our personalized voice into existing assistive communication devices. Toward that end, we worked with two different manufacturers on the iOS and windows platform to provide a personalized voice to 3 beta users. These individuals have reported changes in the quality and quantity of communication exchanges and improvements in overall quality of life related to social integration and access to educational opportunities. Additional broader impacts include novel insights into modeling speech production, new approaches to sparse sample voice conversion and increased awareness of speech and voice impairment as well as the limitations of current assistive communication technologies.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/27/2014<br>      Modified by: Rupal&nbsp;Patel</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ We are surrounded by a world in which we interact with computers using speech. For individuals who rely on assistive communication technologies, synthetic speech offers them a voice. Unfortunately, the voice options are limited and do not reflect the user; it is not uncommon for several children in a classroom or adults in a workplace to user the same voice. Individuals may also use the same voice their whole life despite having grown and changed devices over a dozen times. This work addressed this lack of individuation of prosthetic voice technology.  The intellectual merit of this work relate to scientific advances in speech production and speech technologies and include new algorithm development and deployment. To develop personalized voices for individuals with limited vocal output, we began by adapting a concatenative synthesizer built using a database of recordings from a healthy donor and samples of residual vocalizations from the end user. Our empirical findings and recent developments in Hidden Markov Model (HMM) based synthesis suggested that alternative approaches may improve personalized synthesis and also provide unique insights into the nature of a target talkerÆs speech production deficits. The current grant extended our earlier work by integrating acoustic and articulatory features to 1) extract vocal source characteristics that convey speaker identity, 2) extract vocal tract filter characteristics to further preserve speaker identity and 3) statistical modeling of natural, intelligible speech using aggregates of acoustic and articulatory dynamics across speakers. We found that modeling acoustic and articulatory similarities across speakers resulted in fluent natural-sounding synthetic speech. We then made advances toward integrating our personalized voice into existing assistive communication devices. Toward that end, we worked with two different manufacturers on the iOS and windows platform to provide a personalized voice to 3 beta users. These individuals have reported changes in the quality and quantity of communication exchanges and improvements in overall quality of life related to social integration and access to educational opportunities. Additional broader impacts include novel insights into modeling speech production, new approaches to sparse sample voice conversion and increased awareness of speech and voice impairment as well as the limitations of current assistive communication technologies.           Last Modified: 10/27/2014       Submitted by: Rupal Patel]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
