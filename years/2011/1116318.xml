<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>TC: Small: Theory and Applications of Min-Entropy Leakage</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>486236.00</AwardTotalIntnAmount>
<AwardAmount>502236</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In this age of identity theft, Facebook, and TSA screening, protecting confidential information from improper disclosure has emerged as a fundamental issue for trustworthy computing, involving both technical and social dimensions. While it is sometimes possible to stop undesirable information flows completely, it is perhaps more typical that some undesirable flows are unavoidable. For instance an ATM machine that rejects an incorrect PIN thereby reveals that the secret PIN is not the one that was entered. Similarly, revealing the tally of votes in an election reveals some information about the secret ballots that were cast. More subtly, the amount of time taken by a cryptographic operation may be observable by an adversary, and may inadvertently reveal information about the secret key. As a result, there is growing interest in quantitative theories of information flow, which allow us to talk about "how much" information is leaked and (perhaps) allow us to tolerate "small" leaks. But while it is tempting to base such theories on classic information-theoretic concepts like Shannon entropy and mutual information, these turn out not to provide very satisfactory confidentiality guarantees. As a result, several researchers have developed an alternative theory, based instead on Renyi's min-entropy, which gives strong, direct operational security guarantees.&lt;br/&gt;&lt;br/&gt;This project aims to deepen our understanding of the theory and applications of min-entropy leakage by pursuing several themes concurrently. In the theory of min-entropy leakage, uncertainty is measured in terms of a random variable's vulnerability to being guessed in one try by an adversary; note that this is the complement of the Bayes Risk. The mathematical properties of this theory will be explored for both deterministic and probabilistic systems, with the goal of better understanding the relationship to other theories, such as mutual information leakage and differential privacy. A variant called smooth min-entropy leakage will be developed, giving a measure that is less sensitive to extremely unlikely events. To support the compositional analysis of complex systems, leakage bounds will be established for channels in cascade, where the output of one channel becomes the input to another; moreover, algorithms will be developed for approximately factoring a channel into a cascade, giving a technique for automatic sanitization. Techniques will be developed for computing the min-entropy leakage of systems, giving a way to verify whether they conform to a given quantitative flow policy; both model-checking and statistical-sampling based techniques will be considered. Finally, applications of min-entropy leakage will be explored, for example to accountability systems that use logging and auditing to identify misbehaving entities. These efforts will develop new theory, enforcement techniques, and applications of min-entropy leakage, contributing broadly to a rigorous science of quantitative information flow.</AbstractNarration>
<MinAmdLetterDate>07/18/2011</MinAmdLetterDate>
<MaxAmdLetterDate>04/17/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1116318</AwardID>
<Investigator>
<FirstName>Geoffrey</FirstName>
<LastName>Smith</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Geoffrey S Smith</PI_FULL_NAME>
<EmailAddress>smithg@cis.fiu.edu</EmailAddress>
<PI_PHON>3053486037</PI_PHON>
<NSF_ID>000477976</NSF_ID>
<StartDate>07/18/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Florida International University</Name>
<CityName>Miami</CityName>
<ZipCode>331990001</ZipCode>
<PhoneNumber>3053482494</PhoneNumber>
<StreetAddress>11200 SW 8TH ST</StreetAddress>
<StreetAddress2><![CDATA[MARC 430]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL26</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>071298814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>FLORIDA INTERNATIONAL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Florida International University]]></Name>
<CityName>Miami</CityName>
<StateCode>FL</StateCode>
<ZipCode>331990001</ZipCode>
<StreetAddress><![CDATA[11200 SW 8TH ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>26</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL26</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7795</Code>
<Text>TRUSTWORTHY COMPUTING</Text>
</ProgramElement>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>7795</Code>
<Text>TRUSTWORTHY COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~486236</FUND_OBLG>
<FUND_OBLG>2012~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>A fundamental and vexing problem in cybersecurity is to prevent systems from improperly leaking the sensitive information that they process. But the problem defies simple solutions, because it is frequently necessary in practice to tolerate some leakage. Consider, for example, an election system. Individual ballots are normally considered to be confidential, but the election system needs to output the tally of votes, and this reveals some information about the individual ballots&mdash;in the case of a unanimous election, for example, this reveals how everyone voted.</p> <p>The goal of this project has been to develop the theory of <em>quantitative information flow</em>, making it possible to quantify the amount of leakage caused by a system, so that some leaks can be judged to be &ldquo;small&rdquo; and therefore tolerable. Consider a system taking a secret input <em>X</em> and producing a public output <em>Y</em>. The secrecy of <em>X</em> is modeled using a probability distribution that specifies the probability, from the adversary&rsquo;s perspective, of each of <em>X</em>&rsquo;s possible values. Then the <em>prior vulnerability</em> of <em>X</em> can be defined simply as the maximum probability in the distribution&mdash;note that this is an optimal adversary&rsquo;s probability of correctly guessing the value of <em>X</em> in one try. Now consider how an adversary benefits by seeing the output <em>Y</em>. It turns out that this allows the adversary to update the distribution on <em>X</em>, giving a <em>posterior vulnerability</em> which may be larger than the prior vulnerability. It is then natural to define the leakage caused by the system by comparing the posterior and prior vulnerabilities; this gives a measure known as <em>min-entropy leakage</em>.</p> <p>Min-entropy leakage has the virtue of providing definite guarantees about an adversary&rsquo;s probability of guessing the secret correctly in one try, but of course this may not necessarily reflect the actual operational scenario. Perhaps the adversary is allowed to make multiple guesses, or is penalized for incorrect guesses, or values some parts of the secret more than others. Each such scenario can be modeled using what is called a <em>gain function</em> <em>g</em>, and this gives rise to the family of <em>g-leakage</em> measures.</p> <p>But if there are many possible leakage measures, how can we know which one to use in analyzing a particular system? Can leakage be analyzed robustly, in a way that does not depend on questionable assumptions about the adversary? The theory developed in this project gives two ways of addressing this concern. First, one can robustly <em>compare</em> two systems, asking whether the first never leaks more than the second, no matter the scenario. It turns out that this robust ordering has an exact structural characterization: the leakage ordering holds precisely if the first system is equivalent to the second followed by some post-processing. A second approach to robustness considers <em>capacity</em>, which is the maximum possible leakage over all possible scenarios. The theory of capacity turns out to be interesting and fruitful, since it is feasible to demonstrate that certain systems have small leakage in all scenarios.</p> <p>To conclude, this project has led to the development of foundational techniques for analyzing the amount of leakage caused by computer systems, supporting the long-term goal of a cyber infrastructure in which sensitive information is kept secure from improper disclosure. Finally, the significance of this work is indicated by the recent selection of one of the project&rsquo;s paper as the winner of the NSA&rsquo;s 3rd Annual Best Scientific Cybersecurity Paper Competition:</p> <p><span><a href="https://www.nsa.gov/public_info/press_room/2015/Annual_Cyber_Research_Paper_COMP_Winner.shtml">https://www.nsa.gov/public_info/press_room/2015/Annual_Cyber...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ A fundamental and vexing problem in cybersecurity is to prevent systems from improperly leaking the sensitive information that they process. But the problem defies simple solutions, because it is frequently necessary in practice to tolerate some leakage. Consider, for example, an election system. Individual ballots are normally considered to be confidential, but the election system needs to output the tally of votes, and this reveals some information about the individual ballots&mdash;in the case of a unanimous election, for example, this reveals how everyone voted.  The goal of this project has been to develop the theory of quantitative information flow, making it possible to quantify the amount of leakage caused by a system, so that some leaks can be judged to be "small" and therefore tolerable. Consider a system taking a secret input X and producing a public output Y. The secrecy of X is modeled using a probability distribution that specifies the probability, from the adversaryÆs perspective, of each of XÆs possible values. Then the prior vulnerability of X can be defined simply as the maximum probability in the distribution&mdash;note that this is an optimal adversaryÆs probability of correctly guessing the value of X in one try. Now consider how an adversary benefits by seeing the output Y. It turns out that this allows the adversary to update the distribution on X, giving a posterior vulnerability which may be larger than the prior vulnerability. It is then natural to define the leakage caused by the system by comparing the posterior and prior vulnerabilities; this gives a measure known as min-entropy leakage.  Min-entropy leakage has the virtue of providing definite guarantees about an adversaryÆs probability of guessing the secret correctly in one try, but of course this may not necessarily reflect the actual operational scenario. Perhaps the adversary is allowed to make multiple guesses, or is penalized for incorrect guesses, or values some parts of the secret more than others. Each such scenario can be modeled using what is called a gain function g, and this gives rise to the family of g-leakage measures.  But if there are many possible leakage measures, how can we know which one to use in analyzing a particular system? Can leakage be analyzed robustly, in a way that does not depend on questionable assumptions about the adversary? The theory developed in this project gives two ways of addressing this concern. First, one can robustly compare two systems, asking whether the first never leaks more than the second, no matter the scenario. It turns out that this robust ordering has an exact structural characterization: the leakage ordering holds precisely if the first system is equivalent to the second followed by some post-processing. A second approach to robustness considers capacity, which is the maximum possible leakage over all possible scenarios. The theory of capacity turns out to be interesting and fruitful, since it is feasible to demonstrate that certain systems have small leakage in all scenarios.  To conclude, this project has led to the development of foundational techniques for analyzing the amount of leakage caused by computer systems, supporting the long-term goal of a cyber infrastructure in which sensitive information is kept secure from improper disclosure. Finally, the significance of this work is indicated by the recent selection of one of the projectÆs paper as the winner of the NSAÆs 3rd Annual Best Scientific Cybersecurity Paper Competition:  https://www.nsa.gov/public_info/press_room/2015/Annual_Cyber_Research_Paper_COMP_Winner.shtml.          Last Modified: 09/16/2015       Submitted by: Geoffrey S Smith]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
