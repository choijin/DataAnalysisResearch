<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>TC: Small: Collaborative Research: An Argumentation-based Framework for Security Management</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>249596.00</AwardTotalIntnAmount>
<AwardAmount>264596</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Computer users are increasingly faced with decisions that impact their personal privacy and the security of the systems they manage. The range of users confronting these challenges has broadened from the early days of computing to include everyone from home users to administrators of large enterprise networks. Privacy policies are frequently obscure, and security settings are typically complex. Missing from the options presented to a user is a decision support mechanism that can assist her in making informed choices. Being presented with the consequences of decisions she is asked to make, among other information, is a necessary component that is currently lacking.&lt;br/&gt;&lt;br/&gt;This work introduces formal argumentation as a framework for helping users make informed decisions about the security of their computer systems and the privacy of their electronically stored information. &lt;br/&gt;Argumentation, a mature theoretical discipline, provides a mechanism for reaching substantiated conclusions when faced with incomplete and inconsistent information. It provides the basis for presenting arguments to a user for or against a position, along with well-founded methods for assessing the outcome of interactions among the arguments. An elegant theory of argumentation has been developed based on meta rules characterizing relationships between arguments. Rules for argument construction and evaluation have been devised for specific domains such as medical diagnosis. This project investigates argumentation as the basis for helping users make informed security- and privacy-related decisions about their computer systems. Three specific aims are addressed:&lt;br/&gt;1) Implementation of an inference engine that reasons using argumentation,&lt;br/&gt;2) Facilitate security management through an argumentation inference engine, a rule base specialized for security management, and sensors providing security alerts all enhanced with an interactive front-end.&lt;br/&gt;3) Reason about the consistency and completeness of domain knowledge, as it evolves.&lt;br/&gt;To understand the kinds of domain-specific inference rules required, diverse security applications are studied, such as determining if an attack imperils a particular system, finding the root cause of an attack, deciding on appropriate actions to take in the presence of an uncertain diagnosis of an attack, and deciding on privacy settings. &lt;br/&gt;Emerging from this project will be a prototype towards the practice of usable security. The team is working with organizations responsible for the security administration of large enterprise networks and will make the prototype tools available to these organizations. The team is working with everyday users from a cross-section of community members. &lt;br/&gt;Curricular modules that cover the intersection of argumentation and security are being developed and shared.</AbstractNarration>
<MinAmdLetterDate>07/14/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1117761</AwardID>
<Investigator>
<FirstName>Elizabeth</FirstName>
<LastName>Sklar</LastName>
<PI_MID_INIT>I</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Elizabeth I Sklar</PI_FULL_NAME>
<EmailAddress>elizabeth.sklar@kcl.ac.uk</EmailAddress>
<PI_PHON/>
<NSF_ID>000106884</NSF_ID>
<StartDate>07/14/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Simon</FirstName>
<LastName>Parsons</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Simon Parsons</PI_FULL_NAME>
<EmailAddress>parsons@sci.brooklyn.cuny.edu</EmailAddress>
<PI_PHON>7189515622</PI_PHON>
<NSF_ID>000119932</NSF_ID>
<StartDate>07/14/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>CUNY Brooklyn College</Name>
<CityName>Brooklyn</CityName>
<ZipCode>112102889</ZipCode>
<PhoneNumber>7189515622</PhoneNumber>
<StreetAddress>Office of Research &amp; Sponsored P</StreetAddress>
<StreetAddress2><![CDATA[2900 Bedford Ave]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>620127691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION OF THE CITY UNIVERSITY OF NEW YORK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073268849</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[CUNY Brooklyn College]]></Name>
<CityName>Brooklyn</CityName>
<StateCode>NY</StateCode>
<ZipCode>112102889</ZipCode>
<StreetAddress><![CDATA[Office of Research &amp; Sponsor]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7795</Code>
<Text>TRUSTWORTHY COMPUTING</Text>
</ProgramElement>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>7795</Code>
<Text>TRUSTWORTHY COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~249596</FUND_OBLG>
<FUND_OBLG>2012~15000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This was a project in computational argumentation. Argumentation is the study of the kind of reasoning that people often engage in, in which conclusions are reached by putting forward arguments for and against options, and weighing up these pros and cons to decide what the right conclusions are. Computational argumentation is the study of formal mathematical models of this kind of reasoning. Building mathematical models allows us to study their properties, and so make sure that they behave in suitable ways. We can then write algorithms to operationalize the models, and use these in software that can help people solve problems.</p> <p>The project started with the hypothesis that using computational argumentation could help people reach decisions in complex scenarios, in particular in the area of cybersecurity. To do this, we created a software system, called ArgTrust, which allows the arguments that are relevant to a particular decision, and the interactions between those arguments, to be visualized. Part of the interface to ArgTrust, including the visualizations of some arguments, is shown in the first figure. (The figure shows an example from a military scenario.) ArgTrust applies some well-established techniques from the computational argumentation literature to make recommendations, and allows the arguments to be viewed at different levels of detail, as in the second figure. We carried out a study in which people were presented with scenarios and asked to reach decisions with and without using ArgTrust. Our main finding was that people considered their decisions more carefully when they used ArgTrust.<br /><br />In addition, we investigated the application of argumentation models to two problems from cybersecurity. One of&nbsp; these was firewall configuration. To configure a packet-filtering firewall, a common variety of firewall, one has to write down a set of rules about what data is allowed to pass through the firewall. The resulting set of rules can easily become very complex. As a result it is hard for the human configurer to understand exactly what a given set of rules will allow and what it will deny. We showed how argumentation can be used to answer this question, and how a particular form of argumentation, metalevel argumentation, can be used to provide an explanation of why particular decisions are made. The other application we studied was attributing the source of a cyberattack. When a cyberattack is detected, it is often unclear who carried it out. (Typically attackers do not want to be identified, and so try hard to obscure the origin of the attack). However, there are often a number of pieces of evidence about the source of an attack that can be pieced together to decide who was responsible. This evidence is frequently contradictory (because of obfustication by the attackers), and this makes argumentation, which can deal with conflicting evidence, a natural fit for the problem.<br /><br />Carrying out the work outlined above made us realise that existing argumentation models were not sufficient to deal with the problems we were studying. We therefore extended those models. In particular, we studied: how to capture arguments that support each other; how to incorporate probability into arguments (in particular where the probability is associated with the premises of the arguments); and what the limits are of only dealing with arguments that are defeasible, meaning that all their conclusions can be overturned if there is sufficiently strong evidence against them.</p><br> <p>            Last Modified: 04/30/2020<br>      Modified by: Simon&nbsp;Parsons</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1117761/1117761_10108068_1588263336858_argtrust-anaconda-interface--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1117761/1117761_10108068_1588263336858_argtrust-anaconda-interface--rgov-800width.jpg" title="argtrust-interface"><img src="/por/images/Reports/POR/2020/1117761/1117761_10108068_1588263336858_argtrust-anaconda-interface--rgov-66x44.jpg" alt="argtrust-interface"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The Interface of the ArgTrust System.</div> <div class="imageCredit">Salvit/Wall/Sklar/Parsons</div> <div class="imageSubmitted">Simon&nbsp;Parsons</div> <div class="imageTitle">argtrust-interface</div> </div> </li> <li> <a href="/por/images/Reports/POR/2020/1117761/1117761_10108068_1588264168669_zooming--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1117761/1117761_10108068_1588264168669_zooming--rgov-800width.jpg" title="argtrust-zoom-levels"><img src="/por/images/Reports/POR/2020/1117761/1117761_10108068_1588264168669_zooming--rgov-66x44.jpg" alt="argtrust-zoom-levels"></a> <div class="imageCaptionContainer"> <div class="imageCaption">ArgTrust allows arguments to be viewed at different levels of detail.</div> <div class="imageCredit">Salvit/Wall/Sklar/Parsons</div> <div class="imagePermisssions">Public Domain</div> <div class="imageSubmitted">Simon&nbsp;Parsons</div> <div class="imageTitle">argtrust-zoom-levels</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This was a project in computational argumentation. Argumentation is the study of the kind of reasoning that people often engage in, in which conclusions are reached by putting forward arguments for and against options, and weighing up these pros and cons to decide what the right conclusions are. Computational argumentation is the study of formal mathematical models of this kind of reasoning. Building mathematical models allows us to study their properties, and so make sure that they behave in suitable ways. We can then write algorithms to operationalize the models, and use these in software that can help people solve problems.  The project started with the hypothesis that using computational argumentation could help people reach decisions in complex scenarios, in particular in the area of cybersecurity. To do this, we created a software system, called ArgTrust, which allows the arguments that are relevant to a particular decision, and the interactions between those arguments, to be visualized. Part of the interface to ArgTrust, including the visualizations of some arguments, is shown in the first figure. (The figure shows an example from a military scenario.) ArgTrust applies some well-established techniques from the computational argumentation literature to make recommendations, and allows the arguments to be viewed at different levels of detail, as in the second figure. We carried out a study in which people were presented with scenarios and asked to reach decisions with and without using ArgTrust. Our main finding was that people considered their decisions more carefully when they used ArgTrust.  In addition, we investigated the application of argumentation models to two problems from cybersecurity. One of  these was firewall configuration. To configure a packet-filtering firewall, a common variety of firewall, one has to write down a set of rules about what data is allowed to pass through the firewall. The resulting set of rules can easily become very complex. As a result it is hard for the human configurer to understand exactly what a given set of rules will allow and what it will deny. We showed how argumentation can be used to answer this question, and how a particular form of argumentation, metalevel argumentation, can be used to provide an explanation of why particular decisions are made. The other application we studied was attributing the source of a cyberattack. When a cyberattack is detected, it is often unclear who carried it out. (Typically attackers do not want to be identified, and so try hard to obscure the origin of the attack). However, there are often a number of pieces of evidence about the source of an attack that can be pieced together to decide who was responsible. This evidence is frequently contradictory (because of obfustication by the attackers), and this makes argumentation, which can deal with conflicting evidence, a natural fit for the problem.  Carrying out the work outlined above made us realise that existing argumentation models were not sufficient to deal with the problems we were studying. We therefore extended those models. In particular, we studied: how to capture arguments that support each other; how to incorporate probability into arguments (in particular where the probability is associated with the premises of the arguments); and what the limits are of only dealing with arguments that are defeasible, meaning that all their conclusions can be overturned if there is sufficiently strong evidence against them.       Last Modified: 04/30/2020       Submitted by: Simon Parsons]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
