<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF:Small:RUI:New directions in Fourier analysis, noise sensitivity, and learning theory</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2011</AwardEffectiveDate>
<AwardExpirationDate>05/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>231480.00</AwardTotalIntnAmount>
<AwardAmount>231480</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Balasubramanian Kalyanasundaram</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>One of the major concerns that practitioners have about theoretical machine learning is the focus on distributions where the attributes are independent: in other words, knowing one attribute gives no information about any others.  As an example, while it is plausible that height and eye color are independent, it is much less believable that height and weight are independent.  Thus, the output given by any algorithm that is based on the assumption that the attributes of a person (such as height and weight) are independent cannot be trusted. The goal of this project is to extend what we know about the theory of such problems while removing some of the mathematically convenient assumptions such as independence.  &lt;br/&gt;&lt;br/&gt;The tools used focus on discrete Fourier analysis, but involve many other techniques from mathematics such as functional analysis and representation theory of finite groups.  One recurring technique is the application of the "noise sensitivity" method, which quantifies the complexity of a function based on how similar the value of the function is on some input to the values of that input's neighbors.  In many cases, the goal is to show that the Fourier spectrum of certain classes of functions is predictable; often, this predictability is a key component of algorithms for machine learning. &lt;br/&gt;&lt;br/&gt;The broader goal of this project is to discover new connections between mathematics and computer science with a special focus on questions motivated by machine learning. Answers to the underlying questions would be useful to theoreticians and could lead to better applied machine learning algorithms.  Also, the mathematical questions raised are interesting independently of the machine learning connection.  The problems considered in this project will provide an invigorating research opportunity for undergraduate and Master's students.</AbstractNarration>
<MinAmdLetterDate>06/03/2011</MinAmdLetterDate>
<MaxAmdLetterDate>06/03/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1117079</AwardID>
<Investigator>
<FirstName>Karl</FirstName>
<LastName>Wimmer</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karl E Wimmer</PI_FULL_NAME>
<EmailAddress>wimmerk@duq.edu</EmailAddress>
<PI_PHON>4123966326</PI_PHON>
<NSF_ID>000554999</NSF_ID>
<StartDate>06/03/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duquesne University</Name>
<CityName>Pittsburgh</CityName>
<ZipCode>152193016</ZipCode>
<PhoneNumber>4123961537</PhoneNumber>
<StreetAddress>Room 310 Administration Building</StreetAddress>
<StreetAddress2><![CDATA[600 Forbes Avenue]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>004501193</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUQUESNE UNIVERSITY OF THE HOLY SPIRIT</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004501193</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duquesne University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152193016</ZipCode>
<StreetAddress><![CDATA[Room 310 Administration Building]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9229</Code>
<Text>RES IN UNDERGRAD INST-RESEARCH</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~231480</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project concerns the study of binary valued functions over a set of binary attributes. &nbsp;One major driving force behind studying such functions is the application to learning from data. &nbsp;One can think of these functions as a rule, where the possible inputs represent people, the attributes are answers to yes/no questions about each person (such as "Is this person married?" or "Does this person weigh more than 175 lbs?"), and the value of the function corresponds to some target question (such as "Is there a 90% or greater chance that this person will need major surgery in the next year?"). &nbsp;The goal of learning is to produce such a rule that is correct as often as possible.</p> <p><br />The method of study is an analysis of the level of rigorous mathematical "difficulty" or complexity of such rules. &nbsp;Intuitively, less complex functions should require less resources in order to produce a good hypothesis about the data. &nbsp;Thus, the mathematical study here is the analysis of such functions. &nbsp;There are two focal points to this project:</p> <p><br />1. &nbsp;An assumption about the underlying function: &nbsp;one plausible assumption about these rules is that if an attribute is changed from "no" to "yes", the target answer should never change from "yes" to "no". &nbsp;We refer to such functions as monotone (or sometimes monotone increasing).</p> <p><br />2. &nbsp;An assumption about the underlying data: &nbsp;in much of the existing literature, an assumption is made that the attributes are independent. &nbsp;While this allows for powerful mathematical machinery to be applied, the breadth of application is greatly diminished. &nbsp;In this project, the PI studies the case where the number of "yes" attributes is constant. &nbsp;This pushes the boundaries of the mathematical tools that can be applied, while moving towards more practical settings.</p> <p>(As an aside, we note that the assumptions in (1) and (2) do not mix directly: &nbsp;if the number of "yes" attributes is constant as in (2), then the monotone assumption from (1) does not make sense.)</p> <p>In the case of (1), a negative result was achieved as a result of this project. &nbsp;This results applies to statistical query algorithms, which are algorithms that only interact with the data by making correlations, without looking at the specific data points themselves. &nbsp;Almost all learning algorithms used today fall into this category. &nbsp;The outcome is a proof that every statistical query algorithm that produces a rule competitive with the best monotone rule over a few attributes must use a large amount of resources.</p> <p>Two success are outlined here with respect to (2). &nbsp;The first is a structural theorem about functions that are not very "sensitive on average". &nbsp;More specifically, if the average number of pairs of attributes whose "yes" and "no" can be switched to change the value of the function is small, then the function must agree over 99% of the time with another function that only depends on a small number of attributes. &nbsp;This result serves as a sort of dimension reduction for functions with low average sensitivity, which a priori has no relation to the number of attributes that the function depends on.</p> <p>For the second, a bit more description is in order. &nbsp;To discuss the analytical tools here, we identify "yes" with +1 and "no" with -1, so that the binary classification rule is a function that takes a vector of +1's and -1's as input and outputs +1 or -1. &nbsp;(This identification and these tools are also used above. &nbsp;However, as demonstrated above, the overall idea can be conveyed without them.) &nbsp;&nbsp;</p> <p>The study here concerns low degree polynomials, which are some of the most useful, most studied, and most well-understood functions in mathematics. &nbsp;Every function that takes vectors of +1's and -1's to +1 as input and...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project concerns the study of binary valued functions over a set of binary attributes.  One major driving force behind studying such functions is the application to learning from data.  One can think of these functions as a rule, where the possible inputs represent people, the attributes are answers to yes/no questions about each person (such as "Is this person married?" or "Does this person weigh more than 175 lbs?"), and the value of the function corresponds to some target question (such as "Is there a 90% or greater chance that this person will need major surgery in the next year?").  The goal of learning is to produce such a rule that is correct as often as possible.   The method of study is an analysis of the level of rigorous mathematical "difficulty" or complexity of such rules.  Intuitively, less complex functions should require less resources in order to produce a good hypothesis about the data.  Thus, the mathematical study here is the analysis of such functions.  There are two focal points to this project:   1.  An assumption about the underlying function:  one plausible assumption about these rules is that if an attribute is changed from "no" to "yes", the target answer should never change from "yes" to "no".  We refer to such functions as monotone (or sometimes monotone increasing).   2.  An assumption about the underlying data:  in much of the existing literature, an assumption is made that the attributes are independent.  While this allows for powerful mathematical machinery to be applied, the breadth of application is greatly diminished.  In this project, the PI studies the case where the number of "yes" attributes is constant.  This pushes the boundaries of the mathematical tools that can be applied, while moving towards more practical settings.  (As an aside, we note that the assumptions in (1) and (2) do not mix directly:  if the number of "yes" attributes is constant as in (2), then the monotone assumption from (1) does not make sense.)  In the case of (1), a negative result was achieved as a result of this project.  This results applies to statistical query algorithms, which are algorithms that only interact with the data by making correlations, without looking at the specific data points themselves.  Almost all learning algorithms used today fall into this category.  The outcome is a proof that every statistical query algorithm that produces a rule competitive with the best monotone rule over a few attributes must use a large amount of resources.  Two success are outlined here with respect to (2).  The first is a structural theorem about functions that are not very "sensitive on average".  More specifically, if the average number of pairs of attributes whose "yes" and "no" can be switched to change the value of the function is small, then the function must agree over 99% of the time with another function that only depends on a small number of attributes.  This result serves as a sort of dimension reduction for functions with low average sensitivity, which a priori has no relation to the number of attributes that the function depends on.  For the second, a bit more description is in order.  To discuss the analytical tools here, we identify "yes" with +1 and "no" with -1, so that the binary classification rule is a function that takes a vector of +1's and -1's as input and outputs +1 or -1.  (This identification and these tools are also used above.  However, as demonstrated above, the overall idea can be conveyed without them.)     The study here concerns low degree polynomials, which are some of the most useful, most studied, and most well-understood functions in mathematics.  Every function that takes vectors of +1's and -1's to +1 as input and evaluates to +1 and -1 (even any number) can be written as a polynomial of some finite (but possibly large) degree.  In the case that such a function can be written as a polynomial of low degree, more can be said. As a result of this project, the PI and his c...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
