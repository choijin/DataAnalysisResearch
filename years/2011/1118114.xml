<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Query and Goal Driven Entity Resolution Framework</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>12/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>569406</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Data cleaning technologies, traditionally designed to improve quality of data in back-end data warehouses, are fast emerging as a vital component of real-time information access. As the Web evolves towards supporting interactive analytics and basic search migrates from simple keyword retrieval to retrieval based on semantically richer concepts (e.g., entities) extracted from web pages, the need for "on-the-fly" cleaning techniques that can help alleviate data quality challenges is rapidly increasing. This project explores three new innovations that will help advance data cleaning towards becoming an embedded enabling technology for real-time information access. The first innovation is "query-aware data cleaning" which is based on the observation that the specificity of the real-time task such as a query can be exploited significantly to bring new optimizations to the data cleaning process. The second innovation is a data cleaning framework that migrates from the "best-effort" adhoc setup of today's systems into a principled approach that exposes and exploits a fundamental tradeoff between the cost of cleaning and quality of results achieved. Finally, since results of cleaning need to be fed to the end-user or analysis code, the proposal postulates and addresses approaches towards how results processed through data cleaning code can be presented to the end-recipient. The primary contribution is mechanisms to hide the uncertainty in the data and determinize the results while maximizing the end application goals. &lt;br/&gt;&lt;br/&gt;The proposed research is intended to bring transformative improvements in interactive analytics and search on the web by facilitating real-time data cleaning and data quality enhancements. The project also aims to benefit the research community by incorporating mechanisms developed as part of this research into the Web People Search Technology (WEST), enabling WEST to become a real-time on-the-fly web people search tool. The goal is to support WEST as a plug-and-play system wherein other researchers could embed and test their data cleaning algorithms and tools. Finally, the planned research, system development, and educational activities are going to significantly enhance the educational experience of students, preparing them for a brighter future in the today's knowledge driven society.&lt;br/&gt;&lt;br/&gt;For further information see the project web site at the URL: http://sherlock.ics.uci.edu</AbstractNarration>
<MinAmdLetterDate>08/05/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/25/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1118114</AwardID>
<Investigator>
<FirstName>Sharad</FirstName>
<LastName>Mehrotra</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sharad Mehrotra</PI_FULL_NAME>
<EmailAddress>sharad@ics.uci.edu</EmailAddress>
<PI_PHON>9498245975</PI_PHON>
<NSF_ID>000491471</NSF_ID>
<StartDate>08/05/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dmitri</FirstName>
<LastName>Kalashnikov</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dmitri V Kalashnikov</PI_FULL_NAME>
<EmailAddress>dmitri.vk@gmail.com</EmailAddress>
<PI_PHON>8584809385</PI_PHON>
<NSF_ID>000241133</NSF_ID>
<StartDate>08/05/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Irvine</Name>
<CityName>Irvine</CityName>
<ZipCode>926977600</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress>160 Aldrich Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA45</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>046705849</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, IRVINE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Irvine]]></Name>
<CityName>Irvine</CityName>
<StateCode>CA</StateCode>
<ZipCode>926977600</ZipCode>
<StreetAddress><![CDATA[160 Aldrich Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>45</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA45</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~193018</FUND_OBLG>
<FUND_OBLG>2012~306982</FUND_OBLG>
<FUND_OBLG>2014~69406</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of the project was to explore the theory and algorithms for two innovations in &nbsp; data cleaning -- &nbsp;a) query-driven data cleaning where one &nbsp;cleans just enough data needed to answer the user-specified query, and b) adaptive approach to data cleaning that explores a tradeoff between cost and quality. These innovations represent a paradigm shift in data cleaning technologies that have traditionally beed designed as an offline preprocessing step in creating a data warehouse. Such a offline approach, while suitable for traditional warehousing applications such as business intelligence, planning, etc. is entirely unsuitable in the modern era of interactive applications that require &nbsp;data analyses to be performed &nbsp;as soon as the data is produced / collected. The project focused on one aspect of the data cleaning problem -- viz. entity resolution.&nbsp;</p> <p>The project led to a large number of signifiant outcomes.</p> <p>First, Hotham Altwairjy wrote a dissertation that explored the theoretical underpinnings for interactive data cleaning in the context of a&nbsp;<span style="white-space: pre;"> </span>query. The work explored the context of vestigiality that identifies data, cleaning of which will not have an impact on the outcome of the query and can, as a result, be pruned to optimize the cleaning cost. The dissertation then developed the algebraic framework and implementation of the algebra to co-optimize data cleaning and query processing in the context of SQL.</p> <p>&nbsp;</p> <p>In another dissertation, Yasser Altowin explored a new pay-as-you-go cost-vs-quality data cleaning system that can trade of cost (i.e.,efficiency) of entity resolution with its quality. His work also explored scaling of such an adaptive cleaning approach using cluster computing.&nbsp;</p> <p>A related dissertation was written by Jie Xu who explored the challenge of optimally determinizing uncertain data. &nbsp;Data cleaning algorithms, when applied to data, often &nbsp;first generate&nbsp;<em>intermediate probabilistic results</em>, which reflects their uncertainty in various disambiguation choices. The intermediate probabilistic results are then&nbsp;<em>deteminized&nbsp;</em>&nbsp;by the data cleaning algorithm, often using a simple interpretation procedure, to generate the final (deterministic) disambiguation answer. Jie Xu's dissertation developed a new approach to deteminization that exploits the knowledge of the analyses a user would perform (i.e., the workload on the determinized data) in order to improve the overall accuracy of the data analyses.</p> <p>The project, in addition to meeting &nbsp;the above three goals that wer identified in the proposal, proved to be a catalyst to several new unanticicated research directions.&nbsp;</p> <p>&nbsp;</p> <p>In particular, Liyan Zhang's Ph.D. dissertation applied data cleaning techniques to the area of multimedia analyses -- the work that resulted in a best paper award at ACM International Conference of Multimedia Retrieval. This work led to another new direction of work related to exploring human-in-the-loop for data cleaning. &nbsp;Another direction, explored in the dissertation by Mehdi Sadri is on exploring data cleaning in the streaming social media data. Both these work represent impact of the project on the other disciplines (viz. multimedia, and social media).&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/25/2016<br>      Modified by: Sharad&nbsp;Mehrotra</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of the project was to explore the theory and algorithms for two innovations in   data cleaning --  a) query-driven data cleaning where one  cleans just enough data needed to answer the user-specified query, and b) adaptive approach to data cleaning that explores a tradeoff between cost and quality. These innovations represent a paradigm shift in data cleaning technologies that have traditionally beed designed as an offline preprocessing step in creating a data warehouse. Such a offline approach, while suitable for traditional warehousing applications such as business intelligence, planning, etc. is entirely unsuitable in the modern era of interactive applications that require  data analyses to be performed  as soon as the data is produced / collected. The project focused on one aspect of the data cleaning problem -- viz. entity resolution.   The project led to a large number of signifiant outcomes.  First, Hotham Altwairjy wrote a dissertation that explored the theoretical underpinnings for interactive data cleaning in the context of a  query. The work explored the context of vestigiality that identifies data, cleaning of which will not have an impact on the outcome of the query and can, as a result, be pruned to optimize the cleaning cost. The dissertation then developed the algebraic framework and implementation of the algebra to co-optimize data cleaning and query processing in the context of SQL.     In another dissertation, Yasser Altowin explored a new pay-as-you-go cost-vs-quality data cleaning system that can trade of cost (i.e.,efficiency) of entity resolution with its quality. His work also explored scaling of such an adaptive cleaning approach using cluster computing.   A related dissertation was written by Jie Xu who explored the challenge of optimally determinizing uncertain data.  Data cleaning algorithms, when applied to data, often  first generate intermediate probabilistic results, which reflects their uncertainty in various disambiguation choices. The intermediate probabilistic results are then deteminized  by the data cleaning algorithm, often using a simple interpretation procedure, to generate the final (deterministic) disambiguation answer. Jie Xu's dissertation developed a new approach to deteminization that exploits the knowledge of the analyses a user would perform (i.e., the workload on the determinized data) in order to improve the overall accuracy of the data analyses.  The project, in addition to meeting  the above three goals that wer identified in the proposal, proved to be a catalyst to several new unanticicated research directions.      In particular, Liyan Zhang's Ph.D. dissertation applied data cleaning techniques to the area of multimedia analyses -- the work that resulted in a best paper award at ACM International Conference of Multimedia Retrieval. This work led to another new direction of work related to exploring human-in-the-loop for data cleaning.  Another direction, explored in the dissertation by Mehdi Sadri is on exploring data cleaning in the streaming social media data. Both these work represent impact of the project on the other disciplines (viz. multimedia, and social media).           Last Modified: 03/25/2016       Submitted by: Sharad Mehrotra]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
