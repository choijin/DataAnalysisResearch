<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Scheduling and Resource Allocation in the Cloud using Graphical Models and Randomized Algorithms</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2012</AwardEffectiveDate>
<AwardExpirationDate>06/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>447865.00</AwardTotalIntnAmount>
<AwardAmount>447865</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Scalability is crucial for cloud computing to be widely adopted. Failure to scale has resulted in the demise of the social networking giant MySpace, the inability to support a user load greater than 10% of system capacity, a $4.5 billion annual power expenditure on data centers and a lack of any sophisticated measurement and monitoring systems in the cloud. The size of a cloud, which often consists of tens of thousands of machines, compels the use of low-complexity algorithms, which, when naively designed, cause significant performance degradation as the system grows large. Existing algorithms are often designed for exact, optimal solutions for smaller in-house systems and do not scale due to their centralized high-complexity nature.&lt;br/&gt;&lt;br/&gt;This research overcomes the limitation of existing work by designing a suite of low-complexity algorithms for web services, data management, and measurement and monitoring in the cloud, which are exactly optimal only as the system size grows to infinity, but very close to optimal in finite and large systems. The algorithms are designed to address the challenges with dynamic scaling, multi-tenancy and data-intensiveness in the cloud, and for different application workloads including search, social networks and map-reduce. The research draws upon and contributes to the fields of graphical models and randomized algorithms, both of which exchange sparse information locally to achieve complex global objectives in large systems. The project also includes significant outreach programs in the form of workshops and lab open-houses, to promote undergraduate research, and the participation of women and under-represented minorities.</AbstractNarration>
<MinAmdLetterDate>06/28/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/09/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1150080</AwardID>
<Investigator>
<FirstName>Yi</FirstName>
<LastName>Lu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yi Lu</PI_FULL_NAME>
<EmailAddress>yilu4@illinois.edu</EmailAddress>
<PI_PHON>2173332187</PI_PHON>
<NSF_ID>000569767</NSF_ID>
<StartDate>06/28/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName/>
<StateCode>IL</StateCode>
<ZipCode>618207473</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~85170</FUND_OBLG>
<FUND_OBLG>2013~176705</FUND_OBLG>
<FUND_OBLG>2014~185990</FUND_OBLG>
<FUND_OBLG>2016~0</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Data-parallel applications have become prevalent for processing large-scale data sets. A fundamental problem to all data-parallel applications is data locality, as the processing speed for data-processing tasks varies with data location. There are multiple levels of data locality, depending on whether the data block resides in local memory or disk, within a rack or a data center, or across data centers. Scheduling with data locality is an affinity scheduling problem, albeit with an explosive number of task types. Therefore, existing algorithms do not apply in this setting.</p> <p>For systems with two-level locality, our work show that a simple priority algorithm is delay-optimal in heavy-traffic regime. We find that going from two to three levels of locality changes the problem drastically, as a trade-off between performance and throughput emerges. With multi-level locality, we propose a novel algorithm that achieves delay optimality. This solves a version of an open problem in affinity scheduling, where we want to minimize delay without knowing job arrival rates. For both cases, the proof idea relies on the construction of an appropriate ideal load decomposition that allows the separate treatment of different subsystems, which future reveals an interesting state-space collapse. We implemented the algorithm in Hadoop clusters and demonstrated that it achieves an order of magnitude improvement over existing schedulers.</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/27/2017<br>      Modified by: Yi&nbsp;Lu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Data-parallel applications have become prevalent for processing large-scale data sets. A fundamental problem to all data-parallel applications is data locality, as the processing speed for data-processing tasks varies with data location. There are multiple levels of data locality, depending on whether the data block resides in local memory or disk, within a rack or a data center, or across data centers. Scheduling with data locality is an affinity scheduling problem, albeit with an explosive number of task types. Therefore, existing algorithms do not apply in this setting.  For systems with two-level locality, our work show that a simple priority algorithm is delay-optimal in heavy-traffic regime. We find that going from two to three levels of locality changes the problem drastically, as a trade-off between performance and throughput emerges. With multi-level locality, we propose a novel algorithm that achieves delay optimality. This solves a version of an open problem in affinity scheduling, where we want to minimize delay without knowing job arrival rates. For both cases, the proof idea relies on the construction of an appropriate ideal load decomposition that allows the separate treatment of different subsystems, which future reveals an interesting state-space collapse. We implemented the algorithm in Hadoop clusters and demonstrated that it achieves an order of magnitude improvement over existing schedulers.          Last Modified: 07/27/2017       Submitted by: Yi Lu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
