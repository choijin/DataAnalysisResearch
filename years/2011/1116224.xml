<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small: RUI: A Fast Backing Store System on top of Network RAM, Flash, and other Cluster-wide Storage</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>09/30/2016</AwardExpirationDate>
<AwardTotalIntnAmount>213224.00</AwardTotalIntnAmount>
<AwardAmount>213224</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>It is increasingly important to design cluster systems that efficiently support data-intensive computing. Parallel scientific and multimedia applications often perform large amounts of I/O either indirectly, due to swapping, or directly, due to temporary file accesses.  As the disparity between the speeds of magnetic disk and other hardware such as RAM, interconnection networks, and flash continues to grow, the cost of accessing disk will increasingly become the bottleneck to system performance.  &lt;br/&gt;&lt;br/&gt;The objective of our project is to design and implement a fast backing storage system for clusters and local area networks.  Our system provides a single device interface on top of multiple heterogeneous cluster-wide physical storage, particularly targeting fast random-access storage such as Network RAM and flash.  Our work will free the OS from requiring  multiple device-specific policies in its paging and file systems for every combination of underlying physical storage devices.  As new technologies are developed, the low-level part of our system will incorporate them into its set of heterogeneous physical storage, but the OS will continue to use the same high-level interface of our system, that of a single, fast, random-access device.&lt;br/&gt;&lt;br/&gt;The main goals of our work are: to take advantage of all types of fast storage in a cluster; to provide a single device interface that is independent of the underlying physical storage; to create a storage system that can adapt to changes in resource utilization and capacity; and to develop and examine policies for data placement and data migration between underlying devices.</AbstractNarration>
<MinAmdLetterDate>08/27/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1116224</AwardID>
<Investigator>
<FirstName>Tia</FirstName>
<LastName>Newhall</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tia Newhall</PI_FULL_NAME>
<EmailAddress>newhall@cs.swarthmore.edu</EmailAddress>
<PI_PHON>6103288000</PI_PHON>
<NSF_ID>000065841</NSF_ID>
<StartDate>08/27/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Swarthmore College</Name>
<CityName>Swarthmore</CityName>
<ZipCode>190811390</ZipCode>
<PhoneNumber>6103288000</PhoneNumber>
<StreetAddress>500 COLLEGE AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073755381</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>SWARTHMORE COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Swarthmore College]]></Name>
<CityName>Swarthmore</CityName>
<StateCode>PA</StateCode>
<ZipCode>190811390</ZipCode>
<StreetAddress><![CDATA[500 COLLEGE AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramReference>
<Code>1714</Code>
<Text>SPECIAL PROJECTS - CISE</Text>
</ProgramReference>
<ProgramReference>
<Code>7354</Code>
<Text>COMPUTER SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9229</Code>
<Text>RES IN UNDERGRAD INST-RESEARCH</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~213224</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In the era of Big Data computing, applications often create and process data whose cumulative size is too large to fit into the aggregate memory (RAM) of the set of computers on which they run.&nbsp; When these data-intensive applications are run on general-purpose parallel cluster computers, they may result in data being moved temporarily to slower secondary storage devices (traditionally magnetic disk), in order to free up some RAM space to store data that is being currently processed by the application.&nbsp;&nbsp; This movement to and from secondary storage could be the result of the virtual memory system swapping on individual cluster nodes, or the result of applications using the file system to create and store temporary files containing partial results (external sorting algorithms, and MapReduce are two examples that make use of temporary files).<br /><br />At the same time that applications are stressing the memory hierarchy, the types of secondary storage devices is increasingly diverse.&nbsp; It is common for clusters to have both HDD and SDD drives, other emerging storage technologies, such as phase change memory (PCM), may become more common in the future.&nbsp; In addition, imbalances in application workload across cluster nodes results in some nodes having idle RAM while others have an insufficient amount of RAM to run their local workload.&nbsp; This idle RAM can implement network RAM storage that can be used to store swap or file data.&nbsp; As the disparity continues to grow in the speed of HDD versus other devices such as RAM, network, SSD and PCM, the cost of accessing disk---either as a backing store for swapped pages or files---is increasingly the bottleneck to achieving good performance in these systems.&nbsp; To support efficient data intensive computing, it is is important that cluster node OSs make the most effective use of all types of available fast backing store in a cluster.<br /><br />The goal of our project is to design and implement a system that provides fast backing storage for swap and file data by combining heterogeneous storage devices that are increasingly common in clusters.&nbsp; Additionally, we want to hide the complexity of managing heterogeneous storage from node OSs, allowing this storage to be used in a wide range of contexts without having to modify OS subsystems or applications to use.&nbsp; Finally, we want to provide an adaptable system that easily incorporates new storage devices and provides tunable and adaptable policies to dynamically take advantage of available cluster resources, and to take advantage of the strengths of different types of underlying storage media.<br /><br />We designed, implemented and tested a fully working version of our Nswap2L system.&nbsp; It can be added as backing store for swap or file systems on cluster nodes running unmodified Linux 4.0 kernels.&nbsp;&nbsp; Nswap2L is designed to manage multiple underlying storage devices, and any type of underlying storage device that can be added as a device partition.&nbsp; We have tested Nswap2L with underlying magnetic disk, flash, and Network RAM storage.&nbsp; Our extensive /sys interface to the Nswap2L backing "device" allows users to add new underlying storage devices on the fly, and to change data placement and migration policies as Nswap2L runs.&nbsp; This project also supported five undergraduate students at Swarthmore College, two of whom are co-authors on publications resulting from their work.&nbsp;</p><br> <p>            Last Modified: 02/26/2017<br>      Modified by: Tia&nbsp;Newhall</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In the era of Big Data computing, applications often create and process data whose cumulative size is too large to fit into the aggregate memory (RAM) of the set of computers on which they run.  When these data-intensive applications are run on general-purpose parallel cluster computers, they may result in data being moved temporarily to slower secondary storage devices (traditionally magnetic disk), in order to free up some RAM space to store data that is being currently processed by the application.   This movement to and from secondary storage could be the result of the virtual memory system swapping on individual cluster nodes, or the result of applications using the file system to create and store temporary files containing partial results (external sorting algorithms, and MapReduce are two examples that make use of temporary files).  At the same time that applications are stressing the memory hierarchy, the types of secondary storage devices is increasingly diverse.  It is common for clusters to have both HDD and SDD drives, other emerging storage technologies, such as phase change memory (PCM), may become more common in the future.  In addition, imbalances in application workload across cluster nodes results in some nodes having idle RAM while others have an insufficient amount of RAM to run their local workload.  This idle RAM can implement network RAM storage that can be used to store swap or file data.  As the disparity continues to grow in the speed of HDD versus other devices such as RAM, network, SSD and PCM, the cost of accessing disk---either as a backing store for swapped pages or files---is increasingly the bottleneck to achieving good performance in these systems.  To support efficient data intensive computing, it is is important that cluster node OSs make the most effective use of all types of available fast backing store in a cluster.  The goal of our project is to design and implement a system that provides fast backing storage for swap and file data by combining heterogeneous storage devices that are increasingly common in clusters.  Additionally, we want to hide the complexity of managing heterogeneous storage from node OSs, allowing this storage to be used in a wide range of contexts without having to modify OS subsystems or applications to use.  Finally, we want to provide an adaptable system that easily incorporates new storage devices and provides tunable and adaptable policies to dynamically take advantage of available cluster resources, and to take advantage of the strengths of different types of underlying storage media.  We designed, implemented and tested a fully working version of our Nswap2L system.  It can be added as backing store for swap or file systems on cluster nodes running unmodified Linux 4.0 kernels.   Nswap2L is designed to manage multiple underlying storage devices, and any type of underlying storage device that can be added as a device partition.  We have tested Nswap2L with underlying magnetic disk, flash, and Network RAM storage.  Our extensive /sys interface to the Nswap2L backing "device" allows users to add new underlying storage devices on the fly, and to change data placement and migration policies as Nswap2L runs.  This project also supported five undergraduate students at Swarthmore College, two of whom are co-authors on publications resulting from their work.        Last Modified: 02/26/2017       Submitted by: Tia Newhall]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
