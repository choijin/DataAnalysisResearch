<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Medium: Compiling Parallel Algorithms to Memory Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2012</AwardEffectiveDate>
<AwardExpirationDate>03/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1199884.00</AwardTotalIntnAmount>
<AwardAmount>1206634</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project aims to improve the practice of parallel programming - perhaps the central problem facing computer science in the 21st century.  While the sequential model first introduced by Von Neumann and others has served us well, its inefficiency has been brought into sharp focus by the availability of billion-transistor chips, which are greatly underutilized yet power-hungry when running sequential algorithms.&lt;br/&gt;&lt;br/&gt;This project aims to improve the programmability and efficiency of distributed memory systems, a key issue in the execution of parallel algorithms.  While it is fairly easy to put, say, thousands of independent adders on a single chip, it is far more difficult to supply them with useful data to add, a task that falls to the memory system.  This research will develop compiler optimization algorithms able to configure and orchestrate parallel memory systems able to utilize such parallel computational resources.&lt;br/&gt;&lt;br/&gt;To make more than incremental progress, this project departs from existing hegemony in two important ways.  First, its techniques will be applied only to algorithms expressed in the functional style, a more abstract, mathematically sound representation that enables precise reasoning about parallel algorithms and very aggressive optimizations.  Second, it targets field-programmable gate arrays (FPGAs) rather than existing parallel computing platforms.  FPGAs provide a highly flexible platform that enables exploring parallel architectures far different than today's awkward solutions, which are largely legacy sequential architectures glued together.  While FPGAs are far too flexible and power-hungry to be the long-term "solution"&lt;br/&gt;to the parallel computer architecture question, their use grounds this project in physical reality and will produce useful hardware synthesis algorithms as a side-effect.&lt;br/&gt;&lt;br/&gt;Judicious and efficient data movement is the linchpin of parallel computing.  This project attacks that challenge head on, establishing the constructs and algorithms necessary for hardware and software to efficiently manipulate data together.  This research will lay the groundwork for the next generation of storage and instruction set architectures, compilers, and programming paradigms -- the bedrock of today's mainstream computing.</AbstractNarration>
<MinAmdLetterDate>04/05/2012</MinAmdLetterDate>
<MaxAmdLetterDate>01/18/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1162124</AwardID>
<Investigator>
<FirstName>Stephen</FirstName>
<LastName>Edwards</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stephen A Edwards</PI_FULL_NAME>
<EmailAddress>sedwards@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397019</PI_PHON>
<NSF_ID>000277894</NSF_ID>
<StartDate>04/05/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Martha</FirstName>
<LastName>Kim</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Martha A Kim</PI_FULL_NAME>
<EmailAddress>martha@cs.columbia.edu</EmailAddress>
<PI_PHON>2129397094</PI_PHON>
<NSF_ID>000528915</NSF_ID>
<StartDate>04/05/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100276902</ZipCode>
<StreetAddress><![CDATA[2960 Broadway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~294382</FUND_OBLG>
<FUND_OBLG>2013~296962</FUND_OBLG>
<FUND_OBLG>2014~303102</FUND_OBLG>
<FUND_OBLG>2015~312188</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of the FHW project was to produce a compiler able to translate programs written in a functional language (we chose Haskell) into synthesizable RTL (we chose SystemVerilog)suitable for execution on an FPGA or ASIC that was highly parallel. We ultimately producedsuch a compiler, relying on the Glasgow Haskell Compiler (GHC) as a front-end and writingour own back-end that performed a series of lowering transformations to restructure suchconstructs as recursion, polymorphism, and frst-order functions, into a form suitable forhardware, then transform the now-restricted functional IR into a datafow representation that is then finally transformed into synthesizable System Verilog.</p> <p>In addition to the compiler transformations described above, we developed a library of compositional dataflow blocks and an "assembler" able to translate dataflow descriptions generated by our compiler into synthesizable System Verilog suitable for synthesis on an FPGA. The advantage of our technique is that in the systems it generates data is never dropped, ignored, or accidentally duplicated, unlike in classical RTL coding, and the computed results are always the same, unlike in certain other parallel computing systems that are inherently nondeterministic.</p> <p>The broader impact of this work will be to enable the generation of custom, efficient hardware from higher-level descriptions, simplifying the lives of hardware designers and putting the power of custom hardware design into the hands of designers more familiar with software.</p><br> <p>            Last Modified: 08/04/2019<br>      Modified by: Stephen&nbsp;A&nbsp;Edwards</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of the FHW project was to produce a compiler able to translate programs written in a functional language (we chose Haskell) into synthesizable RTL (we chose SystemVerilog)suitable for execution on an FPGA or ASIC that was highly parallel. We ultimately producedsuch a compiler, relying on the Glasgow Haskell Compiler (GHC) as a front-end and writingour own back-end that performed a series of lowering transformations to restructure suchconstructs as recursion, polymorphism, and frst-order functions, into a form suitable forhardware, then transform the now-restricted functional IR into a datafow representation that is then finally transformed into synthesizable System Verilog.  In addition to the compiler transformations described above, we developed a library of compositional dataflow blocks and an "assembler" able to translate dataflow descriptions generated by our compiler into synthesizable System Verilog suitable for synthesis on an FPGA. The advantage of our technique is that in the systems it generates data is never dropped, ignored, or accidentally duplicated, unlike in classical RTL coding, and the computed results are always the same, unlike in certain other parallel computing systems that are inherently nondeterministic.  The broader impact of this work will be to enable the generation of custom, efficient hardware from higher-level descriptions, simplifying the lives of hardware designers and putting the power of custom hardware design into the hands of designers more familiar with software.       Last Modified: 08/04/2019       Submitted by: Stephen A Edwards]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
