<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHB: Large: Collaborative Research: Companionbots for Proactive Therapeutic Dialog on Depression</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>282389.00</AwardTotalIntnAmount>
<AwardAmount>282389</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This collaborative research investigates a new class of dialog-based, home robotic healthcare assistants to facilitate a new level of in-home, real-time care to elderly and depressed patients, providing lower total costs and higher quality of life. An emotive, physical avatar, called a companionbot, which possesses the ability to engage humans in a way that is unobtrusive and suspends disbelief will be built in this project. The companionbot will be an integration of human language technology, vision, other sensory processing and emotive robotic technology to proactively recognize and dialog with isolated and elderly patients suffering from depression. The companionbot will utilize proactive or companionable dialog based on the context with users suffering from depression. This will require the first multimodal integration of a user model, environment model, and temporal processing with spoken dialog understanding and generation to produce dynamic dialog and emotive interaction, beyond the traditional scripted dialog and emotion. Object recognition, facial expression recognition, and human activity recognition will augment natural language processing to provide current and historical context important to dynamic dialog. &lt;br/&gt;&lt;br/&gt;A team of skilled researchers, assembled from the University of Colorado Boulder, University of Denver, CU Anschutz Medical Campus, and Boulder Language Technologies, will work together to achieve the project goals. The investigators will use the companionbots as a tool to run clinical trials to monitor and dialog with their partners to detect signs of physical and emotional deterioration. The companionbots can then notify remote caregivers, as necessary, provide warnings, reminders, life coaching and therapeutic dialog, extending independence and quality of life, and even saving lives. The other benefits of such a system include continuous, annotated data to improve doctor-patient interaction and analysis, real-time monitoring of mental state for remote healthcare providers and, ultimately, real-time intervention as part of a comprehensive treatment strategy.&lt;br/&gt;&lt;br/&gt;In addition, this research will promote both STEM practice and research education at the graduate and the undergraduate levels of the affiliated institutions. The companionbots are ideal for teaching the next generation of engineers and scientists in critical emerging technologies, as they permit either a deep focus on specific topics or an interdisciplinary perspective while providing a simple high-level interface to manage everything else. Furthermore, the project will develop related educational material to support others and will provide public outreach to K-12 classes in the area.</AbstractNarration>
<MinAmdLetterDate>09/02/2011</MinAmdLetterDate>
<MaxAmdLetterDate>09/26/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1111544</AwardID>
<Investigator>
<FirstName>Wayne</FirstName>
<LastName>Ward</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Wayne H Ward</PI_FULL_NAME>
<EmailAddress>wayne.ward@colorado.edu</EmailAddress>
<PI_PHON>3037355070</PI_PHON>
<NSF_ID>000264998</NSF_ID>
<StartDate>09/02/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Bolanos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Dr</PI_SUFX_NAME>
<PI_FULL_NAME>Daniel Bolanos</PI_FULL_NAME>
<EmailAddress>dani@bltek.com</EmailAddress>
<PI_PHON>3035799605</PI_PHON>
<NSF_ID>000581822</NSF_ID>
<StartDate>09/02/2011</StartDate>
<EndDate>09/26/2013</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Boulder Language Technologies</Name>
<CityName>Boulder</CityName>
<ZipCode>803015406</ZipCode>
<PhoneNumber>3035799605</PhoneNumber>
<StreetAddress>2690 Center Green Ct S Ste 200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>791197622</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOULDER LANGUAGE TECHNOLOGIES (BLT), INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Boulder Language Technologies]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803015406</ZipCode>
<StreetAddress><![CDATA[2690 Center Green Ct S Ste 200]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>8018</Code>
<Text>Smart and Connected Health</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~282389</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-392fec3b-821e-8e6b-546d-9f200442a9e4"> </span></p> <p dir="ltr"><span>Companionbots for Proactive Therapeutic Dialog on Depression</span></p> <p dir="ltr"><span>Boulder Language Technologies</span></p> <p dir="ltr"><span>This is a collaborative project between Boulder Language Technologies (BLT) and the University of North Texas (UNT). The long-term goal of the team&rsquo;s research is to develop a perceptive, emotive, animate, robot that can engage humans in natural conversations. This basic technology will be used to deliver home-based healthcare-related education, training, motivation, and therapy.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Goals</span></p> <p dir="ltr"><span>Primary goals of the project are to: 1) Develop the architecture and processing modules for a conversational robot that can provide select therapy to isolated elderly suffering from depression, and 2) Evaluate the feasibility of the system.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>BLT&rsquo;s activities in the project were to:</span></p> <ol> <li dir="ltr"> <p dir="ltr"><span>Develop audio data collection software and work with researchers at DU to integrate it into the Kinect-based data collection platform.</span></p> </li> <li dir="ltr"> <p dir="ltr"><span>Transcribe and annotate data collected in wizard-of-oz sessions between patient and experimenter</span></p> </li> <li dir="ltr"> <p dir="ltr"><span>Work with the DU team to integrate a new avatar into the system</span></p> </li> <li dir="ltr"> <p dir="ltr"><span>Develop and evaluate a speech recognition module for the system</span></p> </li> </ol> <p dir="ltr"><span><span> </span></span><span><span> </span></span><span><span> </span></span><span><span> </span></span><span><span> </span></span></p> <p dir="ltr"><span>The system is trained by modeling observed sessions of patients interacting with an experimenter. It records the session audio and video data for use in developing an automatic system. The recorded data were transcribed and annotated to create training and test sets for system evaluation.</span></p> <p dir="ltr"><span><span> </span></span><span><span> </span></span><span><span> </span></span><span><span> </span></span><span><span> </span></span></p> <p dir="ltr"><span>Automatic Speech Recognition Evaluation</span></p> <p dir="ltr"><span>The automatic speech recognition (ASR) module of the &nbsp;system uses the KinectSDK environment for reading audio data from the Kinect microphone array. The transcribed data from wizard-of-oz sessions were used to evaluate ASR performance. The sessions were divided into training and test sets, that were used in various combinations to evaluate ASR performance and robustness. A language model was trained by interpolating a general model trained on data scraped from the internet with a model trained on the training set of transcripts.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>The corpus used consisted of sessions from three patients called Tang (36 hours), Chimp (8 hours) and Panda (0.6 hours). &nbsp;As most data came from speaker Tang, it was used to conduct the initial evaluation. As the amount of data are extremely sparse for language modeling, various smoothing algorithms were evaluated , with a smoothed topic-specific model interpolated with a more general model working slightly better than the others. Training acoustic and language models on the Tang training data and testing on a held-out test set resulted in a word error rate (WER) of 40.8%. Adding the test set data to the language model training data (not acoustic) reduced WER to 21.0%. The out of vocabulary word (OOV) rate for the test set was only 1.64%, so words not in the system lexicon were not a significant problem. The large improvement gained from adding language model test data to the training set indicates that the language model is a significant problem. This isn&rsquo;t surprising as it g...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Companionbots for Proactive Therapeutic Dialog on Depression Boulder Language Technologies This is a collaborative project between Boulder Language Technologies (BLT) and the University of North Texas (UNT). The long-term goal of the teamÆs research is to develop a perceptive, emotive, animate, robot that can engage humans in natural conversations. This basic technology will be used to deliver home-based healthcare-related education, training, motivation, and therapy.    Goals Primary goals of the project are to: 1) Develop the architecture and processing modules for a conversational robot that can provide select therapy to isolated elderly suffering from depression, and 2) Evaluate the feasibility of the system.    BLTÆs activities in the project were to:   Develop audio data collection software and work with researchers at DU to integrate it into the Kinect-based data collection platform.   Transcribe and annotate data collected in wizard-of-oz sessions between patient and experimenter   Work with the DU team to integrate a new avatar into the system   Develop and evaluate a speech recognition module for the system         The system is trained by modeling observed sessions of patients interacting with an experimenter. It records the session audio and video data for use in developing an automatic system. The recorded data were transcribed and annotated to create training and test sets for system evaluation.       Automatic Speech Recognition Evaluation The automatic speech recognition (ASR) module of the  system uses the KinectSDK environment for reading audio data from the Kinect microphone array. The transcribed data from wizard-of-oz sessions were used to evaluate ASR performance. The sessions were divided into training and test sets, that were used in various combinations to evaluate ASR performance and robustness. A language model was trained by interpolating a general model trained on data scraped from the internet with a model trained on the training set of transcripts.    The corpus used consisted of sessions from three patients called Tang (36 hours), Chimp (8 hours) and Panda (0.6 hours).  As most data came from speaker Tang, it was used to conduct the initial evaluation. As the amount of data are extremely sparse for language modeling, various smoothing algorithms were evaluated , with a smoothed topic-specific model interpolated with a more general model working slightly better than the others. Training acoustic and language models on the Tang training data and testing on a held-out test set resulted in a word error rate (WER) of 40.8%. Adding the test set data to the language model training data (not acoustic) reduced WER to 21.0%. The out of vocabulary word (OOV) rate for the test set was only 1.64%, so words not in the system lexicon were not a significant problem. The large improvement gained from adding language model test data to the training set indicates that the language model is a significant problem. This isnÆt surprising as it generally requires much more data to train a general language model, as the topics of conversation are open. Testing on the training data gave a WER of 13.9%, which gives an upper bound on how good we can expect models to get using the current architecture and continuing to add more data.    Additional experiments were conducted to show the result of training on one speaker and testing on another. Training on data from Tang and testing on unseen data from the same speaker gave a WER of around 40%. Training on Tang and testing on a different speaker (Chimp) gave an error rate of 73.0. Adding a modest amount of data from the training set for Chimp dropped the WER from 73.0 to 34.9.  These results are consistent with observations from from the literature that acoustic models can pretty quickly adapt to a user, but language models from sparse data are a problem. One possibility is to collect much more data from other sources to build better interpolated models using standard n-gr...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
