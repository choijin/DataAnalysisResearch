<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2012</AwardEffectiveDate>
<AwardExpirationDate>05/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>1251644.00</AwardTotalIntnAmount>
<AwardAmount>1251644</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Robert Beverly</SignBlockName>
<PO_EMAI>rbeverly@nsf.gov</PO_EMAI>
<PO_PHON>7032927068</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Message Passing Interface (MPI) is a very widely used parallel programming model on modern High-End Computing (HEC) systems. Many performance aspects of MPI libraries, such as latency, bandwidth, scalability, memory footprint, cache pollution, overlap of computation and communication etc. are highly dependent on system configuration and application requirements. Additionally, modern clusters are changing rapidly with the growth of multi-core processors and commodity networking technologies such as InfiniBand and 10GigE/iWARP. They are becoming diverse and heterogeneous with varying number of processor cores, processor speed, memory speed, multi-generation network adapters/switches, I/O interface technologies, and accelerators (GPGPUs), etc.  Typically, any MPI library deals with the above kind of diversity in platforms and sensitivity of applications by employing various runtime parameters. These parameters are tuned during its release, or by&lt;br/&gt;system administrators, or by end-users.  These default parameters may or may not be optimal for all system configurations and applications.&lt;br/&gt;&lt;br/&gt;The MPI library of a typical proprietary system goes through heavy performance tuning for a range of applications.  Since commodity clusters provide greater flexibility in their configurations (processor, memory and network), it is very hard to achieve optimal tuning using released version of any MPI library, with its default settings. This leads to the following broad challenge: "Can a comprehensive performance tuning framework be designed for MPI library so that the next generation InfiniBand, 10GigE/iWARP and RoCE clusters and applications will be able to extract `bare-metal' performance and maximum scalability?"  The investigators, involving computer&lt;br/&gt;scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) as well as computational scientists from the Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), University of California San Diego (UCSD), will be addressing the above challenge with innovative solutions.&lt;br/&gt;&lt;br/&gt;The investigators will specifically address the following challenges: 1) Can a set of static tools be designed to optimize performance of an MPI library during installation time?  2) Can a set of dynamic tools with low overhead be designed to optimize performance on a per-user and per-application basis during production runs?  3) How to incorporate the proposed performance tuning framework with the upcoming MPIT interface?  4) How to configure MPI libraries on a given system to deliver different optimizations to a set of driving applications?  and 5) What kind of benefits (in terms of performance, scalability, memory efficiency and reduction in cache pollution) can be achieved by the proposed tuning framework?  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on the TACC Ranger and other systems at OSC, SDSC and OSU.  The proposed designs will be integrated into the open-source MVAPICH2 library.</AbstractNarration>
<MinAmdLetterDate>06/04/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/04/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1148371</AwardID>
<Investigator>
<FirstName>Dhabaleswar</FirstName>
<LastName>Panda</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dhabaleswar K Panda</PI_FULL_NAME>
<EmailAddress>panda@cse.ohio-state.edu</EmailAddress>
<PI_PHON>6142925199</PI_PHON>
<NSF_ID>000487085</NSF_ID>
<StartDate>06/04/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Karen</FirstName>
<LastName>Tomko</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Karen A Tomko</PI_FULL_NAME>
<EmailAddress>ktomko@osc.edu</EmailAddress>
<PI_PHON>6142921091</PI_PHON>
<NSF_ID>000330142</NSF_ID>
<StartDate>06/04/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888735</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1960 Kenny Road]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832127323</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001964634</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio State University]]></Name>
<CityName/>
<StateCode>OH</StateCode>
<ZipCode>432101206</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramReference>
<ProgramReference>
<Code>8009</Code>
<Text>Scientifc Software Integration</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~1251644</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The largest computers today use thousands of computing units (referred<br />to as cores) to run complex scientific simulations. Applications<br />running on these systems predominately use the Message Passing<br />Interface (MPI) programming model to exchange data between cores. The<br />cores are organized several cores per socket, and commonly two or four<br />sockets per server, with the servers connected by a high-performance<br />network, the system's interconnect. The focus of this project is<br />performance tuning of the MVAPICH2 MPI library, a high performance,<br />open-source implementation of the MPI standard that delivers the best<br />performance, scalability, and fault tolerance for high-end computing<br />systems and servers that use one of the following interconnect network<br />technologies: InfiniBand, Intel Omni-Path, 10-40GigE/iWARP or<br />RoCE. The overarching goal of the project is to reduce the time to<br />solution for complex science simulations which use MVAPICH2 or other<br />MPI libraries on these large systems.</p> <p>Most large computer systems (clusters) are built up from commodity<br />components, but the technology is changing rapidly with frequent<br />introduction of new multi-core processors and networking<br />technologies. Open source MPI libraries must run well across multiple<br />generations of processors and interconnects, and additionally support<br />accelerators such as NVIDIA GPGPUS and Intel Xeon Phi<br />processors. Typically, any MPI library deals with diverse platforms<br />and a wide range of applications by employing various runtime<br />parameters. These parameters are either set during software release,<br />by system administrators, or end-users. Considering the nature of<br />commodity clusters, it is difficult to apply one set of common tuning<br />parameters that will allow an MPI library to extract the best<br />performance on all computer systems. This leads to a broad challenge:<br />Can a comprehensive performance tuning framework be designed for MPI<br />libraries for modern commodity clusters?</p> <p>In this project we have addressed this challenge in three areas: 1)<br />Developed architecture specific tuning for MPI communication<br />operations between pairs of cores (point-to-point operations) or<br />groups of cores (collective operations).&nbsp; 2) Developed a performance<br />profiling layer within MVAPICH2, including support for the MPI-3<br />standard's MPI_T interface.&nbsp; 3) Studied application sensitivity to MPI<br />parameters and developed applications specific tuning strategies.</p> <p>Developments were made in all three areas and evaluated with a range<br />of science applications including: HoomdBlue, SMG2000, Neuron, Amber,<br />MiniAMR, MILC, LULESH and HPCCG. These scientific applications and<br />mathematical libraries are from biochemistry, neuroscience, high<br />energy physics, computational fluid dynamics, and numerical linear<br />algebra.&nbsp; Some highlights of these results are that<br />architecture-specific tuning improves collective operation latency by<br />up to 58%. The GPU tuning methods were able to provide a 2X<br />improvement in the execution time of HoomdBlue application. The<br />transport protocol-based tuning shows 27% improvement for the Neuron<br />application example. With HPCG and the LULESH application kernel, the<br />collective tuning for partial subscription available on hybrid<br />MPI+OpenMP programming model improves the execution time by 24% at 512 cores.</p> <p>The results of this research (tuned designs, performance results,<br />benchmarks, etc.) have been made available to the community through<br />the open-source MVAPICH2 library (1.9, 2.0, 2.1, and 2.2 release<br />series including alpha, beta and RC versions). The latest version is<br />currently running on many large-scale XSEDE systems including TACC<br />Stampede, SDSC Gordon and SDSC Comet. Currently, the MVAPICH2 library is being used by more than 2,650 organizations in 81 countries. The<br />MVAPICH2 library and the enhancements are being used by a large number<br />of users of these systems.</p> <p>In each of these releases, information about the tuned designs for<br />various components (such as point-to-point, collectives, GPU-GPU<br />communication, etc.) has been shared with the MVAPICH2 user community<br />through mailing lists. The applications-based tuning results have been<br />made available to the community through the "Best Practices" link of<br />the MVAPICH project web page. In order to achieve direct face-to-face<br />discussion with MVAPICH2 users and get their feedback, in 2013 we<br />started holding an MVAPICH2 User Group Meeting (MUG) each year in<br />August in Columbus Ohio. This meeting has been continuing successfully<br />for the last four years and has helped to disseminate the results of<br />this research to a wider community.&nbsp; In addition to the software<br />distribution and the MUG events, the results have been presented at<br />various conferences and events through talks and tutorials.&nbsp; Multiple<br />Ph.D and Masters students have performed research work and received<br />their Ph.D and M.S. degrees as a part of this project.</p><br> <p>            Last Modified: 08/30/2016<br>      Modified by: Dhabaleswar&nbsp;K&nbsp;Panda</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The largest computers today use thousands of computing units (referred to as cores) to run complex scientific simulations. Applications running on these systems predominately use the Message Passing Interface (MPI) programming model to exchange data between cores. The cores are organized several cores per socket, and commonly two or four sockets per server, with the servers connected by a high-performance network, the system's interconnect. The focus of this project is performance tuning of the MVAPICH2 MPI library, a high performance, open-source implementation of the MPI standard that delivers the best performance, scalability, and fault tolerance for high-end computing systems and servers that use one of the following interconnect network technologies: InfiniBand, Intel Omni-Path, 10-40GigE/iWARP or RoCE. The overarching goal of the project is to reduce the time to solution for complex science simulations which use MVAPICH2 or other MPI libraries on these large systems.  Most large computer systems (clusters) are built up from commodity components, but the technology is changing rapidly with frequent introduction of new multi-core processors and networking technologies. Open source MPI libraries must run well across multiple generations of processors and interconnects, and additionally support accelerators such as NVIDIA GPGPUS and Intel Xeon Phi processors. Typically, any MPI library deals with diverse platforms and a wide range of applications by employing various runtime parameters. These parameters are either set during software release, by system administrators, or end-users. Considering the nature of commodity clusters, it is difficult to apply one set of common tuning parameters that will allow an MPI library to extract the best performance on all computer systems. This leads to a broad challenge: Can a comprehensive performance tuning framework be designed for MPI libraries for modern commodity clusters?  In this project we have addressed this challenge in three areas: 1) Developed architecture specific tuning for MPI communication operations between pairs of cores (point-to-point operations) or groups of cores (collective operations).  2) Developed a performance profiling layer within MVAPICH2, including support for the MPI-3 standard's MPI_T interface.  3) Studied application sensitivity to MPI parameters and developed applications specific tuning strategies.  Developments were made in all three areas and evaluated with a range of science applications including: HoomdBlue, SMG2000, Neuron, Amber, MiniAMR, MILC, LULESH and HPCCG. These scientific applications and mathematical libraries are from biochemistry, neuroscience, high energy physics, computational fluid dynamics, and numerical linear algebra.  Some highlights of these results are that architecture-specific tuning improves collective operation latency by up to 58%. The GPU tuning methods were able to provide a 2X improvement in the execution time of HoomdBlue application. The transport protocol-based tuning shows 27% improvement for the Neuron application example. With HPCG and the LULESH application kernel, the collective tuning for partial subscription available on hybrid MPI+OpenMP programming model improves the execution time by 24% at 512 cores.  The results of this research (tuned designs, performance results, benchmarks, etc.) have been made available to the community through the open-source MVAPICH2 library (1.9, 2.0, 2.1, and 2.2 release series including alpha, beta and RC versions). The latest version is currently running on many large-scale XSEDE systems including TACC Stampede, SDSC Gordon and SDSC Comet. Currently, the MVAPICH2 library is being used by more than 2,650 organizations in 81 countries. The MVAPICH2 library and the enhancements are being used by a large number of users of these systems.  In each of these releases, information about the tuned designs for various components (such as point-to-point, collectives, GPU-GPU communication, etc.) has been shared with the MVAPICH2 user community through mailing lists. The applications-based tuning results have been made available to the community through the "Best Practices" link of the MVAPICH project web page. In order to achieve direct face-to-face discussion with MVAPICH2 users and get their feedback, in 2013 we started holding an MVAPICH2 User Group Meeting (MUG) each year in August in Columbus Ohio. This meeting has been continuing successfully for the last four years and has helped to disseminate the results of this research to a wider community.  In addition to the software distribution and the MUG events, the results have been presented at various conferences and events through talks and tutorials.  Multiple Ph.D and Masters students have performed research work and received their Ph.D and M.S. degrees as a part of this project.       Last Modified: 08/30/2016       Submitted by: Dhabaleswar K Panda]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
