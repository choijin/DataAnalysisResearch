<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Small: Computational Entropy</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2011</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jack S. Snoeyink</SignBlockName>
<PO_EMAI>jsnoeyin@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In his 1948 paper that founded information theory, Shannon introduced a notion of "entropy" to measure the amount of "randomness" in a process.  However, to a computer with bounded  resources, the amount of randomness can appear to be very different from the Shannon entropy.  Indeed, various measures of "computational entropy" have been very useful in computational complexity and the foundations of cryptography. Recent work by the PI and others have introduced new measures of computational entropy, increased our understanding the new and old computational entropy measures, and found greater applicability of the these measures in cryptography and complexity theory.&lt;br/&gt;&lt;br/&gt;This project aims to refine our understanding of computational entropy, use computational entropy to seek unified and optimal constructions of cryptographic primitives, bring us closer to resolving the power of randomness in space-bounded computation, and identify new applications of computational entropy in the theory of computation.&lt;br/&gt;&lt;br/&gt;This research is closely integrated with the PI's educational efforts.   The PI continues to develop courses and online lecture notes related to the project (on topics such as Pseudorandomness, Cryptography, and Applied Algebra).   Graduate students are involved in all aspects of the research, and undergraduates participate regularly as well.   The PI is also very active in service to the scientific community, including outreach efforts such as "vision nuggets" that convey research directions in theoretical computer science to a broad audience.</AbstractNarration>
<MinAmdLetterDate>07/06/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/06/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1116616</AwardID>
<Investigator>
<FirstName>Salil</FirstName>
<LastName>Vadhan</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Salil P Vadhan</PI_FULL_NAME>
<EmailAddress>salil_vadhan@harvard.edu</EmailAddress>
<PI_PHON>6174960439</PI_PHON>
<NSF_ID>000138824</NSF_ID>
<StartDate>07/06/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard University]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021385369</ZipCode>
<StreetAddress><![CDATA[1033 MASSACHUSETTS AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7927</Code>
<Text>COMPLEXITY &amp; CRYPTOGRAPHY</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In 1948, Claude Shannon gave a precise mathematical definition of &ldquo;entropy&rdquo; as a measure of randomness in probability distributions, motivated by the electrical engineering problem of designing efficient communication systems.&nbsp; According to Shannon&rsquo;s definition, the result of a fair coin toss has less entropy than a roll of a fair six-sided die, but more entropy than a biased coin toss.&nbsp; &nbsp;This project is about the interaction of Shannon&rsquo;s concept with computer science.&nbsp; Specifically, to a computer algorithm that has a bounded amount of time to compute, the amount of &ldquo;randomness&rdquo; in a distribution can appear to be very different than what Shannon&rsquo;s measure would suggest.&nbsp;</p> <p>With this perspective, we have introduced several new measures of &ldquo;computational entropy&rdquo; and used them to attack fundamental problems in computer science.&nbsp; First, they have enabled us to construct new cryptographic algorithms (such as used for encryption to protect personal data and electronic commerce) under the weakest possible assumptions.&nbsp; These algorithms are substantially simpler and more efficient than previous ones.&nbsp; Second, we have used forms of computational entropy to make progress on understanding whether randomization can reduce the amount of memory that computers need to solve problems, a long-standing open problem in computer science.</p> <p>Two Ph.D. students, two postdocs, and one undergraduate were trained under this project, most of whom achieved publications and/or presentations at leading conferences and journals, and have continued in research after graduation.&nbsp; The PI taught five courses related to the project, whose materials (including lecture notes and problem sets) are publicly available from the PI&rsquo;s webpage (http://seas.harvard.edu/~salil). &nbsp;One of these was a new introductory course in theoretical computer science co-developed by the PI. &nbsp;The PI also completed a 336-page monograph on Pseudorandomness, which is designed to allow non-experts to learn about this&nbsp;research area - including new students in theoretical computer science and researchers from other fields.&nbsp;&nbsp; The monograph has been used by faculty to teach graduate courses at a number of other universities around the world.&nbsp; The PI engaged in a variety of other outreach activities, including chairing the SIGACT Committee on the Advancement for Theoretical Computer Science, one of whose goals to make the accomplishments and future promise of research in theoretical computer science accessible to a wide audience.</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/01/2015<br>      Modified by: Salil&nbsp;P&nbsp;Vadhan</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2015/1116616/1116616_10105559_1443686920902_Wuerfel5--rgov-214x142.jpg" original="/por/images/Reports/POR/2015/1116616/1116616_10105559_1443686920902_Wuerfel5--rgov-800width.jpg" title="Dice"><img src="/por/images/Reports/POR/2015/1116616/1116616_10105559_1443686920902_Wuerfel5--rgov-66x44.jpg" alt="Dice"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The more sides a die has, the more randomness it has according to Shannon's classic measure of entropy.  But the same need not be true for "computational entropy" - randomness as perceived by computer algorithms with limited computing time.</div> <div class="imageCredit">https://commons.wikimedia.org/wiki/File:Wuerfe...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In 1948, Claude Shannon gave a precise mathematical definition of "entropy" as a measure of randomness in probability distributions, motivated by the electrical engineering problem of designing efficient communication systems.  According to ShannonÆs definition, the result of a fair coin toss has less entropy than a roll of a fair six-sided die, but more entropy than a biased coin toss.   This project is about the interaction of ShannonÆs concept with computer science.  Specifically, to a computer algorithm that has a bounded amount of time to compute, the amount of "randomness" in a distribution can appear to be very different than what ShannonÆs measure would suggest.   With this perspective, we have introduced several new measures of "computational entropy" and used them to attack fundamental problems in computer science.  First, they have enabled us to construct new cryptographic algorithms (such as used for encryption to protect personal data and electronic commerce) under the weakest possible assumptions.  These algorithms are substantially simpler and more efficient than previous ones.  Second, we have used forms of computational entropy to make progress on understanding whether randomization can reduce the amount of memory that computers need to solve problems, a long-standing open problem in computer science.  Two Ph.D. students, two postdocs, and one undergraduate were trained under this project, most of whom achieved publications and/or presentations at leading conferences and journals, and have continued in research after graduation.  The PI taught five courses related to the project, whose materials (including lecture notes and problem sets) are publicly available from the PIÆs webpage (http://seas.harvard.edu/~salil).  One of these was a new introductory course in theoretical computer science co-developed by the PI.  The PI also completed a 336-page monograph on Pseudorandomness, which is designed to allow non-experts to learn about this research area - including new students in theoretical computer science and researchers from other fields.   The monograph has been used by faculty to teach graduate courses at a number of other universities around the world.  The PI engaged in a variety of other outreach activities, including chairing the SIGACT Committee on the Advancement for Theoretical Computer Science, one of whose goals to make the accomplishments and future promise of research in theoretical computer science accessible to a wide audience.          Last Modified: 10/01/2015       Submitted by: Salil P Vadhan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
