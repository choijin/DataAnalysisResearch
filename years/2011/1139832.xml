<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III:  EAGER:  Collaborative Research:  A Community Experiment Platform for Reproducibility and Generalizability</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>190000.00</AwardTotalIntnAmount>
<AwardAmount>190000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>A hallmark of the scientific method has been that experiments should be described in enough detail that they can be repeated and perhaps generalized.  This implies the possibility of repeating results on nominally equal configurations and then generalizing the results by replaying them on new data sets, and seeing how they vary with different parameters.  In principle, this should be easier for computational experiments than for natural science experiments, because not only can computational processes be automated but also computational systems do not suffer from the "biological variation" that plagues the life sciences.  Unfortunately, the state of the art falls far short of this goal.  Most computational experiments are specified only informally in papers, where experimental results are briefly described in figure captions; the code that produced the results is seldom available; and configuration parameters change results in unforeseen ways. Because important scientific discoveries are often the result of sequences of smaller, less significant steps, the ability to publish results that are fully documented and reproducible is necessary for advancing science.  While concern about repeatability and generalizability cuts across virtually all natural, computational, and social science fields, no single field has identified this concern as a target of a research effort.&lt;br/&gt;&lt;br/&gt;This collaborative project between the University of Utah and New York University consists of tools and infrastructure that supports the process of sharing, testing and re-using scientific experiments and results by leveraging and extending the infrastructure provided by provenance-enabled scientific workflow systems. The project explores three key research questions: (1) How to package and publish compendia of scientific results that are reproducible and generalizable. (2) What are appropriate algorithms and interfaces for exploring, comparing, re-using the results or potentially discovering better approaches for a given problem? 3) How to aid reviewers to generate experiments that are most informative given a time/resource limit.&lt;br/&gt;&lt;br/&gt;An expected result of this work is a software infrastructure that allows authors to create workflows that encode the computational processes that derive the results (including data used, configuration parameters set, and underlying software), publish and connect these to publications where the results are reported. Testers (or reviewers) can repeat and validate results, ask questions anonymously, and modify experimental conditions.  Researchers, who want to build upon previous works, are able to search, reproduce, compare and analyze experiments and results. The infrastructure supports scientists, in many disciplines, to derive, publish and share reproducible results. Results of this research, including developed software will be available via the project web site ( http://www.vistrails.org/index.php/RepeatabilityCentral).</AbstractNarration>
<MinAmdLetterDate>06/08/2011</MinAmdLetterDate>
<MaxAmdLetterDate>06/08/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1139832</AwardID>
<Investigator>
<FirstName>Juliana</FirstName>
<LastName>Freire</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Juliana Freire</PI_FULL_NAME>
<EmailAddress>juliana.freire@nyu.edu</EmailAddress>
<PI_PHON>6469974128</PI_PHON>
<NSF_ID>000183765</NSF_ID>
<StartDate>06/08/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Polytechnic University of New York</Name>
<CityName>Brooklyn</CityName>
<ZipCode>112013826</ZipCode>
<PhoneNumber>7182603360</PhoneNumber>
<StreetAddress>15 Metrotech Center</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>066076225</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>POLYTECHNIC INSTITUTE OF NEW YORK UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Polytechnic University of New York]]></Name>
<CityName>Brooklyn</CityName>
<StateCode>NY</StateCode>
<ZipCode>112013840</ZipCode>
<StreetAddress><![CDATA[SIX MetroTech Center]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~190000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Ever since Francis Bacon, a hallmark of the scientific method has been that experiments should be described in enough detail that they can be repeated and perhaps generalized.&nbsp; When Newton said that he could see farther because he stood on the shoulders of giants, he depended on the truth of his predecessors' observations and the correctness of their calculations.&nbsp; In modern terms, this implies the possibility of repeating results on nominally equal configurations and then generalizing the results by replaying them on new data sets, and seeing how they vary with different parameters.&nbsp; In principle, this should be easier for computational experiments than for natural science experiments, because not only can computational processes be automated but also computational systems do not suffer from the ``biological variation'' that plagues the life sciences.&nbsp; Unfortunately, the state of the art falls far short of this goal.&nbsp; Most computational experiments are specified only informally in papers, where experimental results are briefly described in figure captions; the code and actual experiments that produced the results are seldom available.&nbsp; In the absence of reproducibility, it becomes hard and sometimes impossible to verify scientific results.&nbsp; Furthermore, scientific discoveries do not happen in isolation. Important advances are often the result of sequences of smaller, less significant steps. If results are not fully documented, reproducible, and generalizable, it becomes hard to re-use and extend them. While there has been a renewed interest in the publication of reproducible results, a major roadblock to the more widespread adoption of this practice is the fact that it is hard both to derive a compendium that encapsulates all the components (e.g., data, code, parameter settings) needed to reproduce a result.&nbsp; Under NSF sponsorship, our goal in this EAGER award was to design new tools and infrastructure to simplify the process of creating, sharing and evaluating reproducible experiments.<br /><br />Intellectual Merit<br />-We have developed and released the first version of the proposed reproducibility infrastructure. This infrastructure uses the open-source VisTrails system (http://www.vistrails.org) to support the life-cycle of publications: their creation, review and re-use. As scientists explore a given problem, VisTrails systematically captures the provenance of the exploration, including the workflows created and versions of source code and libraries used. The infrastructure also includes methods to link results to their provenance, reproduce results, explore parameter spaces, interact with results through a Web-based interface, and upgrade the specification of computational experiments to work in different environments and with newer versions of software. Documents (including LaTeX, PowerPoint, Word, wiki and HTML pages) can be created that link to provenance information that allows the results to be reproduced. We have also extended VisTrails to reduce the barrier to entry for novice users. In particular, we created a new tool, CLTools, to help users wrap and package their experiments within VisTrails.&nbsp; This infrastructure was selected as a finalist in the Executable Paper Challenge (http://www.executablepapers.com/finalists.html).<br />-We have studied the life-cycle of reproducible experiments and introduced the notion of axes of reproducibility, which we then used to categorized reproducible experiments based on levels of reproducibility along the different axes.<br />-We have developed and released ReproZip (https://github.com/fchirigati), an open-source tool that automatically captures the provenance of experiments and packs all the necessary files, library dependencies and variables to reproduce the results. Reviewers can then unpack and run the experiments without having to install any additional software....]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Ever since Francis Bacon, a hallmark of the scientific method has been that experiments should be described in enough detail that they can be repeated and perhaps generalized.  When Newton said that he could see farther because he stood on the shoulders of giants, he depended on the truth of his predecessors' observations and the correctness of their calculations.  In modern terms, this implies the possibility of repeating results on nominally equal configurations and then generalizing the results by replaying them on new data sets, and seeing how they vary with different parameters.  In principle, this should be easier for computational experiments than for natural science experiments, because not only can computational processes be automated but also computational systems do not suffer from the ``biological variation'' that plagues the life sciences.  Unfortunately, the state of the art falls far short of this goal.  Most computational experiments are specified only informally in papers, where experimental results are briefly described in figure captions; the code and actual experiments that produced the results are seldom available.  In the absence of reproducibility, it becomes hard and sometimes impossible to verify scientific results.  Furthermore, scientific discoveries do not happen in isolation. Important advances are often the result of sequences of smaller, less significant steps. If results are not fully documented, reproducible, and generalizable, it becomes hard to re-use and extend them. While there has been a renewed interest in the publication of reproducible results, a major roadblock to the more widespread adoption of this practice is the fact that it is hard both to derive a compendium that encapsulates all the components (e.g., data, code, parameter settings) needed to reproduce a result.  Under NSF sponsorship, our goal in this EAGER award was to design new tools and infrastructure to simplify the process of creating, sharing and evaluating reproducible experiments.  Intellectual Merit -We have developed and released the first version of the proposed reproducibility infrastructure. This infrastructure uses the open-source VisTrails system (http://www.vistrails.org) to support the life-cycle of publications: their creation, review and re-use. As scientists explore a given problem, VisTrails systematically captures the provenance of the exploration, including the workflows created and versions of source code and libraries used. The infrastructure also includes methods to link results to their provenance, reproduce results, explore parameter spaces, interact with results through a Web-based interface, and upgrade the specification of computational experiments to work in different environments and with newer versions of software. Documents (including LaTeX, PowerPoint, Word, wiki and HTML pages) can be created that link to provenance information that allows the results to be reproduced. We have also extended VisTrails to reduce the barrier to entry for novice users. In particular, we created a new tool, CLTools, to help users wrap and package their experiments within VisTrails.  This infrastructure was selected as a finalist in the Executable Paper Challenge (http://www.executablepapers.com/finalists.html). -We have studied the life-cycle of reproducible experiments and introduced the notion of axes of reproducibility, which we then used to categorized reproducible experiments based on levels of reproducibility along the different axes. -We have developed and released ReproZip (https://github.com/fchirigati), an open-source tool that automatically captures the provenance of experiments and packs all the necessary files, library dependencies and variables to reproduce the results. Reviewers can then unpack and run the experiments without having to install any additional software. ReproZip greatly simplifies the process of making experiments reproducible.  - We have designed a benchmark to help categorize and ...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
