<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Studies on the visual phonetic basis of some language sound patterns</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2012</AwardEffectiveDate>
<AwardExpirationDate>12/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>300173.00</AwardTotalIntnAmount>
<AwardAmount>300173</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Badecker</SignBlockName>
<PO_EMAI>wbadecke@nsf.gov</PO_EMAI>
<PO_PHON>7032925069</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Over the years linguists have studied and explained the sound systems of language in terms of "sounds".  Our words for this area of study "Phonetics" and "Phonology" even employ the Greek root for sound: phon.  Yet, language is acquired in face-to-face communication in which children watch as well as listen as adults model language.  The PI has found that phonetic explanations of sound change sometimes fail because they refer only to muscular or acoustic properties of speech neglecting what talking faces look like. The proposed research identifies five such problem spots in current thinking, presents the rationale for hypothesizing that visual phonetic information may be involved, and outlines proposed tests of these hypotheses.  The ultimate goal is a workable theory of language change that will make it possible to predict future trends.&lt;br/&gt;&lt;br/&gt;The first set of three audiovisual perception studies in this project will examine the potential impact of visual phonetic cues in historical sound changes involving place of articulation.  These include studies on the "unmarked" status of coronal consonants, the emergence of glottal stop from homorganic consonant clusters, and the role of secondary labialization in the preservation of bilabial and labiodental consonants.  The second set of two studies will again use audiovisual speech perception experiments but in this set the emphasis is on dialect variation in American English, dealing specifically with the role of visual phonetic cues for vowels and fricative consonants.  Two visual phonetic descriptive studies, one on low vowels and one on interdental fricatives will be conducted and to provide the empirical basis for the perception experiments.&lt;br/&gt;&lt;br/&gt;In addition to its contribution to the understanding of how humans perceive spoken language and the effect that visual information has on speech perception, this research project will integrate education and research by providing valuable research opportunities for undergraduates and graduate students. It will broaden the participation of under-represented groups through the involvement of students at community colleges.  It will also result in the broad dissemination of research through publications in specialist journals and timely web publication of results and raw materials.</AbstractNarration>
<MinAmdLetterDate>07/16/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/16/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1147583</AwardID>
<Investigator>
<FirstName>Keith</FirstName>
<LastName>Johnson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Keith Johnson</PI_FULL_NAME>
<EmailAddress>keithjohnson@berkeley.edu</EmailAddress>
<PI_PHON>5103331575</PI_PHON>
<NSF_ID>000362546</NSF_ID>
<StartDate>07/16/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>947202650</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~300173</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project explored how seeing the face of a talker may contribute to phonetic knowledge. &nbsp;One way that we tested this was to look at how phonetic properties that are acoustically weak may be anchored in visual cues. &nbsp;There is some reason to believe that such visual anchoring may make it possible to discover and make use of weak acoustic cues. &nbsp;This was tested in a study of "place of articulation" where confusions arise between /p/ and /k/ sounds are robustly resolved by visual information. &nbsp;The cogntive alignment of auditory and visual information was also tested. &nbsp;Because the speed of sound is an order of magnitude slower than the speed of light, perceptual systems must be able to adapt to audio-visual asynchrony. &nbsp;There is evidence that this adaptation process is variable - influenced by attention to modality. &nbsp;We tested the effect of audio-visual asynchrony on speech perception and found that some common historical sound changes can be related to the pattern of results we found. &nbsp;Both of these lines of research - visual anchoring and A-V asynchrony - highlight the contribution of visual phonetic information in the formation of language.</p> <p>There is an important practical application of this research for language teaching. &nbsp;One key element of successful language teaching is pronunciation training - helping students pronounce the words of a language like a native speaker. &nbsp;Our research suggests that visual anchoring is important in the development of phonetic categories. &nbsp;So, in language education, it is vitally important to practice with visual phonetic input - seeing the face of the talker. This has obvious implications for web-based language education programs, which should be designed to show students what it looks like to say the words of the language.&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/12/2016<br>      Modified by: Keith&nbsp;Johnson</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project explored how seeing the face of a talker may contribute to phonetic knowledge.  One way that we tested this was to look at how phonetic properties that are acoustically weak may be anchored in visual cues.  There is some reason to believe that such visual anchoring may make it possible to discover and make use of weak acoustic cues.  This was tested in a study of "place of articulation" where confusions arise between /p/ and /k/ sounds are robustly resolved by visual information.  The cogntive alignment of auditory and visual information was also tested.  Because the speed of sound is an order of magnitude slower than the speed of light, perceptual systems must be able to adapt to audio-visual asynchrony.  There is evidence that this adaptation process is variable - influenced by attention to modality.  We tested the effect of audio-visual asynchrony on speech perception and found that some common historical sound changes can be related to the pattern of results we found.  Both of these lines of research - visual anchoring and A-V asynchrony - highlight the contribution of visual phonetic information in the formation of language.  There is an important practical application of this research for language teaching.  One key element of successful language teaching is pronunciation training - helping students pronounce the words of a language like a native speaker.  Our research suggests that visual anchoring is important in the development of phonetic categories.  So, in language education, it is vitally important to practice with visual phonetic input - seeing the face of the talker. This has obvious implications for web-based language education programs, which should be designed to show students what it looks like to say the words of the language.              Last Modified: 04/12/2016       Submitted by: Keith Johnson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
