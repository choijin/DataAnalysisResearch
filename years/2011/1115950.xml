<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Building Up the Optimization Algorithmic Infrastructure for Data-Driven Knowledge Discovery and Recovery</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2011</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>184999.00</AwardTotalIntnAmount>
<AwardAmount>184999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Junping Wang</SignBlockName>
<PO_EMAI>jwang@nsf.gov</PO_EMAI>
<PO_PHON>7032924488</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The investigator proposes to study a number of optimization models and algorithms &lt;br/&gt;that are broadly useful in data dimension reduction, latent information extraction&lt;br/&gt;and hidden knowledge discovery. The focus is on problems involving low-rank &lt;br/&gt;matrices of very large sizes, including the fundamental problem of computing &lt;br/&gt;principal singular value decompositions for unstructured dense matrices, as well as &lt;br/&gt;3D-image processing techniques with applications to hyperspectral data processing and &lt;br/&gt;wireless video networks.  The overall goal is to develop reliable algorithms that are &lt;br/&gt;much faster (by one order of magnitude or more on large problems) than those in use today. Since many of the proposed algorithms are extensions to the classic augmented Lagrangian alternating direction method (ALADM) originally designed for certain convex programs, a part of the project is devoted to a theoretical investigation on establishing a convergence theory for ALADM in more general settings.&lt;br/&gt;&lt;br/&gt;Modern technologies, such as 4D CT-scans, satellite remote sensing and DNA microarrays, are creating an explosion of data made available in massive quantities and at fast rates. Mathematical and computational techniques play a crucial role in helping make sense out of such massive data sets in a timely fashion and with minimal human interventions. The PI's work involves studying and designing new algorithms for solving several classes of mathematical models designed to help discover and extract the most useful information buried or hidden in large amounts of data. New algorithms have the potential to run much much faster than today's state-of-the-art methods, thus providing more processing power and speed to numerous data-driven applications such as medical diagnoses, agriculture planning, environment surveillance or genetic research in biosciences.</AbstractNarration>
<MinAmdLetterDate>06/23/2011</MinAmdLetterDate>
<MaxAmdLetterDate>06/23/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1115950</AwardID>
<Investigator>
<FirstName>Yin</FirstName>
<LastName>Zhang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yin Zhang</PI_FULL_NAME>
<EmailAddress>yzhang@rice.edu</EmailAddress>
<PI_PHON>7133485744</PI_PHON>
<NSF_ID>000176875</NSF_ID>
<StartDate>06/23/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>William Marsh Rice University</Name>
<CityName>Houston</CityName>
<ZipCode>770051827</ZipCode>
<PhoneNumber>7133484820</PhoneNumber>
<StreetAddress>6100 MAIN ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>050299031</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WILLIAM MARSH RICE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>050299031</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[William Marsh Rice University]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>770051827</ZipCode>
<StreetAddress><![CDATA[6100 MAIN ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~184999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Today's technologies such as internet, medical imaging, hyperspectral remote sensing, DNA microarrays and various sensor networks are creating massive amounts of data on daily basis. &nbsp;These data sets inevitably carry large mounts of redundancy. &nbsp;Mathematical and computational techniques play a crucial role in extracting and understanding critical information or knowledge from such massive data sets. &nbsp;Most mathematical and statistical models involved in data problems lead to so-called optimization models where one seeks to obtain an optimal solution with regard to a given criterion while imposing conditions to try to keep obtained solutions faithful to reality. &nbsp;Therefore, the current data explosion situation creates high demands for efficient and reliable optimization algorithms and software tools that are also capable of handling larger and larger data sets.&nbsp;</p> <p><br />In this project, the PI, with his collaborator and students, studied a diverse set of optimization models useful in data size reduction, data recovery, and knowledge extraction. &nbsp;A common objective in these models is to remove less relevant or redundant data components from a large data set so that (a) the reduced data set retains the essential information but becomes more manageable, and (b) useful information is better exposed and represented. &nbsp;In this project, studied problems include both generally applicable methodologies and some practical applications. &nbsp;</p> <p><br />At the conclusion of the project, several algorithms and software tools have been developed that proved to be more efficient and reliable at least in many cases than those previously available or in use by practicing scientists. &nbsp;Based on works supported fully or in part by this project, eleven (11) journal papers have been published and four (4) software packages have been released. &nbsp;For the sake of space, let us concentrate on two software packages and related papers. &nbsp;First, TVAL3 is a software package for image recovery based on solving an optimization model called total variation regularization. &nbsp;The package, built on a new algorithm, is now widely used by various researchers in image processing fields. &nbsp;We used an extension of TVAL3 to solve a difficult and large data processing problem that involves four-dimensional data (3 space dimensions plus a spectrum dimension). &nbsp;The resulting paper (Li, Sun, Kelly and Zhang: A Compressive Sensing and Unmixing Scheme for Hyperspectral Data Processing, IEEE Transactions on Image Processing. Vol. 21, pp. 1200-1210. 2012) was featured as one of the two highlight papers in IEEE Signal Processing Magazine in September 2012. &nbsp;Another package we mention here is call LMafit that features one of the fastest algorithms for recovering datasets in which a significant percentage of observations is missing. &nbsp; A variation of the algorithm has been extended by us to solving an even more difficult data recovery problem where severe noise exists. &nbsp; &nbsp;The resulting paper (&ldquo;Augmented Lagrangian alternating direction method for matrix separation based on low-rank factorization&rdquo;, published online 13 Jul 2012) was awarded the Charles Broyden Prize for 2014 by the Journal of Optimization Methods and Software.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 10/12/2015<br>      Modified by: Yin&nbsp;Zhang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Today's technologies such as internet, medical imaging, hyperspectral remote sensing, DNA microarrays and various sensor networks are creating massive amounts of data on daily basis.  These data sets inevitably carry large mounts of redundancy.  Mathematical and computational techniques play a crucial role in extracting and understanding critical information or knowledge from such massive data sets.  Most mathematical and statistical models involved in data problems lead to so-called optimization models where one seeks to obtain an optimal solution with regard to a given criterion while imposing conditions to try to keep obtained solutions faithful to reality.  Therefore, the current data explosion situation creates high demands for efficient and reliable optimization algorithms and software tools that are also capable of handling larger and larger data sets.    In this project, the PI, with his collaborator and students, studied a diverse set of optimization models useful in data size reduction, data recovery, and knowledge extraction.  A common objective in these models is to remove less relevant or redundant data components from a large data set so that (a) the reduced data set retains the essential information but becomes more manageable, and (b) useful information is better exposed and represented.  In this project, studied problems include both generally applicable methodologies and some practical applications.     At the conclusion of the project, several algorithms and software tools have been developed that proved to be more efficient and reliable at least in many cases than those previously available or in use by practicing scientists.  Based on works supported fully or in part by this project, eleven (11) journal papers have been published and four (4) software packages have been released.  For the sake of space, let us concentrate on two software packages and related papers.  First, TVAL3 is a software package for image recovery based on solving an optimization model called total variation regularization.  The package, built on a new algorithm, is now widely used by various researchers in image processing fields.  We used an extension of TVAL3 to solve a difficult and large data processing problem that involves four-dimensional data (3 space dimensions plus a spectrum dimension).  The resulting paper (Li, Sun, Kelly and Zhang: A Compressive Sensing and Unmixing Scheme for Hyperspectral Data Processing, IEEE Transactions on Image Processing. Vol. 21, pp. 1200-1210. 2012) was featured as one of the two highlight papers in IEEE Signal Processing Magazine in September 2012.  Another package we mention here is call LMafit that features one of the fastest algorithms for recovering datasets in which a significant percentage of observations is missing.   A variation of the algorithm has been extended by us to solving an even more difficult data recovery problem where severe noise exists.    The resulting paper ("Augmented Lagrangian alternating direction method for matrix separation based on low-rank factorization", published online 13 Jul 2012) was awarded the Charles Broyden Prize for 2014 by the Journal of Optimization Methods and Software.           Last Modified: 10/12/2015       Submitted by: Yin Zhang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
