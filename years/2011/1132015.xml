<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Medium Node: Reducing Error in Computerized Survey Data Collection</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2011</AwardEffectiveDate>
<AwardExpirationDate>12/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>2967347.00</AwardTotalIntnAmount>
<AwardAmount>3446353</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Cheryl Eavey</SignBlockName>
<PO_EMAI>ceavey@nsf.gov</PO_EMAI>
<PO_PHON>7032927269</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Rising costs of survey data collection and researchers' growing concerns about the quality of survey data necessitate the development of innovative approaches to data collection.  This research will focus on improving survey data collected from computer-assisted methods.  The project will include research related to internet data collection and computer-assisted telephone interviews (CATI) data collection systems, along with the variants of these computer-assisted data collection systems.  With an overall goal of improving the quality of data derived from surveys, the research will focus on accomplishing three objectives.  First, the study will evaluate the use of four diagnostic tools for identifying measurement errors in computer-assisted, interviewer-administered data collection instruments, and it will use these findings to inform visual redesign of common features of computer-assisted, interviewer-administered data collection instruments.  Second, the study will evaluate the use of adaptive/responsive designs in which a dynamic modeling of collected data is used to modify the questionnaire as the data are being collected.  Third, the study will evaluate the application of calendar- and time diary-based data collection methods to aid in the accuracy of behavioral self-reports by tailoring questions to the needs of individual respondents.  The tasks under these objectives will be integrative, allowing researchers to improve survey designs/instruments and enhance data quality by reducing interviewer and respondent burden.  The interdisciplinary project team includes experts in statistics, psychology, sociology, survey research and methodology, and computer science.  The team will leverage long-term collaborations and partner with industry leaders -- Gallup and Abt SRBI -- as well as Census to accomplish its goal and objectives.&lt;br/&gt;&lt;br/&gt;This research will advance scientific knowledge in survey methodology and related fields.  Reducing measurement errors in survey data is critical to accurate inference from survey data, and identifying new tools that minimize or reduce measurement errors in survey data will improve conclusions made from surveys across fields.  In addition to advancing discovery and understanding in survey data collection, the project will contribute significantly to the training of future survey research professionals through implementation of an education plan designed to: (1) recruit and retain a diverse methodologically-oriented student pool from related social science and statistical disciplines; (2) integrate research into existing and new curricula at UNL; (3) provide all participating students with hands-on research opportunities; and (4) develop student researchers' professional skills.  The results will be broadly relevant and broadly disseminated at multidisciplinary and interdisciplinary conferences and workshops, to the federal statistical agencies, and to researchers in the survey industry.  This activity is supported by the NSF-Census Research Network funding opportunity.</AbstractNarration>
<MinAmdLetterDate>07/28/2011</MinAmdLetterDate>
<MaxAmdLetterDate>09/22/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1132015</AwardID>
<Investigator>
<FirstName>Allan</FirstName>
<LastName>McCutcheon</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Allan L McCutcheon</PI_FULL_NAME>
<EmailAddress>amccutcheon1@unl.edu</EmailAddress>
<PI_PHON>4024727758</PI_PHON>
<NSF_ID>000393098</NSF_ID>
<StartDate>07/28/2011</StartDate>
<EndDate>06/18/2015</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Allan</FirstName>
<LastName>McCutcheon</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Allan L McCutcheon</PI_FULL_NAME>
<EmailAddress>amccutcheon1@unl.edu</EmailAddress>
<PI_PHON>4024727758</PI_PHON>
<NSF_ID>000393098</NSF_ID>
<StartDate>06/18/2015</StartDate>
<EndDate>06/30/2016</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Belli</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert F Belli</PI_FULL_NAME>
<EmailAddress>bbelli2@unl.edu</EmailAddress>
<PI_PHON>4024585583</PI_PHON>
<NSF_ID>000160226</NSF_ID>
<StartDate>07/28/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Leen-Kiat</FirstName>
<LastName>Soh</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Leen-Kiat Soh</PI_FULL_NAME>
<EmailAddress>lksoh@cse.unl.edu</EmailAddress>
<PI_PHON>4024726738</PI_PHON>
<NSF_ID>000409977</NSF_ID>
<StartDate>07/28/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kristen</FirstName>
<LastName>Olson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kristen Olson</PI_FULL_NAME>
<EmailAddress>kolson5@unl.edu</EmailAddress>
<PI_PHON>4024726057</PI_PHON>
<NSF_ID>000614500</NSF_ID>
<StartDate>09/22/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kristen</FirstName>
<LastName>Olson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kristen Olson</PI_FULL_NAME>
<EmailAddress>kolson5@unl.edu</EmailAddress>
<PI_PHON>4024726057</PI_PHON>
<NSF_ID>000614500</NSF_ID>
<StartDate>07/28/2011</StartDate>
<EndDate>06/08/2015</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jolene</FirstName>
<LastName>Smyth</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jolene Smyth</PI_FULL_NAME>
<EmailAddress>jsmyth2@unl.edu</EmailAddress>
<PI_PHON>4024720662</PI_PHON>
<NSF_ID>000625806</NSF_ID>
<StartDate>07/28/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Nebraska-Lincoln</Name>
<CityName>Lincoln</CityName>
<ZipCode>685031435</ZipCode>
<PhoneNumber>4024723171</PhoneNumber>
<StreetAddress>151 Prem S. Paul Research Center</StreetAddress>
<StreetAddress2><![CDATA[2200 Vine St]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<StateCode>NE</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NE01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555456995</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOARD OF REGENTS OF THE UNIVERSITY OF NEBRASKA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068662618</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Nebraska-Lincoln]]></Name>
<CityName>Lincoln</CityName>
<StateCode>NE</StateCode>
<ZipCode>685880430</ZipCode>
<StreetAddress><![CDATA[312 North 14th Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NE01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>J449</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>P357</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~2929174</FUND_OBLG>
<FUND_OBLG>2016~517178</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of Nebraska NSF-Census Research Network (that is, the Nebraska node) research was to improve understanding of and methods for collecting high quality survey data from internet and computer-assisted interviewer-administered surveys. Such insights can be used by a wide range of disciplines that rely on survey data (e.g., sociology, psychology, economics, health research, political science, etc.) and a wide range of survey researchers, including academic, government, and private survey researchers and methodologists. Findings from this project can help improve the quality of data used to inform important policy decisions.</p> <p>Nebraska-node researchers studied survey responses, audio recordings of survey interviews (i.e., what interviewers and respondents say and do in a survey interview), and computer-generated information about the process respondents used to answer surveys including &nbsp;keyboard keystrokes, mouse clicks, and timing data to better understand the role questions, interviewers, and respondents play in survey data quality. They found that the design of survey questions and interviewer and respondent verbal interactions consistently play a greater role in predicting survey data quality than demographic characteristics of interviewers or respondents. For example, the majority of the differences among respondents in the time taken to answer survey questions was related to survey question characteristics such as question sensitivity, question reading level, and type of question, not to differences among interviewers or respondents. In addition, Nebraska-node researchers examined how interview behaviors differ between landline and cell phone interviews, the relationship between question characteristics and interviewer question reading and respondents' answers, and the effects of interviewer misreadings on respondent behaviors. They also provided and published new empirical insights into specific types of survey questions and question features, including agree/disagree response scales, alignment between question stems and response options, question emphasis, and groups of questions that form a battery.</p> <p>Recalling information about past behaviors and experiences in a survey is a challenging task. Nebraska-node researchers examined the strategies that interviewers and respondents use in a survey that asks respondents about past behaviors and experiences. This examination focused on reducing the rate of missing survey responses due to gaps in a respondent's memory. Interviewers who emphasized remembering aspects of events such as who participated in notable events and where they occurred allowed respondents to draw on more easily remembered parts of autobiographical memory. These notable event cues then allowed respondents to more easily remember these difficult-to-remember events, filling in the missing responses. These findings have a direct application to U.S. Census Bureau-administered surveys such as the Survey of Income and Program Participation (SIPP), the American Time Use Survey (ATUS), and any other survey asking about past behaviors. &nbsp;Nebraska-node researchers also used machine learning techniques such as Markov chains and recurrent neural networks to predict survey outcomes such as breakoffs in Web surveys. These machine learning models effectively identified when and where survey researchers could intervene automatically during a survey to prevent respondents from quitting.</p> <p>Work from the Nebraska node has been used to inform the design of computer-assisted telephone instruments. In particular, this work identifies methods to make recommendations to the interviewer during the course of the interview about what questions should be asked. It will also inform the development of artificial intelligent agents that will monitor interview progress and make recommendations to the interviewer to help streamline data entry, improve the effectiveness and efficiency of interviewer-software interactions, and predict and try to avoid respondent breakoffs in web surveys. In order to build more flexible surveys tailored to the needs of respondents, the Nebraska node demonstrated the benefits of using real-time tracking of keystrokes and other paradata relevant to interviewer-respondent interactions. &nbsp; &nbsp;</p> <p>The Nebraska node also collaborated with Census Bureau staff. First, Nebraska-node researchers identified ways to revise the American Time Use Survey user interface and interviewer training procedures to help interviewers more effectively collect data. Second, Nebraska-node researchers presented findings to the Census Bureau's Center for Survey Measurement on detecting measurement error through the analysis of computer-generated information. Through its involvement and training of undergraduates, master's degree, and Ph.D. students as well as four postdoctoral associates from sociology, computer science, psychology, and survey methodology, the Nebraska node also contributed more broadly to developing the next generation of survey methodologists and data scientists. Students and postdocs learned the fundamentals of research and contributed to the science in these fields. Finally, much of the data produced by the Nebraska node was time-consuming and expensive to produce because of the richness of information and level of detail. These data include experimental treatments, interviewer and respondent behavior codes, computer-generated paradata, question characteristics, and interviewer characteristics.&nbsp; This rare data has been placed in a data archive so that other researchers and the scientific field can continue to benefit from this work.</p><br> <p>            Last Modified: 03/20/2019<br>      Modified by: Kristen&nbsp;Olson</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of Nebraska NSF-Census Research Network (that is, the Nebraska node) research was to improve understanding of and methods for collecting high quality survey data from internet and computer-assisted interviewer-administered surveys. Such insights can be used by a wide range of disciplines that rely on survey data (e.g., sociology, psychology, economics, health research, political science, etc.) and a wide range of survey researchers, including academic, government, and private survey researchers and methodologists. Findings from this project can help improve the quality of data used to inform important policy decisions.  Nebraska-node researchers studied survey responses, audio recordings of survey interviews (i.e., what interviewers and respondents say and do in a survey interview), and computer-generated information about the process respondents used to answer surveys including  keyboard keystrokes, mouse clicks, and timing data to better understand the role questions, interviewers, and respondents play in survey data quality. They found that the design of survey questions and interviewer and respondent verbal interactions consistently play a greater role in predicting survey data quality than demographic characteristics of interviewers or respondents. For example, the majority of the differences among respondents in the time taken to answer survey questions was related to survey question characteristics such as question sensitivity, question reading level, and type of question, not to differences among interviewers or respondents. In addition, Nebraska-node researchers examined how interview behaviors differ between landline and cell phone interviews, the relationship between question characteristics and interviewer question reading and respondents' answers, and the effects of interviewer misreadings on respondent behaviors. They also provided and published new empirical insights into specific types of survey questions and question features, including agree/disagree response scales, alignment between question stems and response options, question emphasis, and groups of questions that form a battery.  Recalling information about past behaviors and experiences in a survey is a challenging task. Nebraska-node researchers examined the strategies that interviewers and respondents use in a survey that asks respondents about past behaviors and experiences. This examination focused on reducing the rate of missing survey responses due to gaps in a respondent's memory. Interviewers who emphasized remembering aspects of events such as who participated in notable events and where they occurred allowed respondents to draw on more easily remembered parts of autobiographical memory. These notable event cues then allowed respondents to more easily remember these difficult-to-remember events, filling in the missing responses. These findings have a direct application to U.S. Census Bureau-administered surveys such as the Survey of Income and Program Participation (SIPP), the American Time Use Survey (ATUS), and any other survey asking about past behaviors.  Nebraska-node researchers also used machine learning techniques such as Markov chains and recurrent neural networks to predict survey outcomes such as breakoffs in Web surveys. These machine learning models effectively identified when and where survey researchers could intervene automatically during a survey to prevent respondents from quitting.  Work from the Nebraska node has been used to inform the design of computer-assisted telephone instruments. In particular, this work identifies methods to make recommendations to the interviewer during the course of the interview about what questions should be asked. It will also inform the development of artificial intelligent agents that will monitor interview progress and make recommendations to the interviewer to help streamline data entry, improve the effectiveness and efficiency of interviewer-software interactions, and predict and try to avoid respondent breakoffs in web surveys. In order to build more flexible surveys tailored to the needs of respondents, the Nebraska node demonstrated the benefits of using real-time tracking of keystrokes and other paradata relevant to interviewer-respondent interactions.      The Nebraska node also collaborated with Census Bureau staff. First, Nebraska-node researchers identified ways to revise the American Time Use Survey user interface and interviewer training procedures to help interviewers more effectively collect data. Second, Nebraska-node researchers presented findings to the Census Bureau's Center for Survey Measurement on detecting measurement error through the analysis of computer-generated information. Through its involvement and training of undergraduates, master's degree, and Ph.D. students as well as four postdoctoral associates from sociology, computer science, psychology, and survey methodology, the Nebraska node also contributed more broadly to developing the next generation of survey methodologists and data scientists. Students and postdocs learned the fundamentals of research and contributed to the science in these fields. Finally, much of the data produced by the Nebraska node was time-consuming and expensive to produce because of the richness of information and level of detail. These data include experimental treatments, interviewer and respondent behavior codes, computer-generated paradata, question characteristics, and interviewer characteristics.  This rare data has been placed in a data archive so that other researchers and the scientific field can continue to benefit from this work.       Last Modified: 03/20/2019       Submitted by: Kristen Olson]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
