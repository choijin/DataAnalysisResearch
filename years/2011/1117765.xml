<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Small: Collaborative Research: Algorithms and Information-Theoretic Limits for Data-Limited Inference</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>236361.00</AwardTotalIntnAmount>
<AwardAmount>236361</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Cozzens</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>It is an irony of our time that despite living in the 'information age' we are often data-limited. After decades of research, scientists still debate the causes and effects of climate change, and recent work has shown that a significant fraction of the most influential medical studies over the past 13 years have been subsequently found to be inaccurate, largely due to insufficient data. One reason for this apparent paradox is that modeling complex, real-world information sources requires rich probabilistic models that cannot be accurately learned even from very large data sets. On a deeper level, research inherently resides at the edge of the possible, and seeks to address questions that available data can only partially answer. It is therefore reasonable to expect that we will always be data-limited.&lt;br/&gt;&lt;br/&gt;This research involves developing new algorithms and performance bounds for data-limited inference. Prior work of the PIs has shown that, by taking an information-theoretic approach, one can develop new algorithms that are tailored specifically to the data-limited regime and perform better than was previously known, and in some cases are provably optimal. This project advances the goal of developing a general theory for data-limited inference by considering a suite of problems spanning multiple application areas, such as classification; determining whether two data sets were generated by the same distribution or by different distributions; distribution estimation from event timings; entropy estimation; and communication over complex and unknown channels. Whereas these problems have all been studied before in isolation, prior work of the PIs has shown it is fruitful to view them as instances of the same underlying problem: data-limited inference.</AbstractNarration>
<MinAmdLetterDate>08/09/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/09/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1117765</AwardID>
<Investigator>
<FirstName>Alon</FirstName>
<LastName>Orlitsky</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alon Orlitsky</PI_FULL_NAME>
<EmailAddress>alon@ucsd.edu</EmailAddress>
<PI_PHON>8588220228</PI_PHON>
<NSF_ID>000445517</NSF_ID>
<StartDate>08/09/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[Office of Contract &amp; Grant A]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7935</Code>
<Text>COMM &amp; INFORMATION THEORY</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~236361</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Everyone knows how to estimate the bias of a coin. Just flip it a few times and observe the fraction of times it turns up heads. But most modern technological endeavors involve random phenomena with many more than two possible outcomes. For example, in natural-language processing applications such as speech recognition or machine translation, there are about a million possible English words, and in genomic analysis, there are billions of possible sequences.&nbsp;</p> <p><br />This project concerned learning distributions over such large alphabets. The most established way of quantifying how well we learn the distribution is compression. If we can learn a distribution well then we can compress its possible outcomes, and vice versa. The least number of bits required to describe the outcomes of a distribution is the distribution's entropy. The increase in compression rate over th entropy, or the quality of learning the distribution, is called the redundancy.&nbsp;</p> <p><br />We therefore studied the redundancy of learning distributions over large alphabets. It is easy to see that the difficulty of learning a distribution, namely the redundancy, increases with the alphabet size. We took two approaches to address this issue.</p> <p><br />First, we considered more structured distributions. We looked for classes of distribution for which we can find compression algorithms whose redundancy grows slowly with the alphabet size. Second, we studied learning aspects of distributions that can be learned for any distribution.</p> <p><br />We considered the natural classes of monotone distributions where the probability of the i'th element decreases monotonically with i, m-modal distributions where the sequence of probabilities has at most m local maxima, and envelope distributions, these are distributions where the probability of any element can be bounded above by some known envelope.&nbsp;</p> <p><br />For monotone and m-modal distributions, we showed that the per-symbol redundancy decreases to 0 as the sequence length grows so long as the alphabet size grows sub-exponentially in n/log n. For envelope classes we improved on existing results and showed for example that for the ubiquitous power-law envelopes where the probabilities decrease as n^(-a) for a&gt;1, the per-symbol redundancy is at most n^(1/a - 1), hence decreases to zero as n increases, regardless of the alphabet size.</p> <p><br />For the second type of problems we considered learning the pattern of a distribution, namely the order in which the elements appear, and improved on previous results to show that the pattern has per-symbol redundancy of n^(-2/3), again decreasing to zero regardless of the alphabet size.</p> <p><br />These results show that with some minor assumptions, or small decrease in the information addressed, even distributions over large alphabets can be learned efficiently. Learning such distributions is the cornerstone to many important applications in a wide range of scientific, engineering, and commercial endeavors.</p> <p>&nbsp;</p><br> <p>            Last Modified: 03/18/2016<br>      Modified by: Alon&nbsp;Orlitsky</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Everyone knows how to estimate the bias of a coin. Just flip it a few times and observe the fraction of times it turns up heads. But most modern technological endeavors involve random phenomena with many more than two possible outcomes. For example, in natural-language processing applications such as speech recognition or machine translation, there are about a million possible English words, and in genomic analysis, there are billions of possible sequences.    This project concerned learning distributions over such large alphabets. The most established way of quantifying how well we learn the distribution is compression. If we can learn a distribution well then we can compress its possible outcomes, and vice versa. The least number of bits required to describe the outcomes of a distribution is the distribution's entropy. The increase in compression rate over th entropy, or the quality of learning the distribution, is called the redundancy.    We therefore studied the redundancy of learning distributions over large alphabets. It is easy to see that the difficulty of learning a distribution, namely the redundancy, increases with the alphabet size. We took two approaches to address this issue.   First, we considered more structured distributions. We looked for classes of distribution for which we can find compression algorithms whose redundancy grows slowly with the alphabet size. Second, we studied learning aspects of distributions that can be learned for any distribution.   We considered the natural classes of monotone distributions where the probability of the i'th element decreases monotonically with i, m-modal distributions where the sequence of probabilities has at most m local maxima, and envelope distributions, these are distributions where the probability of any element can be bounded above by some known envelope.    For monotone and m-modal distributions, we showed that the per-symbol redundancy decreases to 0 as the sequence length grows so long as the alphabet size grows sub-exponentially in n/log n. For envelope classes we improved on existing results and showed for example that for the ubiquitous power-law envelopes where the probabilities decrease as n^(-a) for a&gt;1, the per-symbol redundancy is at most n^(1/a - 1), hence decreases to zero as n increases, regardless of the alphabet size.   For the second type of problems we considered learning the pattern of a distribution, namely the order in which the elements appear, and improved on previous results to show that the pattern has per-symbol redundancy of n^(-2/3), again decreasing to zero regardless of the alphabet size.   These results show that with some minor assumptions, or small decrease in the information addressed, even distributions over large alphabets can be learned efficiently. Learning such distributions is the cornerstone to many important applications in a wide range of scientific, engineering, and commercial endeavors.          Last Modified: 03/18/2016       Submitted by: Alon Orlitsky]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
