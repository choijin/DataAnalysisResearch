<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Embodied Attention: Attentional Guidance by Body Position</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2012</AwardEffectiveDate>
<AwardExpirationDate>02/29/2016</AwardExpirationDate>
<AwardTotalIntnAmount>452555.00</AwardTotalIntnAmount>
<AwardAmount>479315</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>catherine arrington</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Finding the ketchup in the refrigerator is an everyday activity that requires the use of visual attention.  When faced with a cluttered visual environment (the refrigerator), the human visual system can isolate a single, behaviorally relevant item (the ketchup).  Visual attention reduces the complexity of the visual world, enabling us to successfully navigate and function without being overwhelmed with the sheer amount of visual information.  But how does attention know what to select?  Some theoretical accounts propose that attention is directed toward salient (conspicuous) objects, whereas other accounts state that attention is directed toward goal-relevant objects.  One limitation of both of these accounts is that they fail to acknowledge that attention is typically used in conjunction with action, which involves coordinating your body with objects in the environment.  The goal of this research project is to understand how the body and its position (such as an outstretched arm reaching into the refrigerator) controls visual attention.  The research team will investigate this notion of 'embodied attention' using both behavioral and brain imaging (event-related potential) methods.&lt;br/&gt;&lt;br/&gt;Research on embodied attention is important in providing a fuller account of how the mind and body interact to function successfully in the world. The research also offers the possibility to improve the lives of older adults and those with neuropsychological deficits, because these people often have impairments in both motor function and the use of attention. It may be that understanding the connection between body position and attention will lead to treatments that increase the ability of these target populations to function in the world.</AbstractNarration>
<MinAmdLetterDate>02/13/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/25/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1151209</AwardID>
<Investigator>
<FirstName>Catherine</FirstName>
<LastName>Reed</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Catherine L Reed</PI_FULL_NAME>
<EmailAddress>creed@du.edu</EmailAddress>
<PI_PHON>3038714622</PI_PHON>
<NSF_ID>000203631</NSF_ID>
<StartDate>02/13/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Shaun</FirstName>
<LastName>Vecera</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shaun Vecera</PI_FULL_NAME>
<EmailAddress>shaun-vecera@uiowa.edu</EmailAddress>
<PI_PHON>3193350839</PI_PHON>
<NSF_ID>000413169</NSF_ID>
<StartDate>02/13/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>J. Toby</FirstName>
<LastName>Mordkoff</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>J. Toby Mordkoff</PI_FULL_NAME>
<EmailAddress>jonathan-mordkoff@uiowa.edu</EmailAddress>
<PI_PHON>3193843387</PI_PHON>
<NSF_ID>000558265</NSF_ID>
<StartDate>02/13/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Iowa</Name>
<CityName>IOWA CITY</CityName>
<ZipCode>522421320</ZipCode>
<PhoneNumber>3193352123</PhoneNumber>
<StreetAddress>2 GILMORE HALL</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<StateCode>IA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>062761671</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF IOWA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>062761671</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Iowa]]></Name>
<CityName>Iowa City</CityName>
<StateCode>IA</StateCode>
<ZipCode>522421320</ZipCode>
<StreetAddress><![CDATA[2 Gilmore Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramElement>
<Code>9150</Code>
<Text>EPSCoR Co-Funding</Text>
</ProgramElement>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~464755</FUND_OBLG>
<FUND_OBLG>2013~14560</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div> <p>The major goal of this project was to understand how bodily position, such as the position of the hand, influences the direction of visual selective attention. Human behavior regularly involves the coordination of different sensory systems, such as vision, touch, and body position (proprioception), as evidenced in acts as regular as using a mouse to navigate to a computer file. Relatively little is known about how visual attention is guided by body position, an interaction critical to the coordination of vision and proprioception.</p> <p>&nbsp;</p> <p>Previous research developed sound methods for measuring body-directed attention, that is, how the position of the hand directs visual attention. When an observer places her hand adjacent to a computer monitor, the observer identifies targets near the hand faster than those far from the hand. We sought to understand the cognitive and neural mechanisms of body-directed attention, which have allowed us to begin linking behavior and human neural processes. Our research advanced the basic research on body-guided attention by clarifying the exact role of the hand and its influence on attention. In particular, we asked if the impact of hand position on visual processing occurred automatically or in a strategic and task-dependent fashion. By asking observers to respond to a target at one location (the center of a display) while their hand was placed at another location (in the periphery), we found that visual stimuli near the hand affected central target identification, indicating that hand position has an automatic influence on visual attention. This result, however, depended on the nature of the stimuli near the hand: These stimuli needed to be useful for central target identification. If the non-target stimuli were not helpful in performing central target identification, non-targets, including those near the hand, could be effectively ignored. These findings indicate a rapid, automatic coordination between visual attention and body position when the two work in a coordinated (that is, congruent) manner.</p> <p>&nbsp;</p> <p>Beyond the intellectual merit of our findings, our project had many broader impacts. Most important, our research advanced scientific discovery and increase our understanding of visual attention through student training and mentoring. We supported a postdoctoral research associate, a graduate student research assistant, and eight undergraduate research assistants. Four of the undergraduate research assistants were funded by supplemental Research Experiences for Undergraduate (REU) funds. Our trainees have secured positions as postdoctoral research associates at Columbia University and Vanderbilt University, and another member of our research team trained in data science at Galvanize Inc. and is now a data scientist at Microsoft. Our trainees included members of underrepresented groups, providing research experience to undergraduate students who otherwise might not have had an opportunity to experience research directly.</p> </div><br> <p>            Last Modified: 04/04/2016<br>      Modified by: Shaun&nbsp;P&nbsp;Vecera</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The major goal of this project was to understand how bodily position, such as the position of the hand, influences the direction of visual selective attention. Human behavior regularly involves the coordination of different sensory systems, such as vision, touch, and body position (proprioception), as evidenced in acts as regular as using a mouse to navigate to a computer file. Relatively little is known about how visual attention is guided by body position, an interaction critical to the coordination of vision and proprioception.     Previous research developed sound methods for measuring body-directed attention, that is, how the position of the hand directs visual attention. When an observer places her hand adjacent to a computer monitor, the observer identifies targets near the hand faster than those far from the hand. We sought to understand the cognitive and neural mechanisms of body-directed attention, which have allowed us to begin linking behavior and human neural processes. Our research advanced the basic research on body-guided attention by clarifying the exact role of the hand and its influence on attention. In particular, we asked if the impact of hand position on visual processing occurred automatically or in a strategic and task-dependent fashion. By asking observers to respond to a target at one location (the center of a display) while their hand was placed at another location (in the periphery), we found that visual stimuli near the hand affected central target identification, indicating that hand position has an automatic influence on visual attention. This result, however, depended on the nature of the stimuli near the hand: These stimuli needed to be useful for central target identification. If the non-target stimuli were not helpful in performing central target identification, non-targets, including those near the hand, could be effectively ignored. These findings indicate a rapid, automatic coordination between visual attention and body position when the two work in a coordinated (that is, congruent) manner.     Beyond the intellectual merit of our findings, our project had many broader impacts. Most important, our research advanced scientific discovery and increase our understanding of visual attention through student training and mentoring. We supported a postdoctoral research associate, a graduate student research assistant, and eight undergraduate research assistants. Four of the undergraduate research assistants were funded by supplemental Research Experiences for Undergraduate (REU) funds. Our trainees have secured positions as postdoctoral research associates at Columbia University and Vanderbilt University, and another member of our research team trained in data science at Galvanize Inc. and is now a data scientist at Microsoft. Our trainees included members of underrepresented groups, providing research experience to undergraduate students who otherwise might not have had an opportunity to experience research directly.        Last Modified: 04/04/2016       Submitted by: Shaun P Vecera]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
