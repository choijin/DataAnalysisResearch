<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SDCI Net: Collaborative Research: An integrated study of datacenter networking and 100 GigE wide-area networking in support of distributed scientific computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>498378.00</AwardTotalIntnAmount>
<AwardAmount>498378</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As supercomputing speeds increase to peta- and exaflops, scientists are increasing their scale and range of simulations, which are resulting in ever growing datasets that need to be moved to local computers at the scientists' own laboratories. The first goal of this project is to identify bottlenecks that result in poor and/or inconsistent end-to-end application-level throughput using data collection and analysis by working in conjunction with scientists in the Community Earth System Model (CESM) project. With knowledge of the weakest components in the end-to-end chain, we plan to experiment in a controlled environment using a testbed that consists of a high-end cluster at NERSC, which is capable of sourcing/sinking data to disks at close to 100Gbps speeds, and other high-performance computing systems connected via the DOE 100Gbps Advanced Networking Initiative (ANI) prototype network. Multiple datacenter networking technologies such as Remote Direct Memory Access (RDMA) over Converged Ethernet (RoCE) and Internet Wide-Area RDMA Protocol (iWARP) will be combined with high- speed (100Gb/s) wide-area networking solutions, such as dedicated virtual circuits and IP-routed paths, respectively, for a comparative performance study of file transfers and wide-area MPI I/O. A new software module of the Extended-Sockets API (ES-API), which offers RDMA features such as zero-copy operations, will be prototyped and integrated into file transfer applications. Finally, trials will be organized to transfer the best identified solutions to CESM and other scientists. The intellectual merit of the proposed project consists of: i) a systematic scientific approach to determine the reasons for poor end-to-end application-level performance experienced by CESM scientists, ii) development of integrated datacenter and wide-area networking solutions to address the identified problems, and iii) the enabling of these solutions to be utilized by CESM and other science projects. The broader impacts of the proposed activities consist of i) the creation of a course module on datacenter networking, and the involvement of undergraduate students in this research at all three institutions, ii) diversity and outreach programs, and iii) the active promotion of the developed solutions to the CESM project and other scientists.</AbstractNarration>
<MinAmdLetterDate>09/07/2011</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1127340</AwardID>
<Investigator>
<FirstName>Malathi</FirstName>
<LastName>Veeraraghavan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Malathi Veeraraghavan</PI_FULL_NAME>
<EmailAddress>mv5g@virginia.edu</EmailAddress>
<PI_PHON>4349822208</PI_PHON>
<NSF_ID>000254309</NSF_ID>
<StartDate>09/07/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Virginia Main Campus</Name>
<CityName>CHARLOTTESVILLE</CityName>
<ZipCode>229044195</ZipCode>
<PhoneNumber>4349244270</PhoneNumber>
<StreetAddress>P.O.  BOX 400195</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>065391526</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RECTOR &amp; VISITORS OF THE UNIVERSITY OF VIRGINIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>065391526</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Virginia Main Campus]]></Name>
<CityName>Charlottesville</CityName>
<StateCode>VA</StateCode>
<ZipCode>229044743</ZipCode>
<StreetAddress><![CDATA[POB 400743, Thornton Hall C222]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7683</Code>
<Text>SOFTWARE DEVELOPEMENT FOR CI</Text>
</ProgramElement>
<ProgramReference>
<Code>7683</Code>
<Text>SOFTWARE DEVELOPEMENT FOR CI</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~498378</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Scientific computing in fields such as Climate Science, Astronomy, Physics, Molecular Biology, Genomics, requires high-performance computing and high-speed networking. Scientists residing at various universities and research laboratories use super-computers at NSF and DOE funded centers such as Pittsburgh Supercomputing Center, and Argonne Leadership Computing Facility near Chicago. After their complex, parallelized software applications complete execution on these high-performance computers, scientists need to download results files back to their university storage servers. Such transfers require high-speed networking solutions to move the data across wide-area network paths that typically traverse networks that are owned and operated by different organizations. Our high-speed networks research group at the University of Virginia pioneered solutions for moving such datasets fast so that scientists can focus on their research projects rather than on having to deal with data-movement issues. We collaborated with the Globus organization, NERSC and SLAC (two DOE supercomputing sites), to obtain GridFTP logs, which offer a record of file transfers executed using a widely used high-performance file transfer application called GridFTP. Through data analysis, and experiments (between NERSC and SLAC data transfer nodes) as well as controlled experiments on the NSF GENI testbed and NSF DYNES distributed testbed, we developed a comprehensive cross-layer design for fast data movement. This work resulted in a publication in the prestigious IEEE/ACM SuperComputing conference, two peer-reviewed IEEE conference papers, and one IEEE/ACM workshop papers. All these papers are accessible through IEEE Xplore database, and we expect this work to have a significant impact on other researchers involved in supporting scientists in their data-movement needs.</p> <p>In an complementary effort, we studied congestion effects within networks that are used in high-performance computing systems to interconnect large numbers of nodes. For example, the Yellowstone high-performance computing system maintained by the National Center for Atmospheric Research (NCAR) Wyoming Supercomputing Center (NWSC) uses a network technology called InfiniBand. This network uses a link-by-link flow control mechanism to offer applications lossless data communications. An unexpected consequence of this flow-control mechanism is that it can cause flows whose paths do not traverse a congested port to become &ldquo;victims,&rdquo; i.e., suffer a lowered throughput or experience increased packet delays. We performed experimental work on a University of New Hampshire IB testbed, and a data collection and analysis study on Yellowstone. Together, these studies showed that congestion does occur on highly utilized clusters such as Yellowstone, and that a dynamic congestion management system is feasible to counter the effects of congestion. If congestion effects can be reduced, communication delays for large-core count applications such as Climate science models can be reduced, leading to faster scientific discovery.</p> <p>This project also supported diversity enhancing activities through UVA's Center for Diversity in Engineering, which is a school-wide (School of Engineering and Applied Sciences) center. These activities include an National Society of Black Engineers (NSBE) Walk for Education, and a Graduate Lab meeting at the Univ. of Maryland. This project offered an opportunity for an African-American student, Fabrice Mizero, and a female graduate student, Fatma Al-Ali, to learn how to conduct research, publish papers, and contribute to the scientific community. Fatma Al-Ali completed her Masters of Science in Computer Engineering and Fabrice Mizero completed his Masters of Engineering in Computer Engineering in 2016. Al-Ali is continuing her studies toward a PhD degree.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/26/2016<br>      Modified by: Malathi&nbsp;Veeraraghavan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Scientific computing in fields such as Climate Science, Astronomy, Physics, Molecular Biology, Genomics, requires high-performance computing and high-speed networking. Scientists residing at various universities and research laboratories use super-computers at NSF and DOE funded centers such as Pittsburgh Supercomputing Center, and Argonne Leadership Computing Facility near Chicago. After their complex, parallelized software applications complete execution on these high-performance computers, scientists need to download results files back to their university storage servers. Such transfers require high-speed networking solutions to move the data across wide-area network paths that typically traverse networks that are owned and operated by different organizations. Our high-speed networks research group at the University of Virginia pioneered solutions for moving such datasets fast so that scientists can focus on their research projects rather than on having to deal with data-movement issues. We collaborated with the Globus organization, NERSC and SLAC (two DOE supercomputing sites), to obtain GridFTP logs, which offer a record of file transfers executed using a widely used high-performance file transfer application called GridFTP. Through data analysis, and experiments (between NERSC and SLAC data transfer nodes) as well as controlled experiments on the NSF GENI testbed and NSF DYNES distributed testbed, we developed a comprehensive cross-layer design for fast data movement. This work resulted in a publication in the prestigious IEEE/ACM SuperComputing conference, two peer-reviewed IEEE conference papers, and one IEEE/ACM workshop papers. All these papers are accessible through IEEE Xplore database, and we expect this work to have a significant impact on other researchers involved in supporting scientists in their data-movement needs.  In an complementary effort, we studied congestion effects within networks that are used in high-performance computing systems to interconnect large numbers of nodes. For example, the Yellowstone high-performance computing system maintained by the National Center for Atmospheric Research (NCAR) Wyoming Supercomputing Center (NWSC) uses a network technology called InfiniBand. This network uses a link-by-link flow control mechanism to offer applications lossless data communications. An unexpected consequence of this flow-control mechanism is that it can cause flows whose paths do not traverse a congested port to become "victims," i.e., suffer a lowered throughput or experience increased packet delays. We performed experimental work on a University of New Hampshire IB testbed, and a data collection and analysis study on Yellowstone. Together, these studies showed that congestion does occur on highly utilized clusters such as Yellowstone, and that a dynamic congestion management system is feasible to counter the effects of congestion. If congestion effects can be reduced, communication delays for large-core count applications such as Climate science models can be reduced, leading to faster scientific discovery.  This project also supported diversity enhancing activities through UVA's Center for Diversity in Engineering, which is a school-wide (School of Engineering and Applied Sciences) center. These activities include an National Society of Black Engineers (NSBE) Walk for Education, and a Graduate Lab meeting at the Univ. of Maryland. This project offered an opportunity for an African-American student, Fabrice Mizero, and a female graduate student, Fatma Al-Ali, to learn how to conduct research, publish papers, and contribute to the scientific community. Fatma Al-Ali completed her Masters of Science in Computer Engineering and Fabrice Mizero completed his Masters of Engineering in Computer Engineering in 2016. Al-Ali is continuing her studies toward a PhD degree.          Last Modified: 11/26/2016       Submitted by: Malathi Veeraraghavan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>