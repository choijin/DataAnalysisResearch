<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Harmonic Analysis, Geometric Measure Theory and Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2011</AwardEffectiveDate>
<AwardExpirationDate>05/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>135000.00</AwardTotalIntnAmount>
<AwardAmount>135000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Bruce P. Palka</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Mathematically speaking, the project deals with Geometric Measure Theory (GMT) and Harmonic Analysis.  More specifically, we study questions on the interface of both these fields, and use harmonic analysis techniques to study questions in GMT. We study problems of the form "Find a biLipschitz map from a significant part of a given metric space to Euclidean space", "Characterize biLipschitz images of the Euclidean plane" or "When is a metric space built from biLipschitz images of standard pieces and how do we find these pieces?"  On the harmonic analysis side, this is related to questions of the form "When is a function decomposable into a sum of nice functions, and how do you construct this decomposition?" This last question is a standard one in Littlewood-Paley and wavelet analysis, and transferring the methods from these rich theories into the setting of GMT yields a significant toolbox.  This type of study of GMT is very related to many questions that arise in applications. In many applications one is given a large data set represented as a subset of a metric space, such as a high dimensional Euclidean space, and one seeks to faithfully represent a large portion of this data set as a subset of a low dimensional Euclidean space. Faithfully here, means that one can still perform the same data mining tasks on the image of the data portion. It is because of this connection to data mining that the above task has thus far yielded much attention from computer scientists and applied mathematicians using a wide range of approaches.  The framework of dimensionality reduction also includes data compression and data approximation. These have applications in many areas of science; for examples document analysis, face recognition, clustering, machine learning nonlinear image denoising, segmentation and processing.&lt;br/&gt;&lt;br/&gt;The project is geared towards a better understanding of the geometry of collections of points in a given space.  In many applications one is given data which we think of as points in a metric space, and we want to map them to a low dimensional space which we understand better (such as a low dimensional Euclidean space). This data-mining task is typically called dimensionality reduction, and this framework includes data compression and data approximation. These have applications in many areas of science; for examples document analysis, face recognition, clustering, machine learning nonlinear image denoising, segmentation and processing.  A key point is that quite often the data enjoys nice geometric properties and has more structure then a random set of points would have.  One can study these geometric structures and use them to perform data mining tasks. There is a rich mathematical theory behind the study of such geometric structures, and this is what we develop.</AbstractNarration>
<MinAmdLetterDate>04/01/2011</MinAmdLetterDate>
<MaxAmdLetterDate>04/01/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1100008</AwardID>
<Investigator>
<FirstName>Raanan</FirstName>
<LastName>Schul</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Raanan Schul</PI_FULL_NAME>
<EmailAddress>schul@math.sunysb.edu</EmailAddress>
<PI_PHON>6316328265</PI_PHON>
<NSF_ID>000007561</NSF_ID>
<StartDate>04/01/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Stony Brook</Name>
<CityName>Stony Brook</CityName>
<ZipCode>117940001</ZipCode>
<PhoneNumber>6316329949</PhoneNumber>
<StreetAddress>WEST 5510 FRK MEL LIB</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804878247</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Stony Brook]]></Name>
<CityName>Stony Brook</CityName>
<StateCode>NY</StateCode>
<ZipCode>117940001</ZipCode>
<StreetAddress><![CDATA[WEST 5510 FRK MEL LIB]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1281</Code>
<Text>ANALYSIS PROGRAM</Text>
</ProgramElement>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~135000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1">The mathematics&nbsp; in this project was geared towards a better understanding of the geometry of collections of points in a given space. In many applications one is given data which we think of as points in a metric space, and we want to map them to a low dimensional space which we understand better (such as a low dimensional Euclidean space). This data-mining task is typically called dimensionality reduction, and this framework includes data compression and data approximation. These have applications in many areas of science; for examples document analysis, face recognition, clustering, machine learning, nonlinear image denoising, segmentation and processing. It is because of the connection to data mining that the above task has thus far yielded much attention from computer scientists and applied mathematicians using a wide range of approaches. The framework of dimensionality reduction also includes data compression and data approximation.</p> <p class="p2">As a concrete example, consider the collection of all images which are 256 by 256 pixels. Of those, throw away all images which are not of a face of a person. Each image can be thought of as having 256 times 256 = 65,536 greyscale pixels, and so is a point in a 65,536 dimensional space. On the other hand, you certainly do not need 65,536 parameters to describe a face; the distance between the eyes, nose, ears and mouth, basic color scheme, and general shapes are probably enough to generate a decent sketch of the person, so there is a much lower dimensional (say, 20) representation of this collection of images using these salient statistics. Many of these pictures will be of the same person, but from various angles. Natural questions are: describe the geometric properties of the collection of points that represent (a) the collection of all faces in the 65,536 dimensional space, (b) the collection of all the head shots of a single person or (c) a carefully chosen collection which has one face for each person. Knowing how to do this well helps with tasks like facial recognition, or data compression. Understanding the mapping from the high dimensional (65,536) to the low dimensional space corresponds to the question what are the salient statistics we should keep if we want to be able to reconstruct someones face in a recognizable way?</p> <p class="p2">Going back to the general data mining framework: a key point is that quite often the data enjoys nice geometric properties and has more structure then a random set of points would have. One can study these geometric structures and use them to perform data mining tasks. There is a rich mathematical theory behind the study of such geometric structures, and this is what we worked on. A basic mathematical task is to try and faithfully represent a large portion of a data set as a subset of a low dimensional Euclidean space. Faithfully here, means that one can still perform the same data mining tasks on the image of the data portion. There is a lot of interesting interplay with what exact meaning can be put into the words faithfully and large.</p> <p class="p1">&nbsp;</p> <p class="p3">For the mathematically inclined: The proposed work dealt with Geometric Measure Theory (GMT) and Harmonic Analysis. More specifically, we study questions on the interface of both these fields, and use harmonic analysis techniques to study questions in GMT. We studied problems of the form &ldquo;Find a bi-Lipschitz map from a significant part of a given metric measure space to a Euclidean space&rdquo;, &ldquo;Characterize bi-Lipschitz images of the Euclidean plane&rdquo; , &ldquo;When is a metric space built from bi-Lipschitz images of standard pieces and how do we find these pieces?&rdquo; or &ldquo;Describe the geometric properties of a large portion of the fibers of a Lipschitz map&rdquo;. On the harmonic analysis side, this is related to questions of the form &ldquo;When is a...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[The mathematics  in this project was geared towards a better understanding of the geometry of collections of points in a given space. In many applications one is given data which we think of as points in a metric space, and we want to map them to a low dimensional space which we understand better (such as a low dimensional Euclidean space). This data-mining task is typically called dimensionality reduction, and this framework includes data compression and data approximation. These have applications in many areas of science; for examples document analysis, face recognition, clustering, machine learning, nonlinear image denoising, segmentation and processing. It is because of the connection to data mining that the above task has thus far yielded much attention from computer scientists and applied mathematicians using a wide range of approaches. The framework of dimensionality reduction also includes data compression and data approximation. As a concrete example, consider the collection of all images which are 256 by 256 pixels. Of those, throw away all images which are not of a face of a person. Each image can be thought of as having 256 times 256 = 65,536 greyscale pixels, and so is a point in a 65,536 dimensional space. On the other hand, you certainly do not need 65,536 parameters to describe a face; the distance between the eyes, nose, ears and mouth, basic color scheme, and general shapes are probably enough to generate a decent sketch of the person, so there is a much lower dimensional (say, 20) representation of this collection of images using these salient statistics. Many of these pictures will be of the same person, but from various angles. Natural questions are: describe the geometric properties of the collection of points that represent (a) the collection of all faces in the 65,536 dimensional space, (b) the collection of all the head shots of a single person or (c) a carefully chosen collection which has one face for each person. Knowing how to do this well helps with tasks like facial recognition, or data compression. Understanding the mapping from the high dimensional (65,536) to the low dimensional space corresponds to the question what are the salient statistics we should keep if we want to be able to reconstruct someones face in a recognizable way? Going back to the general data mining framework: a key point is that quite often the data enjoys nice geometric properties and has more structure then a random set of points would have. One can study these geometric structures and use them to perform data mining tasks. There is a rich mathematical theory behind the study of such geometric structures, and this is what we worked on. A basic mathematical task is to try and faithfully represent a large portion of a data set as a subset of a low dimensional Euclidean space. Faithfully here, means that one can still perform the same data mining tasks on the image of the data portion. There is a lot of interesting interplay with what exact meaning can be put into the words faithfully and large.   For the mathematically inclined: The proposed work dealt with Geometric Measure Theory (GMT) and Harmonic Analysis. More specifically, we study questions on the interface of both these fields, and use harmonic analysis techniques to study questions in GMT. We studied problems of the form "Find a bi-Lipschitz map from a significant part of a given metric measure space to a Euclidean space", "Characterize bi-Lipschitz images of the Euclidean plane" , "When is a metric space built from bi-Lipschitz images of standard pieces and how do we find these pieces?" or "Describe the geometric properties of a large portion of the fibers of a Lipschitz map". On the harmonic analysis side, this is related to questions of the form "When is a function decomposable into a sum of nice functions, and how do you construct this decomposition?" This last question is a standard one in Littlewood-Paley and wavelet analysis, and transferring the methods fro...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
