<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2012</AwardEffectiveDate>
<AwardExpirationDate>05/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>449995.00</AwardTotalIntnAmount>
<AwardAmount>449995</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rajiv Ramnath</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The Message Passing Interface (MPI) is a very widely used parallel programming model on modern High-End Computing (HEC) systems. Many performance aspects of MPI libraries, such as latency, bandwidth, scalability, memory footprint, cache pollution, overlap of computation and communication etc. are highly dependent on system configuration and application requirements. Additionally, modern clusters are changing rapidly with the growth of multi-core processors and commodity networking technologies such as InfiniBand and 10GigE/iWARP. They are becoming diverse and heterogeneous with varying number of processor cores, processor speed, memory speed, multi-generation network adapters/switches, I/O interface technologies, and accelerators (GPGPUs), etc.  Typically, any MPI library deals with the above kind of diversity in platforms and sensitivity of applications by employing various runtime parameters. These parameters are tuned during its release, or by&lt;br/&gt;system administrators, or by end-users.  These default parameters may or may not be optimal for all system configurations and applications.&lt;br/&gt;&lt;br/&gt;The MPI library of a typical proprietary system goes through heavy performance tuning for a range of applications.  Since commodity clusters provide greater flexibility in their configurations (processor, memory and network), it is very hard to achieve optimal tuning using released version of any MPI library, with its default settings. This leads to the following broad challenge: "Can a comprehensive performance tuning framework be designed for MPI library so that the next generation InfiniBand, 10GigE/iWARP and RoCE clusters and applications will be able to extract `bare-metal' performance and maximum scalability?"  The investigators, involving computer&lt;br/&gt;scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) as well as computational scientists from the Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), University of California San Diego (UCSD), will be addressing the above challenge with innovative solutions.&lt;br/&gt;&lt;br/&gt;The investigators will specifically address the following challenges: 1) Can a set of static tools be designed to optimize performance of an MPI library during installation time?  2) Can a set of dynamic tools with low overhead be designed to optimize performance on a per-user and per-application basis during production runs?  3) How to incorporate the proposed performance tuning framework with the upcoming MPIT interface?  4) How to configure MPI libraries on a given system to deliver different optimizations to a set of driving applications?  and 5) What kind of benefits (in terms of performance, scalability, memory efficiency and reduction in cache pollution) can be achieved by the proposed tuning framework?  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on the TACC Ranger and other systems at OSC, SDSC and OSU.  The proposed designs will be integrated into the open-source MVAPICH2 library.</AbstractNarration>
<MinAmdLetterDate>06/04/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/04/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1148424</AwardID>
<Investigator>
<FirstName>Tommy</FirstName>
<LastName>Minyard</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tommy K Minyard</PI_FULL_NAME>
<EmailAddress>minyard@tacc.utexas.edu</EmailAddress>
<PI_PHON>5124759411</PI_PHON>
<NSF_ID>000371000</NSF_ID>
<StartDate>06/04/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>William</FirstName>
<LastName>Barth</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William L Barth</PI_FULL_NAME>
<EmailAddress>bbarth@tacc.utexas.edu</EmailAddress>
<PI_PHON>5124716424</PI_PHON>
<NSF_ID>000596862</NSF_ID>
<StartDate>06/04/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787137726</ZipCode>
<StreetAddress><![CDATA[PO Box 7726]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>21</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX21</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramReference>
<ProgramReference>
<Code>8009</Code>
<Text>Scientifc Software Integration</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~449995</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The largest computers today use thousands of computing units (referredto as cores) to run complex scientific simulations. Applicationsrunning on these systems predominately use the Message PassingInterface (MPI) programming model to exchange data between cores. Thecores are organized several cores per socket, and commonly two or foursockets per server, with the servers connected by a high-performancenetwork, the system's interconnect. The focus of this project isperformance tuning of the MVAPICH2 MPI library, a high performance,open-source implementation of the MPI standard that delivers the bestperformance, scalability, and fault tolerance for high-end computingsystems and servers that use one of the following interconnect networktechnologies: InfiniBand, Intel Omni-Path, 10-40GigE/iWARP orRoCE. The overarching goal of the project is to reduce the time tosolution for complex science simulations which use MVAPICH2 or otherMPI libraries on these large systems.<br /><br />Most large computer systems (clusters) are built up from commoditycomponents, but the technology is changing rapidly with frequentintroduction of new multi-core processors and networkingtechnologies. Open source MPI libraries must run well across multiplegenerations of processors and interconnects, and additionally supportaccelerators such as NVIDIA GPGPUS and Intel Xeon Phiprocessors. Typically, any MPI library deals with diverse platformsand a wide range of applications by employing various runtimeparameters. These parameters are either set during software release,by system administrators, or end-users. Considering the nature ofcommodity clusters, it is difficult to apply one set of common tuningparameters that will allow an MPI library to extract the bestperformance on all computer systems. This leads to a broad challenge:Can a comprehensive performance tuning framework be designed for MPIlibraries for modern commodity clusters?<br /><br />In this project we have addressed this challenge in three areas: 1) Developed architecture specific tuning for MPI communicationoperations between pairs of cores (point-to-point operations) orgroups of cores (collective operations).&nbsp; 2) Developed a performanceprofiling layer within MVAPICH2, including support for the MPI-3standard's MPI_T interface.&nbsp; 3) Studied application sensitivity to MPIparameters and developed applications specific tuning strategies.<br /><br />Developments were made in all three areas and evaluated with a rangeof science applications including: HoomdBlue, SMG2000, Neuron, Amber,MiniAMR, MILC, LULESH and HPCCG. These scientific applications and mathematical libraries are from biochemistry, neuroscience, high energy physics, computational fluid dynamics, and numerical linear algebra. Some highlights of these results arethat architecture-specific tuning improves collective operationlatency by up to 58%. The GPU tuning methods were able to provide a 2Ximprovement in the execution time of HoomdBlue application. Thetransport protocol-based tuning shows 27% improvement for the Neuronapplication example. With HPCG and the LULESH application kernel, thecollective tuning for partial subscription available on hybridMPI+OpenMP programming model improves the execution time by 24% at 512<br />cores.<br />The results of this research (tuned designs, performance results,benchmarks, etc.) have been made available to the community throughthe open-source MVAPICH2 library (1.9, 2.0, 2.1, and 2.2 releaseseries including alpha, beta and RC versions). The latest version iscurrently running on many large-scale XSEDE systems including TACC Stampede, SDSC Gordon and SDSC Comet. Currently,the MVAPICH2 library is being used by more than 2,650 organizations in 81 countries. The MVAPICH2 library and the enhancements are being used by a large numberof users of these systems.<br /><br />In each of these releases, information about the tuned designs forvarious components (such as point-to-point, collectives, GPU-GPUcommunication, etc.) has been shared with the MVAPICH2 user communitythrough mailing lists. The applications-based tuning results have beenmade available to the community through the "Best Practices" link ofthe MVAPICH project web page. In order to achieve direct face-to-facediscussion with MVAPICH2 users and get their feedback, in 2013 westarted holding an MVAPICH2 User Group Meeting (MUG) each year inAugust in Columbus Ohio. This meeting has been continuing successfullyfor the last four years and has helped to disseminate the results ofthis research to a wider community.&nbsp; In addition to the softwaredistribution and the MUG events, the results have been presented atvarious conferences and events through talks and tutorials. Multiple Ph.D and Masters students have performed research work and received their Ph.D and M.S. degrees as a part of this project.</p><br> <p>            Last Modified: 08/30/2016<br>      Modified by: William&nbsp;L&nbsp;Barth</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The largest computers today use thousands of computing units (referredto as cores) to run complex scientific simulations. Applicationsrunning on these systems predominately use the Message PassingInterface (MPI) programming model to exchange data between cores. Thecores are organized several cores per socket, and commonly two or foursockets per server, with the servers connected by a high-performancenetwork, the system's interconnect. The focus of this project isperformance tuning of the MVAPICH2 MPI library, a high performance,open-source implementation of the MPI standard that delivers the bestperformance, scalability, and fault tolerance for high-end computingsystems and servers that use one of the following interconnect networktechnologies: InfiniBand, Intel Omni-Path, 10-40GigE/iWARP orRoCE. The overarching goal of the project is to reduce the time tosolution for complex science simulations which use MVAPICH2 or otherMPI libraries on these large systems.  Most large computer systems (clusters) are built up from commoditycomponents, but the technology is changing rapidly with frequentintroduction of new multi-core processors and networkingtechnologies. Open source MPI libraries must run well across multiplegenerations of processors and interconnects, and additionally supportaccelerators such as NVIDIA GPGPUS and Intel Xeon Phiprocessors. Typically, any MPI library deals with diverse platformsand a wide range of applications by employing various runtimeparameters. These parameters are either set during software release,by system administrators, or end-users. Considering the nature ofcommodity clusters, it is difficult to apply one set of common tuningparameters that will allow an MPI library to extract the bestperformance on all computer systems. This leads to a broad challenge:Can a comprehensive performance tuning framework be designed for MPIlibraries for modern commodity clusters?  In this project we have addressed this challenge in three areas: 1) Developed architecture specific tuning for MPI communicationoperations between pairs of cores (point-to-point operations) orgroups of cores (collective operations).  2) Developed a performanceprofiling layer within MVAPICH2, including support for the MPI-3standard's MPI_T interface.  3) Studied application sensitivity to MPIparameters and developed applications specific tuning strategies.  Developments were made in all three areas and evaluated with a rangeof science applications including: HoomdBlue, SMG2000, Neuron, Amber,MiniAMR, MILC, LULESH and HPCCG. These scientific applications and mathematical libraries are from biochemistry, neuroscience, high energy physics, computational fluid dynamics, and numerical linear algebra. Some highlights of these results arethat architecture-specific tuning improves collective operationlatency by up to 58%. The GPU tuning methods were able to provide a 2Ximprovement in the execution time of HoomdBlue application. Thetransport protocol-based tuning shows 27% improvement for the Neuronapplication example. With HPCG and the LULESH application kernel, thecollective tuning for partial subscription available on hybridMPI+OpenMP programming model improves the execution time by 24% at 512 cores. The results of this research (tuned designs, performance results,benchmarks, etc.) have been made available to the community throughthe open-source MVAPICH2 library (1.9, 2.0, 2.1, and 2.2 releaseseries including alpha, beta and RC versions). The latest version iscurrently running on many large-scale XSEDE systems including TACC Stampede, SDSC Gordon and SDSC Comet. Currently,the MVAPICH2 library is being used by more than 2,650 organizations in 81 countries. The MVAPICH2 library and the enhancements are being used by a large numberof users of these systems.  In each of these releases, information about the tuned designs forvarious components (such as point-to-point, collectives, GPU-GPUcommunication, etc.) has been shared with the MVAPICH2 user communitythrough mailing lists. The applications-based tuning results have beenmade available to the community through the "Best Practices" link ofthe MVAPICH project web page. In order to achieve direct face-to-facediscussion with MVAPICH2 users and get their feedback, in 2013 westarted holding an MVAPICH2 User Group Meeting (MUG) each year inAugust in Columbus Ohio. This meeting has been continuing successfullyfor the last four years and has helped to disseminate the results ofthis research to a wider community.  In addition to the softwaredistribution and the MUG events, the results have been presented atvarious conferences and events through talks and tutorials. Multiple Ph.D and Masters students have performed research work and received their Ph.D and M.S. degrees as a part of this project.       Last Modified: 08/30/2016       Submitted by: William L Barth]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
