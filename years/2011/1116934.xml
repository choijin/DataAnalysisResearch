<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>TC: Small: Effective Security Warning Dialogs</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project focuses on improving the effectiveness of computer security warning dialogs ? on-screen prompts that warn users about a potential security risks and give users a choice between two or more courses of action. Security dialogs should help users avoid unsafe actions while allowing them to take safe actions by presenting information that allows users to make informed decisions that the system cannot make with user input. This research takes a novel approach to the design and rigorous evaluation of computer security warning dialogs, with the goal of developing generalizable guidelines for designing effective warning dialogs for software products. This has the potential to help end users make better security decisions that keep their information and computer systems safer, and improve the computer security ecosystem.&lt;br/&gt;Based on the Carnegie-Mellon team?s previous work, review of the literature, and discussions with  collaborators, they have developed a set of candidate features that will have a significant impact on the effectiveness of security dialogs. For example, these features include: amount and placemen of text, severity of tone, how to help users decide, describing risks and consequences, use of recommended and default options, and more. They plan to systematically study each feature, applied to a variety of security dialogs, to determine the impact of each (individually and in combination) and to develop guidelines on how to use each feature to best effect. They will follow an iterative design and evaluation approach that will involve five types of studies: exploratory interviews, Mechanical Turk studies, laboratory studies, field studies, and interface designer studies. In the Mechanical Turk studies, participants will be provided with a scenario and a security dialog triggered by that scenario and asked how they would be most likely to respond. They will also be asked follow-up questions to learn why they made that decision, their perception of the risks associated with each warning dialog, their understanding of the warning dialog, their beliefs about how well they think they understand the warning dialog, and their knowledge of the concepts and vocabulary included in each dialog. We will measure the tendency for users to take the recommended action in risky scenarios and the non-recommended action in benign scenarios. The follow-up questions will help determine why users behave the way they do and how to most effectively design security warning dialogs to influence that behavior. It is important to determine how to communicate effectively about the risks and consequences, but also to determine how much users need to be able to understand before they make appropriate decisions. It is anticipated that of some aspects of the situation will be correlated with behavior, but that there will be some information that increases understanding with little or no impact on behavior. In addition, the features are likely to have varying impacts on understanding risks and consequences, motivation to take the safe course of action, and behavior. To test the generalizability of the guidelines, a large set of security dialogs from a wide range of software products will be collected. As candidate guidelines emerge, they will apply them to a variety of dialogs in their catalog and also observe which guidelines seem generally applicable and which seem to apply to only certain types of dialogs in our collection. Based on the final set of guidelines, the team will provide a number of example redesigns in a final project report and security dialog design tutorial that they will make publicly available.</AbstractNarration>
<MinAmdLetterDate>07/18/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/18/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1116934</AwardID>
<Investigator>
<FirstName>Lorrie</FirstName>
<LastName>Cranor</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lorrie Cranor</PI_FULL_NAME>
<EmailAddress>lorrie@acm.org</EmailAddress>
<PI_PHON>4122687534</PI_PHON>
<NSF_ID>000252230</NSF_ID>
<StartDate>07/18/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>PITTSBURGH</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7795</Code>
<Text>TRUSTWORTHY COMPUTING</Text>
</ProgramElement>
<ProgramReference>
<Code>7795</Code>
<Text>TRUSTWORTHY COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~500000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project has resulted in a novel methodology to study behavioral responses to security dialogs in a realistic, ethical way with high levels of ecological validity, and a novel technique to increase and retain attention to security dialogs, even in the presence of habituation. Both outcomes are reported in:</p> <p>C. Bravo-Lillo. Improving Computer Security Dialogs: An Exploration of Attention and Habituation PhD Thesis, Engineering &amp; Public Policy Department, Carnegie Mellon University, Pittsburgh, PA, May 2014.</p> <p>&nbsp;</p> <p>We designed and tested attractors for computer security dialogs: user-interface modications used to draw users&rsquo; attention to the most important information for making decisions. Some of these modifications were purely visual, while others temporarily inhibited potentially-dangerous behaviors to redirect users&rsquo; attention to salient information. We conducted three between-subjects experiments to test the effectiveness of the attractors. In the first two experiments, we sent participants to perform a task on what appeared to be a third-party site that required installation of a browser plugin. We presented them with what appeared to be an installation dialog from their operating system. In both experiments, participants who saw dialogs that employed attractors were significantly less likely than those in the control group to ignore clues that installing this software might be harmful. In the third experiment, we attempted to habituate participants to dialogs that they knew were part of the experiment. We used attractors to highlight a field that was of no value during habituation trials and contained critical information after the habituation period. Participants exposed to inhibitive attractors were two to three times more likely to make an informed decision than those in the control condition.</p> <p>In the third experiment described above, we habituated participants to the contents of a pop-up dialog by asking them to respond to it repeatedly, and then measured participants&rsquo; ability to notice when a text field within the dialog changed. The experimental treatments included various attractors: interface elements designed to draw or force users&rsquo; attention to a text field within the dialog. In all treatments, we exposed participants to a large number of repetitions of the dialog before introducing the change that participants were supposed to notice. As a result, we could not measure how habituation affects attention, or measure the ability of attractors to counter these effects; we could only compare the performance of attractors under high levels of habituation. We subsequently replicated and improved upon our previous experiment, adding the low-habituation conditions essential to measure reductions in attention that result from increasing habituation. In the absence of attractors, increasing habituation caused a three-fold decrease in the proportion of participants who responded to the change in the dialog. As with the prior study, a greater proportion of participants responded to the change in the dialog in treatments using attractors that delayed participants&rsquo; ability to dismiss the dialog. We found that, like the control, increasing habituation reduced the proportion of participants who noticed the change with some attractors. However, for the two attractors that forced the user to interact with the text field containing the change, increasing the level of habituation did not decrease the proportion of participants who responded to the change. These attractors appeared resilient to habituation.</p> <p>These results were reported in:</p> <p>Cristian Bravo-Lillo, Lorrie Cranor, Saranga Komanduri, Stuart Schechter, and Manya Sleeper. Harder to Ignore? Revisiting Pop-up Fatigue and Approaches to Prevent It. SOUPS 2014. Menlo Park, CA, July 9-11, 2014.</p> <p>&nbsp;</p> <p>We published a technical rep...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has resulted in a novel methodology to study behavioral responses to security dialogs in a realistic, ethical way with high levels of ecological validity, and a novel technique to increase and retain attention to security dialogs, even in the presence of habituation. Both outcomes are reported in:  C. Bravo-Lillo. Improving Computer Security Dialogs: An Exploration of Attention and Habituation PhD Thesis, Engineering &amp; Public Policy Department, Carnegie Mellon University, Pittsburgh, PA, May 2014.     We designed and tested attractors for computer security dialogs: user-interface modications used to draw usersÆ attention to the most important information for making decisions. Some of these modifications were purely visual, while others temporarily inhibited potentially-dangerous behaviors to redirect usersÆ attention to salient information. We conducted three between-subjects experiments to test the effectiveness of the attractors. In the first two experiments, we sent participants to perform a task on what appeared to be a third-party site that required installation of a browser plugin. We presented them with what appeared to be an installation dialog from their operating system. In both experiments, participants who saw dialogs that employed attractors were significantly less likely than those in the control group to ignore clues that installing this software might be harmful. In the third experiment, we attempted to habituate participants to dialogs that they knew were part of the experiment. We used attractors to highlight a field that was of no value during habituation trials and contained critical information after the habituation period. Participants exposed to inhibitive attractors were two to three times more likely to make an informed decision than those in the control condition.  In the third experiment described above, we habituated participants to the contents of a pop-up dialog by asking them to respond to it repeatedly, and then measured participantsÆ ability to notice when a text field within the dialog changed. The experimental treatments included various attractors: interface elements designed to draw or force usersÆ attention to a text field within the dialog. In all treatments, we exposed participants to a large number of repetitions of the dialog before introducing the change that participants were supposed to notice. As a result, we could not measure how habituation affects attention, or measure the ability of attractors to counter these effects; we could only compare the performance of attractors under high levels of habituation. We subsequently replicated and improved upon our previous experiment, adding the low-habituation conditions essential to measure reductions in attention that result from increasing habituation. In the absence of attractors, increasing habituation caused a three-fold decrease in the proportion of participants who responded to the change in the dialog. As with the prior study, a greater proportion of participants responded to the change in the dialog in treatments using attractors that delayed participantsÆ ability to dismiss the dialog. We found that, like the control, increasing habituation reduced the proportion of participants who noticed the change with some attractors. However, for the two attractors that forced the user to interact with the text field containing the change, increasing the level of habituation did not decrease the proportion of participants who responded to the change. These attractors appeared resilient to habituation.  These results were reported in:  Cristian Bravo-Lillo, Lorrie Cranor, Saranga Komanduri, Stuart Schechter, and Manya Sleeper. Harder to Ignore? Revisiting Pop-up Fatigue and Approaches to Prevent It. SOUPS 2014. Menlo Park, CA, July 9-11, 2014.     We published a technical report that includes a set of guidelines aimed at helping software designers and developers in designing more effective warning dialogs. These guidelines...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
