<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Medium: Mobile multiview video: compression, rendering, and transmission</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2012</AwardEffectiveDate>
<AwardExpirationDate>03/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>1188174.00</AwardTotalIntnAmount>
<AwardAmount>1188174</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Cozzens</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The prominence of 3D video technology has skyrocketed. The 2009 movie Avatar in 3D became the highest grossing movie of all time. Such a movie requires left and right views of a scene. Many video games which provide a 3D experience require multiple views of a scene. Such data is costly to store and to transmit.&lt;br/&gt;&lt;br/&gt;This research studies how to efficiently compress multiple-view video data, how to allow the scene to be viewed from any angle at different levels of precision, and how to reliably transmit the data over mobile wireless channels. This research has important applications in science education, traffic monitoring, and surveillance and security.&lt;br/&gt;This research studies efficient encoding, rendering, and transmission of multi-view video, aiming for robust performance at arbitrary speeds of mobile units. The research is applicable both to videos of the real world taken with multiple cameras, and to rendered videos. The investigators study left/right view coding such as in the H.264 MVC standard, and view+depth coding. The latter approach is enhanced by encoding the error signal between the original view and its decoder-synthesized version. To optimally design the system, the techniques use cross-layer optimization, in which physical-layer channel-state information and application-layer distortion-rate or slice-priority information are exploited.  Whenever multiple views are rendered from an underlying 3D virtual world, the application's bit requirements can be hugely altered by rendering parameters which affect the content and level of detail of the scene.  The investigators study user-experience models to quantify the relationship between rendering parameters and user satisfaction, and develop a channel-aware adaptive encoding and rendering algorithm to account for fluctuations caused by transmission over a mobile channel.</AbstractNarration>
<MinAmdLetterDate>03/27/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/29/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1160832</AwardID>
<Investigator>
<FirstName>Laurence</FirstName>
<LastName>Milstein</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Laurence B Milstein</PI_FULL_NAME>
<EmailAddress>milstein@ece.ucsd.edu</EmailAddress>
<PI_PHON>8585343096</PI_PHON>
<NSF_ID>000183980</NSF_ID>
<StartDate>03/27/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Truong</FirstName>
<LastName>Nguyen</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Truong Nguyen</PI_FULL_NAME>
<EmailAddress>nguyent@ece.ucsd.edu</EmailAddress>
<PI_PHON>8588225554</PI_PHON>
<NSF_ID>000481463</NSF_ID>
<StartDate>03/27/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Pamela</FirstName>
<LastName>Cosman</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Pamela C Cosman</PI_FULL_NAME>
<EmailAddress>pcosman@ucsd.edu</EmailAddress>
<PI_PHON>8588220157</PI_PHON>
<NSF_ID>000456426</NSF_ID>
<StartDate>03/27/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sujit</FirstName>
<LastName>Dey</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sujit Dey</PI_FULL_NAME>
<EmailAddress>dey@ece.ucsd.edu</EmailAddress>
<PI_PHON>8585340750</PI_PHON>
<NSF_ID>000295328</NSF_ID>
<StartDate>03/27/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>920930407</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramElement>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<ProgramReference>
<Code>7938</Code>
<Text>SENSOR NETWORKS</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~366726</FUND_OBLG>
<FUND_OBLG>2013~821448</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Images and video can be captured using multiple cameras, which is known as multiview video.&nbsp; Capturing multiview video has the advantage that the multiple views may allow a computer to create a 3D model of the scene, but it has the disadvantage that the multiple views involve a huge amount of data and so can be very slow to transmit and process.&nbsp; Sometimes the data is represented as multiple separate views, but it can also be represented as one view with an associated depth map, which tells how far away each pixel in the scene is from the camera.&nbsp; This latter format is often called video+depth data.</p> <p>This project was primarily concerned with (a) how to compress the multiview data efficiently, for both multiview format and for video+depth format, (b) how to protect the multiview data from channel errors that might occur during transmission, (c) how to estimate the depth of a scene point using multiple views, (d) how to create or render a virtual view of the scene given a nearby view and a depth map, and (e) how to evaluate the visual quality and user satisfaction with all of these different processing operations.</p> <p>This research has applications in many different fields.&nbsp; It can be used in gaming, for example when a player wants to move around inside a virtual world, and the system needs to create intermediate views of what the 3D world looks like from different positions.&nbsp; The same type of system can have many commercial or educational purposes, for example for a virtual real estate tour of a property, or a tour through an art gallery.&nbsp; It can be used in surgery, when multiple laparoscopic cameras capture a view of the patient&rsquo;s organs.&nbsp; In many of these applications, the video may be transmitted wirelessly, so compression and error protection are important, and rendering operations may be computationally expensive and may need to be done in the cloud, rather than on a user&rsquo;s mobile device.&nbsp;</p> <p>Some of the specific accomplishments of this project are:</p> <ol> <li>Improvements to compression for multiview video</li> <li>Determining how many parity bits for error correction to use for different views, or to use for depth maps, so that the resulting video has the highest quality possible when transmitted over a bad channel</li> <li>Development of algorithms that use video data from color and depth cameras to measure change in position over time</li> <li>Determination of scene depth in underwater images using image blurriness, and exploiting this to restore color for underwater images</li> <li>Creation of a user experience model that considers asymmetric rendering, asymmetric encoding and network delay for cloud based virtual immersive applications</li> <li>Development of approaches to simultaneously adapt asymmetric rendering and encoding parameters for cloud-based multiview video according to dynamically changing wireless network conditions so as to ensure high user experience</li> <li>Improvements to image denoising algorithms based on a targeted external database</li> <li>Development of algorithms that reconstruct 3D objects with realistic surface geometry using a handheld RGB-D camera</li> </ol> <p>&nbsp;</p><br> <p>            Last Modified: 06/27/2016<br>      Modified by: Pamela&nbsp;C&nbsp;Cosman</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1160832/1160832_10160228_1467050967520_Overview--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1160832/11608...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Images and video can be captured using multiple cameras, which is known as multiview video.  Capturing multiview video has the advantage that the multiple views may allow a computer to create a 3D model of the scene, but it has the disadvantage that the multiple views involve a huge amount of data and so can be very slow to transmit and process.  Sometimes the data is represented as multiple separate views, but it can also be represented as one view with an associated depth map, which tells how far away each pixel in the scene is from the camera.  This latter format is often called video+depth data.  This project was primarily concerned with (a) how to compress the multiview data efficiently, for both multiview format and for video+depth format, (b) how to protect the multiview data from channel errors that might occur during transmission, (c) how to estimate the depth of a scene point using multiple views, (d) how to create or render a virtual view of the scene given a nearby view and a depth map, and (e) how to evaluate the visual quality and user satisfaction with all of these different processing operations.  This research has applications in many different fields.  It can be used in gaming, for example when a player wants to move around inside a virtual world, and the system needs to create intermediate views of what the 3D world looks like from different positions.  The same type of system can have many commercial or educational purposes, for example for a virtual real estate tour of a property, or a tour through an art gallery.  It can be used in surgery, when multiple laparoscopic cameras capture a view of the patientÆs organs.  In many of these applications, the video may be transmitted wirelessly, so compression and error protection are important, and rendering operations may be computationally expensive and may need to be done in the cloud, rather than on a userÆs mobile device.   Some of the specific accomplishments of this project are:  Improvements to compression for multiview video Determining how many parity bits for error correction to use for different views, or to use for depth maps, so that the resulting video has the highest quality possible when transmitted over a bad channel Development of algorithms that use video data from color and depth cameras to measure change in position over time Determination of scene depth in underwater images using image blurriness, and exploiting this to restore color for underwater images Creation of a user experience model that considers asymmetric rendering, asymmetric encoding and network delay for cloud based virtual immersive applications Development of approaches to simultaneously adapt asymmetric rendering and encoding parameters for cloud-based multiview video according to dynamically changing wireless network conditions so as to ensure high user experience Improvements to image denoising algorithms based on a targeted external database Development of algorithms that reconstruct 3D objects with realistic surface geometry using a handheld RGB-D camera           Last Modified: 06/27/2016       Submitted by: Pamela C Cosman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
