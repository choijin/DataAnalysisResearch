<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CGV: Large: Collaborative Research: Analyzing Images Through Time</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>423732.00</AwardTotalIntnAmount>
<AwardAmount>423732</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This collaborative research project leverages expertise of four research teams (IIS-1111415, Massachusetts Institute of Technology; IIS-1110955, Harvard University; IIS-1111398, Washington University; and IIS-1111534, Cornell University). Understanding time-varying processes and phenomena is fundamental to science and engineering. Due to tremendous progress in digital photography, images and videos (including images from webcams, time- lapse photography captured by scientists, surveillance videos, and Internet photo collections) are becoming an important source of information about our dynamic world. However, techniques for automated understanding and visualization of time-varying processes from images or videos are scarce and underdeveloped, requiring fundamental new models and algorithms for representing changes over time. This research involves creating systems that enable modeling, analysis, and visualization of time-varying processes based on image data. These models and algorithms will form the basis for a new set of tools that can help answer important questions about how our environment is changing, how our cities are evolving, and what significant events are happening around the world.&lt;br/&gt;&lt;br/&gt;Analyzing images over time poses fundamental new technical challenges. This project focuses on developing and demonstrating end-to-end systems consisting of (1) novel representations necessary to model time-varying image datasets; (2) algorithms for estimating long-range temporal correspondence in image datasets; (3) algorithms for decomposing image datasets into intuitive primitives such as shading, illumination, reflectance, and motion; (4) analysis tools for deriving higher level information from the decomposed representations (e.g., trends, repeated patterns, and unusual events); and (5) tools for visualization of the high-level information and methods for re-synthesis of image data.&lt;br/&gt;&lt;br/&gt;This work has the potential to have significant impact in a broad range of areas where images are generated over time, e.g., in ecology, astronomy, urban planning, health, and many others. The results of this research will be broadly disseminated by making source code and datasets publicly available via the project web site (https://groups.csail.mit.edu/vision/image_time/) and offering tutorials and organizing workshops at significant conferences. The project provides educational opportunities and offers hands-on collaborative research experience to students at both the undergraduate and graduate levels and the four institutions.</AbstractNarration>
<MinAmdLetterDate>08/19/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/24/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1110955</AwardID>
<Investigator>
<FirstName>Hanspeter</FirstName>
<LastName>Pfister</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hanspeter Pfister</PI_FULL_NAME>
<EmailAddress>pfister@seas.harvard.edu</EmailAddress>
<PI_PHON>6174968269</PI_PHON>
<NSF_ID>000185558</NSF_ID>
<StartDate>08/19/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard University]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021385369</ZipCode>
<StreetAddress><![CDATA[1033 MASSACHUSETTS AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~218083</FUND_OBLG>
<FUND_OBLG>2013~205649</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>We view and interact with video imagery on a daily basis, through television, film, and through the Web. However, video could hardly be described as a creative medium for novices: the knowledge and skill barrier for entry is high, and video production, especially complex video editing, often requires sophisticated software and many hours of manual work. However, by analysing the temporal processes within videos, it is often possible to automatically or semi-automatically apply creative edits, so saving hours of work and bringing sophisticated tools closer to novice users. In this project, we have worked on many approaches to improve editing of time-varying data, and we highlight three works here: color grading, temporally-consistent filtering, and intrinsic decomposition.</p> <p>The look of video is often determined in a post-process called color grading: after the footage has been shot, color grading software can make subtle changes to the appearance of the imagery, e.g., a blue tint, or even radical changes, e.g., changing a day shot to night. Allowing novices to perform color grading would improve the creative control without increasing the skill barrier. We introduced a system for example-based color grade transfer: the user simply picks the grading style from a video they like, and then we automatically transfer this style to a new video by maintaining that the color statistics are similar.</p> <p>This kind of operation must be temporally consistent - the color transfer must not flicker. In general, there are many operations we might like to apply to a video and be temporally consistent, for instance, any of the multitude of image processing operations, such as filters, found in popular software like Adobe Photoshop. However, naively applying these filters to a video frame by frame often fails, and novices cannot make underlying algorithmic changes necessary to remove this flickering. Instead, we developed 'blind video temporal consistency', in which we create a temporally-consistent result from an initial flickering result, without changing any of the image processing code at all and treating it as a black box. This post-process temporal consistency allows many more effects to be applied to videos and requires almost zero knowledge from the user.</p> <p>For more creative editing of the video content, such as to the lighting or the shadows in the scene, it is useful to try and separate the video into two layers: all the illumination effects from the lighting, and all the fundamental colors of objects as if they received no light. So-called intrinsic decomposition would then allow us to easily change the color of an object, without changing the lighting, or vice versa. Currently, these techniques are good but not perfect, and so we developed a fast interactive method for intrinsic video decomposition which allows the user to quickly correct any mistakes the automatic software makes. This greatly speeds up the time it takes to perform these complex edits.</p> <p>These three techniques show that knowing the temporal changes within imagery can greatly improve the ability of software - and so the user - to edit video. The underlying correspondence techniques are broadly applicable to different applications, and so we hope these works will spur future improvements to the analysis of images through time, and to video as a creative medium for all.</p><br> <p>            Last Modified: 09/30/2015<br>      Modified by: Hanspeter&nbsp;Pfister</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div clas...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ We view and interact with video imagery on a daily basis, through television, film, and through the Web. However, video could hardly be described as a creative medium for novices: the knowledge and skill barrier for entry is high, and video production, especially complex video editing, often requires sophisticated software and many hours of manual work. However, by analysing the temporal processes within videos, it is often possible to automatically or semi-automatically apply creative edits, so saving hours of work and bringing sophisticated tools closer to novice users. In this project, we have worked on many approaches to improve editing of time-varying data, and we highlight three works here: color grading, temporally-consistent filtering, and intrinsic decomposition.  The look of video is often determined in a post-process called color grading: after the footage has been shot, color grading software can make subtle changes to the appearance of the imagery, e.g., a blue tint, or even radical changes, e.g., changing a day shot to night. Allowing novices to perform color grading would improve the creative control without increasing the skill barrier. We introduced a system for example-based color grade transfer: the user simply picks the grading style from a video they like, and then we automatically transfer this style to a new video by maintaining that the color statistics are similar.  This kind of operation must be temporally consistent - the color transfer must not flicker. In general, there are many operations we might like to apply to a video and be temporally consistent, for instance, any of the multitude of image processing operations, such as filters, found in popular software like Adobe Photoshop. However, naively applying these filters to a video frame by frame often fails, and novices cannot make underlying algorithmic changes necessary to remove this flickering. Instead, we developed 'blind video temporal consistency', in which we create a temporally-consistent result from an initial flickering result, without changing any of the image processing code at all and treating it as a black box. This post-process temporal consistency allows many more effects to be applied to videos and requires almost zero knowledge from the user.  For more creative editing of the video content, such as to the lighting or the shadows in the scene, it is useful to try and separate the video into two layers: all the illumination effects from the lighting, and all the fundamental colors of objects as if they received no light. So-called intrinsic decomposition would then allow us to easily change the color of an object, without changing the lighting, or vice versa. Currently, these techniques are good but not perfect, and so we developed a fast interactive method for intrinsic video decomposition which allows the user to quickly correct any mistakes the automatic software makes. This greatly speeds up the time it takes to perform these complex edits.  These three techniques show that knowing the temporal changes within imagery can greatly improve the ability of software - and so the user - to edit video. The underlying correspondence techniques are broadly applicable to different applications, and so we hope these works will spur future improvements to the analysis of images through time, and to video as a creative medium for all.       Last Modified: 09/30/2015       Submitted by: Hanspeter Pfister]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
