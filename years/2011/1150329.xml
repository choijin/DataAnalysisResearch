<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Integrated systems for light field capture and analysis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/15/2012</AwardEffectiveDate>
<AwardExpirationDate>01/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>400000.00</AwardTotalIntnAmount>
<AwardAmount>400000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Mahmoud Fallahi</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The objective of this project is to develop integrated, low cost systems able to capture and characterize light from 3-D scenes while minimizing or eliminating the need for off-chip computation. &lt;br/&gt;&lt;br/&gt;The intellectual merit of this research is in understanding the capabilities and limitations of imaging systems using diffractive angle-sensitive pixels to analyze the light field. This includes:  1) Developing models of how scene statistics are reflected in the output statistics of angle sensitive pixel arrays, and so developing algorithms to extract useful information about the scene, such as depth, 3-D directional motion, and object recognition. 2) Understanding how to optimize arrays of angle sensitive pixels to encode scenes at various focal depths, as well as how to best deploy signal selection, conditioning and processing circuits to take advantage of these arrays? signal properties when encoding real scenes.  3) Investigating new integrated diffractive structures that enhance ASP quantum efficiency and spatial resolution, and that filter the light field in new ways. &lt;br/&gt; &lt;br/&gt;The broader impacts of this research will include enhanced imaging systems with applications in security, automation, health care, and scientific research, from wildlife tracking to microscopy. This work will also impact the commercial sphere by providing new capabilities in consumer electronics, helping to boost economic growth. These impacts are increased because these systems use standard CMOS manufacturing, and so can be mass-produced at very low size and cost.  This research provides an easily-understood example of low-level physics driving useful high-level function, providing compelling examples for recruiting and inspiring future engineers.</AbstractNarration>
<MinAmdLetterDate>02/10/2012</MinAmdLetterDate>
<MaxAmdLetterDate>02/10/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1150329</AwardID>
<Investigator>
<FirstName>Alyosha</FirstName>
<LastName>Molnar</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alyosha C Molnar</PI_FULL_NAME>
<EmailAddress>am699@cornell.edu</EmailAddress>
<PI_PHON>6072555014</PI_PHON>
<NSF_ID>000498150</NSF_ID>
<StartDate>02/10/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148535401</ZipCode>
<StreetAddress><![CDATA[402 Phillips Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1517</Code>
<Text>EPMD-ElectrnPhoton&amp;MagnDevices</Text>
</ProgramElement>
<ProgramReference>
<Code>094E</Code>
<Text>Optoelectronic devices</Text>
</ProgramReference>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~400000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Overall, the goal of this project was to develop a new class of chips to capture light from scenes in new ways by using a new kind of pixel (the &ldquo;angle sensitive pixel&rdquo; or ASP).&nbsp; &nbsp;ASPs use diffractive structures placed over each light sensor to process and filter light based on its incident angle.&nbsp; ASPs, when combined with appropriate circuits and signal processing, can be used to extract information about the 2- and 3-D structure of a scene, even without a lens.&nbsp; Specifically, in this project we worked to 1) better understand the basic properties of ASPs and their response to real scenes, 2) to enhance ASP&rsquo;s performance with respect to sensitivity, and by enabling them to detect additional properties of light, and 3) by building arrays of these enhanced ASPs to capture real scenes, and processing those scenes to perform 3D reconstructions.</p> <p>One important aspect of ASP behavior that we explored was how ASPs capture not only incident angle, but also polarization angle.&nbsp; Since ASPs make use of nanoscale diffraction gratings of different orientations to get at incident angle, they also inherently extract polarization angle, and do so in a way that can be isolated from incident angle.&nbsp; Incident angle information allows us to characterize where a set of light rays originate (essentially by triangulation) and so find the 3-D location of an object.&nbsp; However, if the light rays are reflecting off an object and that reflection is specular (that is, if the object is shiny), then simple triangulation algorithms can become confused.&nbsp; However, many forms of specular reflection also result in a mild polarization of the reflected light.&nbsp; Thus, by merging polarization with angular information, we are able to better estimate where an object is, even if it is shiny. &nbsp;(See figures 1+2).</p> <p>A second important aspect of this work involved building better ASPs by changing how we generated the diffraction patterns that ASPs analyze to extract incident angle.&nbsp; Early versions of ASPs used &ldquo;amplitude gratings&rdquo; made up of stripes of metal separated by transparent (glass) gaps each 100&rsquo;s of nanometers wide.&nbsp; Unfortunately, this kind of structure reflects quite a bit of the incoming light (at least 50%), making ASPs work less well in the dark.&nbsp; However, it is known that replacing strips of metal with strips of glass of different thicknesses can generate similar effects by slowing the light down in some places more than others: these are called &ldquo;phase gratings&rdquo;.&nbsp; Although phase- and amplitude gratings generate somewhat different diffraction patterns when their widths are much greater than the wavelength of light, we found they acted very similarly when built on the same scale as the wavelength of light.&nbsp; Although this kind of research finding is not as exciting-sounding as building new kinds of cameras, this kind of careful study of such phenomena is critical for designing and building good sensors and cameras.</p> <p>A third, and perhaps most exciting result came from combining ASPs with single photon avalanche diodes (SPADs).&nbsp; SPADs are a semiconductor structure that allows the absorption of single photons to be detected as discrete events, and encodes then as bits changing from a &ldquo;0&rdquo; to a &ldquo;1&rdquo; in less than 1ns.&nbsp; Combining ASPs and SPADs in a single structure (figure 3) allows us to estimate both when single photons arrive, and where they came from, although doing this accurately requires gathering statistics across many photons.&nbsp; This kind of structure is interesting in that it inherently uses both the wave- and particle aspects of light: the wave aspect to do diffraction (to find angle), the particle part to extract time information.&nbsp; This is also a useful combination of capabilities for a variety of applications.&nbsp; In this project we demonstrated that an array of such angel-sensitive SPADs (Figure 4) could be used to eliminate both the lens and filter from a fluorescent microscope.&nbsp; Angle sensitivity allows an array of ASPs to capture enough information to locate fluorescent objects in a 3-D volume adjacent to a chip (within 1-3mm) without a lens (figure 5).&nbsp; Fine-grain timing information can be used to separate the excitation of fluorescent molecules by a brief flash of blue of violet light, and the subsequent fluorescent re-emission of green or red light in the nanoseconds that follow the flash.&nbsp; Furthermore, since different kinds of fluorescent molecule glow with different color and with different delays, time information allows them to be distinguished based on location and decay rate (Figure 6).&nbsp; To our knowledge, this was the first chip-scale, lens-less, filter-less fluorescent microscope.&nbsp;</p> <p>One final important aspect of this project was that it provided many students an opportunity to learn about optics, electronics, and image processing, and gave them hands-on experience developing new technologies.&nbsp; This included 4 PhD Students 2 Masters students, and more than 10 undergraduates.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/08/2017<br>      Modified by: Alyosha&nbsp;C&nbsp;Molnar</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494299066150_CAREER_fig1--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494299066150_CAREER_fig1--rgov-800width.jpg" title="Polarization from ASPs"><img src="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494299066150_CAREER_fig1--rgov-66x44.jpg" alt="Polarization from ASPs"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Polarization read-out from real image with specular reflection (a).  In fact the polarization is inhomogenous everywhere in the image (b), but is only strongly so in certain regions, mostly associated with specular reflection (c)</div> <div class="imageCredit">Optics Letters</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alyosha&nbsp;C&nbsp;Molnar</div> <div class="imageTitle">Polarization from ASPs</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494299178727_CAREER_fig2--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494299178727_CAREER_fig2--rgov-800width.jpg" title="Fig. 2 Combined polarization and ranging"><img src="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494299178727_CAREER_fig2--rgov-66x44.jpg" alt="Fig. 2 Combined polarization and ranging"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The image could be segmented based on contrast (a),  and the depth of the contours could then be extracted based on angular inhomogeneity in the lightfield (b).  Critically, some of these contours correspond to (and completely overlap with) polarized regions, indicating a specular artifact, allowing</div> <div class="imageCredit">Optics Letters</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alyosha&nbsp;C&nbsp;Molnar</div> <div class="imageTitle">Fig. 2 Combined polarization and ranging</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494300051561_CAREER_fig4--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494300051561_CAREER_fig4--rgov-800width.jpg" title="Angle-sensitive SPAD array"><img src="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494300051561_CAREER_fig4--rgov-66x44.jpg" alt="Angle-sensitive SPAD array"></a> <div class="imageCaptionContainer"> <div class="imageCaption">An array of angle-sensitive single-photon avalanche diodes, used to demonstrate lensless, filterless 3-D Fluorescent microscopy.</div> <div class="imageCredit">VLSI Symposium</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alyosha&nbsp;C&nbsp;Molnar</div> <div class="imageTitle">Angle-sensitive SPAD array</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494300266070_CAREER_fig3--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494300266070_CAREER_fig3--rgov-800width.jpg" title="Fig. 3 Angle sensitive SPAD cross-section"><img src="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494300266070_CAREER_fig3--rgov-66x44.jpg" alt="Fig. 3 Angle sensitive SPAD cross-section"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Cross-sectional diagram of an angle sensitive single photon avalanche diode, which is made up of a pair of diffraction gratings placed over a heavily reverse-biased silicon diode.</div> <div class="imageCredit">VLSI Symposium</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alyosha&nbsp;C&nbsp;Molnar</div> <div class="imageTitle">Fig. 3 Angle sensitive SPAD cross-section</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494300392125_CAREER_fig5--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494300392125_CAREER_fig5--rgov-800width.jpg" title="Fig. 5: 3-D localization of Fluorescent beads"><img src="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494300392125_CAREER_fig5--rgov-66x44.jpg" alt="Fig. 5: 3-D localization of Fluorescent beads"></a> <div class="imageCaptionContainer"> <div class="imageCaption">a) 3-D localization of two different fluorescent beads, localized based on the angular responses of A-SPADs, shown in (b), and identified based on decay time (form Fig. 6)</div> <div class="imageCredit">Sensors</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Alyosha&nbsp;C&nbsp;Molnar</div> <div class="imageTitle">Fig. 5: 3-D localization of Fluorescent beads</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494300624932_CAREER_fig6--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494300624932_CAREER_fig6--rgov-800width.jpg" title="Fig. 6: extracted decay times from different locations"><img src="/por/images/Reports/POR/2017/1150329/1150329_10152961_1494300624932_CAREER_fig6--rgov-66x44.jpg" alt="Fig. 6: extracted decay times from different locations"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Different kinds of fluorescent beads show different decay times, extracted based on localization (fig. 5) and time of arrival of photons.</div> <div class="imageCredit">Sensors</div> <div class="imageSubmitted">Alyosha&nbsp;C&nbsp;Molnar</div> <div class="imageTitle">Fig. 6: extracted decay times from different locations</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Overall, the goal of this project was to develop a new class of chips to capture light from scenes in new ways by using a new kind of pixel (the "angle sensitive pixel" or ASP).   ASPs use diffractive structures placed over each light sensor to process and filter light based on its incident angle.  ASPs, when combined with appropriate circuits and signal processing, can be used to extract information about the 2- and 3-D structure of a scene, even without a lens.  Specifically, in this project we worked to 1) better understand the basic properties of ASPs and their response to real scenes, 2) to enhance ASP?s performance with respect to sensitivity, and by enabling them to detect additional properties of light, and 3) by building arrays of these enhanced ASPs to capture real scenes, and processing those scenes to perform 3D reconstructions.  One important aspect of ASP behavior that we explored was how ASPs capture not only incident angle, but also polarization angle.  Since ASPs make use of nanoscale diffraction gratings of different orientations to get at incident angle, they also inherently extract polarization angle, and do so in a way that can be isolated from incident angle.  Incident angle information allows us to characterize where a set of light rays originate (essentially by triangulation) and so find the 3-D location of an object.  However, if the light rays are reflecting off an object and that reflection is specular (that is, if the object is shiny), then simple triangulation algorithms can become confused.  However, many forms of specular reflection also result in a mild polarization of the reflected light.  Thus, by merging polarization with angular information, we are able to better estimate where an object is, even if it is shiny.  (See figures 1+2).  A second important aspect of this work involved building better ASPs by changing how we generated the diffraction patterns that ASPs analyze to extract incident angle.  Early versions of ASPs used "amplitude gratings" made up of stripes of metal separated by transparent (glass) gaps each 100?s of nanometers wide.  Unfortunately, this kind of structure reflects quite a bit of the incoming light (at least 50%), making ASPs work less well in the dark.  However, it is known that replacing strips of metal with strips of glass of different thicknesses can generate similar effects by slowing the light down in some places more than others: these are called "phase gratings".  Although phase- and amplitude gratings generate somewhat different diffraction patterns when their widths are much greater than the wavelength of light, we found they acted very similarly when built on the same scale as the wavelength of light.  Although this kind of research finding is not as exciting-sounding as building new kinds of cameras, this kind of careful study of such phenomena is critical for designing and building good sensors and cameras.  A third, and perhaps most exciting result came from combining ASPs with single photon avalanche diodes (SPADs).  SPADs are a semiconductor structure that allows the absorption of single photons to be detected as discrete events, and encodes then as bits changing from a "0" to a "1" in less than 1ns.  Combining ASPs and SPADs in a single structure (figure 3) allows us to estimate both when single photons arrive, and where they came from, although doing this accurately requires gathering statistics across many photons.  This kind of structure is interesting in that it inherently uses both the wave- and particle aspects of light: the wave aspect to do diffraction (to find angle), the particle part to extract time information.  This is also a useful combination of capabilities for a variety of applications.  In this project we demonstrated that an array of such angel-sensitive SPADs (Figure 4) could be used to eliminate both the lens and filter from a fluorescent microscope.  Angle sensitivity allows an array of ASPs to capture enough information to locate fluorescent objects in a 3-D volume adjacent to a chip (within 1-3mm) without a lens (figure 5).  Fine-grain timing information can be used to separate the excitation of fluorescent molecules by a brief flash of blue of violet light, and the subsequent fluorescent re-emission of green or red light in the nanoseconds that follow the flash.  Furthermore, since different kinds of fluorescent molecule glow with different color and with different delays, time information allows them to be distinguished based on location and decay rate (Figure 6).  To our knowledge, this was the first chip-scale, lens-less, filter-less fluorescent microscope.   One final important aspect of this project was that it provided many students an opportunity to learn about optics, electronics, and image processing, and gave them hands-on experience developing new technologies.  This included 4 PhD Students 2 Masters students, and more than 10 undergraduates.          Last Modified: 05/08/2017       Submitted by: Alyosha C Molnar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
