<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Small: Distributed System Diagnosis via Request Flow Comparison</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>487764.00</AwardTotalIntnAmount>
<AwardAmount>487764</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This work builds on recent instrumentation approaches that provide low-overhead end-to-end tracing (e.g., Stardust and Dapper) that can capture the flow (i.e., path and timing) of individual requests within and across the components of a distributed system.  &lt;br/&gt;&lt;br/&gt;In addition to tools for profiling and examining system behavior, the PIs are creating tools that compare request flows between two executions to guide understanding of changes in performance, as contrasted with determining why a system has always been slow.  Comparing request flows can help diagnose a large class of performance problems, such as degradations resulting from software changes/upgrades or from usage over time (e.g., due to resource leakage or workload changes).  &lt;br/&gt;&lt;br/&gt;This research is evaluating the efficacy of this approach, addressing scalability challenges in instrumentation data collection and analysis, and exploring the effects of virtualization on variability.</AbstractNarration>
<MinAmdLetterDate>08/01/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/01/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1117567</AwardID>
<Investigator>
<FirstName>Gregory</FirstName>
<LastName>Ganger</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gregory R Ganger</PI_FULL_NAME>
<EmailAddress>ganger@ece.cmu.edu</EmailAddress>
<PI_PHON>4122681297</PI_PHON>
<NSF_ID>000328442</NSF_ID>
<StartDate>08/01/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Chuck</FirstName>
<LastName>Cranor</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Chuck Cranor</PI_FULL_NAME>
<EmailAddress>chuck@andrew.cmu.edu</EmailAddress>
<PI_PHON>4122685426</PI_PHON>
<NSF_ID>000255876</NSF_ID>
<StartDate>08/01/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie-Mellon University]]></Name>
<CityName>PITTSBURGH</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7354</Code>
<Text>COMPUTER SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~487764</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The causes of performance changes in a distributed software system often elude even its developers. &nbsp;This project developed, demonstrated, and evaluated a new technique for gaining insight into such changes: comparing request flows from two executions (e.g., of two system versions or time periods). &nbsp;Building on end-to-end request-flow tracing within and across software components of a distributed service (e.g., an Internet service), algorithms were developed for identifying and ranking changes in the flow and/or timing of request processing. &nbsp;These algorithms were implemented in a tool called Spectroscope, which was used for a broad array of evaluations. &nbsp;Example case studies used Spectroscope to diagnose performance problems in a distributed storage service caused by code changes, configuration modifications, and component degradations, demonstrating the value and efficacy of comparing request flows. &nbsp;The project also transferred the algorithms to some companies and reported on their use to diagnose performance changes within select Google services.</p> <p>In addition to developing this new technique and evaluating its overall efficacy, in multiple real distributed services and real performance problem case studies, this project explored (via user studies) visualization approaches for helping users interactively diagnose complex problems and software development approaches for improving the effectiveness of automated problem diagnosis tools. &nbsp;Our findings indicate that no single visualization approach is best for all users and problem types, but that a production navigation interface can be effective by supporting multiple styles (inc. side-by-side comparison of graphs, user-controlled animation, and overlay) and allowing a user to switch among them at will. &nbsp;Our findings also indicate that the statistical methods underlying automated problem diagnosis are confounded by excessive variance in non-problem request flow timings, making it crucial for software systems to be predictable within nodes of a request flow. &nbsp;To assist with this, the project developed techniques for guiding developers toward points in a software system where variance is a problem.</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/16/2015<br>      Modified by: Gregory&nbsp;R&nbsp;Ganger</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The causes of performance changes in a distributed software system often elude even its developers.  This project developed, demonstrated, and evaluated a new technique for gaining insight into such changes: comparing request flows from two executions (e.g., of two system versions or time periods).  Building on end-to-end request-flow tracing within and across software components of a distributed service (e.g., an Internet service), algorithms were developed for identifying and ranking changes in the flow and/or timing of request processing.  These algorithms were implemented in a tool called Spectroscope, which was used for a broad array of evaluations.  Example case studies used Spectroscope to diagnose performance problems in a distributed storage service caused by code changes, configuration modifications, and component degradations, demonstrating the value and efficacy of comparing request flows.  The project also transferred the algorithms to some companies and reported on their use to diagnose performance changes within select Google services.  In addition to developing this new technique and evaluating its overall efficacy, in multiple real distributed services and real performance problem case studies, this project explored (via user studies) visualization approaches for helping users interactively diagnose complex problems and software development approaches for improving the effectiveness of automated problem diagnosis tools.  Our findings indicate that no single visualization approach is best for all users and problem types, but that a production navigation interface can be effective by supporting multiple styles (inc. side-by-side comparison of graphs, user-controlled animation, and overlay) and allowing a user to switch among them at will.  Our findings also indicate that the statistical methods underlying automated problem diagnosis are confounded by excessive variance in non-problem request flow timings, making it crucial for software systems to be predictable within nodes of a request flow.  To assist with this, the project developed techniques for guiding developers toward points in a software system where variance is a problem.          Last Modified: 08/16/2015       Submitted by: Gregory R Ganger]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
