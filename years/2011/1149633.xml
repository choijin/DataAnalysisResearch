<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Non-Parametric Image Parsing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2012</AwardEffectiveDate>
<AwardExpirationDate>02/28/2019</AwardExpirationDate>
<AwardTotalIntnAmount>499999.00</AwardTotalIntnAmount>
<AwardAmount>499999</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project develops new techniques for visually interpreting an image in a way that specifically leverages large image collections, now common on the web and elsewhere. The research team uses an approach whose performance directly scales with the size of the dataset, unlike many existing approaches to image understanding. The basic approach is to build a copy of a query image by assembling pieces of image from a large set of training images, in the manner of a jigsaw. Each region in the query is classified by copying labels from the matched regions. The larger the training set, the more jigsaw pieces there are to choose from, thus the more accurate the match.&lt;br/&gt;&lt;br/&gt;The initial work of the project focuses on developing efficient methods for performing the matching that allow the incorporating of various desirable constraints. The approach is then extended to handle training data with incomplete labels -- important since few datasets have labels for every region. The research plan also includes building better embeddings for the regions which place semantically similar regions closer together than current representations do, and developing efficient binary matching schemes along with further work on the region embeddings. &lt;br/&gt;&lt;br/&gt;Robust techniques for visual recognition have widespread applicability, in such areas as image search, robotics and surveillance. The project also involves extensive outreach activities, including high-school internships and the organization of a NY-area vision day for students and researchers</AbstractNarration>
<MinAmdLetterDate>02/29/2012</MinAmdLetterDate>
<MaxAmdLetterDate>04/23/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1149633</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Fergus</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Fergus</PI_FULL_NAME>
<EmailAddress>fergus@cs.nyu.edu</EmailAddress>
<PI_PHON>2129982121</PI_PHON>
<NSF_ID>000490043</NSF_ID>
<StartDate>02/29/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041968306</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEW YORK UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041968306</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[New York University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100121110</ZipCode>
<StreetAddress><![CDATA[251 Mercer Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~86820</FUND_OBLG>
<FUND_OBLG>2013~100032</FUND_OBLG>
<FUND_OBLG>2014~102145</FUND_OBLG>
<FUND_OBLG>2015~211002</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to build visual recognition systems that can interpret real-world scenes robustly, giving detailed labels for each region of the image. Early on in the project, the seminal paper of Krizhevsky et al. [1] appeared, which ushered in the Deep Learning revolution in vision. Correspondingly, the project refocused around Deep Learning methods and the work produced in the project (fig 1) helped to persuade the community of their merits.&nbsp; Specifically, [2] showed how the features learned within deep convolutional models show an intuitive hierarchy of composition (edges into simple shapes into small object parts into large object parts into full objects), something that was always hoped for in deep models, but not previously demonstrated. These results have been widely used by other in explaining deep learning to the general public. We also probed the convnet models, gaining insights into their operation, leading to new architectures that won the ImageNet 2013 classification challenge, the leading competition benchmark.</p> <p>&nbsp;</p> <p>Moving beyond classification, we also introduced a widely-used dataset for indoor scene understanding [3], which combines RGB and depth information. Applying deep nets to this dataset, we showed how semantic labels, depth and surface orientation could be robustly predicted [4] (fig 2). &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p> <p>&nbsp;</p> <p>We also explored generative models of images (models that will draw an image) [5]. This work was the first to reproduce the results of the original GAN [6] paper and build up the method, demonstrating, at the time, a big leap in performance over the existing state-of-the-art in image generation (fig 3). This work helped to spur significant follow-on work, the latest of which is able to generate photo-realistic facial images using an approach very similar to our 2015 work. &nbsp;&nbsp;</p> <p>&nbsp;</p> <p>Figure captions:</p> <p>&nbsp;</p> <p>Fig 1: Caption included.</p> <p>&nbsp;</p> <p>Fig 2.: Our system is able to take a standard RGB image of an indoor scene and predict maps for object class labels (chair, sofa, etc.), depth from camera and surface orientation.</p> <p>&nbsp;</p> <p>Fig 3: 64x64 generations from our model, showing 10 samples for three different seeds. Notice how our LAPGAN model is able to recompose images smoothly. At the time of publication, these were state-of-the-art results in image generation.</p> <p>&nbsp;</p> <p>[1] <a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=2071317309766942398&amp;btnI=1&amp;hl=en">Imagenet classification with deep convolutional neural networks</a></p> <p>A Krizhevsky, I Sutskever, GE Hinton&nbsp;- Advances in neural information processing systems, 2012</p> <p>&nbsp;</p> <p>[2] <a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=7533260230303233181&amp;btnI=1&amp;hl=en">Visualizing and understanding convolutional networks</a></p> <p>MD Zeiler, R Fergus&nbsp;- European conference on computer vision, 2014</p> <p>&nbsp;</p> <p>[3] <a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=17561312155189214994&amp;btnI=1&amp;hl=en">Indoor segmentation and support inference from RGB-D images</a></p> <p>N Silberman, D Hoiem, P Kohli, R Fergus&nbsp;- European Conference on Computer Vision, 2012</p> <p>&nbsp;</p> <p>[4] <a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=12162254258128213973&amp;btnI=1&amp;hl=en">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</a>. D Eigen, R Fergus&nbsp;- Proceedings of the IEEE international conference on&nbsp;Computer Vision, 2015</p> <p>&nbsp;</p> <p>[5] <a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=8031319294003741632&amp;btnI=1&amp;hl=en">Deep generative image models using a laplacian pyramid of adversarial networks</a></p> <p>EL Denton, S Chintala, R Fergus&nbsp;- Advances in neural information processing systems, 2015</p> <p>&nbsp;</p> <p>[6] A Style-Based Generator Architecture for Generative Adversarial Networks</p> <p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Karras%2C+T">Tero Karras</a>,&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Laine%2C+S">Samuli Laine</a>,&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Aila%2C+T">Timo Aila</a>, arXiv:1812.04948, 2018</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/02/2019<br>      Modified by: Robert&nbsp;Fergus</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2019/1149633/1149633_10155971_1572668969249_fig1--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1149633/1149633_10155971_1572668969249_fig1--rgov-800width.jpg" title="Convnet feature visualizations"><img src="/por/images/Reports/POR/2019/1149633/1149633_10155971_1572668969249_fig1--rgov-66x44.jpg" alt="Convnet feature visualizations"></a> <div class="imageCaptionContainer"> <div class="imageCaption">See main text.</div> <div class="imageCredit">Matt Zeiler and Rob Fergus</div> <div class="imagePermisssions">Copyright owner is an institution with an existing agreement allowing use by NSF</div> <div class="imageSubmitted">Robert&nbsp;Fergus</div> <div class="imageTitle">Convnet feature visualizations</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1149633/1149633_10155971_1572669034092_fig2--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1149633/1149633_10155971_1572669034092_fig2--rgov-800width.jpg" title="Overview figure for Eigen et al. 2015"><img src="/por/images/Reports/POR/2019/1149633/1149633_10155971_1572669034092_fig2--rgov-66x44.jpg" alt="Overview figure for Eigen et al. 2015"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Our system is able to take a standard RGB image of an indoor scene and predict maps for object class labels (chair, sofa, etc.), depth from camera and surface orientation.</div> <div class="imageCredit">David Eigen and Rob Fergus</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Robert&nbsp;Fergus</div> <div class="imageTitle">Overview figure for Eigen et al. 2015</div> </div> </li> <li> <a href="/por/images/Reports/POR/2019/1149633/1149633_10155971_1572669083380_fig3--rgov-214x142.jpg" original="/por/images/Reports/POR/2019/1149633/1149633_10155971_1572669083380_fig3--rgov-800width.jpg" title="Generations from LAPGAN model"><img src="/por/images/Reports/POR/2019/1149633/1149633_10155971_1572669083380_fig3--rgov-66x44.jpg" alt="Generations from LAPGAN model"></a> <div class="imageCaptionContainer"> <div class="imageCaption">64x64 generations from our model, showing 10 samples for three different seeds. Notice how our LAPGAN model is able to recompose images smoothly. At the time of publication, these were state-of-the-art results in image generation.</div> <div class="imageCredit">Emily Denton and Rob Fergus</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Robert&nbsp;Fergus</div> <div class="imageTitle">Generations from LAPGAN model</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to build visual recognition systems that can interpret real-world scenes robustly, giving detailed labels for each region of the image. Early on in the project, the seminal paper of Krizhevsky et al. [1] appeared, which ushered in the Deep Learning revolution in vision. Correspondingly, the project refocused around Deep Learning methods and the work produced in the project (fig 1) helped to persuade the community of their merits.  Specifically, [2] showed how the features learned within deep convolutional models show an intuitive hierarchy of composition (edges into simple shapes into small object parts into large object parts into full objects), something that was always hoped for in deep models, but not previously demonstrated. These results have been widely used by other in explaining deep learning to the general public. We also probed the convnet models, gaining insights into their operation, leading to new architectures that won the ImageNet 2013 classification challenge, the leading competition benchmark.     Moving beyond classification, we also introduced a widely-used dataset for indoor scene understanding [3], which combines RGB and depth information. Applying deep nets to this dataset, we showed how semantic labels, depth and surface orientation could be robustly predicted [4] (fig 2).           We also explored generative models of images (models that will draw an image) [5]. This work was the first to reproduce the results of the original GAN [6] paper and build up the method, demonstrating, at the time, a big leap in performance over the existing state-of-the-art in image generation (fig 3). This work helped to spur significant follow-on work, the latest of which is able to generate photo-realistic facial images using an approach very similar to our 2015 work.        Figure captions:     Fig 1: Caption included.     Fig 2.: Our system is able to take a standard RGB image of an indoor scene and predict maps for object class labels (chair, sofa, etc.), depth from camera and surface orientation.     Fig 3: 64x64 generations from our model, showing 10 samples for three different seeds. Notice how our LAPGAN model is able to recompose images smoothly. At the time of publication, these were state-of-the-art results in image generation.     [1] Imagenet classification with deep convolutional neural networks  A Krizhevsky, I Sutskever, GE Hinton - Advances in neural information processing systems, 2012     [2] Visualizing and understanding convolutional networks  MD Zeiler, R Fergus - European conference on computer vision, 2014     [3] Indoor segmentation and support inference from RGB-D images  N Silberman, D Hoiem, P Kohli, R Fergus - European Conference on Computer Vision, 2012     [4] Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. D Eigen, R Fergus - Proceedings of the IEEE international conference on Computer Vision, 2015     [5] Deep generative image models using a laplacian pyramid of adversarial networks  EL Denton, S Chintala, R Fergus - Advances in neural information processing systems, 2015     [6] A Style-Based Generator Architecture for Generative Adversarial Networks  Tero Karras, Samuli Laine, Timo Aila, arXiv:1812.04948, 2018             Last Modified: 11/02/2019       Submitted by: Robert Fergus]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
