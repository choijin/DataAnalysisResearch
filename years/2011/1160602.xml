<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Large: Collaborative Research: PXGL: Cyberinfrastructure for Scalable Graph Execution</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>700000.00</AwardTotalIntnAmount>
<AwardAmount>700000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The most powerful computing systems in the world have historically been dedicated to solving scientific problems. Until recently, the computations performed by these systems have typically been simulations of various physical phenomena. However, a new paradigm for scientific discovery has been steadily rising in importance, namely, data-intensive science, which focuses sophisticated analysis techniques on the enormous (and ever increasing) amounts of data being produced in scientific, commercial, and social endeavors. Important research based on data-intensive science include areas as diverse as knowledge discovery, bioinformatics, proteomics and genomics, data mining and search, electronic design automation, computer vision, and Internet routing. Unfortunately, the computational approaches needed for data-intensive science differ markedly from those that have been so effective for simulation-based supercomputing. To enable and facilitate efficient execution of data-intensive scientific problems, this project will develop a comprehensive hardware and software supercomputing system for data-intensive science.&lt;br/&gt;Graph algorithms and data structures are fundamental to data-intensive computations and, consequently, this project is focused on providing fundamental, new understandings of the basics of large-scale graph processing and how to build scalable systems to efficiently solve large-scale graph problems. In particular, this work will characterize processing overheads and the limits of graph processing scalability, develop performance models that properly capture graph algorithms, define the (co-design) process for developing graph-specific hardware, and experimentally verify our approach with a prototype execution environment. Key capabilities of our system include: a novel fine-grained parallel programming model, a scalable library of graph algorithms and data structures, a graph-optimized core architecture, and a scalable graph execution platform. The project will also address the programming challenges involved in constructing scalable and reliable software for data-intensive problems.</AbstractNarration>
<MinAmdLetterDate>10/12/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/02/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1160602</AwardID>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Sterling</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas Sterling</PI_FULL_NAME>
<EmailAddress>tron@indiana.edu</EmailAddress>
<PI_PHON>8128564597</PI_PHON>
<NSF_ID>000119080</NSF_ID>
<StartDate>10/12/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Indiana University</Name>
<CityName>Bloomington</CityName>
<ZipCode>474013654</ZipCode>
<PhoneNumber>3172783473</PhoneNumber>
<StreetAddress>509 E 3RD ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>006046700</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF INDIANA UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>006046700</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Indiana University]]></Name>
<CityName>Bloomington</CityName>
<StateCode>IN</StateCode>
<ZipCode>474021847</ZipCode>
<StreetAddress><![CDATA[P.O. Box 1847]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~525000</FUND_OBLG>
<FUND_OBLG>2014~175000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Louisiana State University, under the direction of Hartmut Kaiser and PI Thomas Sterling at Indiana University have developed the High Performance ParalleX (HPX) runtime system with improved general performance in distributed operations. The scope of these improvements was dictated by the requirements of efficient support for distributed graph operations. They include integration of AM++ active message library developed at IU and message coalescing with AM++ HPX runtime to optimize access to graph segments located on remote nodes. The supported messaging layer options include RDMA (Remote Direct Memory Access), which is the prevalent high-speed message passing technique used in modern networks, and shared memory based communication for efficient message exchange within individual nodes.&nbsp; A novel distributed garbage collector has been implemented to facilitate efficient memory management across the entire system (its performance scales linearly with the number of affected object references). This feature is critical for dynamic graph problems, in which constant changes to graph structure may result in premature exhaustion of available memory if memory reclamation schemes are lacking. Finally, a key component of the execution model, Active Global Address Space, that permits referencing the components of computation transparently to their physical location, has been implemented and tuned for performance. To increase the portability of the applied solutions, the XPI interface layer that abstracts the application programming constructs from the details of low level runtime system implementation, has also been developed.&nbsp; The performance of the HPX code has been measured and verified using several irregular applications, including 1-D and 3-D Adaptive Mesh Refinement (AMR) simulations as well as alpha-beta search used to find the best next move in a game of chess, and FFT (Fast Fourier Transform) computation in which pre-sorting of input data is arranged in binary trees spanning multiple nodes. Two formal releases of the HPX library were developed during the final project reporting period: HPX v0.9.9 (released November 2014) and HPX v0.9.10 (released March 2015). Both versions are publicly available for download from github and HPX web site at LSU.</p> <p>&nbsp;The project provided considerable training opportunities for students.</p> <p>Besides the PI and two senior research staff at LSU, four graduate students have been employed throughout various stages of the project. They have been exposed to advanced concepts in High Performance Computing, such as alternative parallel execution models and runtime systems, and methodologies of supporting dynamic and unstructured problems on parallel architectures. Mastering these research concepts also develops marketable job skills and resulted in at least one case in employment of the participating student as a postdoctoral scholar at a national laboratory.&nbsp; The collaboration with Indiana University and New Mexico State University yielded many improvements to the HPX code base while providing additional outreach opportunities. Specifically, the runtime code developed at LSU has been used as a non-trivial but accessible test bed for student experiments at NMSU oriented on performance analysis and tuning. The research results detailing impacts of various implementation strategies of runtime system internals on parallel application performance were disseminated through talks and publications in conference proceedings.</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/21/2015<br>      Modified by: Thomas&nbsp;Sterling</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Louisiana State University, under the direction of Hartmut Kaiser and PI Thomas Sterling at Indiana University have developed the High Performance ParalleX (HPX) runtime system with improved general performance in distributed operations. The scope of these improvements was dictated by the requirements of efficient support for distributed graph operations. They include integration of AM++ active message library developed at IU and message coalescing with AM++ HPX runtime to optimize access to graph segments located on remote nodes. The supported messaging layer options include RDMA (Remote Direct Memory Access), which is the prevalent high-speed message passing technique used in modern networks, and shared memory based communication for efficient message exchange within individual nodes.  A novel distributed garbage collector has been implemented to facilitate efficient memory management across the entire system (its performance scales linearly with the number of affected object references). This feature is critical for dynamic graph problems, in which constant changes to graph structure may result in premature exhaustion of available memory if memory reclamation schemes are lacking. Finally, a key component of the execution model, Active Global Address Space, that permits referencing the components of computation transparently to their physical location, has been implemented and tuned for performance. To increase the portability of the applied solutions, the XPI interface layer that abstracts the application programming constructs from the details of low level runtime system implementation, has also been developed.  The performance of the HPX code has been measured and verified using several irregular applications, including 1-D and 3-D Adaptive Mesh Refinement (AMR) simulations as well as alpha-beta search used to find the best next move in a game of chess, and FFT (Fast Fourier Transform) computation in which pre-sorting of input data is arranged in binary trees spanning multiple nodes. Two formal releases of the HPX library were developed during the final project reporting period: HPX v0.9.9 (released November 2014) and HPX v0.9.10 (released March 2015). Both versions are publicly available for download from github and HPX web site at LSU.   The project provided considerable training opportunities for students.  Besides the PI and two senior research staff at LSU, four graduate students have been employed throughout various stages of the project. They have been exposed to advanced concepts in High Performance Computing, such as alternative parallel execution models and runtime systems, and methodologies of supporting dynamic and unstructured problems on parallel architectures. Mastering these research concepts also develops marketable job skills and resulted in at least one case in employment of the participating student as a postdoctoral scholar at a national laboratory.  The collaboration with Indiana University and New Mexico State University yielded many improvements to the HPX code base while providing additional outreach opportunities. Specifically, the runtime code developed at LSU has been used as a non-trivial but accessible test bed for student experiments at NMSU oriented on performance analysis and tuning. The research results detailing impacts of various implementation strategies of runtime system internals on parallel application performance were disseminated through talks and publications in conference proceedings.          Last Modified: 09/21/2015       Submitted by: Thomas Sterling]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
