<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Emotional Speech Production: Analysis, Modeling and Synthesis</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Encoding of emotions in speech is achieved by vocal modulations that require an intricate control of human voicing and vocal tract articulation. The aim of this research is to identify and model articulatory processes for emotional speech production based on advanced speech production data acquisition technologies including Electromagnetic articulography (EMA) and a real-time Magnetic resonance imaging (rt-MRI). The research focuses on directly measuring and modeling articulatory kinematics and their interplay with prosodic modulation of pitch, loudness and segmental durations in speech emotion expression in order to understand the emotional speech production strategies across emotion types as well as across speakers. The validity of the emotional speech production models is verified by using a software articulatory synthesizer in an analysis-by-synthesis fashion. Theoretical implications of the findings are interpreted in relation to the Hyper and Hypo theory and the Converter/Distributor (C/D) model of speech production.&lt;br/&gt;&lt;br/&gt;Detailed knowledge on the effects of emotion on the human speech articulatory and prosodic patterning has transformative potential in developing improved speech processing technologies for emotional speech recognition and synthesis that are critical for the development of natural and robust man-machine interfaces. This goal also critically includes informing quantitative assessment of expressive speech to characterize atypical or distressed vocal behavior in diverse populations, for instance, children with Autism Spectrum Disorder (ASD). Finally, a natural by-product of this research effort is the unique articulatory database that will be shared freely with the community for further expansion of the knowledge of human speech production.</AbstractNarration>
<MinAmdLetterDate>07/29/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/28/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1116076</AwardID>
<Investigator>
<FirstName>Shrikanth</FirstName>
<LastName>Narayanan</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shrikanth S Narayanan</PI_FULL_NAME>
<EmailAddress>shri@sipi.usc.edu</EmailAddress>
<PI_PHON>2137406432</PI_PHON>
<NSF_ID>000377152</NSF_ID>
<StartDate>07/29/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sungbok</FirstName>
<LastName>Lee</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sungbok Lee</PI_FULL_NAME>
<EmailAddress>sungbokl@usc.edu</EmailAddress>
<PI_PHON>2137407762</PI_PHON>
<NSF_ID>000518868</NSF_ID>
<StartDate>07/29/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890001</ZipCode>
<StreetAddress><![CDATA[University Park]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main goal of this project is to investigate the human speech production mechanism by which a set of categorical emotions are being encoded in parallel with the linguistic rendering for generating spoken utterances. For the purpose of the study two articulatory databases have been created in this project period using a real-time magnetic resonance imaging technology (rt-MRI, 10 subjects, <a href="http://sail.usc.edu/span/usc-emo-mri/index.html">http://sail.usc.edu/span/usc-emo-mri/index.html</a> ) and the electromagnetic articulography (EMA, 4 subjects, <a href="http://sail.usc.edu/ema_web/ema_release.htm">http://sail.usc.edu/ema_web/ema_release.htm</a> ). They are available for public use through the aforementioned two websites, also with emotion evaluation data by human listeners and a Matlab-based MRI vocal tract segmentation software. It has been found that (1) Speakers may exploit the linguistic freedom of noncritical articulators for emotion encoding over the linguistic articulation, (2) Emotion encoding may change stiffness of active speech articulators and such a changes is mainly correlated with the activation, or the arousal, dimension of emotion, (3) Timing variability associated with emotion expression is distributed along the whole utterance interval but spatial variability is more or less localized in some specific locations in emotional utterances, (4) Pitch (F0) change in emotional speech is correlated more with the articulatory movement strength (i.e., movement range and speed) in the high arousal emotion (e.g., hot-anger) than in the high valence emotional speech (e.g., happy), and (5) Emotion expression in the valance dimension shows a more independent control of the F0 contour than in the arousal dimension. These findings can be integrated into a modeling framework of emotional speech production, including an implementation of articulatory synthesis of emotional speech, as a future extension of the current project. This project has produced nine conference papers, including one best student paper award, and two journal papers. They can be accessed from the website: <a href="http://sail.usc.edu/publications.php">http://sail.usc.edu/publications.php</a>.</p><br> <p>            Last Modified: 10/26/2016<br>      Modified by: Sungbok&nbsp;Lee</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main goal of this project is to investigate the human speech production mechanism by which a set of categorical emotions are being encoded in parallel with the linguistic rendering for generating spoken utterances. For the purpose of the study two articulatory databases have been created in this project period using a real-time magnetic resonance imaging technology (rt-MRI, 10 subjects, http://sail.usc.edu/span/usc-emo-mri/index.html ) and the electromagnetic articulography (EMA, 4 subjects, http://sail.usc.edu/ema_web/ema_release.htm ). They are available for public use through the aforementioned two websites, also with emotion evaluation data by human listeners and a Matlab-based MRI vocal tract segmentation software. It has been found that (1) Speakers may exploit the linguistic freedom of noncritical articulators for emotion encoding over the linguistic articulation, (2) Emotion encoding may change stiffness of active speech articulators and such a changes is mainly correlated with the activation, or the arousal, dimension of emotion, (3) Timing variability associated with emotion expression is distributed along the whole utterance interval but spatial variability is more or less localized in some specific locations in emotional utterances, (4) Pitch (F0) change in emotional speech is correlated more with the articulatory movement strength (i.e., movement range and speed) in the high arousal emotion (e.g., hot-anger) than in the high valence emotional speech (e.g., happy), and (5) Emotion expression in the valance dimension shows a more independent control of the F0 contour than in the arousal dimension. These findings can be integrated into a modeling framework of emotional speech production, including an implementation of articulatory synthesis of emotional speech, as a future extension of the current project. This project has produced nine conference papers, including one best student paper award, and two journal papers. They can be accessed from the website: http://sail.usc.edu/publications.php.       Last Modified: 10/26/2016       Submitted by: Sungbok Lee]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
