<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Ab Initio Models of Solar Activity</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2012</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>37434.00</AwardTotalIntnAmount>
<AwardAmount>37434</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Irene Qualters</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The scientific objectives of this research are to understand how internal solar magnetoconvection powers the Suns activity, and how that activity heats the chromosphere and corona and accelerates charged particles to relativistic energies.&lt;br/&gt;The approach uses three-dimensional, compressible, finite difference, magnetohydrodynamic (MHD) codes to solve the equations for mass, momentum and internal energy in conservative form plus the induction equation for the magnetic field. A particle in-cell plasma code is used to calculate charged particle acceleration and radiation in regions of reconnecting magnetic fields.  The project has requested and been granted a substantial allocation of computer resources on the leadership class Blue Waters system at NCSA.</AbstractNarration>
<MinAmdLetterDate>08/03/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/03/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1144506</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Stein</LastName>
<PI_MID_INIT>F</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert F Stein</PI_FULL_NAME>
<EmailAddress>stein@pa.msu.edu</EmailAddress>
<PI_PHON>5178845613</PI_PHON>
<NSF_ID>000166522</NSF_ID>
<StartDate>08/03/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>William</FirstName>
<LastName>Abbett</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William P Abbett</PI_FULL_NAME>
<EmailAddress>abbett@ssl.berkeley.edu</EmailAddress>
<PI_PHON>5106426880</PI_PHON>
<NSF_ID>000331859</NSF_ID>
<StartDate>08/03/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Bart</FirstName>
<LastName>De Pontieu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bart De Pontieu</PI_FULL_NAME>
<EmailAddress>bdp@lmsal.com</EmailAddress>
<PI_PHON>6504243094</PI_PHON>
<NSF_ID>000571294</NSF_ID>
<StartDate>08/03/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mats</FirstName>
<LastName>Carlsson</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Dr.</PI_SUFX_NAME>
<PI_FULL_NAME>Mats Carlsson</PI_FULL_NAME>
<EmailAddress>mats.carlsson@astro.uio.no</EmailAddress>
<PI_PHON>22856536</PI_PHON>
<NSF_ID>000059910</NSF_ID>
<StartDate>08/03/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Aake</FirstName>
<LastName>Nordlund</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Aake Nordlund</PI_FULL_NAME>
<EmailAddress>aake@nbi.dk</EmailAddress>
<PI_PHON>0045536213</PI_PHON>
<NSF_ID>000595720</NSF_ID>
<StartDate>08/03/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Michigan State University</Name>
<CityName>East Lansing</CityName>
<ZipCode>488242600</ZipCode>
<PhoneNumber>5173555040</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[426 Administration Bldg, Rm2]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>193247145</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MICHIGAN STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>053343976</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Michigan State University]]></Name>
<CityName>East Lansing</CityName>
<StateCode>MI</StateCode>
<ZipCode>488242600</ZipCode>
<StreetAddress><![CDATA[Office of Sponsored Programs]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7781</Code>
<Text>Leadership-Class Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7781</Code>
<Text>PETASCALE - TRACK 1</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~37434</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Attemping to run an existing radiation-magneto-hydrodynamic code on the massively parallel Blue Waters supercomputer showed us that a fundamentally new approach was needed to efficiently use it and the even larger exascale computers that will be available in the near future. &nbsp;To this end we started developing the DISPATCH computing framework.</p> <p>DISPATCH is a revolutionary new task-based, adaptive mesh refinement framework which overcomes the limitations of traditional methods using four novel concepts: (A) small (in index space), adaptively refined Cartesian patches. (B) Asynchronous evolution of these patches in time and space, (C) OpenMP task-based scheduling on each node, and (D) overlapping non-blocking point-to-point MPI communications between nodes with computations. Dispatch uses the existing stagger and other MHD solvers.</p> <p>DISPATCH geometrically sub-divides the simulation domain for the purpose of MPI decomposition. Each MPI rank then sub-divides itself into small curvilinear meshes (patches). By choosing small patch sizes (16^3 - 32^3 cells) one can achieve good cache performance on modern CPUs, take full advantage of the vectorization properties of these CPUs, and communicate full patches to other MPI processes in negligible time. Each patch is updated independently, once guard zone values have been loaded from neighboring patches. The maximum allowed time step is determined locally, with a Courant number (ratio of time step to time for sound waves to cross on grid cell) of about 0.2 suitable for the stagger MHD solver. Because of the locally determined time step, a number of time slices (typically 5) are saved in a rotating buffer, for use when interpolating guard zone values to the times required by neighboring patches. By letting the patches evolve asynchronously one avoids the problem with exceptional local time steps affecting the entire simulation. Coupled with task based scheduling, where each patch is a task, a local slowdown in one task only has a minor effect on simulation performance as a whole, and, contrary to conventional codes, the impact decreases further as the number of cores increases, thus leading to exceptional parallel scalability, expected to continue to millions of cores in the near-future exa-scale era. By having many geometrically small patches at the surface and fewer, geometrically larger patches in the interior, computing power is concentrated where it is most needed. A small fraction of the patches -- particularly above sunspots -- require very small time steps, but their number is not large enough to increase the overall update cost much. This is in contrast to traditional codes, where the smallest time step would be imposed on the entire simulation volume. In principle, patches cannot update until neighboring patches are updated, which could lead to a situation where only one patch (the oldest one) was eligible for update at any one time. &nbsp;To alleviate this constraint, a grace parameter is introduced, as the fraction of a time step one is allowed to extrapolate neighboring patch values in time, in order to fill the guard zones. A setting of 0.05 is typically sufficient to allow a large number of patches to update without delay.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/16/2016<br>      Modified by: Robert&nbsp;F&nbsp;Stein</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Attemping to run an existing radiation-magneto-hydrodynamic code on the massively parallel Blue Waters supercomputer showed us that a fundamentally new approach was needed to efficiently use it and the even larger exascale computers that will be available in the near future.  To this end we started developing the DISPATCH computing framework.  DISPATCH is a revolutionary new task-based, adaptive mesh refinement framework which overcomes the limitations of traditional methods using four novel concepts: (A) small (in index space), adaptively refined Cartesian patches. (B) Asynchronous evolution of these patches in time and space, (C) OpenMP task-based scheduling on each node, and (D) overlapping non-blocking point-to-point MPI communications between nodes with computations. Dispatch uses the existing stagger and other MHD solvers.  DISPATCH geometrically sub-divides the simulation domain for the purpose of MPI decomposition. Each MPI rank then sub-divides itself into small curvilinear meshes (patches). By choosing small patch sizes (16^3 - 32^3 cells) one can achieve good cache performance on modern CPUs, take full advantage of the vectorization properties of these CPUs, and communicate full patches to other MPI processes in negligible time. Each patch is updated independently, once guard zone values have been loaded from neighboring patches. The maximum allowed time step is determined locally, with a Courant number (ratio of time step to time for sound waves to cross on grid cell) of about 0.2 suitable for the stagger MHD solver. Because of the locally determined time step, a number of time slices (typically 5) are saved in a rotating buffer, for use when interpolating guard zone values to the times required by neighboring patches. By letting the patches evolve asynchronously one avoids the problem with exceptional local time steps affecting the entire simulation. Coupled with task based scheduling, where each patch is a task, a local slowdown in one task only has a minor effect on simulation performance as a whole, and, contrary to conventional codes, the impact decreases further as the number of cores increases, thus leading to exceptional parallel scalability, expected to continue to millions of cores in the near-future exa-scale era. By having many geometrically small patches at the surface and fewer, geometrically larger patches in the interior, computing power is concentrated where it is most needed. A small fraction of the patches -- particularly above sunspots -- require very small time steps, but their number is not large enough to increase the overall update cost much. This is in contrast to traditional codes, where the smallest time step would be imposed on the entire simulation volume. In principle, patches cannot update until neighboring patches are updated, which could lead to a situation where only one patch (the oldest one) was eligible for update at any one time.  To alleviate this constraint, a grace parameter is introduced, as the fraction of a time step one is allowed to extrapolate neighboring patch values in time, in order to fill the guard zones. A setting of 0.05 is typically sufficient to allow a large number of patches to update without delay.                   Last Modified: 11/16/2016       Submitted by: Robert F Stein]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
