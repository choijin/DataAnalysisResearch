<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Small: Fast Stagewise Learning of Sparse Hierarchical Data Representations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2011</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>372000.00</AwardTotalIntnAmount>
<AwardAmount>372000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Cozzens</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Increasingly, real-world data has many dimensions or features but rather than filling out all dimensions equally, the data distributed on or near a surface of much lower intrinsic dimension. Examples include fMRI data and natural image and video data sets.  This research will push the boundary of what is currently possible in the analysis of such large data sets by incorporating hierarchical structure into what is known as a "sparse dictionary representation" of the data.  The results will include both basic intellectual contributions to machine learning methods and computational advancements that will aid the investigation of complex real world data. &lt;br/&gt; &lt;br/&gt;A fundamental problem in learning the structure of complex data is how to effectively extract a set of features that reflects the underlying structure of the data. In many applications, including face recognition and object recognition, sparse dictionary representations have proved effective for this purpose. However, since solving large-scale sparse representation problems is very expensive, the method is generally limited to problems of moderate scale. This research reformulates the method into an incremental, multi-stage, hierarchical dictionary learning process. This approach incrementally extracts information from the data and uses this to refine the data representation in an organized hierarchical fashion. This enables the building of large-scale dictionaries in a computationally efficient way. The method hence extends the power of sparse dictionary representation methods to a wider variety of real world applications. It also has the flexibility to incorporate an existing state-of-the-art sparse coding algorithm as the basic solver and hence can extend the functionality of existing sparse coding algorithms to multi-stage, hierarchical dictionary learning.</AbstractNarration>
<MinAmdLetterDate>06/30/2011</MinAmdLetterDate>
<MaxAmdLetterDate>06/30/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1116208</AwardID>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Ramadge</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter J Ramadge</PI_FULL_NAME>
<EmailAddress>ramadge@princeton.edu</EmailAddress>
<PI_PHON>6092584645</PI_PHON>
<NSF_ID>000199906</NSF_ID>
<StartDate>06/30/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[Off. of Research &amp; Proj. Adm]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~372000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 1"> <div class="layoutArea"> <div class="column"> <p><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';">The sparse representation of data with respect to a dictionary of features has recently contributed to successful new methods in machine learning, pattern analysis and signal/image processing. At the heart of many sparse representation methods is a least squares problem </span><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';">called the lasso problem. It is common to encounter lasso problems with </span><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';">a large dictionary, data with large data dimension, and in dictionary learning, a large number of dictionary iterations. These factors can make solving the lasso problem a bottleneck in the overall computation.We have investigated structured apprpaoches to this challenge including</span><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';"> an approach known as (dictionary) screening. S</span><span style="font-size: 10.000000pt; font-family: 'CMBX10';">&nbsp;</span><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';">&nbsp;</span><span style="font-size: 10.000000pt; font-family: 'CMMI10';">&nbsp;</span><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';">creening quickly identifies a subset of features that is guaranteed to have zero weight in a sparse solution</span><span style="font-size: 10.000000pt; font-family: 'CMBX10';">&nbsp;</span><span style="font-size: 10.000000pt; font-family: 'CMR10';">&nbsp;</span><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';">. These features can be removed (or &ldquo;rejected&rdquo;) from the dictionary to form a smaller, more readily solved lasso problem. One then obtains a solution of the original problem. Screening has two potential benefits. First, it can be run in an on-line mode with very few features loaded into memory at a given time. By this means, screening can significantly reduce the size of the dictionary that needs to be loaded into memory in order to solve the lasso problem. Second, by quickly reducing the number of features we can often solve problems faster. </span></p> <div class="page" title="Page 17"> <div class="section"><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';">Our reseaerch strongly suggests that more complex screening tests are worthwhile. We have developed new tests that give significant performance improvement beyond existing tests in both rejection</span><span style="font-size: 10.000000pt; font-family: 'CMMI10';">&nbsp;</span><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';">. But </span><span style="font-size: 10.000000pt; font-family: 'CMMI10';">&nbsp;</span><span style="font-size: 7.000000pt; font-family: 'CMR7'; vertical-align: -1.000000pt;">&nbsp;</span><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';">a significant performance gap remains. To this end, we have developed a new sequential screening method called data adaptive sequential screening that offers at least an order of magnitude improvement.</span></div> <div class="section"><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';">In addtion to the above scientific contributions, this grant supported the education of several doctoral students who have since joined the US workforce.<br /></span></div> <div class="section"></div> </div> <p><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';"><br /></span></p> <div class="page" title="Page 1"> <div class="layoutArea"></div> </div> <p><span style="font-size: 10.000000pt; font-family: 'URWPalladioL';"> </span></p> </div> </div> </div><br> <p>            Last Modified: 05/11/2016<br>      Modified by: Peter&nbsp;J&nbsp;Ramadge</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    The sparse representation of data with respect to a dictionary of features has recently contributed to successful new methods in machine learning, pattern analysis and signal/image processing. At the heart of many sparse representation methods is a least squares problem called the lasso problem. It is common to encounter lasso problems with a large dictionary, data with large data dimension, and in dictionary learning, a large number of dictionary iterations. These factors can make solving the lasso problem a bottleneck in the overall computation.We have investigated structured apprpaoches to this challenge including an approach known as (dictionary) screening. S   creening quickly identifies a subset of features that is guaranteed to have zero weight in a sparse solution  . These features can be removed (or "rejected") from the dictionary to form a smaller, more readily solved lasso problem. One then obtains a solution of the original problem. Screening has two potential benefits. First, it can be run in an on-line mode with very few features loaded into memory at a given time. By this means, screening can significantly reduce the size of the dictionary that needs to be loaded into memory in order to solve the lasso problem. Second, by quickly reducing the number of features we can often solve problems faster.   Our reseaerch strongly suggests that more complex screening tests are worthwhile. We have developed new tests that give significant performance improvement beyond existing tests in both rejection . But   a significant performance gap remains. To this end, we have developed a new sequential screening method called data adaptive sequential screening that offers at least an order of magnitude improvement. In addtion to the above scientific contributions, this grant supported the education of several doctoral students who have since joined the US workforce.                      Last Modified: 05/11/2016       Submitted by: Peter J Ramadge]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
