<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: On the Optimal Rewards Problem</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>200000.00</AwardTotalIntnAmount>
<AwardAmount>200000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Todd Leen</SignBlockName>
<PO_EMAI>tleen@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The specification of goals in the form of utility or reward functions is a cornerstone of most approaches to developing autonomous artificial agents, and to understanding and shaping the behavior of natural biological agents - in fields ranging from control, AI, economics, psychology, and ethology. But in practice there are actually two notions of goals: the (human or evolutionary) designer's goals and the (artificial or natural) agent's goals. Should these be the same? The conventional (implicit) answer is "yes", but new work by the PIs shows that the answer may be "no" for computationally bounded agents. We define a new problem in agent design, the optimal rewards problem, whose solution is a reward function to assign to the agent so that in attempting to maximize its reward the agent best achieves the designer's goals.  This project explores optimal rewards, through computational experimentation and analysis, on two broad fronts. We will develop new principles and algorithms for bounded planning agents, demonstrating increased performance over using conventional rewards, even when taking into account the cost of finding the optimal rewards. We will make significant advances in two areas of behavioral economics and ethology: the understanding of subjective utility in humans, and the understanding of foraging behavior in animals, by using optimal reward theory to rigorously derive subjective reward functions that take into account the computational limits of the natural agents. The results of this work, disseminated through published theory and software, should lead to foundational changes in the way we understand and design incentive structures for humans, animals, and artificial agents, and thus to significant practical benefits for any methods in engineering, education, economics, and health that depend on finding good incentives for desired behavioral outcomes.</AbstractNarration>
<MinAmdLetterDate>08/26/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/29/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1148668</AwardID>
<Investigator>
<FirstName>Richard</FirstName>
<LastName>Lewis</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Richard L Lewis</PI_FULL_NAME>
<EmailAddress>rickl@umich.edu</EmailAddress>
<PI_PHON>7347631466</PI_PHON>
<NSF_ID>000484065</NSF_ID>
<StartDate>08/26/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Satinder</FirstName>
<LastName>Baveja</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Satinder S Baveja</PI_FULL_NAME>
<EmailAddress>baveja@umich.edu</EmailAddress>
<PI_PHON>7349362831</PI_PHON>
<NSF_ID>000100173</NSF_ID>
<StartDate>08/26/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Regents of the University of Michigan - Ann Arbor</Name>
<CityName>Ann Arbor</CityName>
<ZipCode>481091274</ZipCode>
<PhoneNumber>7347636438</PhoneNumber>
<StreetAddress>3003 South State St. Room 1062</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<StateCode>MI</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MI12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073133571</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MICHIGAN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073133571</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Michigan Ann Arbor]]></Name>
<CityName>Ann arbor</CityName>
<StateCode>MI</StateCode>
<ZipCode>481092121</ZipCode>
<StreetAddress><![CDATA[2260 hayward]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Michigan</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MI12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8052</Code>
<Text>Inter Com Sci Econ Soc S (ICE)</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~200000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1">For artificial agents to have autonomy their behavior must exhibit the sort of flexibility and adaptiveness exhibited by humans and other animals. One way to achieve this is for the human builders of such artificial agents to endow them with high-level goals as opposed to low-level detailed behaviors, provided these agents implement algorithms that can translate their high-level goals automatically into low-level behaviors that are adaptive to the specific circumstances of the agent. The field of Reinforcement Learning or RL offers a formal mathematical framework in which the high-level goals are represented as reward functions and provides a set of algorithms for translating reward functions into behavior when an RL-agent is placed in some environment. The research focus of this project was on the question: What reward functions should be given to an RL-agent? A starting point for the answer lies in an earlier optimization framework developed by the PIs in which the optimal reward function is one which when given to an RL-agent yields behavior that is optimal with respect to the human-designer&rsquo;s preferences over the agent&rsquo;s behavior.&nbsp;</p> <p class="p1">In this project, the PIs extended the optimal rewards framework in two essential ways. The first extension considered cases in which the RL-agent is to solve a sequence of tasks as expressed via a sequence of human-designer&rsquo;s reward functions. A major contribution was to show that a useful reward-mapping function that maps the human-designer&rsquo;s reward function to the agent&rsquo;s reward function can be learned from experience. The reward-mapping function is useful in that it is of benefit to the human-designer to use it to automatically set the RL-agent&rsquo;s reward function. The second extension considered multiagent settings in which two RL-agents have to cooperate and compete with each other. A major contribution was to show that automatically learned reward functions lead the two agent&rsquo;s to specialize or differentiate their skills (via learning different reward functions) when that is useful in their environment and to learn similar skills (via learning the same reward functions) when that is better in their environment.&nbsp;</p> <p class="p1">In summary, research supported by this project significantly expanded our understanding of how reward functions should be designed for autonomous artificial agents.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/02/2015<br>      Modified by: Satinder&nbsp;S&nbsp;Baveja</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[For artificial agents to have autonomy their behavior must exhibit the sort of flexibility and adaptiveness exhibited by humans and other animals. One way to achieve this is for the human builders of such artificial agents to endow them with high-level goals as opposed to low-level detailed behaviors, provided these agents implement algorithms that can translate their high-level goals automatically into low-level behaviors that are adaptive to the specific circumstances of the agent. The field of Reinforcement Learning or RL offers a formal mathematical framework in which the high-level goals are represented as reward functions and provides a set of algorithms for translating reward functions into behavior when an RL-agent is placed in some environment. The research focus of this project was on the question: What reward functions should be given to an RL-agent? A starting point for the answer lies in an earlier optimization framework developed by the PIs in which the optimal reward function is one which when given to an RL-agent yields behavior that is optimal with respect to the human-designerÆs preferences over the agentÆs behavior.  In this project, the PIs extended the optimal rewards framework in two essential ways. The first extension considered cases in which the RL-agent is to solve a sequence of tasks as expressed via a sequence of human-designerÆs reward functions. A major contribution was to show that a useful reward-mapping function that maps the human-designerÆs reward function to the agentÆs reward function can be learned from experience. The reward-mapping function is useful in that it is of benefit to the human-designer to use it to automatically set the RL-agentÆs reward function. The second extension considered multiagent settings in which two RL-agents have to cooperate and compete with each other. A major contribution was to show that automatically learned reward functions lead the two agentÆs to specialize or differentiate their skills (via learning different reward functions) when that is useful in their environment and to learn similar skills (via learning the same reward functions) when that is better in their environment.  In summary, research supported by this project significantly expanded our understanding of how reward functions should be designed for autonomous artificial agents.           Last Modified: 01/02/2015       Submitted by: Satinder S Baveja]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
