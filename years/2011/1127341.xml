<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SDCI Net: Collaborative Research:  An integrated study of datacenter networking and 100 GigE wide-area networking in support of distributed scientific computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>299679.00</AwardTotalIntnAmount>
<AwardAmount>299679</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As supercomputing speeds increase to peta- and exaflops, scientists are increasing their scale and range of simulations, which are resulting in ever growing datasets that need to be moved to local computers at the scientists' own laboratories. The first goal of this project is to identify bottlenecks that result in poor and/or inconsistent end-to-end application-level throughput using data collection and analysis by working in conjunction with scientists in the Community Earth System Model (CESM) project. With knowledge of the weakest components in the end-to-end chain, we plan to experiment in a controlled environment using a testbed that consists of a high-end cluster at NERSC, which is capable of sourcing/sinking data to disks at close to 100Gbps speeds, and other high-performance computing systems connected via the DOE 100Gbps Advanced Networking Initiative (ANI) prototype network. Multiple datacenter networking technologies such as Remote Direct Memory Access (RDMA) over Converged Ethernet (RoCE) and Internet Wide-Area RDMA Protocol (iWARP) will be combined with high- speed (100Gb/s) wide-area networking solutions, such as dedicated virtual circuits and IP-routed paths, respectively, for a comparative performance study of file transfers and wide-area MPI I/O. A new software module of the Extended-Sockets API (ES-API), which offers RDMA features such as zero-copy operations, will be prototyped and integrated into file transfer applications. Finally, trials will be organized to transfer the best identified solutions to CESM and other scientists. The intellectual merit of the proposed project consists of: i) a systematic scientific approach to determine the reasons for poor end-to-end application-level performance experienced by CESM scientists, ii) development of integrated datacenter and wide-area networking solutions to address the identified problems, and iii) the enabling of these solutions to be utilized by CESM and other science projects. The broader impacts of the proposed activities consist of i) the creation of a course module on datacenter networking, and the involvement of undergraduate students in this research at all three institutions, ii) diversity and outreach programs, and iii) the active promotion of the developed solutions to the CESM project and other scientists.</AbstractNarration>
<MinAmdLetterDate>09/07/2011</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1127341</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Dennis</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John M Dennis</PI_FULL_NAME>
<EmailAddress>dennis@ucar.edu</EmailAddress>
<PI_PHON>3034971809</PI_PHON>
<NSF_ID>000414921</NSF_ID>
<StartDate>09/07/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University Corporation For Atmospheric Res</Name>
<CityName>Boulder</CityName>
<ZipCode>803012252</ZipCode>
<PhoneNumber>3034971000</PhoneNumber>
<StreetAddress>3090 Center Green Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>078339587</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY CORPORATION FOR ATMOSPHERIC RESEARCH</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>078339587</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[National Center for Atmospheric  Research]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803073000</ZipCode>
<StreetAddress><![CDATA[PO Box 3000]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7683</Code>
<Text>SOFTWARE DEVELOPEMENT FOR CI</Text>
</ProgramElement>
<ProgramReference>
<Code>7683</Code>
<Text>SOFTWARE DEVELOPEMENT FOR CI</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~299679</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The major goals of this project are: to understand the reasons for poor end-to-end application performance, and to evaluate the use of intra and wide-area networking solutions to improve performance of scientific MPI applications and large data transfers. This project is a collaborative effort with Malathi Veeraraghavan of University of Virginia and Robert Russell of the University of New Hampshire.&nbsp; A significant motivation for our research agenda revolved around understanding performance pathologies observed on Yellowstone a very large Linux cluster installed at the National Center for Atmospheric Research (NCAR).&nbsp; Specifically factors of 6 x slowdown in the time it takes to pass data between different processes of the Community Earth System Model (CESM) were observed.&nbsp; We explored several causes of these slowdowns including manufacturing defects in a batch of network cables, congestion in the network and operating system interference.&nbsp; During this project, we developed a number of techniques including the use of software developed at the Barcelona Supercomputing Center which allow us to better understanding the source and nature of variation in network performance.&nbsp;</p> <p>During the course of this project we also greatly improved the flexibility of the communication library used by the High Order Methods Modeling Environment (HOMME) a computational module used by CESM.&nbsp; In particular, we implemented new features of the Message Passing Interface standard version 3 (MPI-3) within HOMME.&nbsp; Frequently, supercomputer vendors will not spend much time optimizing features of standards that are not used by the application community.&nbsp; Further, the application community will not implement new features that are not well optimized.&nbsp; This creates a chicken-and-the-egg issue that stifles the impact that new standards might have on the broader high-performance computing community.&nbsp;&nbsp; We hope to break this cycle by implementing several new features into HOMME an important benchmark used by both the National Science Foundation and Department of Energy computing community.</p> <p>A goal of this project was to reduce the cost to execute earth system models.&nbsp; Even modest reductions in computational cost of earth system models can have a significant impact on infrastructure costs, or free up computer cycles for other computational campaigns.&nbsp; Innovations developed during this project will increase the amount of science that can be done on current and future generation NCAR systems by greater then 1%.&nbsp;</p> <p>This project supported access to a 10-week summer internship program for two undergraduate students and one graduate student hosted by NCAR.&nbsp; This SIParCS program (https://www2.cisl.ucar.edu/siparcs) provides significant research opportunities in parallel computational science.&nbsp; It was through this internship opportunity that a student from a Minority Serving Institution (MSI) was inspired to apply to and complete a graduate degree in Electrical and Computer Engineering.&nbsp;</p><br> <p>            Last Modified: 12/12/2016<br>      Modified by: John&nbsp;M&nbsp;Dennis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The major goals of this project are: to understand the reasons for poor end-to-end application performance, and to evaluate the use of intra and wide-area networking solutions to improve performance of scientific MPI applications and large data transfers. This project is a collaborative effort with Malathi Veeraraghavan of University of Virginia and Robert Russell of the University of New Hampshire.  A significant motivation for our research agenda revolved around understanding performance pathologies observed on Yellowstone a very large Linux cluster installed at the National Center for Atmospheric Research (NCAR).  Specifically factors of 6 x slowdown in the time it takes to pass data between different processes of the Community Earth System Model (CESM) were observed.  We explored several causes of these slowdowns including manufacturing defects in a batch of network cables, congestion in the network and operating system interference.  During this project, we developed a number of techniques including the use of software developed at the Barcelona Supercomputing Center which allow us to better understanding the source and nature of variation in network performance.   During the course of this project we also greatly improved the flexibility of the communication library used by the High Order Methods Modeling Environment (HOMME) a computational module used by CESM.  In particular, we implemented new features of the Message Passing Interface standard version 3 (MPI-3) within HOMME.  Frequently, supercomputer vendors will not spend much time optimizing features of standards that are not used by the application community.  Further, the application community will not implement new features that are not well optimized.  This creates a chicken-and-the-egg issue that stifles the impact that new standards might have on the broader high-performance computing community.   We hope to break this cycle by implementing several new features into HOMME an important benchmark used by both the National Science Foundation and Department of Energy computing community.  A goal of this project was to reduce the cost to execute earth system models.  Even modest reductions in computational cost of earth system models can have a significant impact on infrastructure costs, or free up computer cycles for other computational campaigns.  Innovations developed during this project will increase the amount of science that can be done on current and future generation NCAR systems by greater then 1%.   This project supported access to a 10-week summer internship program for two undergraduate students and one graduate student hosted by NCAR.  This SIParCS program (https://www2.cisl.ucar.edu/siparcs) provides significant research opportunities in parallel computational science.  It was through this internship opportunity that a student from a Minority Serving Institution (MSI) was inspired to apply to and complete a graduate degree in Electrical and Computer Engineering.        Last Modified: 12/12/2016       Submitted by: John M Dennis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
