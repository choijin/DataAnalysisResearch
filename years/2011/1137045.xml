<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: 100G Connectivity for Data-Intensive Computing at JHU</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2012</AwardEffectiveDate>
<AwardExpirationDate>12/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>961263.00</AwardTotalIntnAmount>
<AwardAmount>961263</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The project establishes 100G connectivity to the Johns Hopkins University in support of data intensive science. Recently, the NSF has funded several regional centers to bring their connection speed to Internet2 and the NLR to 100G. In Oct 2011 DOE and Internet 2 have announced the world's first transcontinental deployment of a 100G network based on coherent technology. Connections are now operational between (among others) New York, Washington DC, Chicago, and Sunnyvale. This comprehensive transformation includes the MidAtlantic Crossroads (MAX) that provides the JHU connectivity to Internet2.&lt;br/&gt;&lt;br/&gt;Johns Hopkins is also in the process of establishing its own high-speed data overlay research network across the University, connecting 6 locations across the university with multiple 10G connections. These would be aggregated into a single 100G outgoing path to MAX and beyond: to the Teragrid, Internet2, and other national resources supporting computational science.&lt;br/&gt;&lt;br/&gt;JHU has been awarded an NSF MRI grant to build the 5PB Data-Scope, a novel instrument to observe large data sets. The system will not only have large storage, but it will have extreme IO bandwidth (450GBps) and GPU based processing capability (~200TF), aimed at highly data-intensive analyses simply not possible anywhere else today. Data sets to be analyzed include very large simulations from astrophysics (&gt;1PB), ocean circulation models (600TB), computational fluid dynamics (300TB), bioinformatics and neuroscience (2-300TB each). The main difficulty in these challenges is how to bring the data sets to the Data-Scope -- most of these are generated externally, like on the Teragrid, or at the Oak Ridge Jaguar and Kraken systems.&lt;br/&gt;&lt;br/&gt;These amounts of data are pushing the limits of even a 10G connection.  Demonstrating the ability to move PB of data and analyzing them in a timely fashion would encourage others to follow, and would change the way large data problems are approached in all areas of science today. It would also connect the Data-Scope to the whole US community, who can explore how to use such an instrument. It can also serve as a local hub for aggregating data sets for fast uploads into other data intensive national facilities, like the Gordon system in San Diego and the Open Science Data Grid, centered in Chicago. Without the high speed network it will be quite challenging to move petabytes: transferring a 1PB data set at an effective 5Gbps would take 18.5 days. Moving data at 100Gbps would shrink the time to 1 day, a make or break difference for a PB.&lt;br/&gt;&lt;br/&gt;In order to upgrade to 100G the award activities provide end equipment at MAX, the corresponding optics at JHU, Cisco optics, the addition of a 100G capable card to the Cisco Nexus 7K switch at the JHU side, and a 6x40G-based card for the Nexus 7K to link to the internal JHU research network hubs around the University, where data-intensive resources (clusters, instruments, storage) are located. This strategy would enable the delivery of large amounts of data to the most critical locations at JHU.&lt;br/&gt;&lt;br/&gt;The proposed high-speed connectivity would enable JHU and its partners to move petabyte-scale data sets, and enable a wide community to tackle cutting edge problems, from the traditional HPC and CFD to people who study the connectivity of the Internet. Students and researchers could do research on topics involving very large datasets, a very current and timely area, and acquire data management and analysis skills on a scale previously unheard of in a university setting.</AbstractNarration>
<MinAmdLetterDate>10/27/2011</MinAmdLetterDate>
<MaxAmdLetterDate>10/27/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1137045</AwardID>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Szalay</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander S Szalay</PI_FULL_NAME>
<EmailAddress>aszalay1@jhu.edu</EmailAddress>
<PI_PHON>4105167217</PI_PHON>
<NSF_ID>000472256</NSF_ID>
<StartDate>10/27/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mark</FirstName>
<LastName>Robbins</LastName>
<PI_MID_INIT>O</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mark O Robbins</PI_FULL_NAME>
<EmailAddress>mr@pha.jhu.edu</EmailAddress>
<PI_PHON>4105167204</PI_PHON>
<NSF_ID>000093379</NSF_ID>
<StartDate>10/27/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jonathan</FirstName>
<LastName>Bagger</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jonathan A Bagger</PI_FULL_NAME>
<EmailAddress>bagger@aps.org</EmailAddress>
<PI_PHON>3012093200</PI_PHON>
<NSF_ID>000194302</NSF_ID>
<StartDate>10/27/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Andreas</FirstName>
<LastName>Terzis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andreas Terzis</PI_FULL_NAME>
<EmailAddress>terzis@cs.jhu.edu</EmailAddress>
<PI_PHON>4105165847</PI_PHON>
<NSF_ID>000487920</NSF_ID>
<StartDate>10/27/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Dean</FirstName>
<LastName>Zarriello</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dean Zarriello</PI_FULL_NAME>
<EmailAddress>deanz@jhu.edu</EmailAddress>
<PI_PHON>4105168668</PI_PHON>
<NSF_ID>000590877</NSF_ID>
<StartDate>10/27/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<StreetAddress2><![CDATA[Suite B001]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001910777</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>JOHNS HOPKINS UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001910777</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Johns Hopkins University]]></Name>
<CityName>Baltimore</CityName>
<StateCode>MD</StateCode>
<ZipCode>212182683</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7684</Code>
<Text>CESER-Cyberinfrastructure for</Text>
</ProgramElement>
<ProgramReference>
<Code>7684</Code>
<Text>STRATEGIC TECHNOLOGIES FOR CI</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~961263</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project developed an early prototype of a cyberinfrastructure system characterized by tightly coupled integration between high performance compute systems, big data sources, and high-speed network infrastructures.&nbsp; This effort was motivated by scientific use cases from astrophysics, ocean circulation models, computational fluid dynamics, bioinformatics, and others.&nbsp;</p> <p>The prototype development focused on the establishment of 100 Gigabit/s connectivity to the Data-Scope (http://idies.jhu.edu/resources/datascope) cluster at the Johns Hopkins University (JHU).&nbsp; The 6.5PB Data-Scope is a novel instrument to &lsquo;observe&rsquo; large data sets. The system has large storage, extreme IO bandwidth (450GBps), and GPU-based processing capability (~200TF), aimed at highly data intensive analyses which are simply not possible anywhere else today.&nbsp; Data sets to be analyzed include very large simulations from astrophysics (&gt;1PB), ocean circulation models (600TB), computational fluid dynamics (300TB), bioinformatics and neuroscience (2-300TB each).&nbsp; The difficulty in bringing data to this cluster was identified a primary limitation with respect to expanding the utility of this resource across a broader range of distributed scientific domain researchers, data, and applications.&nbsp; This project was motivated by a desire to solve this problem and to develop a general architecture that could be replicated at other facilities within the Research and Education (R&amp;E) space.&nbsp; The high level objective was to enhance the ability to bring data and computer resources together in the most flexible and application specific advantageous manner possible.</p> <p>The University of Maryland (UMD) Mid-Atlantic Crossroads (MAX) (www.maxgigapop.net) designed and deployed the external networking for the Data-Scope facility.&nbsp; MAX is regional network in the Washington D.C. and Baltimore metropolitan areas, which provides high performance network services to the R&amp;E community.&nbsp;&nbsp; For this project, MAX provided a 100 Gigabit/s Dense Wave Division Wavelength (DWDM) service directly to the Data-Scope facility in Baltimore, Maryland.&nbsp; This provided a high-speed gateway to external resources located across the national and global R&amp;E infrastructure including Internet2 (www.internet2.edu) and ESnet (www.es.net).&nbsp;</p> <p>The key challenges for this project included both the design and construction of the cyberinfrastructure and adaptations of the domain science applications to optimize use of the increased data and higher speed access.&nbsp; The cyberinfrastructure challenges revolved around designing methods for &ldquo;frictionless&rdquo; data flow, i.e, performance limiting firewall bypass, between distributed data repositories and computational systems.&nbsp; The solution here leveraged the emerging ScienceDMZ architecture where authorized flows are diverted around firewalls and allowed to progress unimpeded to the destination resource.&nbsp; From a domain science workflow and data analysis algorithm perspective, we found that applications were not always able to directly utilize the increased data volume and network speed.&nbsp; We found that it is important to use large files in science data sets for ideal data transfer throughput. In many cases, it is important to incorporate integrity checks such as strong checksums throughout a data set&rsquo;s lifecycle.&nbsp; We translated our practical lessons learned with the Data-Scope, and other science projects into best practices and end-to-end methodologies.&nbsp; For example, one might generate strong checksums for an important simulation result in parallel as part of the workflow, while still on the supercomputer, rather than by hand after the fact.</p> <p>Overall, the combination of high-speed research networks, robust and user-friendly data movement software like Globus, dedi...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project developed an early prototype of a cyberinfrastructure system characterized by tightly coupled integration between high performance compute systems, big data sources, and high-speed network infrastructures.  This effort was motivated by scientific use cases from astrophysics, ocean circulation models, computational fluid dynamics, bioinformatics, and others.   The prototype development focused on the establishment of 100 Gigabit/s connectivity to the Data-Scope (http://idies.jhu.edu/resources/datascope) cluster at the Johns Hopkins University (JHU).  The 6.5PB Data-Scope is a novel instrument to æobserveÆ large data sets. The system has large storage, extreme IO bandwidth (450GBps), and GPU-based processing capability (~200TF), aimed at highly data intensive analyses which are simply not possible anywhere else today.  Data sets to be analyzed include very large simulations from astrophysics (&gt;1PB), ocean circulation models (600TB), computational fluid dynamics (300TB), bioinformatics and neuroscience (2-300TB each).  The difficulty in bringing data to this cluster was identified a primary limitation with respect to expanding the utility of this resource across a broader range of distributed scientific domain researchers, data, and applications.  This project was motivated by a desire to solve this problem and to develop a general architecture that could be replicated at other facilities within the Research and Education (R&amp;E) space.  The high level objective was to enhance the ability to bring data and computer resources together in the most flexible and application specific advantageous manner possible.  The University of Maryland (UMD) Mid-Atlantic Crossroads (MAX) (www.maxgigapop.net) designed and deployed the external networking for the Data-Scope facility.  MAX is regional network in the Washington D.C. and Baltimore metropolitan areas, which provides high performance network services to the R&amp;E community.   For this project, MAX provided a 100 Gigabit/s Dense Wave Division Wavelength (DWDM) service directly to the Data-Scope facility in Baltimore, Maryland.  This provided a high-speed gateway to external resources located across the national and global R&amp;E infrastructure including Internet2 (www.internet2.edu) and ESnet (www.es.net).   The key challenges for this project included both the design and construction of the cyberinfrastructure and adaptations of the domain science applications to optimize use of the increased data and higher speed access.  The cyberinfrastructure challenges revolved around designing methods for "frictionless" data flow, i.e, performance limiting firewall bypass, between distributed data repositories and computational systems.  The solution here leveraged the emerging ScienceDMZ architecture where authorized flows are diverted around firewalls and allowed to progress unimpeded to the destination resource.  From a domain science workflow and data analysis algorithm perspective, we found that applications were not always able to directly utilize the increased data volume and network speed.  We found that it is important to use large files in science data sets for ideal data transfer throughput. In many cases, it is important to incorporate integrity checks such as strong checksums throughout a data setÆs lifecycle.  We translated our practical lessons learned with the Data-Scope, and other science projects into best practices and end-to-end methodologies.  For example, one might generate strong checksums for an important simulation result in parallel as part of the workflow, while still on the supercomputer, rather than by hand after the fact.  Overall, the combination of high-speed research networks, robust and user-friendly data movement software like Globus, dedicated high performance data-transfer nodes (DTNs), and vibrant knowledge sharing among research networking architects has brought routine, real-world use of 100G-class research networks within reach of many...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
