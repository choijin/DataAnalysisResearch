<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Synthesizing Human-Readable Documentation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>470955.00</AwardTotalIntnAmount>
<AwardAmount>470955</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Developing and maintaining software is a key challenge in computer science,&lt;br/&gt;with failures costing up to one half of one percent of the US GDP each&lt;br/&gt;year. Most code is retained and evolved, rather than created from scratch,&lt;br/&gt;and professional software developers spend over three-fourths of their time&lt;br/&gt;trying to understand existing code. Understandability and documentation&lt;br/&gt;have become key components of software quality, yet they remain poorly&lt;br/&gt;understood by both researchers and practitioners. In a future where the&lt;br/&gt;software engineering focus shifts from implementation to design and&lt;br/&gt;composition concerns, program understandability will become even more&lt;br/&gt;important. This research develops tools and techniques for mechanically&lt;br/&gt;generating documentation to help make programs easier to understand.&lt;br/&gt;&lt;br/&gt;The research follows the insight that modern analysis techniques can form&lt;br/&gt;rich descriptive models of programs that are both precise and succinct.&lt;br/&gt;Human-readable documentation can then be synthesized from such models.&lt;br/&gt;The approach applies to large programs across multiple application domains.&lt;br/&gt;The research focuses on documenting how code should be used correctly, a&lt;br/&gt;critical aspect in an era of components-of-the-shelf development, as well&lt;br/&gt;as documenting how code has changed and evolved over time, a key&lt;br/&gt;part of software maintenance. The research leverages program analysis&lt;br/&gt;techniques, machine learning, and textual synthesis, with results&lt;br/&gt;disseminated through academic publication; the education, training and&lt;br/&gt;mentoring of students; as well as freely-available, open-source tools.</AbstractNarration>
<MinAmdLetterDate>07/19/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/19/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1116289</AwardID>
<Investigator>
<FirstName>Westley</FirstName>
<LastName>Weimer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Westley Weimer</PI_FULL_NAME>
<EmailAddress>weimerw@umich.edu</EmailAddress>
<PI_PHON>7346159916</PI_PHON>
<NSF_ID>000205470</NSF_ID>
<StartDate>07/19/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Virginia Main Campus</Name>
<CityName>CHARLOTTESVILLE</CityName>
<ZipCode>229044195</ZipCode>
<PhoneNumber>4349244270</PhoneNumber>
<StreetAddress>P.O.  BOX 400195</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>VA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>065391526</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RECTOR &amp; VISITORS OF THE UNIVERSITY OF VIRGINIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>065391526</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Virginia Main Campus]]></Name>
<CityName>CHARLOTTESVILLE</CityName>
<StateCode>VA</StateCode>
<ZipCode>229044195</ZipCode>
<StreetAddress><![CDATA[P.O.  BOX 400195]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~470955</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project studied the how humans interact with the source code to computer programs. While the popular perception of programmers involve<em>s </em>writing new code, in practice programmers spend the majority of their time <em>reading</em> and understanding existing code. In addition, while the popular perception of computer programs is very mathematical, in practice they contain significant amounts of natural language in the form of <em>comments</em> or <em>documentation</em>. This project focused on understanding human judgments of code readability, as well as on improving readability, such as through the automatic creation of new documentation.</p> <p>With respect to code readability, this project consturcted, refined and made use of <em>formal models</em> to relate code features to human judgments. That is, in the same way that we can determine that one book is written at a fourth-grade level and another at a tenth-grade level, we can also determine the relative readability of sections of code. These formal models can be used in various ways: for example, defect density correlates with low readability, while more-readable tests can be favored during maintenance.</p> <p>Since readability is so crucial, this project also focused on improving the readability of software artifacts by automatically <em>constructing </em>human-readable documentation. We found that programmers often document both <em>what</em> code does (e.g., "I added safety checks to this array") and <em>why</em> certain decisions were made (e.g., "Management has placed a focus on security, and these array checks prevent certain vulnerabilities"). While synthesizin<em>g why</em> information remains future work, research associated with this award has successfully synthesized <em>what</em> information in multiple contexts. We can synthesize the conditions under which programs will behave unexpectedly or have errors (i.e., synthesize documentation related to exceptions), we can synthesize how best to use certain pieces of code (i.e., synthesize API usage examples), and we can even synthesize descriptions of what has changed in a program over time (i.e., synthesize version control commit messages).</p> <p>In the final year of this project we also investigated a novel use of<em> medical imaging</em> to understand what goes on in human brains as they read code. In a data-driven, exploratory study using functional magnetic resonance imaging we found patterns of neural activation vary between comprehending code and comprehending prose in natural language. Using formal modeling, we can distinguish between programming language and natural language tasks with 79% balanced accuracy. In the popular press, this sort of decoding result is known as a "mind reading" experiment: merely by examining patterns of brain activity we can correctly determine whether a participant was looking at code or prose four times out of five.</p> <p>In general, the research supported by this award was well-received by the academic community. For example, among the many resulting peer-reviewed publications produced, one received a Distinguished Paper Award from the Association for Computing Machinery.</p> <p>This work also supported the <em>mentoring </em>and education of a number of undergraduate and graduate students (leading to bachelor's, master's and Ph.D. degrees). Students supported by this award went on to both research labs (e.g., Apogee Research) and industrial practice (e.g., Google, Microsoft).</p><br> <p>            Last Modified: 11/14/2016<br>      Modified by: Westley&nbsp;Weimer</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project studied the how humans interact with the source code to computer programs. While the popular perception of programmers involves writing new code, in practice programmers spend the majority of their time reading and understanding existing code. In addition, while the popular perception of computer programs is very mathematical, in practice they contain significant amounts of natural language in the form of comments or documentation. This project focused on understanding human judgments of code readability, as well as on improving readability, such as through the automatic creation of new documentation.  With respect to code readability, this project consturcted, refined and made use of formal models to relate code features to human judgments. That is, in the same way that we can determine that one book is written at a fourth-grade level and another at a tenth-grade level, we can also determine the relative readability of sections of code. These formal models can be used in various ways: for example, defect density correlates with low readability, while more-readable tests can be favored during maintenance.  Since readability is so crucial, this project also focused on improving the readability of software artifacts by automatically constructing human-readable documentation. We found that programmers often document both what code does (e.g., "I added safety checks to this array") and why certain decisions were made (e.g., "Management has placed a focus on security, and these array checks prevent certain vulnerabilities"). While synthesizing why information remains future work, research associated with this award has successfully synthesized what information in multiple contexts. We can synthesize the conditions under which programs will behave unexpectedly or have errors (i.e., synthesize documentation related to exceptions), we can synthesize how best to use certain pieces of code (i.e., synthesize API usage examples), and we can even synthesize descriptions of what has changed in a program over time (i.e., synthesize version control commit messages).  In the final year of this project we also investigated a novel use of medical imaging to understand what goes on in human brains as they read code. In a data-driven, exploratory study using functional magnetic resonance imaging we found patterns of neural activation vary between comprehending code and comprehending prose in natural language. Using formal modeling, we can distinguish between programming language and natural language tasks with 79% balanced accuracy. In the popular press, this sort of decoding result is known as a "mind reading" experiment: merely by examining patterns of brain activity we can correctly determine whether a participant was looking at code or prose four times out of five.  In general, the research supported by this award was well-received by the academic community. For example, among the many resulting peer-reviewed publications produced, one received a Distinguished Paper Award from the Association for Computing Machinery.  This work also supported the mentoring and education of a number of undergraduate and graduate students (leading to bachelor's, master's and Ph.D. degrees). Students supported by this award went on to both research labs (e.g., Apogee Research) and industrial practice (e.g., Google, Microsoft).       Last Modified: 11/14/2016       Submitted by: Westley Weimer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
