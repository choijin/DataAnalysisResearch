<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Software and Hardware Integration with Feedback and Transparency for Many-Core Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>139846.00</AwardTotalIntnAmount>
<AwardAmount>139846</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Multi-core processors are becoming prevalent and provide means for continuing to increase our computational capacity. These emerging platforms with multiple processing cores on chip offer the capability to run multiple programs simultaneously, thereby increasing the processing power of computing systems. However, since the running programs often share multi-core resources, execution interference effects between such programs can hurt their performance. Often times, software obliviousness to this underlying hardware behavior exacerbate the multi-core performance problem. Such adverse effects limit our computational progress in areas where interference can be detrimental such as high-performance computing and cloud computing, and could further complicate the predictability and deployment of these advanced processors in mission-critical domains such as avionics and automobiles. Therefore, understanding software behavior and Interference effects on multi-core processors is crucial to harnessing their full potential.&lt;br/&gt;&lt;br/&gt;In this research the investigators do preliminary studies of the issues arising from execution interference between multiple software threads on multi-core processor platforms. This research involves revisiting how hardware resource management can account for application-level constraints (such as performance isolation, fairness, and priority) by enhancing the interface between hardware and the Operating System (OS), and studying the hardware and software overheads. This research lays the groundwork for the hardware-OS interaction based on active fine-grained monitoring of resources and a two-way adaptation between both the hardware and OS to tightly control the effects of interference between threads.</AbstractNarration>
<MinAmdLetterDate>07/19/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/19/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1117243</AwardID>
<Investigator>
<FirstName>Gabriel</FirstName>
<LastName>Parmer</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gabriel A Parmer</PI_FULL_NAME>
<EmailAddress>gparmer@gwu.edu</EmailAddress>
<PI_PHON>2029949741</PI_PHON>
<NSF_ID>000560657</NSF_ID>
<StartDate>07/19/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Guru Prasadh</FirstName>
<LastName>Venkataramani</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Guru Prasadh V Venkataramani</PI_FULL_NAME>
<EmailAddress>guruv@gwu.edu</EmailAddress>
<PI_PHON>2029942980</PI_PHON>
<NSF_ID>000580837</NSF_ID>
<StartDate>07/19/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>George Washington University</Name>
<CityName>Washington</CityName>
<ZipCode>200520086</ZipCode>
<PhoneNumber>2029940728</PhoneNumber>
<StreetAddress>1922 F Street NW</StreetAddress>
<StreetAddress2><![CDATA[4th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<StateCode>DC</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DC00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043990498</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGE WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[George Washington University]]></Name>
<CityName>Washington</CityName>
<StateCode>DC</StateCode>
<ZipCode>200520086</ZipCode>
<StreetAddress><![CDATA[1922 F Street NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>District of Columbia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DC00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7329</Code>
<Text>COMPILERS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~139846</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>For decades now, the basic interaction between hardware and software has remained unchanged: the OS (operating systems) handles the execution of multiple applications, and assumes that smart compilers know how best to exploit hardware resources efficiently. It is often assumed that, as hardware capabilities increase -- with faster processors, and ever increasing numbers of computing cores -- applications will trivially take advantage of the turbo-charged hardware by merely choosing an application to execute (i.e. schedule it) and &ldquo;getting out of the way&rdquo;. Unfortunately, such an approach on modern-day computing platforms like multi-core chips, results in performance artifacts and degradations as applications on different cores interfere with each other and contend for shared hardware resources. &nbsp;It is essential to effectively harness the potential resulting from increasing core counts and continue the progress that computers have enjoyed over the past several decades. &nbsp;This requires a closer collaboration between software and hardware than has traditionally been customary.</p> <p><br />In this project, the Principal Investigators (PIs) and their research groups have performed preliminary studies to better understand execution interference between multiple software computations running on multi-core processor platforms, and have produced research publications that aim to bridge the significant gap between software and hardware.</p> <p><br />First, the PIs have investigated the impact of how execution on different cores impacts the system's consumption of power. Experimental studies show that even after balancing performance on various cores, the power consumption by the respective cores could still vary. Such variations are a result of differences in application code executing on the chip, execution time artifacts resulting from minor variations in the underlying core microarchitecture and so on. This disparity in power consumption both drains batteries faster and affects the processor's heat density -- making it hard to leverage the multi-core processors in most computing environments including mobile platforms. This result has a significant influence on how a programmer writes and tests their programs; in addition to making sure that programs are functionally correct, they must keep track of an application's power consumption behavior. &nbsp;This project has highlighted the need for understanding power in multi-core applications and their interactions with the processor micro-architecture.</p> <p><br />Second, the PIs have researched a novel software architecture that is capable of much finer-grained interactions with hardware. &nbsp;A typical software system is composed of code with large numbers of different patterns and means for using the hardware. &nbsp;The PI's architecture decouples software with different "hardware finger-prints" into disparate components, and opens the possibility for a better matching of components with specific requirements, to hardware that best accommodates those needs. &nbsp;Components, though beneficial for isolating software with specific requirements, also raise challenges that the PIs address. &nbsp;For example, to break software systems into components requires effective and efficient means for them to communicate with each other, a focus of this work. &nbsp;By creating a novel software architecture based on components, the PIs have enabled the potential for a more effective mapping of specialized software to comparably specialized hardware to effectively bridge the gap between hardware and software.</p> <p><br />The PIs have also integrated this research with educational initiatives by engaging students at The George Washington University in this research, and by integrating the insights regarding delicate interplay between software and hardware into the GWU undergraduate and graduate courses on computer ar...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ For decades now, the basic interaction between hardware and software has remained unchanged: the OS (operating systems) handles the execution of multiple applications, and assumes that smart compilers know how best to exploit hardware resources efficiently. It is often assumed that, as hardware capabilities increase -- with faster processors, and ever increasing numbers of computing cores -- applications will trivially take advantage of the turbo-charged hardware by merely choosing an application to execute (i.e. schedule it) and "getting out of the way". Unfortunately, such an approach on modern-day computing platforms like multi-core chips, results in performance artifacts and degradations as applications on different cores interfere with each other and contend for shared hardware resources.  It is essential to effectively harness the potential resulting from increasing core counts and continue the progress that computers have enjoyed over the past several decades.  This requires a closer collaboration between software and hardware than has traditionally been customary.   In this project, the Principal Investigators (PIs) and their research groups have performed preliminary studies to better understand execution interference between multiple software computations running on multi-core processor platforms, and have produced research publications that aim to bridge the significant gap between software and hardware.   First, the PIs have investigated the impact of how execution on different cores impacts the system's consumption of power. Experimental studies show that even after balancing performance on various cores, the power consumption by the respective cores could still vary. Such variations are a result of differences in application code executing on the chip, execution time artifacts resulting from minor variations in the underlying core microarchitecture and so on. This disparity in power consumption both drains batteries faster and affects the processor's heat density -- making it hard to leverage the multi-core processors in most computing environments including mobile platforms. This result has a significant influence on how a programmer writes and tests their programs; in addition to making sure that programs are functionally correct, they must keep track of an application's power consumption behavior.  This project has highlighted the need for understanding power in multi-core applications and their interactions with the processor micro-architecture.   Second, the PIs have researched a novel software architecture that is capable of much finer-grained interactions with hardware.  A typical software system is composed of code with large numbers of different patterns and means for using the hardware.  The PI's architecture decouples software with different "hardware finger-prints" into disparate components, and opens the possibility for a better matching of components with specific requirements, to hardware that best accommodates those needs.  Components, though beneficial for isolating software with specific requirements, also raise challenges that the PIs address.  For example, to break software systems into components requires effective and efficient means for them to communicate with each other, a focus of this work.  By creating a novel software architecture based on components, the PIs have enabled the potential for a more effective mapping of specialized software to comparably specialized hardware to effectively bridge the gap between hardware and software.   The PIs have also integrated this research with educational initiatives by engaging students at The George Washington University in this research, and by integrating the insights regarding delicate interplay between software and hardware into the GWU undergraduate and graduate courses on computer architecture and operating systems. Additionally, the PIs have mentored seniors in their capstone projects, efforts which have resulted in published research wi...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
