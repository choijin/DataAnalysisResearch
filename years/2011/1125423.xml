<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CDI-Type II: Acoustic Sensor Arrays for Understanding Bird Communication</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2011</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>2499673.00</AwardTotalIntnAmount>
<AwardAmount>2499673</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>CDI-Type II. Acoustic Sensor Arrays for Understanding Bird Communication&lt;br/&gt;&lt;br/&gt; &lt;br/&gt;The intent of this project is to permit humans to understand the grammar and meaning of bird songs.   Recent advances in sensor arrays, computation, and computational linguistics finally make this long-sought goal achievable.   The approach taken in this proposal is to: (1) collect  very large amounts of bird song recordings from acoustic sensor arrays in a variety of natural settings; (2) process the data by software, some of which is recent and some of which will be developed using new advances in localizing source with beamforming, then filtering out noise, identifying events of interest, and then classifying them according to species and individual, and combining that with behavioral observations; (3) this information/knowledge will then be stored in a large database that can be shared among the collaborating research groups;  and (4) it will be analyzed by computational-linguistic tools to identify the syntax of the songs,  and combined with information about the context in which it occurred, then analyzed by new software methods to identify the meaning of those songs.  The project  begins testing inferences from those inferences and explore consequences for individual and community ecology. &lt;br/&gt;&lt;br/&gt;The research will be transformational in several ways.  First, it will contribute to a profound transformation that is already underway: the recognition of very sophisticated signaling strategies and syntactic structures in non-human species.  The new tools and methods for collecting and analyzing bird song now allow a level of observation that previously would not have been possible. Scientists are now collecting truly  vast amounts of data from previously inaccessible settings and subjecting data to previously undiscovered sophisticated structural analyses. It will be transformational to computational linguistics if the natural world beyond humans were shown to have languages that are radically different from our own (as seems quite likely).  In addition, the project  will radically expand the range of engineering with voice recognition and classification, which so far has been restricted almost exclusively to humans.   &lt;br/&gt;&lt;br/&gt;  Other contributions will come from the database that will comprise huge amounts of data pertaining to bird songs and the environmental/behavioral context in which it occurs.  Offering both thematic and outreach contributions, the project will bring together people from engineering, ecology, linguistics and art -- and from the US, Mexico and Japan. The educational part  will bring together underserved K6-12 students with the research community and will involve them with well-established educational programs in engineering, biology and art | science. While the science portion of this project is high-payoff --- high-risk, the outreach portion will certainly be effective at furthering appreciation and learning of science.</AbstractNarration>
<MinAmdLetterDate>09/09/2011</MinAmdLetterDate>
<MaxAmdLetterDate>09/09/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1125423</AwardID>
<Investigator>
<FirstName>Kung</FirstName>
<LastName>Yao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kung Yao</PI_FULL_NAME>
<EmailAddress>yao@ee.ucla.edu</EmailAddress>
<PI_PHON>3102064304</PI_PHON>
<NSF_ID>000201422</NSF_ID>
<StartDate>09/09/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Martin</FirstName>
<LastName>Cody</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Martin L Cody</PI_FULL_NAME>
<EmailAddress>mlcody@ucla.edu</EmailAddress>
<PI_PHON>3108251327</PI_PHON>
<NSF_ID>000224027</NSF_ID>
<StartDate>09/09/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Charles</FirstName>
<LastName>Taylor</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Charles E Taylor</PI_FULL_NAME>
<EmailAddress>Taylor@biology.ucla.edu</EmailAddress>
<PI_PHON>3108256850</PI_PHON>
<NSF_ID>000335850</NSF_ID>
<StartDate>09/09/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Edward</FirstName>
<LastName>Stabler</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Edward P Stabler</PI_FULL_NAME>
<EmailAddress>stabler@ucla.edu</EmailAddress>
<PI_PHON>3102065743</PI_PHON>
<NSF_ID>000122789</NSF_ID>
<StartDate>09/09/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Abeer</FirstName>
<LastName>Alwan</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Abeer A Alwan</PI_FULL_NAME>
<EmailAddress>alwan@ee.ucla.edu</EmailAddress>
<PI_PHON>3102062231</PI_PHON>
<NSF_ID>000090848</NSF_ID>
<StartDate>09/09/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Los Angeles</Name>
<CityName>LOS ANGELES</CityName>
<ZipCode>900951406</ZipCode>
<PhoneNumber>3107940102</PhoneNumber>
<StreetAddress>10889 Wilshire Boulevard</StreetAddress>
<StreetAddress2><![CDATA[Suite 700]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA33</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>092530369</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, LOS ANGELES</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Los Angeles]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>900951606</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>33</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA33</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7751</Code>
<Text>CDI TYPE II</Text>
</ProgramElement>
<ProgramReference>
<Code>7721</Code>
<Text>FROM DATA TO KNOWLEDGE</Text>
</ProgramReference>
<ProgramReference>
<Code>7722</Code>
<Text>COMPLEXITY</Text>
</ProgramReference>
<ProgramReference>
<Code>7752</Code>
<Text>CDI NON SOLICITED RESEARCH</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~2499673</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Language is often called a window to the mind. The same might be said of bird songs -- that they are a window to the minds of birds. In the past few years, significant insights have come from linguistic analysis of bird vocalizations.&nbsp; Our research contributes to those insights.<br />&nbsp;&nbsp;&nbsp; Birds vocalize in two ways that are largely, if not always, distinct -- <em>calls</em> and <em>songs</em>. <em>Calls</em> are exemplified by the clucking of chickens, who have different types of clucks for aerial- or ground- predators, and&nbsp; distinct clucks for available food, etc. These calls are typically semantically rich, but lack sequential syntax -- e.g. "a-b" does not mean anything different from "b-a" or "b" and "a" alone. Bird <em>songs</em> are usually more melodic,&nbsp; often pleasant to human ears and sequentially organized in ways that do have syntactic structure. The songs have traditionally been thought to convey little meaning apart from territory defense and mate attraction. But such simplified interpretations are beginning to change. For example,&nbsp; we now know that birds can listen to other birds and classify them as conspecific or heterospecific, neighbor or stranger, mate or non-mate, kin or non-kin, etc.&nbsp; <br />&nbsp;&nbsp;&nbsp; Our research is directed at recording, analyzing and interpreting the songs of birds.&nbsp; We have focused&nbsp; on two species &mdash; California Thrashers (<em>Toxostoma redivivum</em>) and Cassin&rsquo;s Vireos (<em>Vireo cassinii</em>), two species with particularly complex songs.&nbsp; Our program has had 5 specific aims:<br /><br /><strong>Specific Aim 1:</strong> Develop sensor arrays that provide a 3D acoustic view of avian ecology. In earlier years we extended the ability to localize from 2D to 3D, but it was computationally demanding. We developed new algorithms that gave a roughly 10x speedup, with still better methods in the pipeine. We developed a way to use commercially available recorders for determining Direction of Arrival.&nbsp; We have also collaborated with Japanese developers of alternative methods that show promise.<br /><strong>Specific Aim 2:</strong> Develop an annotated database of recorded bird vocalizations. In earlier years of the project we developed such a database, made it publicly available online and published its description. <br /><strong>Specific Aim 3:</strong> Classification and syntactic analysis of recorded bird vocalizations. During earlier years of the project we published descriptions of the syntactic structure of the songs for Cassin's Vireo and California Thrashers. We described new methods for fitting the songs of Cassin's Vireos to mathematical models.&nbsp; We conducted playback experiments with songs judged syntactically correct or not to California Thrashers, and recorded the birds' responses in video and acoustic recordings. Even random song sequences did give some response but the response was greater when the playback syntax was correct. We infer that &ldquo;syntax matters&rdquo;, but is not all that matters. We developed and published several methods for automating birdsong recognition. The most recent of these has proven effective even with noisy backgrounds.<strong></strong></p> <p><strong>Specific Aim 4.</strong> Tie vocalization patterns to ecological and behavioral events in the birds&rsquo; world. The field of artificial intelligence has developed new ways to formally associate meaning with language. We have explored the use of one such method &mdash; Bayesian networks.&nbsp; A paper about semantic analysis of Cassin&rsquo;s Vireo songs using Bayesian Networks is currently in press. We have also had some success integrating sound and localization to develop a narrative from sequential ordering in the songs, using procedures from acoustic scene analysis.<strong></strong></p> <p><strong>Specific Aim 5.</strong> In collaboration with the Art | Science center at UCLA, the University of Tokyo, University of Tsukuba and Harvestworks in NY, we developed an installation relating bird vocalizations to the environment. This participatory artwork was first installed at the UCLA Broad Arts center Sculpture garden in 2014. The following two years, we participated in the New York Electronic Art Festival at Governor&rsquo;s Island. The most evolved version of the installation was in fully immersive Virtual Reality, shown at the&nbsp; University of Tsukuba (2016).&nbsp; Most recently Bird Song Diamond was featured at the 2017 Ars  Electronica&rsquo;s Deep 8K Space. This is the largest media arts festival in  the world, attended by 100,000 people from around the world. Some videos  from these installations are online at http://www.birdsongdiamond.com  .&nbsp;&nbsp;</p> <p>Bird Song Diamond was also integrated into the two-week NanoLab summer Institute for High School Students at the California NanoSystems Institute.</p> <p>We anticipate that the research we have engaged in will contribute to a profound transformation that is already underway: the recognition of very sophisticated signaling strategies and syntactic structures in nonhuman species. Further, the new tools and methods for collecting and analyzing bird song now allow a level of observation that previously would not have been possible.&nbsp; We will radically expand the range of engineering with voice recognition and classification, which so far has been restricted almost exclusively to humans. Finally, if we are successful with our outreach efforts, then involvement of artists with this project will greatly expand appreciation of those transformations.</p><br> <p>            Last Modified: 11/16/2017<br>      Modified by: Charles&nbsp;E&nbsp;Taylor</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Language is often called a window to the mind. The same might be said of bird songs -- that they are a window to the minds of birds. In the past few years, significant insights have come from linguistic analysis of bird vocalizations.  Our research contributes to those insights.     Birds vocalize in two ways that are largely, if not always, distinct -- calls and songs. Calls are exemplified by the clucking of chickens, who have different types of clucks for aerial- or ground- predators, and  distinct clucks for available food, etc. These calls are typically semantically rich, but lack sequential syntax -- e.g. "a-b" does not mean anything different from "b-a" or "b" and "a" alone. Bird songs are usually more melodic,  often pleasant to human ears and sequentially organized in ways that do have syntactic structure. The songs have traditionally been thought to convey little meaning apart from territory defense and mate attraction. But such simplified interpretations are beginning to change. For example,  we now know that birds can listen to other birds and classify them as conspecific or heterospecific, neighbor or stranger, mate or non-mate, kin or non-kin, etc.       Our research is directed at recording, analyzing and interpreting the songs of birds.  We have focused  on two species &mdash; California Thrashers (Toxostoma redivivum) and Cassin?s Vireos (Vireo cassinii), two species with particularly complex songs.  Our program has had 5 specific aims:  Specific Aim 1: Develop sensor arrays that provide a 3D acoustic view of avian ecology. In earlier years we extended the ability to localize from 2D to 3D, but it was computationally demanding. We developed new algorithms that gave a roughly 10x speedup, with still better methods in the pipeine. We developed a way to use commercially available recorders for determining Direction of Arrival.  We have also collaborated with Japanese developers of alternative methods that show promise. Specific Aim 2: Develop an annotated database of recorded bird vocalizations. In earlier years of the project we developed such a database, made it publicly available online and published its description.  Specific Aim 3: Classification and syntactic analysis of recorded bird vocalizations. During earlier years of the project we published descriptions of the syntactic structure of the songs for Cassin's Vireo and California Thrashers. We described new methods for fitting the songs of Cassin's Vireos to mathematical models.  We conducted playback experiments with songs judged syntactically correct or not to California Thrashers, and recorded the birds' responses in video and acoustic recordings. Even random song sequences did give some response but the response was greater when the playback syntax was correct. We infer that "syntax matters", but is not all that matters. We developed and published several methods for automating birdsong recognition. The most recent of these has proven effective even with noisy backgrounds.  Specific Aim 4. Tie vocalization patterns to ecological and behavioral events in the birds? world. The field of artificial intelligence has developed new ways to formally associate meaning with language. We have explored the use of one such method &mdash; Bayesian networks.  A paper about semantic analysis of Cassin?s Vireo songs using Bayesian Networks is currently in press. We have also had some success integrating sound and localization to develop a narrative from sequential ordering in the songs, using procedures from acoustic scene analysis.  Specific Aim 5. In collaboration with the Art | Science center at UCLA, the University of Tokyo, University of Tsukuba and Harvestworks in NY, we developed an installation relating bird vocalizations to the environment. This participatory artwork was first installed at the UCLA Broad Arts center Sculpture garden in 2014. The following two years, we participated in the New York Electronic Art Festival at Governor?s Island. The most evolved version of the installation was in fully immersive Virtual Reality, shown at the  University of Tsukuba (2016).  Most recently Bird Song Diamond was featured at the 2017 Ars  Electronica?s Deep 8K Space. This is the largest media arts festival in  the world, attended by 100,000 people from around the world. Some videos  from these installations are online at http://www.birdsongdiamond.com  .    Bird Song Diamond was also integrated into the two-week NanoLab summer Institute for High School Students at the California NanoSystems Institute.  We anticipate that the research we have engaged in will contribute to a profound transformation that is already underway: the recognition of very sophisticated signaling strategies and syntactic structures in nonhuman species. Further, the new tools and methods for collecting and analyzing bird song now allow a level of observation that previously would not have been possible.  We will radically expand the range of engineering with voice recognition and classification, which so far has been restricted almost exclusively to humans. Finally, if we are successful with our outreach efforts, then involvement of artists with this project will greatly expand appreciation of those transformations.       Last Modified: 11/16/2017       Submitted by: Charles E Taylor]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
