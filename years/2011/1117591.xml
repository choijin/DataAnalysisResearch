<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Ensemble Methods for Structured Prediction</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>407074.00</AwardTotalIntnAmount>
<AwardAmount>407074</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Weng-keen Wong</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Ensemble methods are general techniques in machine learning for combining several hypotheses to create a more accurate predictor.  In the batch learning setting, techniques such as bagging, boosting, stacking, error-correction techniques, Bayesian averaging, or other averaging schemes are common instances of these methods.  These methods often significantly improve performance in practice and often benefit from favorable learning guarantees, typically in terms of the margins of the training samples. &lt;br/&gt;&lt;br/&gt;However, ensemble methods and their theory have been developed primarily for the common binary classification problem, or standard regression tasks where the target labels are real numbers and thus have no structure. These techniques do not readily apply to structured prediction problems such as pronunciation modeling, speech recognition, parsing, machine translation, or image processing. The objective of this proposal is to create the theoretical foundation, large-scale algorithms, and practical techniques for devising effective ensembles of structured prediction techniques. The benefits of these algorithms are likely to be at least as significant as those resulting from ensemble techniques in binary classification.&lt;br/&gt;&lt;br/&gt;Our solutions will be crucial to a broad set of applications and will be made widely accessible through open-source software programs. These software and open-source programs will make the use of our learning algorithms accessible to a broad community of researchers and engineers.  More broadly, our techniques will benefit the society through the discovery of significantly more accurate solutions to a variety of important problems including speech recognition, speech synthesis, and machine translation.</AbstractNarration>
<MinAmdLetterDate>07/29/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/29/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1117591</AwardID>
<Investigator>
<FirstName>Mehryar</FirstName>
<LastName>Mohri</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mehryar Mohri</PI_FULL_NAME>
<EmailAddress>mohri@cims.nyu.edu</EmailAddress>
<PI_PHON>2129983200</PI_PHON>
<NSF_ID>000201880</NSF_ID>
<StartDate>07/29/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041968306</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEW YORK UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041968306</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[New York University]]></Name>
<CityName>NEW YORK</CityName>
<StateCode>NY</StateCode>
<ZipCode>100121019</ZipCode>
<StreetAddress><![CDATA[70 WASHINGTON SQUARE S]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~407074</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Most learning problems found in applications admit some structure. This includes natural language processing tasks such as part-of-speech (POS) tagging and parsing where the target label is a sequence of POS tags or a parse tree, which can be decomposed into subsequence tags or sub-trees. This covers many other natural language tasks such as pronunciation modeling, dependency parsing, named-entity recognition, speech recognition, speech synthesis, but also problems such as image annotation and many other image processing, computer vision problems, and computational biology problems.</p> <p><br />These learning problems are often difficult since the number of possible ways of labeling an input grows exponentially with the size of the input. &nbsp;For instance, the number of possible POS tags is exponential in the length of a sentence. Effective learning algorithms have been given in the past for many of these tasks. But, can one combine these algorithms to derive a more accurate algorithm? For simple binary classification problems, such combination or 'ensemble' solutions have been extensively studied in the past, but those solutions cannot be readily applied to the structured case.</p> <p><br />This project consisted of giving a theoretical, algorithmic, and empirical study of the problem of deriving an accurate ensemble of structured predictors. It has led to a series of significant results in machine learning. This includes new learning algorithms and theoretical guarantees for designing accurate ensembles of structured prediction tasks when the loss function can be decomposed additively with respect to the substructures: several randomized and deterministic algorithms devised by converting on-line learning algorithms to batch ones, and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels, a detailed study of all these algorithms, the description of new on-line-to-batch conversions and learning guarantees, as well as the results of extensive experiments with these algorithms in several structured prediction tasks showing significant improvements in accuracy.</p> <p><br />Our results also include a series of theoretical and algorithmic results in the case where the loss function is non-additive, which is the case of interest in a number of applications. A loss function based on variants of the edit-distance is a commonly used example of such losses. The project led to the design of on-line learning algorithms extending to non-additive losses the standard Follow-the-Perturbed-Leader (FPL) algorithm and Randomized Weighted Majority (RWM) algorithms. We further showed that these algorithms can play a critical role in improving performance in applications such as structured prediction.<br />These constitute significant advances in on-line learning broadening the applicability of online algorithms to critical applications such as machine translation, automatic speech recognition, and computational biology. The theoretical and algorithmic ensemble solutions can be used in a broad set of contexts and serve as the foundation for the exploration of other theoretical questions or the design of alternative algorithms.</p> <p><br />The methods and techniques we have devised apply to all structured prediction problems, including speech pronunciation problems, machine translation, parsing, etc., more generally, all sequence prediction problems in natural language processing and computational biology. As such, our findings are directly relevant and applicable to the NLP and bioinformatics areas. &nbsp;They can benefit the society through the discovery of significantly more accurate solutions to a variety of problems, including speech recognition, speech synthesis, and machine translation, and other related areas where sequential information is critical such as bioinformatics.</p> <p><br />The project also provided several research and training skills to the Research Assistants who worked on this project. It helped them familiarize themselves with several theoretical concepts and techniques in machine learning, in particular those related to on-line learning algorithm and theory, and to weighted automata theory and algorithms. &nbsp;It also helped them become familiar with the design of algorithms for weighted transducers and the software tools OpenFst, which is used in a variety of other applications.<br /><br /></p><br> <p>            Last Modified: 08/19/2016<br>      Modified by: Mehryar&nbsp;Mohri</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Most learning problems found in applications admit some structure. This includes natural language processing tasks such as part-of-speech (POS) tagging and parsing where the target label is a sequence of POS tags or a parse tree, which can be decomposed into subsequence tags or sub-trees. This covers many other natural language tasks such as pronunciation modeling, dependency parsing, named-entity recognition, speech recognition, speech synthesis, but also problems such as image annotation and many other image processing, computer vision problems, and computational biology problems.   These learning problems are often difficult since the number of possible ways of labeling an input grows exponentially with the size of the input.  For instance, the number of possible POS tags is exponential in the length of a sentence. Effective learning algorithms have been given in the past for many of these tasks. But, can one combine these algorithms to derive a more accurate algorithm? For simple binary classification problems, such combination or 'ensemble' solutions have been extensively studied in the past, but those solutions cannot be readily applied to the structured case.   This project consisted of giving a theoretical, algorithmic, and empirical study of the problem of deriving an accurate ensemble of structured predictors. It has led to a series of significant results in machine learning. This includes new learning algorithms and theoretical guarantees for designing accurate ensembles of structured prediction tasks when the loss function can be decomposed additively with respect to the substructures: several randomized and deterministic algorithms devised by converting on-line learning algorithms to batch ones, and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels, a detailed study of all these algorithms, the description of new on-line-to-batch conversions and learning guarantees, as well as the results of extensive experiments with these algorithms in several structured prediction tasks showing significant improvements in accuracy.   Our results also include a series of theoretical and algorithmic results in the case where the loss function is non-additive, which is the case of interest in a number of applications. A loss function based on variants of the edit-distance is a commonly used example of such losses. The project led to the design of on-line learning algorithms extending to non-additive losses the standard Follow-the-Perturbed-Leader (FPL) algorithm and Randomized Weighted Majority (RWM) algorithms. We further showed that these algorithms can play a critical role in improving performance in applications such as structured prediction. These constitute significant advances in on-line learning broadening the applicability of online algorithms to critical applications such as machine translation, automatic speech recognition, and computational biology. The theoretical and algorithmic ensemble solutions can be used in a broad set of contexts and serve as the foundation for the exploration of other theoretical questions or the design of alternative algorithms.   The methods and techniques we have devised apply to all structured prediction problems, including speech pronunciation problems, machine translation, parsing, etc., more generally, all sequence prediction problems in natural language processing and computational biology. As such, our findings are directly relevant and applicable to the NLP and bioinformatics areas.  They can benefit the society through the discovery of significantly more accurate solutions to a variety of problems, including speech recognition, speech synthesis, and machine translation, and other related areas where sequential information is critical such as bioinformatics.   The project also provided several research and training skills to the Research Assistants who worked on this project. It helped them familiarize themselves with several theoretical concepts and techniques in machine learning, in particular those related to on-line learning algorithm and theory, and to weighted automata theory and algorithms.  It also helped them become familiar with the design of algorithms for weighted transducers and the software tools OpenFst, which is used in a variety of other applications.         Last Modified: 08/19/2016       Submitted by: Mehryar Mohri]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
