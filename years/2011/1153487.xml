<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: SMALL: Statistical Linguistic Typology</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>365970.00</AwardTotalIntnAmount>
<AwardAmount>365970</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project considers the unification of two view of language: that&lt;br/&gt;from natural language processing and that from linguistic typology.&lt;br/&gt;Our view is that typological information is both useful for solving&lt;br/&gt;real-world natural language processing thats and automatically&lt;br/&gt;derivable from language data.  This research first explores how to use&lt;br/&gt;typological knowledge to improve performance on problems such as&lt;br/&gt;dependency parsing and machine translation for low density langauges.&lt;br/&gt;Intuitively, our statistical models waste time exploring a hypothesis&lt;br/&gt;space that is too big: the space of realistic grammars is much smaller&lt;br/&gt;than the space of all grammars.  The second part of this research&lt;br/&gt;considers the automatic acquisition and boostrapping of typological&lt;br/&gt;knowledge from raw text.  The outcome of this research is: (a)&lt;br/&gt;improved statistical models for hard natural language processing&lt;br/&gt;problems; and (b) a larger library of typological universals that have&lt;br/&gt;been derived automatically from data.  Our outcomes are empirically&lt;br/&gt;evaluated on the raw language processing tasks and in terms of the&lt;br/&gt;quality of the universal implications mined from data, but comparing&lt;br/&gt;them with known repositories of universals.  &lt;br/&gt;&lt;br/&gt;Our results will impact the fields of natural language processing and linguistics.  From the research side, this research will find applications in a wider variety of problems than the ones we intend to study; in particular, the use of linguistic universals in natural language processing technology&lt;br/&gt;will fundamentally change the way multilinguality is addressed in this&lt;br/&gt;field.  From a linguistics perspective, the goal of this project is to&lt;br/&gt;shed new light on linguistic universals.  This should impact not only&lt;br/&gt;the area of typology, but also the study and preservation of&lt;br/&gt;endangered languages.  By automatically identifying typological&lt;br/&gt;features and implications from data, the process of documenting&lt;br/&gt;endangered languages could be made more efficient: leading to a&lt;br/&gt;smaller loss of knowledge of these languages.</AbstractNarration>
<MinAmdLetterDate>08/21/2011</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1153487</AwardID>
<Investigator>
<FirstName>Hal</FirstName>
<LastName>Daume</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>III</PI_SUFX_NAME>
<PI_FULL_NAME>Hal Daume</PI_FULL_NAME>
<EmailAddress>hal@umiacs.umd.edu</EmailAddress>
<PI_PHON>8015567863</PI_PHON>
<NSF_ID>000445461</NSF_ID>
<StartDate>08/21/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~160298</FUND_OBLG>
<FUND_OBLG>2011~205672</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The linguistic knowledge used and inferred in natural language processing is typically language specific. Despite this, most language processing applications are built one language at a time. And yet, and entire branch of linguistics&mdash;typology&mdash;is devoted to understanding aspects of Language that transcend one particular language. The central goal of this project was to evaluate the hypothesis that typological information is: (1) useful for solving real-world natural language processing tasks and (2) automatically derivable from linguistic data. This project aimed to address both of these hypotheses simultaneously, sharing newly built knowledge between the two halves. One specific form of knowledge we have investigated is (probabilistic) linguistic universals. Such universals date back to their introduction in the mid 1960s by Greenberg. These constrain the space of possible languages (or, in our new statistical perspective, the space of probable languages). The Greenbergian model traditionally discusses concrete aspects of language, such as verb/object order, the presence of tone, or the lack of a dual. These principles are combined in a hierarchical fashion to describe possible languages (e.g., Subject-Object-Verb languages should have Relative Clauses before Nouns).<br /><br />Our first success was the development of a statistical model of linguistic typology. Our model was able to uncover both well-known typological universals, as well as propose others that, with the aid of a trained typologist, we determined were linguistically plausible. Proceeding from there, we developed monolingual and multi-lingual models for inducing syntactic categories of words using typological features. We showed that even without typological features, one can use sophisticated learning techniques to align words and documents across languages without any notion of cross-linguistic correspondence (as well as with some correspondence). Subsequently, we developed a typological model of phonological information in the context of name transliteration. We developed two types of models for accomplishing this: one based on finite state methods and one based on low dimensional projections.<br /><br />All of this work has brought the fields of linguistic typology and natural language processing closer together: in particular, the degree to which linguistic typology has been mentioned in ACL papers since our first published paper in 2007 has increased dramatically. On the other side, we have been invited to provide commentary in the journal Typology for recently proposed statistical typological work by others, published in Nature.</p><br> <p>            Last Modified: 03/03/2014<br>      Modified by: Hal&nbsp;Daume</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The linguistic knowledge used and inferred in natural language processing is typically language specific. Despite this, most language processing applications are built one language at a time. And yet, and entire branch of linguistics&mdash;typology&mdash;is devoted to understanding aspects of Language that transcend one particular language. The central goal of this project was to evaluate the hypothesis that typological information is: (1) useful for solving real-world natural language processing tasks and (2) automatically derivable from linguistic data. This project aimed to address both of these hypotheses simultaneously, sharing newly built knowledge between the two halves. One specific form of knowledge we have investigated is (probabilistic) linguistic universals. Such universals date back to their introduction in the mid 1960s by Greenberg. These constrain the space of possible languages (or, in our new statistical perspective, the space of probable languages). The Greenbergian model traditionally discusses concrete aspects of language, such as verb/object order, the presence of tone, or the lack of a dual. These principles are combined in a hierarchical fashion to describe possible languages (e.g., Subject-Object-Verb languages should have Relative Clauses before Nouns).  Our first success was the development of a statistical model of linguistic typology. Our model was able to uncover both well-known typological universals, as well as propose others that, with the aid of a trained typologist, we determined were linguistically plausible. Proceeding from there, we developed monolingual and multi-lingual models for inducing syntactic categories of words using typological features. We showed that even without typological features, one can use sophisticated learning techniques to align words and documents across languages without any notion of cross-linguistic correspondence (as well as with some correspondence). Subsequently, we developed a typological model of phonological information in the context of name transliteration. We developed two types of models for accomplishing this: one based on finite state methods and one based on low dimensional projections.  All of this work has brought the fields of linguistic typology and natural language processing closer together: in particular, the degree to which linguistic typology has been mentioned in ACL papers since our first published paper in 2007 has increased dramatically. On the other side, we have been invited to provide commentary in the journal Typology for recently proposed statistical typological work by others, published in Nature.       Last Modified: 03/03/2014       Submitted by: Hal Daume]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
