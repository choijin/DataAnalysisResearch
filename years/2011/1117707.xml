<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Algebraic and Spectral Structure of Data in High Dimension</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2011</AwardEffectiveDate>
<AwardExpirationDate>12/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Hector Munoz-Avila</SignBlockName>
<PO_EMAI>hmunoz@nsf.gov</PO_EMAI>
<PO_PHON>7032924481</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Obtaining information from data is one of the most fundamental problems of modern science and technology. The aim of machine learning is to develop algorithms to automatically extract useful information from complex, high-dimensional data.   Making progress toward this aim requires developing an understanding of the aspects of data, which are amenable to analysis and can be learned using computationally efficient methods. In particular, modeling non-linear structures in high-dimensional data has become one of the very challenging and active lines of research, which has seen significant progress over the last ten years.&lt;br/&gt;&lt;br/&gt;The goal of this project is to develop and analyze new mathematical representations for data, based on spectral and algebraic methods. We will explore how different structures in the data, such as cluster, manifold or parametric model structures, are reflected in their spectral and algebraic properties and how they can be extracted algorithmically from data, paying particular attention to the issues of high dimensionality and non-linearity. These insights will be used to build better and more adaptive algorithms for inference and data analysis tasks. &lt;br/&gt;&lt;br/&gt;We will also analyze experimentally and theoretically properties of these algorithms, when data deviates from the posited model structure.  This is a key issue in practical applications, which nearly always involve uncertainty and noise.</AbstractNarration>
<MinAmdLetterDate>06/29/2011</MinAmdLetterDate>
<MaxAmdLetterDate>04/07/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1117707</AwardID>
<Investigator>
<FirstName>Mikhail</FirstName>
<LastName>Belkin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mikhail Belkin</PI_FULL_NAME>
<EmailAddress>mbelkin@ucsd.edu</EmailAddress>
<PI_PHON>6148053884</PI_PHON>
<NSF_ID>000107334</NSF_ID>
<StartDate>06/29/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Ohio State University</Name>
<CityName>Columbus</CityName>
<ZipCode>432101016</ZipCode>
<PhoneNumber>6146888735</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1960 Kenny Road]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832127323</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OHIO STATE UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001964634</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Ohio State University]]></Name>
<CityName>Columbus</CityName>
<StateCode>OH</StateCode>
<ZipCode>432101016</ZipCode>
<StreetAddress><![CDATA[Office of Sponsored Programs]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Modern data often consist of complex and high-dimensional objects, with a large number of different attributes. For example, documents may be represented by frequencies of various words and word combinations, while images are often represented by the intensity and color of individual pixel or sets of more sophisticated characteristics, such as edge locations.&nbsp;&nbsp; To extract useful information from these high-dimensional data we need to develop new classes of methods and theoretical analyses.</p> <p>&nbsp;</p> <p>In the course of this project we concentrated on several different types of structure and approaches to learn from data. In particular, we developed methods for understanding the structure in unlabeled data.&nbsp; We also considered the problem of semi-supervised learning, i.e. of combining labeled and unlabeled data as well as that of scaling these methods to larger datasets.</p> <p>&nbsp;</p> <p>1. We designed methods to understand data by analyzing spectral properties of certain matrices associated to unlabeled points. We provided connections to Fredholm integral equations, a type of inverse problem studied in mathematical analysis. We also showed how these ideas connect to some of these ideas can lead to more powerful and scalable algorithms for data analysis.</p> <p>&nbsp;</p> <p>2. We have developed a class of methods generalizing classical matrix methods, such as&nbsp; power iteration to a non-linear, non-tensorial setting. &nbsp;We developed an understanding of the structure of these optimization problems, connecting them to convex maximization and applied these ideas to tasks such as spectral clustering and image segmentation.</p> <p>&nbsp;</p> <p>3. We developed methods based on hierarchical structures in data. We designed software to visualize such data and developed theoretical foundations to better understand how such structures can be imposed an a theoretically justified way.</p> <p>&nbsp;</p> <p>These finding have been presented and published in the leading venues in the fields of Machine Learning, Learning Theory and&nbsp; Computer Science.</p><br> <p>            Last Modified: 04/25/2016<br>      Modified by: Mikhail&nbsp;Belkin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Modern data often consist of complex and high-dimensional objects, with a large number of different attributes. For example, documents may be represented by frequencies of various words and word combinations, while images are often represented by the intensity and color of individual pixel or sets of more sophisticated characteristics, such as edge locations.   To extract useful information from these high-dimensional data we need to develop new classes of methods and theoretical analyses.     In the course of this project we concentrated on several different types of structure and approaches to learn from data. In particular, we developed methods for understanding the structure in unlabeled data.  We also considered the problem of semi-supervised learning, i.e. of combining labeled and unlabeled data as well as that of scaling these methods to larger datasets.     1. We designed methods to understand data by analyzing spectral properties of certain matrices associated to unlabeled points. We provided connections to Fredholm integral equations, a type of inverse problem studied in mathematical analysis. We also showed how these ideas connect to some of these ideas can lead to more powerful and scalable algorithms for data analysis.     2. We have developed a class of methods generalizing classical matrix methods, such as  power iteration to a non-linear, non-tensorial setting.  We developed an understanding of the structure of these optimization problems, connecting them to convex maximization and applied these ideas to tasks such as spectral clustering and image segmentation.     3. We developed methods based on hierarchical structures in data. We designed software to visualize such data and developed theoretical foundations to better understand how such structures can be imposed an a theoretically justified way.     These finding have been presented and published in the leading venues in the fields of Machine Learning, Learning Theory and  Computer Science.       Last Modified: 04/25/2016       Submitted by: Mikhail Belkin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
