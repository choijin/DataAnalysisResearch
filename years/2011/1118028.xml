<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>249995.00</AwardTotalIntnAmount>
<AwardAmount>249995</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families&lt;br/&gt;Swaminathan Vishwanathan, Purdue University; Manfred Warmuth, University of California, Santa Cruz&lt;br/&gt;&lt;br/&gt;Machine learning is currently indispensible for building predictive models from massive data sets. A large majority of widely used machine learning algorithms  are based on minimizing a convex loss function. A fundamental problem with all such models is that they are not robust to outliers. To address this limitation, this project develops probabilistic models based on a parametric family of distributions, namely, the t-exponential family, that lead to quasi-convex loss functions and yield models that  are robust to outliers. &lt;br/&gt;&lt;br/&gt;The key challenge when working with the t-exponential family of distributions, as in the case of the exponential family, is to compute the log-partition function and perform inference efficiently. The project addresses this challenge in two specific cases.  For problems with small number of classes exact iterative schemes are being developed. For problems where the number of classes is exponentially large, approximate inference techniques are being developed by extending variational methods. &lt;br/&gt;&lt;br/&gt;In partnership with Google, some of the data mining algorithms resulting from this project are being applied to a challenging real-world problem of recognizing text in photos (the PhotoOCR problem). The project offers opportunities for research-based advanced training of graduate students as well as research opportuinities for undergraduates in machine learning and data mining. Algorithms for constructing predictive models from data that are robust in the presence of outliers are likely to find use in a broad range of applications. Open source implementions of algorithms, publications, and data sets resulting from the project are being made available through the project web page at: http://learning.stat.purdue.edu/wiki/tentropy/start</AbstractNarration>
<MinAmdLetterDate>06/03/2011</MinAmdLetterDate>
<MaxAmdLetterDate>10/15/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1118028</AwardID>
<Investigator>
<FirstName>Manfred</FirstName>
<LastName>Warmuth</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Manfred K Warmuth</PI_FULL_NAME>
<EmailAddress>manfred@cse.ucsc.edu</EmailAddress>
<PI_PHON>8314594950</PI_PHON>
<NSF_ID>000416261</NSF_ID>
<StartDate>06/03/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Santa Cruz</Name>
<CityName>Santa Cruz</CityName>
<ZipCode>950641077</ZipCode>
<PhoneNumber>8314595278</PhoneNumber>
<StreetAddress>1156 High Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>125084723</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SANTA CRUZ</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Santa Cruz]]></Name>
<CityName>Santa Cruz</CityName>
<StateCode>CA</StateCode>
<ZipCode>950641077</ZipCode>
<StreetAddress><![CDATA[1156 High Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~249995</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The exponential family of distributions play an important role in statistics and machine learning. They underlie numerous models such as logistic regression and probabilistic graphical models. &nbsp;However, exponential family based probabilistic models are vulnerable to outliers. We focused on the binary and multiclas sclassification problems and proposed a new robust algorithm, t-logistic regression. t-logistic regression is obtained by replacing the exponential family in logistic regression by a more generalized distribution family, namely the t-exponential family from statistical Physics. Unlike the exponential family, the key challenge in working with the t-exponential family is the computation of the log-partition function, for which no general closed form solution exists. We showed how to overcome this difficulty via a computationally efficient iterative algorithm.</p> <p>The t-exponential families are parameterized by a temperature parameter t.The standard logarithm and exponential are the case when t=1.&nbsp;By changing the temperature in say the logistic regression loss, more or less convex version of the logistic loss are created. &nbsp;Actually there are two temperature parameters involved in the logistic regression loss. By letting these two temperatures diverge, non-convex losses are constructed that make inference robust to outliers.&nbsp;</p> <p><br />We demonstrate that our algorithm is empirically stable,&nbsp;even though the empirical risk may have multiple local minima. The key is to carefully tune the two temperature parameters.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/08/2016<br>      Modified by: Manfred&nbsp;K&nbsp;Warmuth</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The exponential family of distributions play an important role in statistics and machine learning. They underlie numerous models such as logistic regression and probabilistic graphical models.  However, exponential family based probabilistic models are vulnerable to outliers. We focused on the binary and multiclas sclassification problems and proposed a new robust algorithm, t-logistic regression. t-logistic regression is obtained by replacing the exponential family in logistic regression by a more generalized distribution family, namely the t-exponential family from statistical Physics. Unlike the exponential family, the key challenge in working with the t-exponential family is the computation of the log-partition function, for which no general closed form solution exists. We showed how to overcome this difficulty via a computationally efficient iterative algorithm.  The t-exponential families are parameterized by a temperature parameter t.The standard logarithm and exponential are the case when t=1. By changing the temperature in say the logistic regression loss, more or less convex version of the logistic loss are created.  Actually there are two temperature parameters involved in the logistic regression loss. By letting these two temperatures diverge, non-convex losses are constructed that make inference robust to outliers.    We demonstrate that our algorithm is empirically stable, even though the empirical risk may have multiple local minima. The key is to carefully tune the two temperature parameters.                Last Modified: 08/08/2016       Submitted by: Manfred K Warmuth]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
