<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project uses statistical models and human judgment to determine dynamic, probabilistic representations of extensible usages of words; these representations are suitable for incorporation into VerbNet, a lexical resource widely used in the Natural Language Processing (NLP) community. Existing lexical resources reflect a binary notion of usages as grammatical or not. However, in actual language use, forms vary in acceptability; moreover, the process of coercion extends words beyond their standard usages. For example, a strictly intransitive action verb such as 'sneeze' may be used as in 'She sneezed the foam off the cappuccino', expressing manner of motion.  This research has a two-pronged approach involving extensive use of machine learning and a fundamental shift in the development and use of VerbNet. Specifically, the research develops probabilistic methods for: (1) analyzing usages of verbs in large corpora and incorporating the resulting probabilistic information into VerbNet classes; and (2) representing information about the likelihood of potential constructional coercions and the productivity of such extensions.  These developments use the Hierarchical Bayesian Model of Parisien and Stevenson, which are an ideal framework for marrying probabilistic reasoning about complex, real-world data within the hierarchically-organized VerbNet lexicon. In addition to statistical models, the representations are also informed by human judgments with respect to the use of such constructions. Thus, this research enriches the current symbolic verb representations in VerbNet with probabilistic distributional information, which becomes salient through the influence of construction grammar. &lt;br/&gt;&lt;br/&gt;Encoding verb knowledge probabilistically provides the necessary flexibility to represent extensional constructions and support their appropriate interpretation by NLP systems.  This is especially useful for interpretation in new domains and genres, leading to advances in NLP technologies, such as question answering and machine translation, thus improving information access. Additionally, insights into statistical properties of constructions gained through this research are valuable for psycholinguistic models of language acquisition and second language learning.</AbstractNarration>
<MinAmdLetterDate>08/05/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/28/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1116782</AwardID>
<Investigator>
<FirstName>Martha</FirstName>
<LastName>Palmer</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Martha S Palmer</PI_FULL_NAME>
<EmailAddress>mpalmer@colorado.edu</EmailAddress>
<PI_PHON>3034921300</PI_PHON>
<NSF_ID>000377893</NSF_ID>
<StartDate>08/05/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Colorado at Boulder</Name>
<CityName>Boulder</CityName>
<ZipCode>803031058</ZipCode>
<PhoneNumber>3034926221</PhoneNumber>
<StreetAddress>3100 Marine Street, Room 481</StreetAddress>
<StreetAddress2><![CDATA[572 UCB]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>007431505</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF COLORADO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>007431505</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Colorado at Boulder]]></Name>
<CityName>Boulder</CityName>
<StateCode>CO</StateCode>
<ZipCode>803031058</ZipCode>
<StreetAddress><![CDATA[3100 Marine Street, Room 481]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~148465</FUND_OBLG>
<FUND_OBLG>2012~151535</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Although the field of natural language processing (NLP) has made considerable strides in the automated processing of typical newswire language, the creativity with which people constantly "coin" new phrases causes great difficulty.&nbsp; Current NLP systems assume they can combine the meaning of individual words into larger units in a compositional manner, relying on probabilities based on previously seen usages. However, the way authors and speakers continually combine words in previously unseen ways baffles the NLP systems. This proposal focused on extending our capabilities, in particular with respect to VerbNet, a pre-existing verb lexicion designed to be used by NLP systems, &nbsp;to better enable the ability of NLP systems to deal with flexible and extensible usages of language.</p> <p><span style="white-space: pre;"> </span>The process of meaning &ldquo;coercion&rdquo; is also central to the notion of flexibility in language. Coercion involves extending the usage of a word to a new context, as in "She blinked the snow off of her eyelashes." &nbsp;Blink is a verb that typically takes only one argument, and refers to an involuntary bodily process of opening and closing one's eyelids. &nbsp;However, this sentence is completely understandable, with the meaning that she removed the snow from her eyelashes by blinking.&nbsp; It is a classic example of a Caused-Motion Construction, similarly to &ldquo;They laughed the speaker off the stage,&rdquo; and &ldquo;They sneered him out of the university.&rdquo;&nbsp; All of the sentences add an unexpected &ldquo;path prepositional phrase,&rdquo; (off her eyelashes, off the stage, out of the university) to a simple verb usage, to &ldquo;coerce&rdquo; the verb into a Caused-Motion Construction. So the laughing caused the speaker to leave the stage in embarrassment, just as the sneering caused him to leave the university in shame. Knowledge of the syntax and semantics of verbs is some of the most highly complex information to be captured in a lexical resource, and appropriate means for capturing and encoding the allowable coercive processes on verbs is key for two reasons. First, an understanding of the role of coercion is necessary to the representation of the full range of allowable verb usages. Second, coercion can be an important organizing principle &ndash; i.e., representing the complexity of verb usages without explicitly representing the operation of coercion will miss generalizations over verb usages and lead to less useful lexical resources. We have added to VerbNet the capability of flexibly producing representations indicated by the presence of the Caused-Motion Construction, and identified all the verb classes where it is a core construction, and the ones where it is easily produced via coercion. This necessitated a major restructuring of our representation of motion in general, especially paths of motion, that has affected over 100 verb classes.&nbsp; We also built classifiers to automatically detect instances of coerced Caused-Motion Construction, so that the appropriate representation could be associated with them (Hwang &amp; Palmer, *SEM 2015; Hwang, Dissertation, 2014; Hwang, et. al., LREC 2014; Bonial, et. al., RELMS 2011).&nbsp; This new version of VerbNet is freely downloadable on the web, and there are several web sites with supporting information.</p> <p><span style="white-space: pre;"> </span>We also explored another thorny problem for NLP systems, light verb constructions such as &ldquo;take a bath, give a bath, give a speech,&rdquo; which are an especially productive type of multi-word expression.&nbsp; These phrases are not be interpreted as someone dragging a bathtub down the hall to hand over to another person, but rather as &ldquo;bathing&rdquo; events, just as &ldquo;give a speech&rdquo; involves a speaking event, rather than someone handing over a speech document.&nbsp; However, for the compu...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Although the field of natural language processing (NLP) has made considerable strides in the automated processing of typical newswire language, the creativity with which people constantly "coin" new phrases causes great difficulty.  Current NLP systems assume they can combine the meaning of individual words into larger units in a compositional manner, relying on probabilities based on previously seen usages. However, the way authors and speakers continually combine words in previously unseen ways baffles the NLP systems. This proposal focused on extending our capabilities, in particular with respect to VerbNet, a pre-existing verb lexicion designed to be used by NLP systems,  to better enable the ability of NLP systems to deal with flexible and extensible usages of language.   The process of meaning "coercion" is also central to the notion of flexibility in language. Coercion involves extending the usage of a word to a new context, as in "She blinked the snow off of her eyelashes."  Blink is a verb that typically takes only one argument, and refers to an involuntary bodily process of opening and closing one's eyelids.  However, this sentence is completely understandable, with the meaning that she removed the snow from her eyelashes by blinking.  It is a classic example of a Caused-Motion Construction, similarly to "They laughed the speaker off the stage," and "They sneered him out of the university."  All of the sentences add an unexpected "path prepositional phrase," (off her eyelashes, off the stage, out of the university) to a simple verb usage, to "coerce" the verb into a Caused-Motion Construction. So the laughing caused the speaker to leave the stage in embarrassment, just as the sneering caused him to leave the university in shame. Knowledge of the syntax and semantics of verbs is some of the most highly complex information to be captured in a lexical resource, and appropriate means for capturing and encoding the allowable coercive processes on verbs is key for two reasons. First, an understanding of the role of coercion is necessary to the representation of the full range of allowable verb usages. Second, coercion can be an important organizing principle &ndash; i.e., representing the complexity of verb usages without explicitly representing the operation of coercion will miss generalizations over verb usages and lead to less useful lexical resources. We have added to VerbNet the capability of flexibly producing representations indicated by the presence of the Caused-Motion Construction, and identified all the verb classes where it is a core construction, and the ones where it is easily produced via coercion. This necessitated a major restructuring of our representation of motion in general, especially paths of motion, that has affected over 100 verb classes.  We also built classifiers to automatically detect instances of coerced Caused-Motion Construction, so that the appropriate representation could be associated with them (Hwang &amp; Palmer, *SEM 2015; Hwang, Dissertation, 2014; Hwang, et. al., LREC 2014; Bonial, et. al., RELMS 2011).  This new version of VerbNet is freely downloadable on the web, and there are several web sites with supporting information.   We also explored another thorny problem for NLP systems, light verb constructions such as "take a bath, give a bath, give a speech," which are an especially productive type of multi-word expression.  These phrases are not be interpreted as someone dragging a bathtub down the hall to hand over to another person, but rather as "bathing" events, just as "give a speech" involves a speaking event, rather than someone handing over a speech document.  However, for the computer to process them appropriately they must first be detected.  Since they are extremely common and very productive, in that new types of light verb constructions appear every day, this is challenging.  We trained a state-of-the-art classifier for light verb detection, and performed several psycho...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
