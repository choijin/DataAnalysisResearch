<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Metaevaluation of REU and RET Evaluations to Assess Quality and Build Capacity Locally and Nationally</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>249778.00</AwardTotalIntnAmount>
<AwardAmount>249778</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Connie Della-Piana</SignBlockName>
<PO_EMAI>cdellapi@nsf.gov</PO_EMAI>
<PO_PHON>7032925309</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project examines NSF-funded project-level evaluations of the REU and RET programs which either bring undergraduates or teachers together for summer research experiences.  &lt;br/&gt;&lt;br/&gt;The project will develop a systematic review tool that incorporates the Program Evaluation Accuracy Standards and moves beyond them to attend to effects on student learning. Using the methodology of meta-evaluation, the project will (1) describe and assess the quality of REU and RET evaluations, (2) test the feasibility of synthesizing and generalizing across evaluations to support conclusions about program-level effectiveness, and (3) lay the groundwork for a resource to build evaluation capacity and align the evaluation needs of local projects and the national programs.&lt;br/&gt;&lt;br/&gt;The REU and RET projects represent an important set of activities across the NSF.  This exploratory study will determine the degree to which the evaluations of these projects can be used to make statements about the effectiveness of these projects overall.</AbstractNarration>
<MinAmdLetterDate>09/09/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/16/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1118777</AwardID>
<Investigator>
<FirstName>Leslie</FirstName>
<LastName>Cooksy</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Leslie Cooksy</PI_FULL_NAME>
<EmailAddress>ljcooksy@udel.edu</EmailAddress>
<PI_PHON>3028316872</PI_PHON>
<NSF_ID>000585735</NSF_ID>
<StartDate>09/09/2011</StartDate>
<EndDate>06/27/2012</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jennifer</FirstName>
<LastName>Buckley</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jennifer Buckley</PI_FULL_NAME>
<EmailAddress>jbuckley@udel.edu</EmailAddress>
<PI_PHON>3028312136</PI_PHON>
<NSF_ID>000602577</NSF_ID>
<StartDate>09/29/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Joan</FirstName>
<LastName>Buttram</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joan Buttram</PI_FULL_NAME>
<EmailAddress>jbuttram@udel.edu</EmailAddress>
<PI_PHON>3028314434</PI_PHON>
<NSF_ID>000620573</NSF_ID>
<StartDate>06/27/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Delaware</Name>
<CityName>Newark</CityName>
<ZipCode>197160099</ZipCode>
<PhoneNumber>3028312136</PhoneNumber>
<StreetAddress>210 Hullihen Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Delaware</StateName>
<StateCode>DE</StateCode>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>DE00</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>059007500</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF DELAWARE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>059007500</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Delaware]]></Name>
<CityName>Newark</CityName>
<StateCode>DE</StateCode>
<ZipCode>197160099</ZipCode>
<StreetAddress><![CDATA[210 Hullihen Hall]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Delaware</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>00</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>DE00</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<ProgramReference>
<Code>9177</Code>
<Text>ELEMENTARY/SECONDARY EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0411</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0412</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~123032</FUND_OBLG>
<FUND_OBLG>2012~126746</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This study conducted a metaevaluation of the Research Experiences for Undergraduates (REU) and Research Experience for Teachers (RET) projects funded by NSF to: 1) describe and assess the quality of their evaluations, 2) test the feasibility of synthesizing and generalizing evaluation results across projects to support conclusions about program level effectiveness, and 3) lay the groundwork for a resource to build evaluation capacity and align the evaluation needs of the local projects and national programs. A total of 63 projects were included in the metaevaluation. The metaevaluation results revealed significant variability in the rigor and quality of the majority of the evaluations. Most evaluations were conducted by project staff or graduate students; few were conducted by experienced program evaluators. Less than half of the evaluations stated project goals explicitly and even fewer described in detail project activities. Assessment of participant outcomes varied. About 78% reported data on participant satisfaction, about 70% on participant knowledge and skills, and about 60% on participants continued participation in STEM. Most projects relied on ratings of satisfaction of faculty and participants at the end of the project. Assessments of participant knowledge or skills often relied on self-reports rather than objective pre- and post-assessments. Some projects collected data related to participants&rsquo; presentations of academic papers and enrollment in later STEM-related graduate programs. Only 30% of the projects used evaluation data to make programmatic changes. A four-level model for evaluating REU and RET outcomes is suggested to structure and organize future evaluations of these projects.</p><br> <p>            Last Modified: 10/08/2015<br>      Modified by: Joan&nbsp;Buttram</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This study conducted a metaevaluation of the Research Experiences for Undergraduates (REU) and Research Experience for Teachers (RET) projects funded by NSF to: 1) describe and assess the quality of their evaluations, 2) test the feasibility of synthesizing and generalizing evaluation results across projects to support conclusions about program level effectiveness, and 3) lay the groundwork for a resource to build evaluation capacity and align the evaluation needs of the local projects and national programs. A total of 63 projects were included in the metaevaluation. The metaevaluation results revealed significant variability in the rigor and quality of the majority of the evaluations. Most evaluations were conducted by project staff or graduate students; few were conducted by experienced program evaluators. Less than half of the evaluations stated project goals explicitly and even fewer described in detail project activities. Assessment of participant outcomes varied. About 78% reported data on participant satisfaction, about 70% on participant knowledge and skills, and about 60% on participants continued participation in STEM. Most projects relied on ratings of satisfaction of faculty and participants at the end of the project. Assessments of participant knowledge or skills often relied on self-reports rather than objective pre- and post-assessments. Some projects collected data related to participantsÃ† presentations of academic papers and enrollment in later STEM-related graduate programs. Only 30% of the projects used evaluation data to make programmatic changes. A four-level model for evaluating REU and RET outcomes is suggested to structure and organize future evaluations of these projects.       Last Modified: 10/08/2015       Submitted by: Joan Buttram]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
