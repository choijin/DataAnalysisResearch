<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:  Immediate Feedback Assessment in Chemistry Courses</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2012</AwardEffectiveDate>
<AwardExpirationDate>04/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>149063.00</AwardTotalIntnAmount>
<AwardAmount>149063</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11040200</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DUE</Abbreviation>
<LongName>Division Of Undergraduate Education</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Dawn Rickey</SignBlockName>
<PO_EMAI>drickey@nsf.gov</PO_EMAI>
<PO_PHON>7032924674</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This collaborative project between the University of Wisconsin-River Falls and Winona State University is gathering evidence on the role timing and type of feedback has on student learning when multiple choice exams are used in large-enrollment general chemistry and organic chemistry lecture courses.  This systematic chemical education research project is using both traditional Scantron and Immediate Feedback Assessment Technique forms (also known as "Answer Until Correct" in which answer spaces are covered with a coating that is scratched off like a lottery ticket).  The variables being investigated include immediate versus delayed feedback, corrective versus non-corrective feedback, test retaking, deliberate increases in cognitive complexity demands, and student confidence in their answers.  The results are providing evidence about student cognitive and metacognitive growth and how different sub-populations vary within the testing regimes.  Volunteer student populations from one doctoral institution, three comprehensive institutions, and two community colleges (Anoka-Ramsey Community College and South Suburban College) are participating.  The intellectual merit of this project includes valid and reliable General Chemistry 1 and Organic Chemistry 1 multiple-choice tests that encompass a spectrum of thinking skills, including higher order cognitive skills.  In addition, the project is documenting how multiple choice exams are used both for real-time student learning as well as assessing a student's knowledge.  The broader impacts of this project are the knowledge that is being created about how the structure of multiple choice questions, coupled with type and timing of feedback, influences student learning for the whole, as well as for different sub-groups.  Given the wide-spread use of multiple-choice exams, this project also is establishing best practices for the construction of online practice and homework problems such that they now can now be formulated better to maximize student learning.</AbstractNarration>
<MinAmdLetterDate>04/22/2012</MinAmdLetterDate>
<MaxAmdLetterDate>04/22/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1140914</AwardID>
<Investigator>
<FirstName>Jamie</FirstName>
<LastName>Schneider</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jamie L Schneider</PI_FULL_NAME>
<EmailAddress>jamie.schneider@uwrf.edu</EmailAddress>
<PI_PHON>7154254590</PI_PHON>
<NSF_ID>000338325</NSF_ID>
<StartDate>04/22/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kristen</FirstName>
<LastName>Murphy</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kristen L Murphy</PI_FULL_NAME>
<EmailAddress>kmurphy@uwm.edu</EmailAddress>
<PI_PHON>4142294468</PI_PHON>
<NSF_ID>000505211</NSF_ID>
<StartDate>04/22/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Arunendu</FirstName>
<LastName>Chatterjee</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Arunendu Chatterjee</PI_FULL_NAME>
<EmailAddress>arunendu.chatterjee@uwrf.edu</EmailAddress>
<PI_PHON>7154253201</PI_PHON>
<NSF_ID>000564543</NSF_ID>
<StartDate>04/22/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Wisconsin-River Falls</Name>
<CityName>River Falls</CityName>
<ZipCode>540225001</ZipCode>
<PhoneNumber>7154253195</PhoneNumber>
<StreetAddress>410 South 3rd Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<StateCode>WI</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WI03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>068191857</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WISCONSIN SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041188822</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Wisconsin-River Falls]]></Name>
<CityName>River Falls</CityName>
<StateCode>WI</StateCode>
<ZipCode>540225010</ZipCode>
<StreetAddress><![CDATA[410 S 3rd St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WI03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1536</Code>
<Text>S-STEM-Schlr Sci Tech Eng&amp;Math</Text>
</ProgramElement>
<ProgramElement>
<Code>7513</Code>
<Text>TUES-Type 1 Project</Text>
</ProgramElement>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0412</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>13XX</Code>
<Name>H-1B FUND, EHR, NSF</Name>
<APP_SYMB_ID>045176</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~149063</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span style="font-family: Times New Roman; font-size: small;"> </span></p> <p style="line-height: 15pt;"><span style="font-family: Times New Roman;"><span style="font-size: small;">Social science research provides much evidence that forced recall through testing can support and even enhance student learning, especially when corrective feedback is provided. A limitation of these studies is that they tend to be in the context of reading comprehension and fact based questioning. Our study sought to apply similar methodology but in the context of first semester general chemistry testing. We proposed that testing feedback would improve student&rsquo;s performance upon repeat testing in chemistry but perhaps not to the same extent as those found in earlier studies due to the complexity of typical chemistry items. This exploratory grant provided funding to establish the methodology for this study in chemistry testing feedback, which included writing two valid and reliable general chemistry exams. The exams were designed to cover four essential first semester general chemistry concepts&nbsp;using typical classroom level multiple-choice questions.&nbsp;The four topics were chemical composition, gram/mole/molecule conversions, reaction stoichiometry, and limiting reactant stoichiometry. The two exams were written to be algorithmic clones, meaning same item wording and thinking but with different chemicals and/or quantitities.&nbsp;Data from our study supported that these exams were valid and reliable and that they funtioned as clones. These exams can be utilized by others for other pre/post testing&nbsp; assessment in first term general chemistry. </span></span></p> <p style="line-height: 15pt;"><span style="font-family: Times New Roman;"><span style="font-size: small;">Once the methodology was established, we collected and analyzed repetitive testing data from a control group that received non-corrective feedback (scores) and three different corrective feedback experimental groups (immediate answer-until-correct, delayed corrective, and delayed answer-until-correct). We chose these feedback techniques based on earlier social science studies and on ease of application into normal classroom practices in future studies. These experiments utilized over a thousand volunteer first semester general chemistry students from three different institutions who were nearing the end of their course term.&nbsp;&nbsp;Although the&nbsp;repetitive testing studies were outside of the classroom schedule and grading practices, they were designed to offer a practice testing opportunity near the final examination period.&nbsp;We found that repeat performance on the cloned chemistry exams was significantly improved when answer-until-correct feedback was offered both immediately during the test and delayed after the test compared to just offering test scores. These effects were however smaller than those previously reported in many social science studies. We also found that giving students their answer and the correct answer alongside each item (delayed corrective feedback) was not effective at enhancing student repeat performance. We propose that passive reading of answers to these moderately complex items is not sufficient to support learning through testing feedback. This evidence contradicts earlier social science studies providing further support for the importance of gathering evidence within different learning contexts. Given the popularity of multiple-choice testing in college science courses (especially for larger class sizes), our grant findings provide important science education research into best practices in testing feedback to maximize student learning. Our research team&rsquo;s long-term goal is to promote a paradigm shift from testing for evaluation of learning to testing for evaluation <em>and</em></span><span style="font-size: small;"> improvement of student learning.</span></span></p> <p><span style="font-family: Times New Roman; font-size: small;"> </span></p><br> <p>            Last Modified: 07/20/2017<br>      Modified by: Jamie&nbsp;L&nbsp;Schneider</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Social science research provides much evidence that forced recall through testing can support and even enhance student learning, especially when corrective feedback is provided. A limitation of these studies is that they tend to be in the context of reading comprehension and fact based questioning. Our study sought to apply similar methodology but in the context of first semester general chemistry testing. We proposed that testing feedback would improve student?s performance upon repeat testing in chemistry but perhaps not to the same extent as those found in earlier studies due to the complexity of typical chemistry items. This exploratory grant provided funding to establish the methodology for this study in chemistry testing feedback, which included writing two valid and reliable general chemistry exams. The exams were designed to cover four essential first semester general chemistry concepts using typical classroom level multiple-choice questions. The four topics were chemical composition, gram/mole/molecule conversions, reaction stoichiometry, and limiting reactant stoichiometry. The two exams were written to be algorithmic clones, meaning same item wording and thinking but with different chemicals and/or quantitities. Data from our study supported that these exams were valid and reliable and that they funtioned as clones. These exams can be utilized by others for other pre/post testing  assessment in first term general chemistry.  Once the methodology was established, we collected and analyzed repetitive testing data from a control group that received non-corrective feedback (scores) and three different corrective feedback experimental groups (immediate answer-until-correct, delayed corrective, and delayed answer-until-correct). We chose these feedback techniques based on earlier social science studies and on ease of application into normal classroom practices in future studies. These experiments utilized over a thousand volunteer first semester general chemistry students from three different institutions who were nearing the end of their course term.  Although the repetitive testing studies were outside of the classroom schedule and grading practices, they were designed to offer a practice testing opportunity near the final examination period. We found that repeat performance on the cloned chemistry exams was significantly improved when answer-until-correct feedback was offered both immediately during the test and delayed after the test compared to just offering test scores. These effects were however smaller than those previously reported in many social science studies. We also found that giving students their answer and the correct answer alongside each item (delayed corrective feedback) was not effective at enhancing student repeat performance. We propose that passive reading of answers to these moderately complex items is not sufficient to support learning through testing feedback. This evidence contradicts earlier social science studies providing further support for the importance of gathering evidence within different learning contexts. Given the popularity of multiple-choice testing in college science courses (especially for larger class sizes), our grant findings provide important science education research into best practices in testing feedback to maximize student learning. Our research team?s long-term goal is to promote a paradigm shift from testing for evaluation of learning to testing for evaluation and improvement of student learning.          Last Modified: 07/20/2017       Submitted by: Jamie L Schneider]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
