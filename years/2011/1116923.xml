<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Indoor Visual Navigation and Recognition for the Blind Using a Motion Sensing Input Device</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>449994.00</AwardTotalIntnAmount>
<AwardAmount>449994</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In this project the goal is to develop a computer vision system to assist visually impaired people when navigating in indoor environments. A novel feature of the vision system is that it uses a motion sensing input device to gather both an image and depth map of the scene as input. By learning joint representations that combine both depth and intensity information, powerful features can be learned that give a dramatic improvement over existing scene understanding algorithms (which rely on intensity information alone). The availability of depth information also allows the recovery of the room geometry and permits the construction of new types of 3D priors on the locations of objects, not currently possible with existing approaches. The output of the vision system will be communicated to the visually impaired person via a number of possible methods: (i) a tactile hand-grip on a cane; (ii) a wearable pad embedded with actuators and (iii) the BrainPort sensor which has an array of tiny actuators that are placed on the tongue.&lt;br/&gt;&lt;br/&gt;The expected results of the project are: (i) a large dataset of indoor scenes with depth maps and dense labels; (ii) new open-source algorithms for fusing depth and intensity information to aid scene understanding and (iii) a prototype vision-based assistive device for visually impaired people. The project aims to assist the approximately 2.5 million people in the US are blind or partially sighted.</AbstractNarration>
<MinAmdLetterDate>08/11/2011</MinAmdLetterDate>
<MaxAmdLetterDate>10/30/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1116923</AwardID>
<Investigator>
<FirstName>Yann</FirstName>
<LastName>LeCun</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yann LeCun</PI_FULL_NAME>
<EmailAddress>yann@cs.nyu.edu</EmailAddress>
<PI_PHON>2129983283</PI_PHON>
<NSF_ID>000201195</NSF_ID>
<StartDate>10/30/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Fergus</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Fergus</PI_FULL_NAME>
<EmailAddress>fergus@cs.nyu.edu</EmailAddress>
<PI_PHON>2129982121</PI_PHON>
<NSF_ID>000490043</NSF_ID>
<StartDate>08/11/2011</StartDate>
<EndDate>10/30/2014</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041968306</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEW YORK UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041968306</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[New York University]]></Name>
<CityName>NEW YORK</CityName>
<StateCode>NY</StateCode>
<ZipCode>100121019</ZipCode>
<StreetAddress><![CDATA[70 WASHINGTON SQUARE S]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~300523</FUND_OBLG>
<FUND_OBLG>2013~149471</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The goal of this project was to develop method for machines to visually understand scenes, with the ultimate purpose of helping people who are visually impaired.&nbsp; That is, given an image get the computer to interpret the pixels and recognize the layout of the room, pieces of furniture and objects. Despite the ease with which we do this, it is very challenging problem for a machine. One particular difficulty is that with a single image, the depth information is lacking. Thus we used a Kinect depth sensing device (which records both depth and an RGB image simultaneously) to gather a large set of training set of indoor scenes (NYU Depth dataset V2.0). Each image had depth and object identity information available at each pixel. From this data, we trained machine learning models to predict this information form the RGB image. After trying several approaches, we settled on a neural network model. This was able to predict depth, surface orientation and object identity for each location in the image In near real-time. The accuracy, while inferior to humans, is sufficiently good to be of practical value in building a device that could help visually impaired people.</p> <p>&nbsp;</p> <p>For more details, please see our paper: http://arxiv.org/abs/1411.4734</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/08/2016<br>      Modified by: Yann&nbsp;Lecun</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The goal of this project was to develop method for machines to visually understand scenes, with the ultimate purpose of helping people who are visually impaired.  That is, given an image get the computer to interpret the pixels and recognize the layout of the room, pieces of furniture and objects. Despite the ease with which we do this, it is very challenging problem for a machine. One particular difficulty is that with a single image, the depth information is lacking. Thus we used a Kinect depth sensing device (which records both depth and an RGB image simultaneously) to gather a large set of training set of indoor scenes (NYU Depth dataset V2.0). Each image had depth and object identity information available at each pixel. From this data, we trained machine learning models to predict this information form the RGB image. After trying several approaches, we settled on a neural network model. This was able to predict depth, surface orientation and object identity for each location in the image In near real-time. The accuracy, while inferior to humans, is sufficiently good to be of practical value in building a device that could help visually impaired people.     For more details, please see our paper: http://arxiv.org/abs/1411.4734          Last Modified: 07/08/2016       Submitted by: Yann Lecun]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
