<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Collaborative Research: Visual Attributes for Identification and Search in Images</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2011</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>250000.00</AwardTotalIntnAmount>
<AwardAmount>250000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The automatic identification in images of people, places, objects, and especially object categories is a central and ongoing challenge within computer vision.  This project addresses this problem using low-level image features to learn intermediate representations, ones in which objects in images are labeled with an extensive list of highly descriptive visual attributes.  This work demonstrates this approach in three domains: faces, plant species, and architecture.  In each domain, it develops techniques for deriving visual attribute vocabularies, training attribute detectors, and building compositional models to automatically label attributes in images.&lt;br/&gt;&lt;br/&gt;The project is making four fundamental contributions to the use of visual attributes. 1) It is developing new methods by which automatic systems and humans can interact to select domain-appropriate attribute vocabularies and label large image collections. 2) It is developing compositional models that capture dependencies between attributes.  This provides more accurate attribute detection and enables inference of global properties of objects.  3) Using compositional models, the project is developing new, localizable attributes that capture the geometric relations between object parts and landmarks.  4) The project is designing algorithms that combine attributes to identify objects, search through image vast collections, and automatically annotate image databases.&lt;br/&gt;&lt;br/&gt;Not only is this research generating large datasets of labeled images that should help catalyze new research, it is also demonstrating the feasibility of new systems for analyzing images in specialized domains such as faces, plants, and architecture. For example, the project develops new software applications for analyzing and searching images of faces as well as free mobile apps for plant species identification.</AbstractNarration>
<MinAmdLetterDate>06/30/2011</MinAmdLetterDate>
<MaxAmdLetterDate>06/30/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1116631</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Jacobs</LastName>
<PI_MID_INIT>W</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David W Jacobs</PI_FULL_NAME>
<EmailAddress>djacobs@cs.umd.edu</EmailAddress>
<PI_PHON>3014050679</PI_PHON>
<NSF_ID>000315613</NSF_ID>
<StartDate>06/30/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~250000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our project addresses the problem of building part- and attribute-based descriptions of objects.&nbsp; We have been applying this work by building systems that recognize and describe plants and animals.&nbsp; Attributes capture properties of an object (is it old?&nbsp; furry? speckled?), while parts allow us to understand and match the common components of objects (eg., their legs, nose, or stem).</p> <p>&nbsp;</p> <p>Our project has developed new algorithms for identifying these parts in images of animals, such as people, dogs or birds.&nbsp; After finding the parts of an animal, we are able to build descriptions of their appearance, using features that are anchored to these parts.&nbsp; This has led to new recognition algorithms that allow us to identify the breed of a dog or the species of a bird from a photograph.</p> <p>&nbsp;</p> <p>We have incorporated these recognition algorithms into new, freely available apps.&nbsp; In creating apps for species identification we build on our previous work in constructing Leafsnap.&nbsp; Developed in collaboration with botanists at the Smithsonian Institution National Museum of Natural History, Leafsnap allows a user to photograph a leaf of a tree from the Northeast US, and then uses shape based search to identify species of trees that best match this leaf.&nbsp; Leafsnap has been downloaded by over 1.5 million users, and used in a variety of biodiversity studies and classrooms.&nbsp; In this project, a new version of Leafsnap, covering the trees of the United Kingdom, has been developed.&nbsp; Using our new recognition algorithms we have also developed Dogsnap and Birdsnap, apps that use computer vision to help identify the breed of a dog or species of a bird in a photograph.&nbsp; These apps have already been downloaded by over 100,000 users.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 09/30/2015<br>      Modified by: David&nbsp;W&nbsp;Jacobs</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our project addresses the problem of building part- and attribute-based descriptions of objects.  We have been applying this work by building systems that recognize and describe plants and animals.  Attributes capture properties of an object (is it old?  furry? speckled?), while parts allow us to understand and match the common components of objects (eg., their legs, nose, or stem).     Our project has developed new algorithms for identifying these parts in images of animals, such as people, dogs or birds.  After finding the parts of an animal, we are able to build descriptions of their appearance, using features that are anchored to these parts.  This has led to new recognition algorithms that allow us to identify the breed of a dog or the species of a bird from a photograph.     We have incorporated these recognition algorithms into new, freely available apps.  In creating apps for species identification we build on our previous work in constructing Leafsnap.  Developed in collaboration with botanists at the Smithsonian Institution National Museum of Natural History, Leafsnap allows a user to photograph a leaf of a tree from the Northeast US, and then uses shape based search to identify species of trees that best match this leaf.  Leafsnap has been downloaded by over 1.5 million users, and used in a variety of biodiversity studies and classrooms.  In this project, a new version of Leafsnap, covering the trees of the United Kingdom, has been developed.  Using our new recognition algorithms we have also developed Dogsnap and Birdsnap, apps that use computer vision to help identify the breed of a dog or species of a bird in a photograph.  These apps have already been downloaded by over 100,000 users.             Last Modified: 09/30/2015       Submitted by: David W Jacobs]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
