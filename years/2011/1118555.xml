<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Advancing Methodological Knowledge in STEM Education Research: An Empirical Investigation of Design Parameters for Planning Cluster Randomized Trials in Science Education</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>762964.00</AwardTotalIntnAmount>
<AwardAmount>907452</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>11090000</Code>
<Directorate>
<Abbreviation>EHR</Abbreviation>
<LongName>Direct For Education and Human Resources</LongName>
</Directorate>
<Division>
<Abbreviation>DRL</Abbreviation>
<LongName>Division Of Research On Learning</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Finbarr Sloane</SignBlockName>
<PO_EMAI>fsloane@nsf.gov</PO_EMAI>
<PO_PHON>7032928465</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Recognizing the need for powerful tools to enhance studies in science education, the project is developing a set of resources to assist researchers in the planning of cluster-randomized trials of science education interventions.  The goals of this full-scale project are to develop the statistical resources to design and conduct rigorous cluster-randomized trials (CRTs) in science education research; increase the accuracy of the range of parameters needed to conduct power analyses; and develop statistical resources for the science research community available through a free power analysis software package. Curriculum developers and researchers from Biological Sciences Curriculum Study and Western Michigan University team-up to address this lack of statistical resources specifically for cluster-randomized trials for science education researchers and evaluators. &lt;br/&gt;&lt;br/&gt;Recognizing that the majority of the work on power analysis estimates has been conducted in mathematics and reading, the research team is developing of a set of estimates of parameters specifically for science education research that increases the accuracy and improves the efficiency of CRTs.  Results from the meta-analysis of research on science education interventions and a multi-level analysis of intra-class correlations (ICC) and covariate outcome correlations (R^2) data provide a foundation for establishing estimates for power analysis. This effort to develop more accurate estimates of ICC, R^2, and effect size (ES) will improve the internal validity of CRTs in science education research.  &lt;br/&gt;&lt;br/&gt;By empirically establishing estimates for the full range of parameters needed to conduct a power analysis for CRTs, researchers and evaluators will be able to find estimates of the parameters necessary to plan rigorous CRTs in science education.  Access to this statistical resource through a free power analysis software package, increases the likelihood that accurate estimates of all three parameters (ICC, R^2, ES) will be used in research planning.  Improving the accuracy of power equations has broad impact on the planning and conducting of large scale science education research.</AbstractNarration>
<MinAmdLetterDate>08/30/2011</MinAmdLetterDate>
<MaxAmdLetterDate>02/27/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.076</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1118555</AwardID>
<Investigator>
<FirstName>Joseph</FirstName>
<LastName>Taylor</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joseph A Taylor</PI_FULL_NAME>
<EmailAddress>jtaylo18@uccs.edu</EmailAddress>
<PI_PHON>7192555145</PI_PHON>
<NSF_ID>000209818</NSF_ID>
<StartDate>08/30/2011</StartDate>
<EndDate>02/27/2015</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Joseph</FirstName>
<LastName>Taylor</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joseph A Taylor</PI_FULL_NAME>
<EmailAddress>jtaylo18@uccs.edu</EmailAddress>
<PI_PHON>7192555145</PI_PHON>
<NSF_ID>000209818</NSF_ID>
<StartDate>02/27/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Susan</FirstName>
<LastName>Kowalski</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Susan M Kowalski</PI_FULL_NAME>
<EmailAddress>skowalski@bscs.org</EmailAddress>
<PI_PHON>7195315550</PI_PHON>
<NSF_ID>000579624</NSF_ID>
<StartDate>02/27/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Jessaca</FirstName>
<LastName>Spybrook</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jessaca Spybrook</PI_FULL_NAME>
<EmailAddress>jessaca.spybrook@wmich.edu</EmailAddress>
<PI_PHON>2693873889</PI_PHON>
<NSF_ID>000585112</NSF_ID>
<StartDate>08/30/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>BSCS Science Learning</Name>
<CityName>Colorado Springs</CityName>
<ZipCode>809183842</ZipCode>
<PhoneNumber>7195315550</PhoneNumber>
<StreetAddress>5415 Mark Dabling Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<StateCode>CO</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CO05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>173848607</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BSCS SCIENCE LEARNING</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[BSCS Science Learning]]></Name>
<CityName>Colorado Springs</CityName>
<StateCode>CO</StateCode>
<ZipCode>809183842</ZipCode>
<StreetAddress><![CDATA[5415 Mark Dabling Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Colorado</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CO05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7261</Code>
<Text>Project &amp; Program Evaluation</Text>
</ProgramElement>
<ProgramReference>
<Code>9177</Code>
<Text>ELEMENTARY/SECONDARY EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0411</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0414</Code>
<Name>NSF Education &amp; Human Resource</Name>
<APP_SYMB_ID>040106</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~762964</FUND_OBLG>
<FUND_OBLG>2014~144488</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project sought to advance knowledge by establishing design parameters for the planning of cluster randomized trials of science education interventions. These three design parameters (intraclass correlation, percent of variance explained by covariate(s), and effect size) were estimated using two separate research techniques. Estimates for intraclass correlations and the percent of variance explained by covariate(s) for science achievement outcomes were estimated using multilevel models applied to state-level achievement databases from Michigan, Wisconsin, and Texas. Estimates for expected effect sizes were generated using a predictive meta-regression model where the analysis used nearly 300 effect sizes extracted from just under 100 intervention studies in science education.</p> <p>In terms of intraclass correlations, the empirical estimates suggest these estimates vary across states. On average, the estimates of the between-school and between-district variance from Texas were lower than both Wisconsin and Michigan. The variance between schools and between districts also appeared to vary across grades, with higher grades tending to have higher values. We also examined the percent of variance explained by : demographics, school-level pretests, and student-level pretests. In general, the explanatory power of the 1-year lag school-level science pretest was the highest. Given that school-level pretests are much less expensive and easier to obtain than student-level demographics or pretests, we suggest use of the 1-year lag school-level science scores.</p> <p>In terms of effect sizes, the meta-regression model estimated that the average effect size for science education interventions is approximately 0.50 standard deviations with larger effect size values coming from published studies, from studies where the outcome assessment was developed by the researcher, and from studies that tested secondary school interventions.</p> <p>Prior to this study, no such analysis of intraclass correlations and percent of variance explained by covariate(s) had been conducted on science outcomes. Further, the handful of meta-analyses that had been done on science education interventions were more limited in scope and did not extract multiple effect sizes per study. The current meta-analysis extracted multiple effect sizes per study and modeled that dependence through robust variance estimation.</p> <p>The design parameters generated in this study will equip study designers with the information required to conduct much more precise a priori power analyses than what was possible in the past. As such, study designers who use these parameters are less likely to design an underpowered or an overpowered trial (too few or too many participants). This maximizes the efficiency of experimental designs and promotes better stewardship of tax dollars and other resources.</p> <p>As researchers use our products to inform the design of their own intervention studies, they are more likely to obtain results that can advance science education research. Advances in science education research will lead to advances in science education broadly, improved scientific literacy among students, a better prepared science workforce, and better science decision-makers in the general public.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/02/2016<br>      Modified by: Susan&nbsp;M&nbsp;Kowalski</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project sought to advance knowledge by establishing design parameters for the planning of cluster randomized trials of science education interventions. These three design parameters (intraclass correlation, percent of variance explained by covariate(s), and effect size) were estimated using two separate research techniques. Estimates for intraclass correlations and the percent of variance explained by covariate(s) for science achievement outcomes were estimated using multilevel models applied to state-level achievement databases from Michigan, Wisconsin, and Texas. Estimates for expected effect sizes were generated using a predictive meta-regression model where the analysis used nearly 300 effect sizes extracted from just under 100 intervention studies in science education.  In terms of intraclass correlations, the empirical estimates suggest these estimates vary across states. On average, the estimates of the between-school and between-district variance from Texas were lower than both Wisconsin and Michigan. The variance between schools and between districts also appeared to vary across grades, with higher grades tending to have higher values. We also examined the percent of variance explained by : demographics, school-level pretests, and student-level pretests. In general, the explanatory power of the 1-year lag school-level science pretest was the highest. Given that school-level pretests are much less expensive and easier to obtain than student-level demographics or pretests, we suggest use of the 1-year lag school-level science scores.  In terms of effect sizes, the meta-regression model estimated that the average effect size for science education interventions is approximately 0.50 standard deviations with larger effect size values coming from published studies, from studies where the outcome assessment was developed by the researcher, and from studies that tested secondary school interventions.  Prior to this study, no such analysis of intraclass correlations and percent of variance explained by covariate(s) had been conducted on science outcomes. Further, the handful of meta-analyses that had been done on science education interventions were more limited in scope and did not extract multiple effect sizes per study. The current meta-analysis extracted multiple effect sizes per study and modeled that dependence through robust variance estimation.  The design parameters generated in this study will equip study designers with the information required to conduct much more precise a priori power analyses than what was possible in the past. As such, study designers who use these parameters are less likely to design an underpowered or an overpowered trial (too few or too many participants). This maximizes the efficiency of experimental designs and promotes better stewardship of tax dollars and other resources.  As researchers use our products to inform the design of their own intervention studies, they are more likely to obtain results that can advance science education research. Advances in science education research will lead to advances in science education broadly, improved scientific literacy among students, a better prepared science workforce, and better science decision-makers in the general public.          Last Modified: 12/02/2016       Submitted by: Susan M Kowalski]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
