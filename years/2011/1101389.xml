<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>ICES: Large: Economic Foundations of Digital Privacy</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>997993.00</AwardTotalIntnAmount>
<AwardAmount>997993</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In the last decade private data has become a commodity - it is gathered, bought and sold, and contributes to the primary business of many Internet and information technology companies. At the same time, various formalizations of the notion of "privacy" have been developed and studied by computer scientists. Nevertheless, to date we lack a theory for the economics of digital privacy, and this project aims to close this important gap.&lt;br/&gt;&lt;br/&gt;Concretely, the project will develop a theory to address the following questions:&lt;br/&gt;&lt;br/&gt;- How should a market for private data be structured? How can one design an auction that accommodates issues specific to private data analysis: that the buyer of private data often wishes to buy from a representative sample from the population; and that individuals' value for their privacy can itself be a very sensitive piece of information?&lt;br/&gt;&lt;br/&gt;- How should other markets be structured to properly account for participants' concerns about privacy? How should privacy be modeled in auction settings, and how should markets be designed to address issues relating to utility for privacy?&lt;br/&gt;&lt;br/&gt;- Studying economic interactions necessitates studying learning - but what is the cost of privacy on agent learning? How does the incomplete information that is the necessary result of privacy-preserving mechanisms affect how individuals engaged in a dynamic interaction can learn and coordinate, and how do perturbed measurements affect learning dynamics in games? How can market research be conducted both usefully and privately?&lt;br/&gt;&lt;br/&gt;Our investigation of these questions will blend models and methods from several relevant fields, including computer science, economics, algorithmic game theory and machine learning.&lt;br/&gt;&lt;br/&gt;This project directly addresses one of the most important tensions that the Internet era has thrust upon society: the tension between the tremendous societal and commercial value of private and potentially sensitive data about individual citizens, and the interests and rights of those individuals to control their data. Despite the attention and controversy this tension has evoked, there is no comprehensive and coherent science for understanding it. Furthermore, science (rather than technology alone) is required, since the technological and social factors underlying data privacy are undergoing perpetual change. Within the field of computer science, the recently introduced subfield of privacy preserving computation has pointed the way to potential advances. This project aims to both broaden and deepen these directions.</AbstractNarration>
<MinAmdLetterDate>05/02/2011</MinAmdLetterDate>
<MaxAmdLetterDate>11/05/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1101389</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Kearns</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Kearns</PI_FULL_NAME>
<EmailAddress>mkearns@cis.upenn.edu</EmailAddress>
<PI_PHON>2158987293</PI_PHON>
<NSF_ID>000440175</NSF_ID>
<StartDate>11/05/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Kearns</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Kearns</PI_FULL_NAME>
<EmailAddress>mkearns@cis.upenn.edu</EmailAddress>
<PI_PHON>2158987293</PI_PHON>
<NSF_ID>000440175</NSF_ID>
<StartDate>05/02/2011</StartDate>
<EndDate>11/05/2012</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sham</FirstName>
<LastName>Kakade</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sham Kakade</PI_FULL_NAME>
<EmailAddress>sham@cs.washington.edu</EmailAddress>
<PI_PHON>2065439344</PI_PHON>
<NSF_ID>000553028</NSF_ID>
<StartDate>05/02/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Mallesh</FirstName>
<LastName>Pai</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mallesh Pai</PI_FULL_NAME>
<EmailAddress>Mallesh.Pai@rice.edu</EmailAddress>
<PI_PHON>7133482289</PI_PHON>
<NSF_ID>000570733</NSF_ID>
<StartDate>05/02/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>AARON</FirstName>
<LastName>ROTH</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>AARON ROTH</PI_FULL_NAME>
<EmailAddress>aaroth@cis.upenn.edu</EmailAddress>
<PI_PHON>2158987293</PI_PHON>
<NSF_ID>000624673</NSF_ID>
<StartDate>11/05/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Pennsylvania</Name>
<CityName>Philadelphia</CityName>
<ZipCode>191046205</ZipCode>
<PhoneNumber>2158987293</PhoneNumber>
<StreetAddress>Research Services</StreetAddress>
<StreetAddress2><![CDATA[3451 Walnut St, 5th Flr Franklin]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042250712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042250712</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Pennsylvania]]></Name>
<CityName>Philadelphia</CityName>
<StateCode>PA</StateCode>
<ZipCode>191046205</ZipCode>
<StreetAddress><![CDATA[Research Services]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8052</Code>
<Text>Inter Com Sci Econ Soc S (ICE)</Text>
</ProgramElement>
<ProgramReference>
<Code>7752</Code>
<Text>CDI NON SOLICITED RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~997993</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Over the course of this award, we developed a theory around a surprising set of connections between data privacy and game theory and market design. These connections were of two types:First, it turned out that the decade-old literature on &ldquo;differential privacy&rdquo; in the algorithms community has provided a number of tools for designing novel mechanisms in game theoretic settings. In hindsight, this is natural: differential privacy provides a tool-kit for designing algorithms whose outcomes are incentivize to the unilateral changes in behavior of single individuals. Using these tools, we were able to design mechanisms for a variety of market settings that incentivize agents to report their true data to the coordinating algorithm, rather than attempting to strategize. These settings include:1)<span> </span>Matching students to schools, in settings in which both the students and schools have preferences over whom they are matched to.&nbsp;2)<span> </span>Choosing among several equilibria in a large interaction &ndash; i.e. an outcome in which nobody has any incentive to change their behavior.&nbsp;3)<span> </span>Pricing items to sell to buyers, in settings in which buyers might have complicated preferences over different bundles of items.&nbsp;4)<span> </span>Dynamically setting tolls on road networks, to manage traffic such that on average, commute time is minimized, in settings in which different drivers have different destinations in mind, and prefer shorter commutes to longer commutes.&nbsp;We also showed how the mathematics of differential privacy can be used to analyze the equilibria (i.e. the expected long-term outcomes) in repeated interactions when every individual has only a small effect on the market.&nbsp;Second, we used the tools of game theoretic analysis to study &ldquo;privacy&rdquo; as a quantity that can be bought and sold, and that has complicated interactions with individual incentives in market settings. For example:1)<span> </span>In a series of papers, we showed how a data buyer can price and procure private data, in settings in which the sellers experience a cost for the loss of their privacy. In this work, we use differential privacy as a tool to quantify the loss in utility that a seller might incur in the worst case, as a result of the private use of her data. By varying the privacy parameter, we can trade off between the costs of the sellers and the utility of the buyer, and mediate these tradeoffs with prices, so that everyone is incentivized to interact honestly with the market. We can do this even when the buyer cannot directly verify the data of the sellers.&nbsp;2)<span> </span>We show that introducing a new privacy technology into a market can have counter-intuitive affects, including actually -decreasing- the amount of privacy available to individuals, and increasing the amount of information that sellers learn about their customers. This is because, in equilibrium, introducing a new privacy technology into a market can cause both buyers and sellers to change their behavior in response, which changes the data that is made available to be processed by the privacy technology.&nbsp;In the course of conducting this research, we trained 4 PhD students and two postdocs, developed several new graduate and undergraduate courses, and disseminated our research broadly to both the algorithms and economics communities.&nbsp;</p><br> <p>            Last Modified: 11/16/2016<br>      Modified by: Aaron&nbsp;Roth</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Over the course of this award, we developed a theory around a surprising set of connections between data privacy and game theory and market design. These connections were of two types:First, it turned out that the decade-old literature on "differential privacy" in the algorithms community has provided a number of tools for designing novel mechanisms in game theoretic settings. In hindsight, this is natural: differential privacy provides a tool-kit for designing algorithms whose outcomes are incentivize to the unilateral changes in behavior of single individuals. Using these tools, we were able to design mechanisms for a variety of market settings that incentivize agents to report their true data to the coordinating algorithm, rather than attempting to strategize. These settings include:1) Matching students to schools, in settings in which both the students and schools have preferences over whom they are matched to. 2) Choosing among several equilibria in a large interaction &ndash; i.e. an outcome in which nobody has any incentive to change their behavior. 3) Pricing items to sell to buyers, in settings in which buyers might have complicated preferences over different bundles of items. 4) Dynamically setting tolls on road networks, to manage traffic such that on average, commute time is minimized, in settings in which different drivers have different destinations in mind, and prefer shorter commutes to longer commutes. We also showed how the mathematics of differential privacy can be used to analyze the equilibria (i.e. the expected long-term outcomes) in repeated interactions when every individual has only a small effect on the market. Second, we used the tools of game theoretic analysis to study "privacy" as a quantity that can be bought and sold, and that has complicated interactions with individual incentives in market settings. For example:1) In a series of papers, we showed how a data buyer can price and procure private data, in settings in which the sellers experience a cost for the loss of their privacy. In this work, we use differential privacy as a tool to quantify the loss in utility that a seller might incur in the worst case, as a result of the private use of her data. By varying the privacy parameter, we can trade off between the costs of the sellers and the utility of the buyer, and mediate these tradeoffs with prices, so that everyone is incentivized to interact honestly with the market. We can do this even when the buyer cannot directly verify the data of the sellers. 2) We show that introducing a new privacy technology into a market can have counter-intuitive affects, including actually -decreasing- the amount of privacy available to individuals, and increasing the amount of information that sellers learn about their customers. This is because, in equilibrium, introducing a new privacy technology into a market can cause both buyers and sellers to change their behavior in response, which changes the data that is made available to be processed by the privacy technology. In the course of conducting this research, we trained 4 PhD students and two postdocs, developed several new graduate and undergraduate courses, and disseminated our research broadly to both the algorithms and economics communities.        Last Modified: 11/16/2016       Submitted by: Aaron Roth]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
