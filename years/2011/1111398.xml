<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CGV: Large: Collaborative Research: Analyzing Images Through Time</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>423714.00</AwardTotalIntnAmount>
<AwardAmount>423714</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This collaborative research project leverages expertise of four research teams (IIS-1111415, Massachusetts Institute of Technology; IIS-1110955, Harvard University; IIS-1111398, Washington University; and IIS-1111534, Cornell University). Understanding time-varying processes and phenomena is fundamental to science and engineering. Due to tremendous progress in digital photography, images and videos (including images from webcams, time- lapse photography captured by scientists, surveillance videos, and Internet photo collections) are becoming an important source of information about our dynamic world. However, techniques for automated understanding and visualization of time-varying processes from images or videos are scarce and underdeveloped, requiring fundamental new models and algorithms for representing changes over time. This research involves creating systems that enable modeling, analysis, and visualization of time-varying processes based on image data. These models and algorithms will form the basis for a new set of tools that can help answer important questions about how our environment is changing, how our cities are evolving, and what significant events are happening around the world.&lt;br/&gt;&lt;br/&gt;Analyzing images over time poses fundamental new technical challenges. This project focuses on developing and demonstrating end-to-end systems consisting of (1) novel representations necessary to model time-varying image datasets; (2) algorithms for estimating long-range temporal correspondence in image datasets; (3) algorithms for decomposing image datasets into intuitive primitives such as shading, illumination, reflectance, and motion; (4) analysis tools for deriving higher level information from the decomposed representations (e.g., trends, repeated patterns, and unusual events); and (5) tools for visualization of the high-level information and methods for re-synthesis of image data.&lt;br/&gt;&lt;br/&gt;This work has the potential to have significant impact in a broad range of areas where images are generated over time, e.g., in ecology, astronomy, urban planning, health, and many others. The results of this research will be broadly disseminated by making source code and datasets publicly available via the project web site (https://groups.csail.mit.edu/vision/image_time/) and offering tutorials and organizing workshops at significant conferences. The project provides educational opportunities and offers hands-on collaborative research experience to students at both the undergraduate and graduate levels and the four institutions.</AbstractNarration>
<MinAmdLetterDate>08/19/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1111398</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Pless</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Pless</PI_FULL_NAME>
<EmailAddress>pless@gwu.edu</EmailAddress>
<PI_PHON>3142390536</PI_PHON>
<NSF_ID>000269630</NSF_ID>
<StartDate>08/19/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Washington University</Name>
<CityName>Saint Louis</CityName>
<ZipCode>631304862</ZipCode>
<PhoneNumber>3147474134</PhoneNumber>
<StreetAddress>CAMPUS BOX 1054</StreetAddress>
<StreetAddress2><![CDATA[1 Brookings Drive]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<StateCode>MO</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MO01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>068552207</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>WASHINGTON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068552207</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Washington University]]></Name>
<CityName>Saint Louis</CityName>
<StateCode>MO</StateCode>
<ZipCode>631304862</ZipCode>
<StreetAddress><![CDATA[CAMPUS BOX 1054]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Missouri</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MO01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~123331</FUND_OBLG>
<FUND_OBLG>2012~300383</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>ANALYZING IMAGES OVER TIME was a collaboration between researchers at MIT, Harvard, Cornell and Washington University in St Louis. &nbsp;The project created new mathematical representations and practical algorithms to interpret video, time-lapse imageryand images taken at different times, in order to understand our dynamic world. &nbsp;At a short time scale, we developed and demonstrated approaches to highlight and measure subtle motion in video caused by breathing or vibrations from sound. &nbsp;At the time scale of days, we developed new photometric stereo approaches to decompose imagery into lighting and surface characteristics, in order to use time-lapse data to reconstruct the shape of an outdoor scene. &nbsp;At time scales up to decades we created approaches to integrate Internet imagery from different viewpoints into coherent time-lapse videos that highlight what stays the same and what changes.</p> <p>&nbsp;</p> <p>This project supported the creation of a number of online resources. &nbsp;The Archive of Many Outdoor Scenes (http://amos.cse.wustl.edu) stores images from publicly available outdoor webcams in order to record a visual archive of our changing world. &nbsp;Our app, RePhoto (http://projectrephoto.org) provides an infrastructure so that anyone can start a visual monitoring program by creating an interface that makes it easy to take pictures that are exactly aligned with previous pictures. &nbsp;Finally, http://geocalibration.org is a web application to give the exact location from which a picture was taken if a user finds a few points that match between the image and Google maps.</p> <p><br />The broader impacts of this project result from both the new analytic tools and the sharing of online resources. &nbsp;Our novel approaches to visual measurement have direct applications to civil engineering and environmental measurement. &nbsp;The online resources are widely available and in use by Citizen Science projects on three continents in a variety of ecology and outreach applications.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/07/2016<br>      Modified by: Robert&nbsp;Pless</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2016/1111398/1111398_10124302_1460012597749_heliometric2--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1111398/1111398_10124302_1460012597749_heliometric2--rgov-800width.jpg" title="Heliometric Stereo"><img src="/por/images/Reports/POR/2016/1111398/1111398_10124302_1460012597749_heliometric2--rgov-66x44.jpg" alt="Heliometric Stereo"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A time-lapse (top row) can be decomposed into lighting, surface color, surface orientation, and orientation detail (second row).  The surface orientation is derived based on how bright surfaces are when the sun is in different locations; they are shown here as a false-color image.</div> <div class="imageCredit">Robert Pless and Austin Abrams</div> <div class="imageSubmitted">Robert&nbsp;Pless</div> <div class="imageTitle">Heliometric Stereo</div> </div> </li> <li> <a href="/por/images/Reports/POR/2016/1111398/1111398_10124302_1460011820508_MCA--rgov-214x142.jpg" original="/por/images/Reports/POR/2016/1111398/1111398_10124302_1460011820508_MCA--rgov-800width.jpg" title="Motion Component Analysis"><img src="/por/images/Reports/POR/2016/1111398/1111398_10124302_1460011820508_MCA--rgov-66x44.jpg" alt="Motion...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ ANALYZING IMAGES OVER TIME was a collaboration between researchers at MIT, Harvard, Cornell and Washington University in St Louis.  The project created new mathematical representations and practical algorithms to interpret video, time-lapse imageryand images taken at different times, in order to understand our dynamic world.  At a short time scale, we developed and demonstrated approaches to highlight and measure subtle motion in video caused by breathing or vibrations from sound.  At the time scale of days, we developed new photometric stereo approaches to decompose imagery into lighting and surface characteristics, in order to use time-lapse data to reconstruct the shape of an outdoor scene.  At time scales up to decades we created approaches to integrate Internet imagery from different viewpoints into coherent time-lapse videos that highlight what stays the same and what changes.     This project supported the creation of a number of online resources.  The Archive of Many Outdoor Scenes (http://amos.cse.wustl.edu) stores images from publicly available outdoor webcams in order to record a visual archive of our changing world.  Our app, RePhoto (http://projectrephoto.org) provides an infrastructure so that anyone can start a visual monitoring program by creating an interface that makes it easy to take pictures that are exactly aligned with previous pictures.  Finally, http://geocalibration.org is a web application to give the exact location from which a picture was taken if a user finds a few points that match between the image and Google maps.   The broader impacts of this project result from both the new analytic tools and the sharing of online resources.  Our novel approaches to visual measurement have direct applications to civil engineering and environmental measurement.  The online resources are widely available and in use by Citizen Science projects on three continents in a variety of ecology and outreach applications.                Last Modified: 04/07/2016       Submitted by: Robert Pless]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
