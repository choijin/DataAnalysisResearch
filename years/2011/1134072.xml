<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Support for Workshop on Advances in Language and Vision</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2011</AwardEffectiveDate>
<AwardExpirationDate>02/29/2012</AwardExpirationDate>
<AwardTotalIntnAmount>28720.00</AwardTotalIntnAmount>
<AwardAmount>28720</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project supports travel expenses for participants at the workshop on advances in language and vision. In the past few years, great progress has been made in the fields of language and computer vision in developing technologies of extracting semantic content from text and imagery respectively. Each field has desires to adapt methods from the other, but often looking to the past literature rather than the current state of the art. This workshop makes significant scientific progress in multimodal representations and methods by bringing together the top researchers in both fields. The well organized brainstorming and discussion sessions contribute new ideas to this emerging area.  The outcome of the workshop provides some guidelines for targeted research in this interdisciplinary area, including anticipated fundamental scientific advances, possible large-scale challenge problems, the needs and prospects for available datasets, and connections to significant applications and their associated long-term economic impact and other societal benefits.</AbstractNarration>
<MinAmdLetterDate>03/25/2011</MinAmdLetterDate>
<MaxAmdLetterDate>11/08/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1134072</AwardID>
<Investigator>
<FirstName>Trevor</FirstName>
<LastName>Darrell</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Trevor J Darrell</PI_FULL_NAME>
<EmailAddress>trevor@eecs.berkeley.edu</EmailAddress>
<PI_PHON>4156900822</PI_PHON>
<NSF_ID>000175078</NSF_ID>
<StartDate>03/25/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[National Science Foundation]]></Name>
<CityName/>
<StateCode>VA</StateCode>
<ZipCode>222031859</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>VA08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~28720</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The fields of language and vision have separately made great progress in recent years, developing automated extraction of semantic content in text and imageryrespectively. The state of the art of these fields are rapidly encroaching each other:language is increasingly focused on how to &ldquo;ground&rdquo; meaning in physical observations,and vision is exploiting ontological structures and trying to &ldquo;tell a story&rdquo; from an image, relating objects, activities, people, and scenes. These potentially transformative open research questions indicate that the time is ripe to start understandingand constructing deeper connections between imagery and its associatedlanguage. And in particular, it is time for these two fields to start communicatingand collaborating in a richer manner.</p> <p>Two workshops were supported in part by this NSF grant and were held to explore future research directions in languageand vision; one at NSF on May 17-18th, 2011, and one held at the NIPS conferenceon December 16th, 2011. Participants observed that considerable progresshas been made in each field by adopting relatively shallow models from the other field, and that this was likely to continue to bear fruit in various near term internetsearch-style applications. However, tranformative progress requires a significantadvance towards a symmetric integrated approach, with corresponding &ldquo;deep&rdquo; semantic representations.</p> <p>Recommendations for several specific research themes along this line included</p> <p>:&bull; &ldquo;understanding understanding&rdquo; - going beyond recognizing individual imageelements to grasp the underlying meaning of a picture. This will involve&nbsp;moving beyond recognition as a list of objects present (ie what is in the picture)to determine what information is useful to extract from an image, possiblyincluding predictions of relationships, appearance characteristics, andwhat is happening or what will happen in the underlying world captured ina photo or video.</p> <p>&bull; &ldquo;visual entailment&rdquo; - algorithms to verify whether statements can be inferredfrom images or videos. Similar to the first goal, this will require adeeper understanding of an image or video and will be necessary for interactionand communication with human consumers of computer vision. Thismay be framed as a visual Jeopardy problem.</p> <p>&bull; &ldquo;seeing between the lines&rdquo; - collecting basic mundane facts about the worldto improve image and language understanding. Current research has movedaway from collecting large quantities of world knowledge, but simple knowledgeabout specific environments, or the world in general may be necessaryfor building effective visual systems that can operate in the real world</p> <p>.&bull;a &ldquo;semantic survival kit&rdquo; - determining what to recognize first. In order tobuild a universal vision system we need to determine a set of basic recognitionunits and priority for what things are most important to be able torecognize. Some suggestions from the workshops included: first wordslearned in a second language, vocabulary words used by young children,words explained in how things work resources.</p> <p>For more details see the workshop materials available on the web the the URL:&nbsp;<a href="https://sites.google.com/site/languagevisionworkshop/">https://sites.google.com/site/languagevisionworkshop/</a></p> <p>&nbsp;</p><br> <p>            Last Modified: 06/21/2012<br>      Modified by: Trevor&nbsp;J&nbsp;Darrell</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The fields of language and vision have separately made great progress in recent years, developing automated extraction of semantic content in text and imageryrespectively. The state of the art of these fields are rapidly encroaching each other:language is increasingly focused on how to "ground" meaning in physical observations,and vision is exploiting ontological structures and trying to "tell a story" from an image, relating objects, activities, people, and scenes. These potentially transformative open research questions indicate that the time is ripe to start understandingand constructing deeper connections between imagery and its associatedlanguage. And in particular, it is time for these two fields to start communicatingand collaborating in a richer manner.  Two workshops were supported in part by this NSF grant and were held to explore future research directions in languageand vision; one at NSF on May 17-18th, 2011, and one held at the NIPS conferenceon December 16th, 2011. Participants observed that considerable progresshas been made in each field by adopting relatively shallow models from the other field, and that this was likely to continue to bear fruit in various near term internetsearch-style applications. However, tranformative progress requires a significantadvance towards a symmetric integrated approach, with corresponding "deep" semantic representations.  Recommendations for several specific research themes along this line included  :&bull; "understanding understanding" - going beyond recognizing individual imageelements to grasp the underlying meaning of a picture. This will involve moving beyond recognition as a list of objects present (ie what is in the picture)to determine what information is useful to extract from an image, possiblyincluding predictions of relationships, appearance characteristics, andwhat is happening or what will happen in the underlying world captured ina photo or video.  &bull; "visual entailment" - algorithms to verify whether statements can be inferredfrom images or videos. Similar to the first goal, this will require adeeper understanding of an image or video and will be necessary for interactionand communication with human consumers of computer vision. Thismay be framed as a visual Jeopardy problem.  &bull; "seeing between the lines" - collecting basic mundane facts about the worldto improve image and language understanding. Current research has movedaway from collecting large quantities of world knowledge, but simple knowledgeabout specific environments, or the world in general may be necessaryfor building effective visual systems that can operate in the real world  .&bull;a "semantic survival kit" - determining what to recognize first. In order tobuild a universal vision system we need to determine a set of basic recognitionunits and priority for what things are most important to be able torecognize. Some suggestions from the workshops included: first wordslearned in a second language, vocabulary words used by young children,words explained in how things work resources.  For more details see the workshop materials available on the web the the URL: https://sites.google.com/site/languagevisionworkshop/          Last Modified: 06/21/2012       Submitted by: Trevor J Darrell]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
