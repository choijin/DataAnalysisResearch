<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Workshop on Questionnaire Design Issues in Longitudinal and Repeated Cross-sectional Surveys: Balancing Continuity and Innovation at Duke University in February 2011</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2011</AwardEffectiveDate>
<AwardExpirationDate>12/31/2011</AwardExpirationDate>
<AwardTotalIntnAmount>38235.00</AwardTotalIntnAmount>
<AwardAmount>38235</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Brian Humes</SignBlockName>
<PO_EMAI>bhumes@nsf.gov</PO_EMAI>
<PO_PHON>7032927284</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project organizes a two-day workshop for survey methodologists and practitioners. The workshop will be held at Duke University in February 2011, and will consist of panels devoted to best practices for proposing, testing, and implementing changes to questionnaires used in longitudinal and repeated cross-sectional surveys. The workshop will feature some fifteen presenters, all of whom are being recruited on the basis of their leadership roles in major longitudinal and cross-sectional surveys and their publishing record in survey methodology topics relevant to the workshop. In total, roughly seventy researchers will participate in the workshop.&lt;br/&gt;&lt;br/&gt;Recurring surveys face a unique set of challenges--most notably, the need to balance comparability and continuity over time with any innovations in the questionnaire. The workshop is expressly designed to help clarify the appropriate standards for considering and implementing questionnaire changes in longitudinal studies, thereby informing the decision making of study investigators. The conference is also intended to encourage a broader dialogue between investigators across various major longitudinal studies, and across academic, government, non-profit, and commercial sectors. Moreover, the workshop aims to enhance future social science research: the guidance on measurement decisions it offers investigators will also inform the substantive analyses of, and conclusions reached by, the user community.&lt;br/&gt;&lt;br/&gt;The workshop makes significant and substantial broader contributions. Large-scale recurring surveys are costly projects often made possible through taxpayer support, either through government agencies (e.g., National Health Interview Survey, Current Population Survey) or through NSF grant support for "infrastructure" recurring surveys--the American National Election Studies (ANES), the General Social Survey (GSS), and the Panel Survey on Income Dynamics (PSID). These surveys serve large and diverse communities of scholars, policy-makers, and businesses, and it is essential that the data they provide be of top quality. Achieving and maintaining this standard is an ongoing challenge; this conference provides a valuable service by creating a forum in which to discuss the current state-of-the-science in updating recurring surveys and to create guidelines for broad use by those implementing such survey protocols.</AbstractNarration>
<MinAmdLetterDate>12/17/2010</MinAmdLetterDate>
<MaxAmdLetterDate>12/17/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1110341</AwardID>
<Investigator>
<FirstName>D. Sunshine</FirstName>
<LastName>Hillygus</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>D. Sunshine Hillygus</PI_FULL_NAME>
<EmailAddress>hillygus@duke.edu</EmailAddress>
<PI_PHON>9196604341</PI_PHON>
<NSF_ID>000498237</NSF_ID>
<StartDate>12/17/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Duke University</Name>
<CityName>Durham</CityName>
<ZipCode>277054010</ZipCode>
<PhoneNumber>9196843030</PhoneNumber>
<StreetAddress>2200 W. Main St, Suite 710</StreetAddress>
<StreetAddress2><![CDATA[Erwin Square]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>044387793</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DUKE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>044387793</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Duke University]]></Name>
<CityName>Durham</CityName>
<StateCode>NC</StateCode>
<ZipCode>277054010</ZipCode>
<StreetAddress><![CDATA[2200 W. Main St, Suite 710]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1371</Code>
<Text>Political Science</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~38235</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Researchers often use surveys to understand how people view the world, their own situation, their communities, politics, marketing techniques, or any number of aspects of everyday life.&nbsp; Many of these surveys are repeated over time to gauge how these things change over time. &nbsp;Updating the questionnaires in such data collections is both essential and problematic. Survey technologies change. Respondent expectations change. Contemporary topics, understandings, and vocabularies change. Budgets shrink; the costs of securing responses seems always to rise. Maintaining the quality of the data gathered &ndash; their relevance, reliability, and validity &ndash; in a context where the intent is to be able to generalize from a sample to the larger population demands that researchers adapt. Yet adapting without losing the ability to make meaningful comparisons across years is a daunting challenge.&nbsp;</p> <p>The key rationale motivating the conference on <em>Questionnaire Issues in Longitudinal and Repeated Cross-Sectional Surveys </em>was the absence of a clear set of standards or guidelines concerning how investigators can best propose, evaluate, and implement questionnaire changes in recurring surveys. The conference took place on the campus of Duke University on February 18, 2011, and the program featured 18 of the most prominent experts on survey research.&nbsp; Over 200 scholars and practitioners from around the country attended the event.</p> <p>The reasons for change and standards for assessing changes were discussed at length, and the insights led to a better understanding of the many factors that affect survey meaning that extend far beyond changes to questionnaires.&nbsp; Experts who work on a variety of surveys discussed how they use experiments and field testing to implement and test changes to their surveys, and recommended several experimental designs and analytical tools to use in the process.&nbsp; A broad recommendation for complete openness and transparency in the process of making survey changes emerged.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p> <p>While there still exists no formal consensus regarding changes to longitudinal and repeated cross-sectional surveys, the conference provided a forum for experts to come together and share their own practices and ideas.&nbsp; The result is that the survey community is much better informed about the practices and procedures being used by experts in the field.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/07/2012<br>      Modified by: D. Sunshine&nbsp;Hillygus</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Researchers often use surveys to understand how people view the world, their own situation, their communities, politics, marketing techniques, or any number of aspects of everyday life.  Many of these surveys are repeated over time to gauge how these things change over time.  Updating the questionnaires in such data collections is both essential and problematic. Survey technologies change. Respondent expectations change. Contemporary topics, understandings, and vocabularies change. Budgets shrink; the costs of securing responses seems always to rise. Maintaining the quality of the data gathered &ndash; their relevance, reliability, and validity &ndash; in a context where the intent is to be able to generalize from a sample to the larger population demands that researchers adapt. Yet adapting without losing the ability to make meaningful comparisons across years is a daunting challenge.   The key rationale motivating the conference on Questionnaire Issues in Longitudinal and Repeated Cross-Sectional Surveys was the absence of a clear set of standards or guidelines concerning how investigators can best propose, evaluate, and implement questionnaire changes in recurring surveys. The conference took place on the campus of Duke University on February 18, 2011, and the program featured 18 of the most prominent experts on survey research.  Over 200 scholars and practitioners from around the country attended the event.  The reasons for change and standards for assessing changes were discussed at length, and the insights led to a better understanding of the many factors that affect survey meaning that extend far beyond changes to questionnaires.  Experts who work on a variety of surveys discussed how they use experiments and field testing to implement and test changes to their surveys, and recommended several experimental designs and analytical tools to use in the process.  A broad recommendation for complete openness and transparency in the process of making survey changes emerged.                 While there still exists no formal consensus regarding changes to longitudinal and repeated cross-sectional surveys, the conference provided a forum for experts to come together and share their own practices and ideas.  The result is that the survey community is much better informed about the practices and procedures being used by experts in the field.             Last Modified: 02/07/2012       Submitted by: D. Sunshine Hillygus]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
