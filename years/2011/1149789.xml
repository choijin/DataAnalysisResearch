<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Machine Learning Methods and Statistical Analysis Tools for Single Network Domains</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2012</AwardEffectiveDate>
<AwardExpirationDate>12/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>496640.00</AwardTotalIntnAmount>
<AwardAmount>496640</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
<PO_EMAI>sspengle@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>CAREER: Machine Learning Methods and Statistical Analysis Tools for Single Network Domains&lt;br/&gt;&lt;br/&gt;Machine learning researchers focus on two distinct learning scenarios for structured network data (i.e., where there are statistical dependencies among the attributes of linked nodes). In the first scenario, the domain consists of a population of structured examples (e.g., chemical compounds) and we can reason about learning algorithms asymptotically, as the number of structured examples increases. In the second scenario, the domain consists of a single, potentially infinite-sized network (e.g., the World Wide Web). In these "single network" domains, an increase in data corresponds to acquiring a larger portion of the underlying network. Even when there are a set of network samples available for learning and prediction, they correspond to subnetworks drawn from the same underlying network and thus may be dependent. &lt;br/&gt;&lt;br/&gt;Although estimation and inference methods from the field of statistical relational learning have been successfully applied in single-network domains, the algorithms were initially developed for populations of networks, and thus the theoretical foundation for learning and inference in single networks is scant. This work focuses on the development of robust statistical methods for single network domains -- since many large network datasets about complex systems rarely have more than a few subnetworks available for model estimation and evaluation. Specifically, the aims of the project include (1) strengthening the theoretical foundation for learning in single network domains, (2) creating accurate methods for determining the significance of discovered patterns and features, (3) formulating novel model selection and evaluation methods, and (4) developing improved approaches for network learning and prediction based on the unique characteristics of single network domains.&lt;br/&gt;&lt;br/&gt;The research will enhance our understanding of the mechanisms that influence the performance of network analysis methods and drive the development novel methods for complex network domains. Expanding the applicability of machine learning techniques for single network domains could have a transformational impact across a broad range of areas (e.g., psychology, communications, education, political science) where current methods limit research to the investigation of processes in dyad or small group settings. Also, the project results will serve as an example application of computer science in the broader network science context, which will attract and retain students that might not otherwise be engaged by conventional CS topics. For more details see:&lt;br/&gt;http://www.cs.purdue.edu/homes/neville/research-nsf-career.html</AbstractNarration>
<MinAmdLetterDate>01/05/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/10/2016</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1149789</AwardID>
<Investigator>
<FirstName>Jennifer</FirstName>
<LastName>Neville</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jennifer Neville</PI_FULL_NAME>
<EmailAddress>neville@cs.purdue.edu</EmailAddress>
<PI_PHON>7654944600</PI_PHON>
<NSF_ID>000171629</NSF_ID>
<StartDate>01/05/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<StreetAddress2><![CDATA[155 S Grant Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072051394</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PURDUE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072051394</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Purdue University]]></Name>
<CityName/>
<StateCode>IN</StateCode>
<ZipCode>479072107</ZipCode>
<StreetAddress><![CDATA[305 N. University Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~91471</FUND_OBLG>
<FUND_OBLG>2013~95167</FUND_OBLG>
<FUND_OBLG>2014~99087</FUND_OBLG>
<FUND_OBLG>2015~103247</FUND_OBLG>
<FUND_OBLG>2016~107668</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Although estimation and inference methods from the field of statistical relational learning have been successfully applied in single-network domains, the theoretical foundation for learning in single network domains is scant. In addition, the development of predictive models has mainly focused on increasing classification accuracy and relatively little attention has been paid to the issues of feature selection and hypothesis testing. Lastly, the structure of a network dataset is affected by both the sampling process and the availability of node/link information. In single network domains, due to the complex dependencies among instances, these factors may have a large impact on the quality of learning/inference. This project aimed to explore these issues to strengthen the foundation for statistical relational learning.</p> <p><strong>Intellectual merit</strong>: We investigated these issues from several angles---analytical, empirical, and algorithmic---while trying to bridge the gap between structured machine learning methods and statistical network analysis. We thought carefully about the underlying assumptions, problem formulations, and methodology used for learning over networks, and used it to develop theoretical characterizations of network learning and inference methods. This analysis informed the development of diverse algorithms spanning the full machine learning spectrum, including sampling, learning, inference, hypothesis testing, model selection, descriptive modeling, and evaluation. We discuss two illustrative efforts from the project below.</p> <p>Much of the work in relational learning has been based on an (implicit) assumption of the availability of a fully labeled network for learning. In this project, we focused on semi-supervised learning in partially-labeled networks. Although relational EM methods can significantly improve predictive performance in densely labeled networks, they could not achieve the same gains in sparsely labeled networks and often can performed worse than methods that ignore the relational information all together. We showed that the fixed-point methods that relational EM uses for approximate learning and inference result in errors that prevent convergence in sparsely labeled networks. A central finding is the tie between the stability of relational inference and the maximal eigenvalue of the inference solution, which can be used to determine if a particular network is prone to convergence issues based on the structure of label availability. As a solution, we develop a relational data augmentation method that represents uncertainty over label predictions and parameter estimates to ensure better convergence in all types of networks. To develop a more practical, scalable solution for massive networks, we had the key idea to augment the inference step of the fixed point EM algorithm to include a maximum entropy constraint, where the predicted label proportions (i.e., percentage predicted positive vs. negative) are forced to align with the proportions observed in the training set. The method adjusts the label predictions at every step of collective inference so they adhere to maximum entropy constraints.&nbsp;</p> <p>Another important task in network analysis is the detection of anomalous events in a network time series. Previous work has focused on detecting anomalies through the use of network statistics that summarize the behavior of the network. However, we show that many common network statistics are dependent on confounding factors like the total number of nodes or edges, and use of these statistics leads to incorrect conclusions (i.e., both false positives and false negatives). We formalize the use of network statistics in graphs of varying sizes and shows that detection errors are due to the use of size inconsistent or size divergent statistics. To remedy the issue, we introduce the statistical property of size consistency for network statistics and shows theoretically how the property ensures accurate detection of changes or anomalies in dynamic networks. We then propose three size consistent network statistics: mass shift, degree shift, and triangle probability, which are provably size consistent. With extensive synthetic and semi-synthetic experiments, we demonstrate that anomaly detectors using the new size-consistent statistics exhibit superior detection performance in dynamic networks where the message and edge counts vary.</p> <p><strong>Broader impact</strong>: The work described above, while published in data mining and machine learning venues, is likely to have a broad impact across network-centric fields where researchers study the properties of complex systems by modeling the data as a network or graph. In these fields, a deeper understanding of the statistical and theoretical properties of analytic methods, as well as improved algorithms, will increase the accuracy and robustness of these investigations. Moreover, this project has had a small, but important, impact on the education and training of several URMs students. As these individuals graduate and move to academic and industrial STEM settings, they will serve as role models in the areas of machine learning, graph mining, and network science more broadly.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/19/2019<br>      Modified by: Jennifer&nbsp;Neville</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Although estimation and inference methods from the field of statistical relational learning have been successfully applied in single-network domains, the theoretical foundation for learning in single network domains is scant. In addition, the development of predictive models has mainly focused on increasing classification accuracy and relatively little attention has been paid to the issues of feature selection and hypothesis testing. Lastly, the structure of a network dataset is affected by both the sampling process and the availability of node/link information. In single network domains, due to the complex dependencies among instances, these factors may have a large impact on the quality of learning/inference. This project aimed to explore these issues to strengthen the foundation for statistical relational learning.  Intellectual merit: We investigated these issues from several angles---analytical, empirical, and algorithmic---while trying to bridge the gap between structured machine learning methods and statistical network analysis. We thought carefully about the underlying assumptions, problem formulations, and methodology used for learning over networks, and used it to develop theoretical characterizations of network learning and inference methods. This analysis informed the development of diverse algorithms spanning the full machine learning spectrum, including sampling, learning, inference, hypothesis testing, model selection, descriptive modeling, and evaluation. We discuss two illustrative efforts from the project below.  Much of the work in relational learning has been based on an (implicit) assumption of the availability of a fully labeled network for learning. In this project, we focused on semi-supervised learning in partially-labeled networks. Although relational EM methods can significantly improve predictive performance in densely labeled networks, they could not achieve the same gains in sparsely labeled networks and often can performed worse than methods that ignore the relational information all together. We showed that the fixed-point methods that relational EM uses for approximate learning and inference result in errors that prevent convergence in sparsely labeled networks. A central finding is the tie between the stability of relational inference and the maximal eigenvalue of the inference solution, which can be used to determine if a particular network is prone to convergence issues based on the structure of label availability. As a solution, we develop a relational data augmentation method that represents uncertainty over label predictions and parameter estimates to ensure better convergence in all types of networks. To develop a more practical, scalable solution for massive networks, we had the key idea to augment the inference step of the fixed point EM algorithm to include a maximum entropy constraint, where the predicted label proportions (i.e., percentage predicted positive vs. negative) are forced to align with the proportions observed in the training set. The method adjusts the label predictions at every step of collective inference so they adhere to maximum entropy constraints.   Another important task in network analysis is the detection of anomalous events in a network time series. Previous work has focused on detecting anomalies through the use of network statistics that summarize the behavior of the network. However, we show that many common network statistics are dependent on confounding factors like the total number of nodes or edges, and use of these statistics leads to incorrect conclusions (i.e., both false positives and false negatives). We formalize the use of network statistics in graphs of varying sizes and shows that detection errors are due to the use of size inconsistent or size divergent statistics. To remedy the issue, we introduce the statistical property of size consistency for network statistics and shows theoretically how the property ensures accurate detection of changes or anomalies in dynamic networks. We then propose three size consistent network statistics: mass shift, degree shift, and triangle probability, which are provably size consistent. With extensive synthetic and semi-synthetic experiments, we demonstrate that anomaly detectors using the new size-consistent statistics exhibit superior detection performance in dynamic networks where the message and edge counts vary.  Broader impact: The work described above, while published in data mining and machine learning venues, is likely to have a broad impact across network-centric fields where researchers study the properties of complex systems by modeling the data as a network or graph. In these fields, a deeper understanding of the statistical and theoretical properties of analytic methods, as well as improved algorithms, will increase the accuracy and robustness of these investigations. Moreover, this project has had a small, but important, impact on the education and training of several URMs students. As these individuals graduate and move to academic and industrial STEM settings, they will serve as role models in the areas of machine learning, graph mining, and network science more broadly.                Last Modified: 06/19/2019       Submitted by: Jennifer Neville]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
