<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Noise and strong analog error-correcting codes in neural computation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2011</AwardEffectiveDate>
<AwardExpirationDate>09/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>175000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project aims to uncover the existence of a qualitatively better class of analog error-correcting codes than previously known in the brain, show how such codes can be used and decoded, and develop the theory for quantifying the performance of such codes.  &lt;br/&gt;&lt;br/&gt;Information theory was introduced into neuroscience relatively early, and the theory of efficient (source) coding has been widely embraced in the sensory neurosciences. However, the second branch of information theory, which deals with the maximally parsimonious addition of redundancy to recover signal from noise, has curiously not made inroads in neuroscience. Shannon's channel coding theorem revealed the existence of codes that make possible error correction at efficiencies previously thought impossible.&lt;br/&gt;&lt;br/&gt;The investigator's central hypothesis is that the brain routinely employs such error correcting codes and the machinery required to decode and work with them. The hypothesis is motivated by a recent analysis of the grid cell code for animal location by the investigator and colleagues, showing it has unprecedented error-correction properties compared to known population codes in the brain (Sreenivasan &amp; Fiete, 2011). The investigator proposes to: 1) Develop definitions and constraints for analog neural codes, to apply the channel coding framework to neural codes and thus characterize their "goodness" on error-correction. 2) Identify high-level coding properties that enable strong error-correction, and search for these properties in observed but poorly understood neural codes. At the same time, explore strong theoretical error-correcting codes that the brain may plausibly implement. 3) Model plausible neural mechanisms for decoding such codes. Decoding is inference, so this question can be more generally thought of as exploring neural mechanisms for hierarchical inference. &lt;br/&gt;&lt;br/&gt;This project is computational and theoretical, and also involves close collaboration with neurophysiologists, to apply quantification techniques to neural data and work with experiments to inform the theories and test predictions.</AbstractNarration>
<MinAmdLetterDate>08/31/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/31/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1148973</AwardID>
<Investigator>
<FirstName>Ila</FirstName>
<LastName>Fiete</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ila Fiete</PI_FULL_NAME>
<EmailAddress>fiete@mit.edu</EmailAddress>
<PI_PHON>6172589363</PI_PHON>
<NSF_ID>000536962</NSF_ID>
<StartDate>08/31/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>170230239</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TEXAS AT AUSTIN</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042000273</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Texas at Austin]]></Name>
<CityName>Austin</CityName>
<StateCode>TX</StateCode>
<ZipCode>787120805</ZipCode>
<StreetAddress><![CDATA[1 University Station, C7000]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~175000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project has focused on the role that noise in neurons can play in degrading memory and learning and understanding whether good coding strategies can mitigate the effects of noise, and to what degree.&nbsp;</p> <p>&nbsp;</p> <p>Because memory involves holding information over time, ongoing noise that&nbsp;perturbs the activity of neurons that store such information will tend to reduce the fidelity of the representation over time. Further, noise during the process of development when networks of neurons must organize into useful computational structures can impede the process.</p> <p>&nbsp;</p> <p>Our work has quantified exactly the time-course of degradation of information stored in a large class of neural networks whose architectures are tuned to represent and store analog variables, as a function of the number of neurons involved in such storage [1]. In a related project, we ask whether, using clever ``coding&rsquo;&rsquo; strategies, it is possible to improve memory accuracy over time even when the noise per neuron is kept fixed, and if so, what is the best possible achievable accuracy. For instance, thanks to the clever coding of music written on a CD, it is possible to get a good rendition of the piece even when the CD is substantially scratched. We combined techniques and results from modern coding theory to models in neuroscience to obtain a bound on best possible accuracy in the presence of noise. In collaboration with psychologists we showed that human memory behavior appears to follow a similar time-dependence as the bound [2]. Finally, we have shown that observed rules for how neural connection strengths change can lead to the development of memory systems, in particular the grid cell system that plays a role in spatial memory and navigation. We show that the very specific architectures required for spatial computation and memory can be learned by these rules, and moreover, are robust to a degree of ongoing neural noise. This work results in a number of testable predictions for experiments on how grid cell activity will gradually evolve during development, and raises a number of new questions for theory and experiment, on the role of noise during network development [3].</p> <p>&nbsp;</p> <p>These projects have led to a better understanding of how noise affects computations in the brain, and how the brain can rein in the undesirable effects of noise with appropriate strategies for development and for representing information.</p> <p>&nbsp;</p> <p>This award has helped to train a number of undergraduate, graduate, and postdoctoral students on how to generate interesting questions and solve new problems, adopt tools from one field to apply in another, generate new ideas and conceptual viewpoints, and to effectively present findings to international audiences.</p> <p>&nbsp;</p> <p>Finally, with help from this award, the PI has developed a new undergraduate class called Quantitative Methods in Neuroscience. The aim is to train a new generation of biology and neurobiology students about quantitative tools, techniques, data analytics, and ways of thinking, that are fast becoming indispensible as biology transitions from being a primarily descriptive field into a quantitative one. Students finding themselves in walks of life outside of academics will find that quantitative tools like the ones developed in this course are in demand in various professions, now that data collection and analysis is becoming an integral part of fields as diverse as marketing and advertising, finance, medicine, fashion, and biotechnology.</p> <p>[1] Y. Burak and&nbsp;<strong>I.R. Fiete</strong>. Fundamental limits on persistent activity in networks of noisy neurons.&nbsp;<em>Proceedings of the National Academy of Sciences</em>&nbsp;109(43) 17645-17650 (2012).</p> <p>[2] O. Koyluoglu, Y. Pertzov, S. Manohar, M. Husain,&nbsp;<strong>I.R. Fiete</strong>. Fundamental limits on capaci...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project has focused on the role that noise in neurons can play in degrading memory and learning and understanding whether good coding strategies can mitigate the effects of noise, and to what degree.      Because memory involves holding information over time, ongoing noise that perturbs the activity of neurons that store such information will tend to reduce the fidelity of the representation over time. Further, noise during the process of development when networks of neurons must organize into useful computational structures can impede the process.     Our work has quantified exactly the time-course of degradation of information stored in a large class of neural networks whose architectures are tuned to represent and store analog variables, as a function of the number of neurons involved in such storage [1]. In a related project, we ask whether, using clever ``codingÆÆ strategies, it is possible to improve memory accuracy over time even when the noise per neuron is kept fixed, and if so, what is the best possible achievable accuracy. For instance, thanks to the clever coding of music written on a CD, it is possible to get a good rendition of the piece even when the CD is substantially scratched. We combined techniques and results from modern coding theory to models in neuroscience to obtain a bound on best possible accuracy in the presence of noise. In collaboration with psychologists we showed that human memory behavior appears to follow a similar time-dependence as the bound [2]. Finally, we have shown that observed rules for how neural connection strengths change can lead to the development of memory systems, in particular the grid cell system that plays a role in spatial memory and navigation. We show that the very specific architectures required for spatial computation and memory can be learned by these rules, and moreover, are robust to a degree of ongoing neural noise. This work results in a number of testable predictions for experiments on how grid cell activity will gradually evolve during development, and raises a number of new questions for theory and experiment, on the role of noise during network development [3].     These projects have led to a better understanding of how noise affects computations in the brain, and how the brain can rein in the undesirable effects of noise with appropriate strategies for development and for representing information.     This award has helped to train a number of undergraduate, graduate, and postdoctoral students on how to generate interesting questions and solve new problems, adopt tools from one field to apply in another, generate new ideas and conceptual viewpoints, and to effectively present findings to international audiences.     Finally, with help from this award, the PI has developed a new undergraduate class called Quantitative Methods in Neuroscience. The aim is to train a new generation of biology and neurobiology students about quantitative tools, techniques, data analytics, and ways of thinking, that are fast becoming indispensible as biology transitions from being a primarily descriptive field into a quantitative one. Students finding themselves in walks of life outside of academics will find that quantitative tools like the ones developed in this course are in demand in various professions, now that data collection and analysis is becoming an integral part of fields as diverse as marketing and advertising, finance, medicine, fashion, and biotechnology.  [1] Y. Burak and I.R. Fiete. Fundamental limits on persistent activity in networks of noisy neurons. Proceedings of the National Academy of Sciences 109(43) 17645-17650 (2012).  [2] O. Koyluoglu, Y. Pertzov, S. Manohar, M. Husain, I.R. Fiete. Fundamental limits on capacity and forgetting in human short-term memory. (In revision for Nature Neuroscience).   [3] J. Widloski and I.R. Fiete. A model of grid cell development based on spatial exploration and spike time-dependent plasticity. Neuron 83(2): 481&ndash;49...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
