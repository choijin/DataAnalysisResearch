<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: Small: Approximate Augmented Lagrangians: First-Order and Parallel Optimization Methods, with Applications to Stochastic Programming</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2011</AwardEffectiveDate>
<AwardExpirationDate>06/30/2015</AwardExpirationDate>
<AwardTotalIntnAmount>358499.00</AwardTotalIntnAmount>
<AwardAmount>358499</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rahul Shah</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Continuous optimization is a mathematical discipline with extensive&lt;br/&gt;applications in engineering design and business/logistical planning.&lt;br/&gt;Its currently most common solution techniques are difficult to adapt&lt;br/&gt;to newly evolving computer architectures comprising dozens to&lt;br/&gt;thousands of processing elements working in parallel.  Combining&lt;br/&gt;several existing techniques with some recent results of the principal&lt;br/&gt;investigator, this project explores a means of solving continuous&lt;br/&gt;optimization problems that should adapt more readily to parallel&lt;br/&gt;computer architectures than present standard solvers, allowing the&lt;br/&gt;architectures' full power to be brought to bear on large,&lt;br/&gt;time-consuming problems.  Without such new solution approaches,&lt;br/&gt;solution of critical design and planning problems may not benefit from&lt;br/&gt;most of the advances in computing power anticipated for the next&lt;br/&gt;decade. The project will also involve cooperative work with the&lt;br/&gt;Brazilian research community.&lt;br/&gt;&lt;br/&gt;The technical approach is to capitalize on recent advances in&lt;br/&gt;augmented Lagrangian and conjugate gradient algorithms to produce a&lt;br/&gt;new kind of modular parallel continuous constrained optimization&lt;br/&gt;solver.  The solver consists of a classical augmented Lagrangian outer&lt;br/&gt;loop, with subproblems solved by the a state-of-the art&lt;br/&gt;box-constrained conjugate gradient method terminated by a recently&lt;br/&gt;developed relative error criterion.  The research consists of three&lt;br/&gt;stages: the goal of stage one is to create an object-oriented, modular&lt;br/&gt;serial implementation, test it extensively, and address some&lt;br/&gt;theoretical issues.  Stage two aims to evolve the stage-one substrate&lt;br/&gt;into a parallel solver for which the user explicitly specifies how to&lt;br/&gt;map the problem structure to multiple processing elements.  Stage&lt;br/&gt;three's goal is to automate the structure detection and mapping&lt;br/&gt;process.  Stages two and three will use stochastic programming&lt;br/&gt;problems as test cases.</AbstractNarration>
<MinAmdLetterDate>07/11/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/11/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1115638</AwardID>
<Investigator>
<FirstName>Jonathan</FirstName>
<LastName>Eckstein</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Dr</PI_SUFX_NAME>
<PI_FULL_NAME>Jonathan Eckstein</PI_FULL_NAME>
<EmailAddress>jeckstei@rci.rutgers.edu</EmailAddress>
<PI_PHON>8484450510</PI_PHON>
<NSF_ID>000195522</NSF_ID>
<StartDate>07/11/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Rutgers University Newark</Name>
<CityName>Newark</CityName>
<ZipCode>071021896</ZipCode>
<PhoneNumber>9739720283</PhoneNumber>
<StreetAddress>Blumenthal Hall, Suite 206</StreetAddress>
<StreetAddress2><![CDATA[249 University Avenue]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>130029205</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001912864</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Rutgers University New Brunswick]]></Name>
<CityName>Piscataway</CityName>
<StateCode>NJ</StateCode>
<ZipCode>088543925</ZipCode>
<StreetAddress><![CDATA[33 Knightsbridge Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7934</Code>
<Text>PARAL/DISTRIBUTED ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~358499</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project concerned developing parallel optimization algorithms based on various forms of a mathematical tool known as an augmented Lagrangian.&nbsp; Optimization problems arise in many areas of data analysis, engineering, and management.&nbsp; Examples of such problems including finding patterns in data, designing an aircraft wing to maximize its performance, or planning the operation of electric power utilities to minimize costs or greenhouse gas emissions.</p> <p>A principal aim of this project is to produce methods for solving such problems that make efficient use of parallel computers, that is, computers with large numbers of semi-independent processing elements.&nbsp; Increasingly, the growth in performance of individual computer processors has slowed, while most of the gain in the processing power of computers of all sizes has come from increasing numbers of parallel processing elements.</p> <p>Currently, a class of algorithms called Newton barrier algorithms is generally considered the most successful approach to solving optimization problems with large numbers of variables.&nbsp; However, such algorithms have difficulty making use of large numbers of processors.&nbsp; When efforts to make extensive use of parallelism in such algorithms have been successful, they are generally the result of extensive programming effort highly specific to a particular application.</p> <p>This project aimed to explore whether augmented Lagrangian methods might provide a better route to exploiting the potential power of parallel computers to solve large optimization problems.&nbsp; One main thrust of the project was to develop a new means of writing computer code to implement optimization algorithms in such a way that the same code could be applied efficiently and without modification to a variety of different problems in which data are spread across multiple computing processors in different ways.&nbsp; By testing on simple data-analysis optimization problems known as LASSO problems, we found that this approach met our expectations.&nbsp; For a large enough problem, we were able to make nearly perfectly efficient use of over 4,000 simultaneous computer processors cooperating to solve a single problem.</p> <p>We continued to develop our techniques to solve more complicated problems, selecting as a test case some problems arising from the planning of electrical power utilities.&nbsp; Here we ran into some technical difficulties.&nbsp; While the basic augmented Lagrangian method seemed to be performing as we expected, we ran into difficulties with what is known as the "subproblem" solver.&nbsp; Augmented Lagrangian methods work by converting a complex optimization into a sequence of simpler optimization problems, which in our case need only be solved approximately.&nbsp; After the first few subproblems, the standard methods we have been using for these subproblems seem to be working very slowly, although they appeared to work well in earlier testing outside of the augmented Lagrangian framework.&nbsp; We are continuing work on trying to identify why this phenomon is occurring, and to identify the right way to approximately solve the subproblems.</p> <p>We also worked on developing some new "decomposition" forms of augmented Lagrangian algorithms.&nbsp; Decomposition algorithms work by splitting large mathematical problems into a connected collection of smaller problems.&nbsp; They then alternate between computations on modified versions of the smaller problems, also called "subproblems", and "coordination" calculations share information between the smaller problems and manipulate them in such a way that one eventually obtains a solution to the overall problem.&nbsp; We developed three new versions of a standard decomposition augmented Lagrangian method called the "ADMM" (alternating direction method of multipliers), which allow the subproblems to be solved in an approximate m...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project concerned developing parallel optimization algorithms based on various forms of a mathematical tool known as an augmented Lagrangian.  Optimization problems arise in many areas of data analysis, engineering, and management.  Examples of such problems including finding patterns in data, designing an aircraft wing to maximize its performance, or planning the operation of electric power utilities to minimize costs or greenhouse gas emissions.  A principal aim of this project is to produce methods for solving such problems that make efficient use of parallel computers, that is, computers with large numbers of semi-independent processing elements.  Increasingly, the growth in performance of individual computer processors has slowed, while most of the gain in the processing power of computers of all sizes has come from increasing numbers of parallel processing elements.  Currently, a class of algorithms called Newton barrier algorithms is generally considered the most successful approach to solving optimization problems with large numbers of variables.  However, such algorithms have difficulty making use of large numbers of processors.  When efforts to make extensive use of parallelism in such algorithms have been successful, they are generally the result of extensive programming effort highly specific to a particular application.  This project aimed to explore whether augmented Lagrangian methods might provide a better route to exploiting the potential power of parallel computers to solve large optimization problems.  One main thrust of the project was to develop a new means of writing computer code to implement optimization algorithms in such a way that the same code could be applied efficiently and without modification to a variety of different problems in which data are spread across multiple computing processors in different ways.  By testing on simple data-analysis optimization problems known as LASSO problems, we found that this approach met our expectations.  For a large enough problem, we were able to make nearly perfectly efficient use of over 4,000 simultaneous computer processors cooperating to solve a single problem.  We continued to develop our techniques to solve more complicated problems, selecting as a test case some problems arising from the planning of electrical power utilities.  Here we ran into some technical difficulties.  While the basic augmented Lagrangian method seemed to be performing as we expected, we ran into difficulties with what is known as the "subproblem" solver.  Augmented Lagrangian methods work by converting a complex optimization into a sequence of simpler optimization problems, which in our case need only be solved approximately.  After the first few subproblems, the standard methods we have been using for these subproblems seem to be working very slowly, although they appeared to work well in earlier testing outside of the augmented Lagrangian framework.  We are continuing work on trying to identify why this phenomon is occurring, and to identify the right way to approximately solve the subproblems.  We also worked on developing some new "decomposition" forms of augmented Lagrangian algorithms.  Decomposition algorithms work by splitting large mathematical problems into a connected collection of smaller problems.  They then alternate between computations on modified versions of the smaller problems, also called "subproblems", and "coordination" calculations share information between the smaller problems and manipulate them in such a way that one eventually obtains a solution to the overall problem.  We developed three new versions of a standard decomposition augmented Lagrangian method called the "ADMM" (alternating direction method of multipliers), which allow the subproblems to be solved in an approximate manner that is more general and convenient than what has been proposed earlier.  We showed that these techniques allow various data analysis problems to be solved faster tha...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
