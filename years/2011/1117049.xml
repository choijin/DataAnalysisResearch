<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NeTS:Small:Understanding the Impact of Unreliable Hardware on the Resilience of Networked Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>439998.00</AwardTotalIntnAmount>
<AwardAmount>439998</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Darleen Fisher</SignBlockName>
<PO_EMAI>dlfisher@nsf.gov</PO_EMAI>
<PO_PHON>7032928950</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Networked systems have always been designed to operate even in the presence of failures, especially in communication links and storage. Until recently other components of such systems had relatively low probabilities of failures and for most networked systems, desired levels of resilience could be achieved using minimal redundancy added in an ad hoc manner. Two opposing trends are likely to make the task of achieving resilience significantly more difficult in the coming years: (a) increasing hardware failure probabilities: with the move towards finer nano-scale fabrication, chips are increasingly vulnerable to soft errors caused by external noise and are increasingly likely to fail early due to fatigue; (b) higher resilience requirements: as critical services continue to migrate to clouds, service providers are compelled into more stringent service-level agreements (SLAs), including higher reliability, higher availability, and tighter guarantees on service times. The above combination can dramatically increase the overhead of existing approaches for achieving desired levels of resilience. &lt;br/&gt;&lt;br/&gt;Intellectual merit: The first outcome of this project will be a holistic roadmap for resilience of networked systems. This resilience roadmap will take the roadmaps from the nano-scale CMOS (trends in chip cost, functionality, performance, power, and resilience that can be attained at chip level) and attempt to realistically project the future cost of currently-used networking and systems techniques for achieving desired level of resilience. The second outcome of this project is to develop resilience methods that scale gracefully in the face of increasing hardware failures. Such techniques will use novel partitioned redundancy strategies that achieve reliability at different levels across hardware and software layers. &lt;br/&gt;Broader Impacts. The resilience roadmap will provide unprecedented understanding of the trends in resilience and a uniquely realistic assessment of challenges and opportunities. This will significantly influence the research in the hardware as well as networking communities. A systematic design of scalable resilience methods will lead to significantly higher levels of resilience, lower costs - capital (equipment) as well recurring (especially, energy), and/or higher levels of performance.  The utilitarian gains to society by the proposed project are likely to be substantial, since networked systems now constitute one of our most critical infrastructures and consume an increasingly large proportion of our resources.&lt;br/&gt;&lt;br/&gt;This project will draw upon two different disciplines, hardware architecture and networked systems, and involve detailed case studies and development of completely new theory and techniques, and will therefore provide unique educational and training opportunities for students and working professionals in these fields.&lt;br/&gt;&lt;br/&gt;Budget Impact Statement: The item numbers in this paragraph refer to those in Figure 9 and Section 3.2 (entitled 'Proposed Research Tasks and Plan') of our original proposal. We will undertake all tasks and sub-tasks proposed in item-1 (and all its sub-items). In item-2, we will undertake the development of a general framework to consider all basic redundancy schemes and alternative ways of deploying them (sub-item-2.1). We will also characterize the associated tradeoffs (sub-item-2.2) and the consequences of realistic constraints (sub-item-2.3). However, we will pursue the development of prototype tools (as outlined in sub-item-2.4), to the extent necessary to demonstrate the benefits of our approach and to conduct case studies (described in item-3). Finally, we will undertake the case studies as originally proposed in item-3.</AbstractNarration>
<MinAmdLetterDate>08/08/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/08/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1117049</AwardID>
<Investigator>
<FirstName>Sandeep</FirstName>
<LastName>Gupta</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sandeep K Gupta</PI_FULL_NAME>
<EmailAddress>sandeep@usc.edu</EmailAddress>
<PI_PHON>2137402251</PI_PHON>
<NSF_ID>000097683</NSF_ID>
<StartDate>08/08/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ramesh</FirstName>
<LastName>Govindan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ramesh Govindan</PI_FULL_NAME>
<EmailAddress>ramesh@usc.edu</EmailAddress>
<PI_PHON>2137404509</PI_PHON>
<NSF_ID>000459031</NSF_ID>
<StartDate>08/08/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890001</ZipCode>
<StreetAddress><![CDATA[University Park]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~439998</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p id="docs-internal-guid-4b148670-5e90-137d-ce4b-591261ac0bb6" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 14.666666666666666px; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">In this project, we have roadmapped unreliable memory technologies, such as SRAM, DRAM, TCAMs, and Flash. Using ITRS projections, manufacturing technology trends, and discussions with experts from industry, we have carefully quantified the projected reliability degradations in these memory technologies over the next decade. These reliability degradations arise from manufacturing process variations resulting from increasing technology density, going from today&rsquo;s technology down to 12nm and beyond. These process variations can cause transient errors in memory cell reads or writes.</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 14.666666666666666px; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">Using these projections, we have explored how key networking and distributed storage technologies are impacted by these findings. End-to-end reliable transport is significantly affected by memory errors, since these are not explicitly accounted for by today&rsquo;s protocols: memory errors can result in packets being dropped either in intermediate routers, or failing end-to-end checksums. In turn, these cause today&rsquo;s transport protocols to throttle themselves significantly, resulting in increased flow completion times. Flash memory reliability affects distributed storage techniques, as does the reliability of end-to-end transport (since distributed storage techniques rely on these). In a distributed storage technique like a key-value store, write failures can occur as a result of transport protocol failures, and read failures can occur if transient Flash errors prevent read quorum consensus.</span></p> <p>&nbsp;</p> <p style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;" dir="ltr"><span style="font-size: 14.666666666666666px; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">The final outcome of the project is a careful quantitative exploration of the space of mitigations. We &nbsp;have devised better memory testing techniques that are designed to weed out errors due to process variations, without significantly increasing testing cost. These approaches can prevent faulty chips from being shipped, but can result in low yield at high-technology densities. An alternative mitigation strategy would be to increase the redundancy in memory chips with hardware error correction capabilities, which can permit higher yield, but increase chip cost. The final set of mitigation strategies places some of the burden of error mitigation on network equipment vendors or protocol designers, and relies on software error correction capabilities, either per hop, or end-to-end.</span></p> <p><br /><span style="font-size: 14.666666666666666px; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;">In the future, our work will inform the choices that the semiconductor and networking industries make in jointly dealing with increasing memory unreliability.</span></p><br> <p>            Last Modified: 12/01/2015<br>      Modified by: Sandeep&nbsp;K&nbsp;Gupta</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[In this project, we have roadmapped unreliable memory technologies, such as SRAM, DRAM, TCAMs, and Flash. Using ITRS projections, manufacturing technology trends, and discussions with experts from industry, we have carefully quantified the projected reliability degradations in these memory technologies over the next decade. These reliability degradations arise from manufacturing process variations resulting from increasing technology density, going from todayÆs technology down to 12nm and beyond. These process variations can cause transient errors in memory cell reads or writes.    Using these projections, we have explored how key networking and distributed storage technologies are impacted by these findings. End-to-end reliable transport is significantly affected by memory errors, since these are not explicitly accounted for by todayÆs protocols: memory errors can result in packets being dropped either in intermediate routers, or failing end-to-end checksums. In turn, these cause todayÆs transport protocols to throttle themselves significantly, resulting in increased flow completion times. Flash memory reliability affects distributed storage techniques, as does the reliability of end-to-end transport (since distributed storage techniques rely on these). In a distributed storage technique like a key-value store, write failures can occur as a result of transport protocol failures, and read failures can occur if transient Flash errors prevent read quorum consensus.    The final outcome of the project is a careful quantitative exploration of the space of mitigations. We  have devised better memory testing techniques that are designed to weed out errors due to process variations, without significantly increasing testing cost. These approaches can prevent faulty chips from being shipped, but can result in low yield at high-technology densities. An alternative mitigation strategy would be to increase the redundancy in memory chips with hardware error correction capabilities, which can permit higher yield, but increase chip cost. The final set of mitigation strategies places some of the burden of error mitigation on network equipment vendors or protocol designers, and relies on software error correction capabilities, either per hop, or end-to-end.   In the future, our work will inform the choices that the semiconductor and networking industries make in jointly dealing with increasing memory unreliability.       Last Modified: 12/01/2015       Submitted by: Sandeep K Gupta]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
