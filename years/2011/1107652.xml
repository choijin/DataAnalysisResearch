<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAPSI:Learning to Improve Near and Far Stereovisual Depth Estimation through Physical Interaction</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2011</AwardEffectiveDate>
<AwardExpirationDate>05/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>5700.00</AwardTotalIntnAmount>
<AwardAmount>5700</AwardAmount>
<AwardInstrument>
<Value>Fellowship Award</Value>
</AwardInstrument>
<Organization>
<Code>01090000</Code>
<Directorate>
<Abbreviation>O/D</Abbreviation>
<LongName>Office Of The Director</LongName>
</Directorate>
<Division>
<Abbreviation>OISE</Abbreviation>
<LongName>Office Of Internatl Science &amp;Engineering</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anne Emig</SignBlockName>
<PO_EMAI>aemig@nsf.gov</PO_EMAI>
<PO_PHON>7032927241</PO_PHON>
</ProgramOfficer>
<AbstractNarration/>
<MinAmdLetterDate>05/11/2011</MinAmdLetterDate>
<MaxAmdLetterDate>05/11/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.079</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1107652</AwardID>
<Investigator>
<FirstName>Timothy</FirstName>
<LastName>Mann</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Timothy A Mann</PI_FULL_NAME>
<EmailAddress/>
<PI_PHON>3157716253</PI_PHON>
<NSF_ID>000578431</NSF_ID>
<StartDate>05/11/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Mann                    Timothy        A</Name>
<CityName>Bryan</CityName>
<ZipCode>778024217</ZipCode>
<PhoneNumber/>
<StreetAddress/>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM/>
<ORG_LGL_BUS_NAME/>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Mann                    Timothy        A]]></Name>
<CityName>Bryan</CityName>
<StateCode>TX</StateCode>
<ZipCode>778024217</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramElement>
<ProgramReference>
<Code>5942</Code>
<Text>KOREA</Text>
</ProgramReference>
<ProgramReference>
<Code>5978</Code>
<Text>EAST ASIA AND PACIFIC PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~5700</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The purpose of this investigation was to determine how an human-like robot, with two cameras functioning as eyes, can learn to improve its guesses of how far an object is from itself, without the aid of a human teacher to measure the distance. Our hypothesis is that a robot system can improve the accuracy of its guesses by moving its head, arms, or the rest of its body to find out how different actions modify what the cameras see.<br /><br />It is well known that the distance to an object can be estimated from two images taken from slightly different vantage points (such as the distance between the left and right eye). However, to estimate distance using two images, the robot system needs to know several properties of the cameras used to capture the images. Our study investigated how a robot can learn information about its cameras by performing its own experiments.<br /><br />We found that a robot system can learn some of these camera properties by performing simple actions (such as rotating its neck while watching the same object). Actions that do not alter the physical distance, but cause a perceptual difference, reveal errors in the assumptions made about the cameras used for depth estimation. These errors can be used to learn the true camera properties.<br /><br />An important and unique benefit of the NSF East Asia &amp; Pacific Summer Institutes (EAPSI), is that it enabled extended on site research collaboration with the Artificial Brain Research (ABR) Laboratory at Kyungpook National University in South Korea, led by Professor Minho Lee. The ABR laboratory has developed a detailed model of visual attention that allowed us to bypass several technical issues and work directly on evaluating our hypothesis. Without support from the EAPSI program this level of collaboration would not have been possible.</p><br> <p>            Last Modified: 11/20/2011<br>      Modified by: Timothy&nbsp;A&nbsp;Mann</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2011/1107652/1107652_10090987_1321849470600_stereo_vision_plus_interaction_distance_estimate--rgov-214x142.jpg" original="/por/images/Reports/POR/2011/1107652/1107652_10090987_1321849470600_stereo_vision_plus_interaction_distance_estimate--rgov-800width.jpg" title="Stereo Vision Diagram"><img src="/por/images/Reports/POR/2011/1107652/1107652_10090987_1321849470600_stereo_vision_plus_interaction_distance_estimate--rgov-66x44.jpg" alt="Stereo Vision Diagram"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Depiction of humanoid with binocular vision interacting with an object.</div> <div class="imageCredit">Timothy A. Mann</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Timothy&nbsp;A&nbsp;Mann</div> <div class="imageTitle">Stereo Vision Diagram</div> </div> </li> <li> <a href="/por/images/Reports/POR/2011/1107652/1107652_10090987_1321849132158_bernardo_headmount_closeup--rgov-214x142.jpg" original="/por/images/Reports/POR/2011/1107652/1107652_10090987_1321849132158_bernardo_headmount_closeup--rgov-800width.jpg" title="Binocular Vision System"><img src="/por/images/Reports/POR/2011/1107652/1107652_10090987_1321849132158_bernardo_headmount_closeup--rgov-66x44.jpg" alt="Binocular Vision System"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Binocular vision system mounted on the head of an Aldebaran Nao humanoid robot.</div> <div class="imageCredit">Timothy A. Mann</div...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The purpose of this investigation was to determine how an human-like robot, with two cameras functioning as eyes, can learn to improve its guesses of how far an object is from itself, without the aid of a human teacher to measure the distance. Our hypothesis is that a robot system can improve the accuracy of its guesses by moving its head, arms, or the rest of its body to find out how different actions modify what the cameras see.  It is well known that the distance to an object can be estimated from two images taken from slightly different vantage points (such as the distance between the left and right eye). However, to estimate distance using two images, the robot system needs to know several properties of the cameras used to capture the images. Our study investigated how a robot can learn information about its cameras by performing its own experiments.  We found that a robot system can learn some of these camera properties by performing simple actions (such as rotating its neck while watching the same object). Actions that do not alter the physical distance, but cause a perceptual difference, reveal errors in the assumptions made about the cameras used for depth estimation. These errors can be used to learn the true camera properties.  An important and unique benefit of the NSF East Asia &amp; Pacific Summer Institutes (EAPSI), is that it enabled extended on site research collaboration with the Artificial Brain Research (ABR) Laboratory at Kyungpook National University in South Korea, led by Professor Minho Lee. The ABR laboratory has developed a detailed model of visual attention that allowed us to bypass several technical issues and work directly on evaluating our hypothesis. Without support from the EAPSI program this level of collaboration would not have been possible.       Last Modified: 11/20/2011       Submitted by: Timothy A Mann]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
