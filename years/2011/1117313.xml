<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Reinforcement Learning for Realistic Statistical Spoken Dialogue Systems - Beyond Slot-Filling Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>449988.00</AwardTotalIntnAmount>
<AwardAmount>449988</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Statistical spoken dialogue systems (SDSs) use reinforcement learning to learn a dialogue policy that decides what to do based on the dialogue context (also called dialogue state). Previous work on this problem has mainly addressed slot-filling dialogue, in which the user presents a complex request (e.g. an appointment booking), and the system tries to fill a set of slots (e.g. date and time) to satisfy the user's request. This project significantly extends and generalizes prior work by allowing automated dialogue policy creation for other genres of dialogue including question-answering and negotiation. The following open research issues are investigated: (1) the extent to which the three very different genres of dialogue (slot-filling, question-answering, and negotiation) can be represented using the same kind of dialogue policy representation; (2) whether state-of-the-art learning techniques, that work well for small state spaces and simple interactions, can scale to the needs of more complex dialogues and larger state spaces; (3) methods for compactly representing the dialogue state and for combining learned and hand-crafted policies; (4) development of automated metrics for measuring the quality of simulated users and learned policies; (5) validation of those  metrics with respect to how well they correlate with human evaluations.&lt;br/&gt;&lt;br/&gt;Statistical SDSs facilitate easier creation of dialogue systems (less hand-crafting by dialogue system experts) that are more tuned in to user behavior (learning policies from data and simulation). This project broadens the types of systems that can be developed with this kind of approach (not just slot-filling, but also simple question-answering and more complex negotiation). The advances made in the project are encoded in a toolkit (to be publicly distributed) specifically designed for statistical dialogue management. This toolkit allows broader access to this technology, by providing the potential to attract more researchers from academia and industry to the field of SDS, and make the use of statistical techniques available to non-experts; it can also be an excellent resource for teaching statistical dialogue management to students.</AbstractNarration>
<MinAmdLetterDate>07/22/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/22/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1117313</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Traum</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David R Traum</PI_FULL_NAME>
<EmailAddress>traum@ict.usc.edu</EmailAddress>
<PI_PHON>3105745729</PI_PHON>
<NSF_ID>000173016</NSF_ID>
<StartDate>07/22/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Kallirroi</FirstName>
<LastName>Georgila</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kallirroi Georgila</PI_FULL_NAME>
<EmailAddress>kgeorgila@ict.usc.edu</EmailAddress>
<PI_PHON>2137407762</PI_PHON>
<NSF_ID>000580549</NSF_ID>
<StartDate>07/22/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890001</ZipCode>
<StreetAddress><![CDATA[University Park]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~449988</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Natural language dialogue systems allow users to interact with computers using language (spoken or written) by trying to simulate how humans interact with one another. The dialogue manager is the component of the dialogue system that decides what to say using a dialogue policy. Designing the dialogue policy is a very complex procedure that requires full knowledge of the application domain and prediction of all the possible ways the user may employ to interact with the dialogue system in various dialogue contexts. This issue has motivated the dialogue research community to examine the use of statistical methods (more specifically reinforcement learning) for automatically learning dialogue policies from data or simulated users, i.e., models that simulate the behavior of real users. To date, the focus of statistical dialogue management has been on slot-filling dialogue systems, in which the role of the system is to fill a number of slots based on information provided by the user (in a restaurant recommendation scenario the slots would be food type, price, location, etc.). This project extended the state-of-the-art by investigating the use of statistical dialogue management in a variety of dialogue genres, beyond slot-filling.</p> <p>We experimented with negotiation and question-answering dialogue genres. We used reinforcement learning to learn a question-answering dialogue policy for a real-world application. We analyzed a corpus of interactions of museum visitors with two virtual characters that served as guides at the Museum of Science in Boston, in order to build a realistic model of user behavior when interacting with these characters. Note that the development of these virtual characters and the corpus collection was supported by the NSF grant #0813541. A simulated user was built based on this model and used for learning the dialogue policy of the virtual characters using reinforcement learning. Our learned policy outperformed two strong baselines (including the original dialogue policy that was used for collecting the corpus).</p> <p>Negotiation domains pose challenges for standard reinforcement learning techniques. For example, in negotiation we need to keep track of what has been agreed offered or rejected so far, the current offers on the table, each negotiator&rsquo;s beliefs about their interlocutors&rsquo; preferences, etc. In practice this means that, from a machine learning point of view, the problem can easily become intractable. To deal with the high complexity of the problem, we developed methods that can abstract away from the specifics of the problem to more general representations (potentially transferrable between similar negotiation problems) that make the learning problem more tractable. Using these methods we learned competitive dialogue policies for multi-issue negotiation that can perform well even against opponents whose behavior has not been seen before. In fact, these learned negotiation dialogue policies were judged by human raters as more rational than competitive manually authored negotiation dialogue policies.</p> <p>When it comes to decision-making and negotiation, different people may weigh different factors depending on their culture, personality, circumstances, etc. We used machine learning techniques, i.e., inverse reinforcement learning, to learn the weights of these factors from real negotiation data from different cultures. The weights learned with our model surpassed both a simple baseline with random weights, and a strong baseline considering only one factor of maximizing gain in own wealth in accounting for the behavior of human negotiators from different cultures. We also showed that the weights learned with our model for one culture outperformed weights learned for other cultures when negotiating against opponents of the first culture. We conclude that decision-making in negotiation is a complex, culture-specific process t...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Natural language dialogue systems allow users to interact with computers using language (spoken or written) by trying to simulate how humans interact with one another. The dialogue manager is the component of the dialogue system that decides what to say using a dialogue policy. Designing the dialogue policy is a very complex procedure that requires full knowledge of the application domain and prediction of all the possible ways the user may employ to interact with the dialogue system in various dialogue contexts. This issue has motivated the dialogue research community to examine the use of statistical methods (more specifically reinforcement learning) for automatically learning dialogue policies from data or simulated users, i.e., models that simulate the behavior of real users. To date, the focus of statistical dialogue management has been on slot-filling dialogue systems, in which the role of the system is to fill a number of slots based on information provided by the user (in a restaurant recommendation scenario the slots would be food type, price, location, etc.). This project extended the state-of-the-art by investigating the use of statistical dialogue management in a variety of dialogue genres, beyond slot-filling.  We experimented with negotiation and question-answering dialogue genres. We used reinforcement learning to learn a question-answering dialogue policy for a real-world application. We analyzed a corpus of interactions of museum visitors with two virtual characters that served as guides at the Museum of Science in Boston, in order to build a realistic model of user behavior when interacting with these characters. Note that the development of these virtual characters and the corpus collection was supported by the NSF grant #0813541. A simulated user was built based on this model and used for learning the dialogue policy of the virtual characters using reinforcement learning. Our learned policy outperformed two strong baselines (including the original dialogue policy that was used for collecting the corpus).  Negotiation domains pose challenges for standard reinforcement learning techniques. For example, in negotiation we need to keep track of what has been agreed offered or rejected so far, the current offers on the table, each negotiatorÆs beliefs about their interlocutorsÆ preferences, etc. In practice this means that, from a machine learning point of view, the problem can easily become intractable. To deal with the high complexity of the problem, we developed methods that can abstract away from the specifics of the problem to more general representations (potentially transferrable between similar negotiation problems) that make the learning problem more tractable. Using these methods we learned competitive dialogue policies for multi-issue negotiation that can perform well even against opponents whose behavior has not been seen before. In fact, these learned negotiation dialogue policies were judged by human raters as more rational than competitive manually authored negotiation dialogue policies.  When it comes to decision-making and negotiation, different people may weigh different factors depending on their culture, personality, circumstances, etc. We used machine learning techniques, i.e., inverse reinforcement learning, to learn the weights of these factors from real negotiation data from different cultures. The weights learned with our model surpassed both a simple baseline with random weights, and a strong baseline considering only one factor of maximizing gain in own wealth in accounting for the behavior of human negotiators from different cultures. We also showed that the weights learned with our model for one culture outperformed weights learned for other cultures when negotiating against opponents of the first culture. We conclude that decision-making in negotiation is a complex, culture-specific process that cannot be explained just by the notion of maximizing oneÆs own utility, but which can be...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
