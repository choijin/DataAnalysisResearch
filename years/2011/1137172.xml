<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EFRI-M3C: Mobility Skill Acquisition and Learning through Alternative and Multimodal Perception for Visually Impaired People</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2018</AwardExpirationDate>
<AwardTotalIntnAmount>1999659.00</AwardTotalIntnAmount>
<AwardAmount>2424671</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07040000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>EFMA</Abbreviation>
<LongName>Emerging Frontiers &amp; Multidisciplinary Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Radhakisan Baheti</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The objective of this EFRI-M3C project is to develop models of sensorimotor control in order to establish a set of design criteria for developing improved assistive technologies for visually impaired people. Multimodal sensory information obtained by wearable machine sensors will substitute for and/or augment impaired vision with somatosensory and other novel stimulation (?transducing?) methods. The project has the following three research threads: (1) Determination of the information needed by visually impaired people to perform wayfinding and arm reaching tasks and the impact on task performance when transducing information through visual, auditory and vibrotactile modalities. (2) Understanding of motor skill acquisition and control through in vivo measurements of brain activity and movement performance. (3) Development of a sensorimotor model through man-machine co-learning, which can be used for improving sensor/display designs and applied to machines. &lt;br/&gt;&lt;br/&gt;The intellectual merit of this research is that it will generate a theoretical foundation for a deeper understanding of the neural mechanisms of sensorimotor integration and motor learning, shedding new light on these functions in humans and machines. This will lead to new design concepts of alternative perception, formulation of required information necessary for successful orientation and wayfinding, and development of cost-effective and revolutionary mechatronic devices to assist visually impaired people in achieving mobility functions comparable to people with normal vision. The interdisciplinary team includes experts in engineering, computer science, psychology, and applied physiology from the City College of New York and Georgia Tech, tackling challenging problems on the boundaries of sensing, cognition, and action. Advisory board members, including experts in (neuro-)ophthalmology and human vision research, as well as counselors at the NYS Commission for the Blind and Visually Handicapped, will provide guidance for the study. &lt;br/&gt;&lt;br/&gt;The broader impacts of this research will be assistive technologies for individuals with sensory impairments, whose numbers have been rising due to the increasing population of older adults in the US and around the world. The project will also create new and expand existing academic programs in assistive technologies, brain computer interfaces and sensorimotor integration. It will increase cross-campus education opportunities for students particularly in under-represented groups of the two campuses.</AbstractNarration>
<MinAmdLetterDate>09/01/2011</MinAmdLetterDate>
<MaxAmdLetterDate>03/09/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1137172</AwardID>
<Investigator>
<FirstName>Kok-Meng</FirstName>
<LastName>Lee</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Kok-Meng Lee</PI_FULL_NAME>
<EmailAddress>kokmeng.lee@me.gatech.edu</EmailAddress>
<PI_PHON>4048947402</PI_PHON>
<NSF_ID>000153834</NSF_ID>
<StartDate>09/01/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Boris</FirstName>
<LastName>Prilutsky</LastName>
<PI_MID_INIT>l</PI_MID_INIT>
<PI_SUFX_NAME>Dr</PI_SUFX_NAME>
<PI_FULL_NAME>Boris l Prilutsky</PI_FULL_NAME>
<EmailAddress>boris.prilutsky@ap.gatech.edu</EmailAddress>
<PI_PHON>4048947659</PI_PHON>
<NSF_ID>000219720</NSF_ID>
<StartDate>09/01/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tony</FirstName>
<LastName>Ro</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tony Ro</PI_FULL_NAME>
<EmailAddress>tro@gc.cuny.edu</EmailAddress>
<PI_PHON>2128171815</PI_PHON>
<NSF_ID>000453113</NSF_ID>
<StartDate>09/01/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Zhigang</FirstName>
<LastName>Zhu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Zhigang Zhu</PI_FULL_NAME>
<EmailAddress>zzhu@ccny.cuny.edu</EmailAddress>
<PI_PHON>4136952795</PI_PHON>
<NSF_ID>000095302</NSF_ID>
<StartDate>09/01/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>YingLi</FirstName>
<LastName>Tian</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>YingLi Tian</PI_FULL_NAME>
<EmailAddress>ytian@ccny.cuny.edu</EmailAddress>
<PI_PHON>2126507046</PI_PHON>
<NSF_ID>000517490</NSF_ID>
<StartDate>09/01/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>CUNY City College</Name>
<CityName>New York</CityName>
<ZipCode>100319101</ZipCode>
<PhoneNumber>2126505418</PhoneNumber>
<StreetAddress>Convent Ave at 138th St</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>603503991</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION OF THE CITY UNIVERSITY OF NEW YORK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073268849</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[CUNY City College]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100319198</ZipCode>
<StreetAddress><![CDATA[138th St and Convent Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7218</Code>
<Text>RET SUPPLEMENTS</Text>
</ProgramElement>
<ProgramElement>
<Code>7633</Code>
<Text>EFRI Research Projects</Text>
</ProgramElement>
<ProgramReference>
<Code>115E</Code>
<Text>RESEARCH EXP FOR TEACHERS</Text>
</ProgramReference>
<ProgramReference>
<Code>7633</Code>
<Text>EFRI RESEARCH PROJECTS</Text>
</ProgramReference>
<ProgramReference>
<Code>9177</Code>
<Text>ELEMENTARY/SECONDARY EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~1999659</FUND_OBLG>
<FUND_OBLG>2012~100000</FUND_OBLG>
<FUND_OBLG>2013~125012</FUND_OBLG>
<FUND_OBLG>2014~100000</FUND_OBLG>
<FUND_OBLG>2015~100000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-0fa317d6-7fff-497f-8cf0-490142ac602d"> </span></p> <p dir="ltr"><span>The goal of the NSF EFRI-M3C research project led by the City &nbsp;College of New York (CCNY) and Georgia Tech is to gain a deeper understanding of the human brain by conducting psychophysical and brain imaging experiments to assess how, when, and where multisensory information is processed. Our overall man-machine co-learning approach has three closely coupled research threads, representing the measuring, modeling, and adapting with machine obtainable multimodal sensory information for assisting visually impaired individuals: (1) Determine critical sensory information and &ldquo;alternative perception&rdquo; for mobility of visually impaired; (2) Understand sensory and motor representations in the human brain; and (3) Construct a man-machine sensorimotor model through man-machine co-learning.</span></p> <p dir="ltr"><span>This research has provided transformative human assistive technologies as well as helping improve machines (including autonomous robots and human-robot collaboration systems) that utilize human-inspired artificial intelligence. Studies in this project have not only provided new insights into sensorimotor integration and skill acquisition in people and robots, but also &nbsp;led to new design concepts of &ldquo;alternative perception&rdquo; and methods to formulate &ldquo;required information&rdquo;, which capitalize on multimodal (e.g., visual, audio, tactile, thermal, etc.) information in developing cost-effective mechatronic devices to assist visually impaired people and autonomous robots in achieving mobility functions comparable to people with normal vision.  The new sensing and stimulation approaches include: proprioception-coupled omnidirectional range field sensing/display, non-invasive TMS-based visual neural stimulation, hybrid mobile sensing and processing for assistive navigation, multimodal emotion recognition using audio, video, EMG and EEG sensors, and computer-vision based scene recognition and transducing. This effort fills the gap between advances in sensing and mechatronic technologies and studies in cognitive neuroscience and biomechanics.</span></p> <p dir="ltr"><span>In terms of platforms and datasets, we have set up a multimodal data collection and sensor evaluation platforms using virtual reality techniques. We have also developed a set of novel sensor packages and algorithms for serving visually impaired people and integrated them into a smart and accessible transportation hub testbed. We have generated a number of multimodal datasets with multimodal sensory data of environments, brain measurement data using MRI, EEG, optical imaging, and biomechanical data from action analysis for closing the loop of environments, brain and action for research and education.</span></p> <p dir="ltr"><span>The social and economic impact of the research is also significant. Based on the 2014 World Health Organization report, there are more than 285 million visually impaired people, of which 39 million are blind. These numbers are increasing with the aging population in US and around the world. This research will have a long term impact on the health and wellness of society, not only for the visually challenged, but for people who often work in dangerous environments with low visibility, such as firefighters, drivers and soldiers. The research could also provide more human friendly machines for home services and education when they are equipped with more adequate sensory systems and behave in ways that are more like human beings.</span></p> <p dir="ltr"><span><span> </span></span><span><span> </span></span><span><span> </span></span><span><span> </span></span></p> <p dir="ltr"><span>Through the EFRI-M3C project, we have attracted a great deal of attention from both the blind and blind serving community and underrepresented groups in New York City. Through our advisory board members, we have had close contact with the New York State Commission for Blind (NYSCB) and Lighthouse Guild, both of which serve blind and visually impaired individuals in the New York City area. CCNY is one of the most diverse campuses in the U.S., with a student body with a significant population of Hispanic and African-American students. As a major urban university in Manhattan, we also host many veterans and students who have various disabilities. Various research and outreach projects have provided an enriching experience for the students, while also benefiting the blind community through research and technology development.</span></p> <p dir="ltr"><span><span> </span></span></p> <p dir="ltr"><span>The multidisciplinary nature of the project has provided a platform to bring together young talent from three traditionally separate disciplines (i.e., engineering, computing and neuroscience) under one unified and exciting research program. The collaboration between CCNY and Georgia Tech increases cross-campus education opportunities for our students by introducing them to this research project across the two campuses.</span></p> <p dir="ltr"><span>In summary, with the support of this project, the REU program and the EFRI supplement Research Experience and Mentoring (REM) program , a total of 26 PhD students, 6 masters students, 35 undergraduate students, 8 &nbsp;high school students, one high school teacher and one community college professor  participated various research activities. Students have gained interdisciplinary knowledge and unique research experience in computer vision, multi-sensor fusion, phycology, control, man-machine sensorimotor, and assistive technologies, as well as real problem solving skills and theoretical model development. The research experiences have prepared them for their advanced degree studies and future careers. In addition, we have published over 100 peer-reviewed papers in conferences and journals, supported in part by this project.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 10/10/2018<br>      Modified by: Zhigang&nbsp;Zhu</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2018/1137172/1137172_10130158_1539047405646_efri-map-vip-diagram--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1137172/1137172_10130158_1539047405646_efri-map-vip-diagram--rgov-800width.jpg" title="Multimodal and alternative perception"><img src="/por/images/Reports/POR/2018/1137172/1137172_10130158_1539047405646_efri-map-vip-diagram--rgov-66x44.jpg" alt="Multimodal and alternative perception"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Multimodal sensing and man-machine learning/modeling with machine obtainable sensory information.</div> <div class="imageCredit">Zhigang Zhu, Tony Ro, Yingli Tian, Kok-Meng Lee, Boris Prilutsky</div> <div class="imageSubmitted">Zhigang&nbsp;Zhu</div> <div class="imageTitle">Multimodal and alternative perception</div> </div> </li> <li> <a href="/por/images/Reports/POR/2018/1137172/1137172_10130158_1539047892663_rem-2015--rgov-214x142.jpg" original="/por/images/Reports/POR/2018/1137172/1137172_10130158_1539047892663_rem-2015--rgov-800width.jpg" title="REM 2015 Cohort at The City College of New York"><img src="/por/images/Reports/POR/2018/1137172/1137172_10130158_1539047892663_rem-2015--rgov-66x44.jpg" alt="REM 2015 Cohort at The City College of New York"></a> <div class="imageCaptionContainer"> <div class="imageCaption">REM 2015 Cohort at The City College of New York</div> <div class="imageCredit">Zhigang Zhu</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Zhigang&nbsp;Zhu</div> <div class="imageTitle">REM 2015 Cohort at The City College of New York</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   The goal of the NSF EFRI-M3C research project led by the City  College of New York (CCNY) and Georgia Tech is to gain a deeper understanding of the human brain by conducting psychophysical and brain imaging experiments to assess how, when, and where multisensory information is processed. Our overall man-machine co-learning approach has three closely coupled research threads, representing the measuring, modeling, and adapting with machine obtainable multimodal sensory information for assisting visually impaired individuals: (1) Determine critical sensory information and "alternative perception" for mobility of visually impaired; (2) Understand sensory and motor representations in the human brain; and (3) Construct a man-machine sensorimotor model through man-machine co-learning. This research has provided transformative human assistive technologies as well as helping improve machines (including autonomous robots and human-robot collaboration systems) that utilize human-inspired artificial intelligence. Studies in this project have not only provided new insights into sensorimotor integration and skill acquisition in people and robots, but also  led to new design concepts of "alternative perception" and methods to formulate "required information", which capitalize on multimodal (e.g., visual, audio, tactile, thermal, etc.) information in developing cost-effective mechatronic devices to assist visually impaired people and autonomous robots in achieving mobility functions comparable to people with normal vision.  The new sensing and stimulation approaches include: proprioception-coupled omnidirectional range field sensing/display, non-invasive TMS-based visual neural stimulation, hybrid mobile sensing and processing for assistive navigation, multimodal emotion recognition using audio, video, EMG and EEG sensors, and computer-vision based scene recognition and transducing. This effort fills the gap between advances in sensing and mechatronic technologies and studies in cognitive neuroscience and biomechanics. In terms of platforms and datasets, we have set up a multimodal data collection and sensor evaluation platforms using virtual reality techniques. We have also developed a set of novel sensor packages and algorithms for serving visually impaired people and integrated them into a smart and accessible transportation hub testbed. We have generated a number of multimodal datasets with multimodal sensory data of environments, brain measurement data using MRI, EEG, optical imaging, and biomechanical data from action analysis for closing the loop of environments, brain and action for research and education. The social and economic impact of the research is also significant. Based on the 2014 World Health Organization report, there are more than 285 million visually impaired people, of which 39 million are blind. These numbers are increasing with the aging population in US and around the world. This research will have a long term impact on the health and wellness of society, not only for the visually challenged, but for people who often work in dangerous environments with low visibility, such as firefighters, drivers and soldiers. The research could also provide more human friendly machines for home services and education when they are equipped with more adequate sensory systems and behave in ways that are more like human beings.      Through the EFRI-M3C project, we have attracted a great deal of attention from both the blind and blind serving community and underrepresented groups in New York City. Through our advisory board members, we have had close contact with the New York State Commission for Blind (NYSCB) and Lighthouse Guild, both of which serve blind and visually impaired individuals in the New York City area. CCNY is one of the most diverse campuses in the U.S., with a student body with a significant population of Hispanic and African-American students. As a major urban university in Manhattan, we also host many veterans and students who have various disabilities. Various research and outreach projects have provided an enriching experience for the students, while also benefiting the blind community through research and technology development.   The multidisciplinary nature of the project has provided a platform to bring together young talent from three traditionally separate disciplines (i.e., engineering, computing and neuroscience) under one unified and exciting research program. The collaboration between CCNY and Georgia Tech increases cross-campus education opportunities for our students by introducing them to this research project across the two campuses. In summary, with the support of this project, the REU program and the EFRI supplement Research Experience and Mentoring (REM) program , a total of 26 PhD students, 6 masters students, 35 undergraduate students, 8  high school students, one high school teacher and one community college professor  participated various research activities. Students have gained interdisciplinary knowledge and unique research experience in computer vision, multi-sensor fusion, phycology, control, man-machine sensorimotor, and assistive technologies, as well as real problem solving skills and theoretical model development. The research experiences have prepared them for their advanced degree studies and future careers. In addition, we have published over 100 peer-reviewed papers in conferences and journals, supported in part by this project.          Last Modified: 10/10/2018       Submitted by: Zhigang Zhu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
