<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CGV: Medium: Collaborative Research: Visualizing Comparisons</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Comparison is an essential part of data analysis and, therefore, of many visualization tasks.  While the published literature provides a wealth of visualization tools for looking at individual objects (graphs, volumes, time series, gene sequences, molecular motions, etc.), there has to date been less consideration of support for comparison.  The PIs argue that comparison tasks are best supported by tools explicitly designed for that purpose.  The problem is that visual comparison becomes more challenging as the number of objects, their size, and the complexity of the objects and/or of the relationships among them increases.  The difficulty is further compounded by our rapidly growing ability to collect and generate data.  In prior work the PIs have developed some encouraging initial examples of comparison tools, but these are specialized successes that offer little guidance for future endeavors.  Addressing a wider range of comparison problems at greater scale with our present limited understanding thus largely remains an art that requires considerable effort.  The PIs' goal in this project is to move towards a science of visual comparison.  By studying visual comparison as a general problem, they will establish a domain-independent foundation for the field that facilitates the design of future tools which allow the creation of more effective and scalable comparisons.  To these ends the team will pursue three interconnected research threads.  They will define theories that are grounded upon principles of visual cognition.  They will explore case studies (derived from real problems suggested by domain collaborators) that challenge and extend these theories, provide examples for empirical study, and suggest or use general concepts.  And they will identify common tasks, designs, and strategies that enable development of generalized techniques, guidelines, and software components.  This approach uniquely combines empirical studies, design explorations, and software development to take the field of visual comparisons to a new level that is both rooted in theory yet viable in practice. &lt;br/&gt;&lt;br/&gt;Broader Impacts:   Because visual comparison plays a key role in diverse domains (including essentially all of the sciences, engineering, and medicine), the potential benefits from an improved science of visual comparison tools are far reaching.  To ensure maximum applicability for project outcomes, the PIs are directly collaborating with physical, biological, social, educational, and medical scientists, as well as with engineers and scholars in the humanities.  The project will generate visualization tools, software components, and resources for visualization development by others.  Visual comparison will serve as a mechanism to expose students at all levels to issues in data understanding.  This project will also provide training for visualization specialists, engage non-technical students in visualization, and explore the role of visualization in public outreach efforts.</AbstractNarration>
<MinAmdLetterDate>05/16/2012</MinAmdLetterDate>
<MaxAmdLetterDate>07/30/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1162013</AwardID>
<Investigator>
<FirstName>Charles</FirstName>
<LastName>Hansen</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Charles D Hansen</PI_FULL_NAME>
<EmailAddress>hansen@cs.utah.edu</EmailAddress>
<PI_PHON>8015813154</PI_PHON>
<NSF_ID>000265298</NSF_ID>
<StartDate>05/16/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Utah</Name>
<CityName>SALT LAKE CITY</CityName>
<ZipCode>841128930</ZipCode>
<PhoneNumber>8015816903</PhoneNumber>
<StreetAddress>75 S 2000 E</StreetAddress>
<StreetAddress2><![CDATA[Second Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009095365</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF UTAH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009095365</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName>Salt Lake City</CityName>
<StateCode>UT</StateCode>
<ZipCode>841128930</ZipCode>
<StreetAddress><![CDATA[75 South 2000 East]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~58940</FUND_OBLG>
<FUND_OBLG>2013~88199</FUND_OBLG>
<FUND_OBLG>2014~90137</FUND_OBLG>
<FUND_OBLG>2015~62724</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Scientists, engineers, and analysts work with increasingly large and complex data sets. Visualization tools are essential to understanding, analyzing, and communicating data. Work with complex data often involves comparison. However, visualization tools rarely support comparison tasks directly, and those that do are highly specialized and often limited in scale.&nbsp; In this project, we have focused on tools for enhancing visual comparison.&nbsp; We have investigated whether depth-of-field provides better visual cues.&nbsp; We found that depth of field had a statistically significant effect if, and only if, the nearest feature was the focus feature.&nbsp; We also investigated multi-attribute data and methods for visualization of multi-variate data and linked views. We used a boundary confidence image (guided by uncertainty) that is blended with slices through the 3D dataset.&nbsp;&nbsp;The user can easily select regions of interest with a guided uncertainty-aware lasso, which automatically snaps to the detected feature boundaries.&nbsp;&nbsp;This provides selection based on visual comparison of the underlying data.&nbsp;&nbsp;We performed a comprehensive review of colormap generation techniques and developed a taxonomy, which is helpful for finding appropriate techniques to use for specific data and applications. We classified colormap generation techniques into four classes: procedural methods, userstudy based methods, rulebased methods, and datadriven&nbsp;methods. We also explored methods that are beyond pure data comprehension purposes. We explored representative sets of visualization techniques that explicitly discuss the use of colormaps and classified these based on the nature of the data in these applications. Research on microscopy data from developing biological samples usually requires tracking individual cells over time. When cells are densely packed in three-dimensions and in a time-dependent scan of volumes, tracking results can become unreliable and uncertain. We developed the uncertainty footprint, an uncertainty quantification and visualization technique that examines nonuniformity at local convergence for an iterative evaluation process on a spatial domain supported by partially overlapping bases. We demonstrate that the patterns revealed by the uncertainty footprint indicate data processing quality in two algorithms from a typical cell tracking workflow - cell identification and association.&nbsp;&nbsp;This allows for visual comparison targeting areas of the data where potentially erroneous tracking has taken place.&nbsp;&nbsp;The user visually validates the underlying system.</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/02/2018<br>      Modified by: Charles&nbsp;D&nbsp;Hansen</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Scientists, engineers, and analysts work with increasingly large and complex data sets. Visualization tools are essential to understanding, analyzing, and communicating data. Work with complex data often involves comparison. However, visualization tools rarely support comparison tasks directly, and those that do are highly specialized and often limited in scale.  In this project, we have focused on tools for enhancing visual comparison.  We have investigated whether depth-of-field provides better visual cues.  We found that depth of field had a statistically significant effect if, and only if, the nearest feature was the focus feature.  We also investigated multi-attribute data and methods for visualization of multi-variate data and linked views. We used a boundary confidence image (guided by uncertainty) that is blended with slices through the 3D dataset.  The user can easily select regions of interest with a guided uncertainty-aware lasso, which automatically snaps to the detected feature boundaries.  This provides selection based on visual comparison of the underlying data.  We performed a comprehensive review of colormap generation techniques and developed a taxonomy, which is helpful for finding appropriate techniques to use for specific data and applications. We classified colormap generation techniques into four classes: procedural methods, userstudy based methods, rulebased methods, and datadriven methods. We also explored methods that are beyond pure data comprehension purposes. We explored representative sets of visualization techniques that explicitly discuss the use of colormaps and classified these based on the nature of the data in these applications. Research on microscopy data from developing biological samples usually requires tracking individual cells over time. When cells are densely packed in three-dimensions and in a time-dependent scan of volumes, tracking results can become unreliable and uncertain. We developed the uncertainty footprint, an uncertainty quantification and visualization technique that examines nonuniformity at local convergence for an iterative evaluation process on a spatial domain supported by partially overlapping bases. We demonstrate that the patterns revealed by the uncertainty footprint indicate data processing quality in two algorithms from a typical cell tracking workflow - cell identification and association.  This allows for visual comparison targeting areas of the data where potentially erroneous tracking has taken place.  The user visually validates the underlying system.          Last Modified: 01/02/2018       Submitted by: Charles D Hansen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
