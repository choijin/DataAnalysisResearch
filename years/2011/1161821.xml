<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Medium: Collaborative Research: Regression Testing Techniques for Real-world Software Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2012</AwardEffectiveDate>
<AwardExpirationDate>06/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>875065.00</AwardTotalIntnAmount>
<AwardAmount>1103853</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Reports estimate that regression testing, which is the activity of retesting a software system after it has been modified, can consume up to 50% of the cost of software development and maintenance. Although there are many techniques that can reduce the cost of regression testing, most of them do not account for important characteristics of modern systems, such as product lines, web applications, service-oriented architectures, and cloud-based applications.  These systems are increasingly heterogeneous: they may come from different sources, may be written in different languages, and may be accessible in different formats (e.g., source code, binary code, or through remote interfaces). Moreover, modern software is often environment dependent: its behavior can be affected not only by changes in the code, but also by changes in its complex environment (e.g., databases, configuration files, and network layouts).  Because most existing regression-testing techniques do not account for these characteristics, the application of these techniques can result in inadequately tested software, problems during maintenance, and ultimately poor software quality.&lt;br/&gt;&lt;br/&gt;The overall goal of this research is to go beyond the state of the art in regression testing by defining novel approaches that can be applied to modern, real-world software and account for its characteristics and complexity. To achieve this goal, the research will first extend analysis techniques on which regression-testing approaches rely, such as system modeling, version differencing, coverage analysis, and impact analysis. The research will then leverage these fundamental techniques to develop, evaluate with industrial partners, and make available a family of regression testing techniques and tools that can (1) build comprehensive models of heterogeneous, environment-dependent software systems, (2) evolve these models throughout the systems' lifetimes, and (3) analyze the changes across models to understand their effects on the systems' behavior and retest them effectively and efficiently. The impact of the research will be manyfold. First, the rigorous, transformative, and highly automated techniques developed will help improve the quality of today's large, complex software systems, thus benefitting all segments of society that depend on software. Second, the release of the produced tools and infrastructure will let other researchers and practitioners build on our results, advancing knowledge and understanding. Finally, the research findings will be integrated in curriculum materials that will be made available to the broader scientific community, which will help prepare a globally competitive workforce and further benefit society.</AbstractNarration>
<MinAmdLetterDate>03/27/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/14/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1161821</AwardID>
<Investigator>
<FirstName>Mary</FirstName>
<LastName>Harrold</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mary J Harrold</PI_FULL_NAME>
<EmailAddress>harrold@cc.gatech.edu</EmailAddress>
<PI_PHON>4043850612</PI_PHON>
<NSF_ID>000248827</NSF_ID>
<StartDate>03/27/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Alessandro</FirstName>
<LastName>Orso</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alessandro Orso</PI_FULL_NAME>
<EmailAddress>orso@cc.gatech.edu</EmailAddress>
<PI_PHON>4043852066</PI_PHON>
<NSF_ID>000489660</NSF_ID>
<StartDate>03/27/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[College of Computing]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320280</ZipCode>
<StreetAddress><![CDATA[801 Atlantic Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramElement>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~191271</FUND_OBLG>
<FUND_OBLG>2013~683794</FUND_OBLG>
<FUND_OBLG>2015~228788</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>A considerable percentage of software development and maintenance costs are due to regression testing?retesting a software system after it is modified to assess whether the changes in the software behave as intended and have not introduced unwanted side effects. Both researchers and practitioners have defined many techniques for supporting regression-testing and improving its effectiveness and efficiency. Many of these techniques, however, make unrealistic assumptions on the software that is being tested or fall short when applied to modern software systems. The overall goal of this project was to go beyond the state of the art in regression testing by defining novel approaches that can be applied to modern, real-world software and account for its characteristics and complexity. The rest of this report summarizes a representative set of the techniques that were developed within the project.</p> <p><em>Improving in-house tests based on observed field behavior</em>. Regression testing, and testing in general, is inherently limited, as in-house tests can typically only exercise a tiny fraction of all possible software behaviors. Typically, developers make decisions on what to test and how based on some assumptions on how the software will be used in the field. When these assumptions are incorrect, which is a common occurrence, the test cases used in-house may not be representative of the software behavior that users actually exercise in the field. Moreover, even when these assumptions are accurate, it is often the case that testers have to compromise on the thoroughness of their tests due time and cost considerations. In fact, a study performed as part of this project investigated the representativeness of developer-written tests and confirmed that tests fail to cover much of the behavior exercised in the field. To make in-house tests more representative, this research developed a technique that aims to bridge the gap between in-house tests and field executions by mimicking observed, untested user behavior. Intuitively, the technique discovers untested behavior occurring in the field and generates new tests that exercise this relevant behavior. Empirical evaluations of the technique show that it can be effective in generating test suites that are representative of the way software is actually used and can therefore reveal more bugs than traditional test suites developed in-house.</p> <p><em>Automated test repair during code evolution</em>. A considerable fraction of the cost of regression testing comes from test-suite maintenance and, in particular, from the effort developers spend repairing ?broken test cases?. When tests break because of a change in the software but are still valid, they should be repaired. Moreover, a broken test should be repaired so that the repaired test exercises the same behavior as the original test. This can require considerable effort in terms of understanding the test cases and the functionality of the program being tested. To reduce the cost of test repair, this research developed a technique that can fix broken tests by performing non-trivial changes to the test code while preserving the intent of the tests. An empirical evaluation of this technique showed that the technique is effective in generating intent-preserving repaired test cases, including in cases where the repair involves dealing with significant program (and test) changes.</p> <p><em>Automated library migration for mobile apps</em>. In addition to regression errors due to changes in a software system, it is not uncommon to witness regressions that are due solely to changes in the environment (i.e., operating/runtime system and libraries). This is particularly frequent in the case of mobile apps, as these apps rely heavily on their underlying environment. Unfortunately, evolving an app to adapt it to environment changes is a tedious, error-prone, and time-consuming task, especially when the app must work on multiple versions of the environment (e.g., multiple versions of the OS). To mitigate this problem, this research developed a technique that can learn how to update an app to adapt it to a given set of environment changes by (1) analyzing examples of corresponding updates on other apps, (2) synthesizing suitable code transformations, and (2) applying these transformations to the app to be updated. Empirical evaluations of this approach show that it can be successfully applied to real-world apps and real changes in their environment, thus potentially saving a considerable amount of developers? effort during software evolution.</p> <p><strong>Broader impact:</strong> In addition to disseminating the results of this research through publications, public presentations, and integration into the curriculum, this research made freely available to researchers and practitioners tools, data, and experiment infrastructure developed within the project, which will help further dissemination and enable future research. More generally, by advancing the state of the art in the areas of regression testing and software engineering in general, this research helped and will help developers build more reliable software systems, ultimately increasing the overall quality of our software infrastructure and providing benefits to all segments of society that depend on software.</p><br> <p>            Last Modified: 01/26/2020<br>      Modified by: Alessandro&nbsp;Orso</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ A considerable percentage of software development and maintenance costs are due to regression testing?retesting a software system after it is modified to assess whether the changes in the software behave as intended and have not introduced unwanted side effects. Both researchers and practitioners have defined many techniques for supporting regression-testing and improving its effectiveness and efficiency. Many of these techniques, however, make unrealistic assumptions on the software that is being tested or fall short when applied to modern software systems. The overall goal of this project was to go beyond the state of the art in regression testing by defining novel approaches that can be applied to modern, real-world software and account for its characteristics and complexity. The rest of this report summarizes a representative set of the techniques that were developed within the project.  Improving in-house tests based on observed field behavior. Regression testing, and testing in general, is inherently limited, as in-house tests can typically only exercise a tiny fraction of all possible software behaviors. Typically, developers make decisions on what to test and how based on some assumptions on how the software will be used in the field. When these assumptions are incorrect, which is a common occurrence, the test cases used in-house may not be representative of the software behavior that users actually exercise in the field. Moreover, even when these assumptions are accurate, it is often the case that testers have to compromise on the thoroughness of their tests due time and cost considerations. In fact, a study performed as part of this project investigated the representativeness of developer-written tests and confirmed that tests fail to cover much of the behavior exercised in the field. To make in-house tests more representative, this research developed a technique that aims to bridge the gap between in-house tests and field executions by mimicking observed, untested user behavior. Intuitively, the technique discovers untested behavior occurring in the field and generates new tests that exercise this relevant behavior. Empirical evaluations of the technique show that it can be effective in generating test suites that are representative of the way software is actually used and can therefore reveal more bugs than traditional test suites developed in-house.  Automated test repair during code evolution. A considerable fraction of the cost of regression testing comes from test-suite maintenance and, in particular, from the effort developers spend repairing ?broken test cases?. When tests break because of a change in the software but are still valid, they should be repaired. Moreover, a broken test should be repaired so that the repaired test exercises the same behavior as the original test. This can require considerable effort in terms of understanding the test cases and the functionality of the program being tested. To reduce the cost of test repair, this research developed a technique that can fix broken tests by performing non-trivial changes to the test code while preserving the intent of the tests. An empirical evaluation of this technique showed that the technique is effective in generating intent-preserving repaired test cases, including in cases where the repair involves dealing with significant program (and test) changes.  Automated library migration for mobile apps. In addition to regression errors due to changes in a software system, it is not uncommon to witness regressions that are due solely to changes in the environment (i.e., operating/runtime system and libraries). This is particularly frequent in the case of mobile apps, as these apps rely heavily on their underlying environment. Unfortunately, evolving an app to adapt it to environment changes is a tedious, error-prone, and time-consuming task, especially when the app must work on multiple versions of the environment (e.g., multiple versions of the OS). To mitigate this problem, this research developed a technique that can learn how to update an app to adapt it to a given set of environment changes by (1) analyzing examples of corresponding updates on other apps, (2) synthesizing suitable code transformations, and (2) applying these transformations to the app to be updated. Empirical evaluations of this approach show that it can be successfully applied to real-world apps and real changes in their environment, thus potentially saving a considerable amount of developers? effort during software evolution.  Broader impact: In addition to disseminating the results of this research through publications, public presentations, and integration into the curriculum, this research made freely available to researchers and practitioners tools, data, and experiment infrastructure developed within the project, which will help further dissemination and enable future research. More generally, by advancing the state of the art in the areas of regression testing and software engineering in general, this research helped and will help developers build more reliable software systems, ultimately increasing the overall quality of our software infrastructure and providing benefits to all segments of society that depend on software.       Last Modified: 01/26/2020       Submitted by: Alessandro Orso]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
