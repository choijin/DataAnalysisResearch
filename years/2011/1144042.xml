<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Proposed Meeting Series:  The Message Passing Interface Forum</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>12/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>70272.00</AwardTotalIntnAmount>
<AwardAmount>70272</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The work of the proposed Message Passing Interface Forum addresses the continuation of activity focused on enhancing the Message Passing Interface (MPI) for passing data between processors in distributed computing systems built from heterogeneous multi-cores, for parallel remote file operations, and for dynamic process control of such parallel applications. The current standard provides a core of routines used for point-to-point and one-sided data exchange, blocking collective communications routines, remote parallel file access and dynamic process control. In addition, these are augmented with support for features such as derived data types, communications contexts, process groups, and virtual process topologies. &lt;br/&gt;The work of this forum will produce a communications standard suitable for addressing the needs of parallel applications on the next generation computing facilities at the petascale level and beyond.</AbstractNarration>
<MinAmdLetterDate>08/08/2011</MinAmdLetterDate>
<MaxAmdLetterDate>08/08/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1144042</AwardID>
<Investigator>
<FirstName>Jack</FirstName>
<LastName>Dongarra</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jack J Dongarra</PI_FULL_NAME>
<EmailAddress>dongarra@icl.utk.edu</EmailAddress>
<PI_PHON>8659748295</PI_PHON>
<NSF_ID>000299281</NSF_ID>
<StartDate>08/08/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>George</FirstName>
<LastName>Bosilca</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>George M Bosilca</PI_FULL_NAME>
<EmailAddress>bosilca@icl.utk.edu</EmailAddress>
<PI_PHON>8659746321</PI_PHON>
<NSF_ID>000348594</NSF_ID>
<StartDate>08/08/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Tennessee Knoxville</Name>
<CityName>Knoxville</CityName>
<ZipCode>379163801</ZipCode>
<PhoneNumber>8659743466</PhoneNumber>
<StreetAddress>1331 CIR PARK DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003387891</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TENNESSEE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003387891</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Tennessee Knoxville]]></Name>
<CityName/>
<StateCode>TN</StateCode>
<ZipCode>379960003</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~70272</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The Message Passing Interface has become, over the last 2 decades, the ubiquitous model for programming parallel applications. It is a major and critical piece of any software stack targeted toward parallel applications, and a building block for most of the higher-level programming paradigms. It laid the foundation for the entire scientific computing field, allowing applications to take advantage of the now-complex architectures and boost the scientific throughput to unforeseen levels. It is difficult to summarize in words the daily impact of this technology, but few scientific discoveries are predicted, made, or proven without analytic computations, an area where MPI is of paramount importance.</p> <p>MPI is not an unmovable standard; it needs to evolve with our understanding of the physical characteristics of the materials used to build computers, with our comprehension of the most efficient ways to describe mathematical algorithms in terms of computer science, with our perception of the most advanced way of maximizing the throughput of the computational platforms we built, and with our interpretation of the short and long-term needs of current and future scientists. Message Passing is an active field of research, and the MPI standard is the tool to expose the needed concepts to the computational scientists, and provide them with a simple and straightforward way of preparing the humankind future through science.</p> <p>This project was targeted to provide funds to scientists from US universities to partially cover the travel cost and encourage them to participate in the meetings of the MPI Forum, the standardization body working on the MPI standard. Their participation is crucial as they are part of the interface between the users and the makers of the MPI standard. Their research is paramount to the novel features added to the MPI standard, they bring new functionality needed by the scientific computing community, and they implement and maintain the MPI libraries. At the same time, these scientists are the storytellers; they help propagate the knowledge about the new features and new capabilities of the MPI implementation to the users&rsquo; communities&mdash;ensuring a fast turn-around of all new MPI features into the science applications.</p> <p>The third version of the MPI standard contains significant extensions to MPI functionality, including non-blocking collectives, new one-sided communication operations, and Fortran 2008 bindings. A new MPI Tool Information Interface was added, as well as neighborhood collective communication routines, to support sparse communication on virtual topology grids. The prototype of some MPI routines has been changed to allow addressing large quantities of information, especially in the I/O routines. New routines to handle message probing / reception atomicity issues, due to multithread environment, have also been added to the standard.</p><br> <p>            Last Modified: 01/29/2013<br>      Modified by: Jack&nbsp;J&nbsp;Dongarra</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Message Passing Interface has become, over the last 2 decades, the ubiquitous model for programming parallel applications. It is a major and critical piece of any software stack targeted toward parallel applications, and a building block for most of the higher-level programming paradigms. It laid the foundation for the entire scientific computing field, allowing applications to take advantage of the now-complex architectures and boost the scientific throughput to unforeseen levels. It is difficult to summarize in words the daily impact of this technology, but few scientific discoveries are predicted, made, or proven without analytic computations, an area where MPI is of paramount importance.  MPI is not an unmovable standard; it needs to evolve with our understanding of the physical characteristics of the materials used to build computers, with our comprehension of the most efficient ways to describe mathematical algorithms in terms of computer science, with our perception of the most advanced way of maximizing the throughput of the computational platforms we built, and with our interpretation of the short and long-term needs of current and future scientists. Message Passing is an active field of research, and the MPI standard is the tool to expose the needed concepts to the computational scientists, and provide them with a simple and straightforward way of preparing the humankind future through science.  This project was targeted to provide funds to scientists from US universities to partially cover the travel cost and encourage them to participate in the meetings of the MPI Forum, the standardization body working on the MPI standard. Their participation is crucial as they are part of the interface between the users and the makers of the MPI standard. Their research is paramount to the novel features added to the MPI standard, they bring new functionality needed by the scientific computing community, and they implement and maintain the MPI libraries. At the same time, these scientists are the storytellers; they help propagate the knowledge about the new features and new capabilities of the MPI implementation to the usersÃ† communities&mdash;ensuring a fast turn-around of all new MPI features into the science applications.  The third version of the MPI standard contains significant extensions to MPI functionality, including non-blocking collectives, new one-sided communication operations, and Fortran 2008 bindings. A new MPI Tool Information Interface was added, as well as neighborhood collective communication routines, to support sparse communication on virtual topology grids. The prototype of some MPI routines has been changed to allow addressing large quantities of information, especially in the I/O routines. New routines to handle message probing / reception atomicity issues, due to multithread environment, have also been added to the standard.       Last Modified: 01/29/2013       Submitted by: Jack J Dongarra]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
