<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: PXFS - A Persistent Storage Model for Extreme Scale</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2011</AwardEffectiveDate>
<AwardExpirationDate>07/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>299796.00</AwardTotalIntnAmount>
<AwardAmount>299796</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The challenges of Exascale computing suggest that a new model of computation is needed upon&lt;br/&gt;which a new framework for mass storage may be built. Such a model currently under development is the&lt;br/&gt;ParalleX model. ParalleX addresses Exascale parallelism by removing most (if not all) global&lt;br/&gt;synchronization and providing a natural means for programs to manage local dependencies that maintain&lt;br/&gt;consistency. In a similar manner, we propose to develop a model of parallel storage that achieves the&lt;br/&gt;desired semantics without global synchronization. At the same time, the model will adopt a new&lt;br/&gt;programmer interface that reflects this approach. This I/O model is named PXFS and is intended to be&lt;br/&gt;integrated with the ParalleX model of computation, both by providing an I/O model for ParalleX and by&lt;br/&gt;being defined in terms of ParalleX.&lt;br/&gt;The goals of the proposed research are to derive a new model of persistent mass storage that&lt;br/&gt;unifies it with active in-memory data and develop PXFS, a proof-of-concept file system to enable&lt;br/&gt;effective and scalable Exascale computing. The objectives of the proposed project are to describe the&lt;br/&gt;PXFS model in complete terms, develop an initial reference implementation integrated with the HPX&lt;br/&gt;implementation of ParalleX, and evaluate the model via the reference implementation.</AbstractNarration>
<MinAmdLetterDate>07/14/2011</MinAmdLetterDate>
<MaxAmdLetterDate>07/14/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1142905</AwardID>
<Investigator>
<FirstName>Walter</FirstName>
<LastName>Ligon</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Walter B Ligon</PI_FULL_NAME>
<EmailAddress>walt@clemson.edu</EmailAddress>
<PI_PHON>8646561224</PI_PHON>
<NSF_ID>000359301</NSF_ID>
<StartDate>07/14/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Clemson University</Name>
<CityName>CLEMSON</CityName>
<ZipCode>296345701</ZipCode>
<PhoneNumber>8646562424</PhoneNumber>
<StreetAddress>230 Kappa Street</StreetAddress>
<StreetAddress2><![CDATA[Suite 200]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<StateCode>SC</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>SC03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>042629816</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CLEMSON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042629816</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Clemson University]]></Name>
<CityName>Clemson</CityName>
<StateCode>SC</StateCode>
<ZipCode>296340001</ZipCode>
<StreetAddress><![CDATA[300 Brackett Hall, Box 345702]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>SC03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~299796</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Science and engineering are increasingly reliant on the execution of realistic simulation models.&nbsp; Simulation is faster and cheaper than physical experimentation, and while it doesn&rsquo;t eliminate the need for experiments it can dramatically reduce the time and cost needed to get to a working solution.&nbsp;&nbsp; Simulations require significant amounts of computation and can both consume and generate very large amounts of data.&nbsp; Today simulations use a computational technique known as parallel processing to provided the needed computational performance.&nbsp; Parallel processing refers to using hundreds or even thousands or more computer processors to work together to solve a single large problem.&nbsp; Parallel processing can be quite difficult to use properly and many problems lose performance dramatically when the number of processing elements gets very large.&nbsp; In order to allow these problems to scale to next generation computer systems, the ParalleX computational model has been proposed.&nbsp; ParalleX describes a way of organizing computer code so that it can scale effectively to extremely large numbers of processors.&nbsp; Until now ParalleX only defined computations, but modern problems perform as much data I/O as computation.&nbsp; This project has focused on defining an I/O framework for ParalleX.<br /><br />Under this project we developed PXFS, an I/O subsystem for a programming environment based on ParalleX called HPX.&nbsp; HPX is a system based on c++ that manages the creation, execution, and interaction of threads on a large distributed computer system.&nbsp; PXFS is based on the Parallel Virtual File System (PVFS) which is an open source system for performing I/O on a large distributed computer system.&nbsp; The primary goal of the project was to develop requirements for PXFS by developing a prototype.&nbsp; The prototype used PVFS file server processes, and part of the PVFS API but built a new interface on top in order to understand how these two complex software systems might interact.<br /><br />One of the main issues discovered in this project was that HPX assumes complete control over threads execution on a given computer in the system.&nbsp; This became problematic, because PVFS regularly interacts with the computer&rsquo;s operating system in a way that would cause all of the HPX threads to stop while I/O was being done.&nbsp; To remedy this it was clear that the HPX/PVFS interface had to employ a asynchronous model.&nbsp; The first approach employed used an asynchronous layer on top of the PVFS file system interface.&nbsp; This layer was constructed to look much like the common POSIX API, but allowed each call to execute in the background and call a completion function when it had finished.&nbsp; This was implemented using a distinct thread which solved the conflict discussed, but was not particularly efficient.</p> <p>Experiments run on the interface showed very little overhead over standard interfaces, and even a performance benefit because I/O could be submitted concurrently.&nbsp; It was theorized that poor interaction with HPX was the culprit.&nbsp; Subsequent experimentation involved powering the asynchronous interface with an HPX thread which allowed HPX to manage the I/O.&nbsp; This produced good results.&nbsp; Additional experiments involved moving the I/O processing from an HPX compute node to a PVFS server node and using HPX active messaging to send data for I/O.&nbsp; This produced even better results because the I/O processing was removed from the compute nodes completely.<br /><br />Thus, the results of this project were to define an appropriate architecture for PXFS as the addition of HPX communication capability to storage nodes rather than the addition of storage processing on compute nodes.&nbsp; It also demonstrated that this can be done with a very low overhead and that the inherent support for highly a...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Science and engineering are increasingly reliant on the execution of realistic simulation models.  Simulation is faster and cheaper than physical experimentation, and while it doesnÃ†t eliminate the need for experiments it can dramatically reduce the time and cost needed to get to a working solution.   Simulations require significant amounts of computation and can both consume and generate very large amounts of data.  Today simulations use a computational technique known as parallel processing to provided the needed computational performance.  Parallel processing refers to using hundreds or even thousands or more computer processors to work together to solve a single large problem.  Parallel processing can be quite difficult to use properly and many problems lose performance dramatically when the number of processing elements gets very large.  In order to allow these problems to scale to next generation computer systems, the ParalleX computational model has been proposed.  ParalleX describes a way of organizing computer code so that it can scale effectively to extremely large numbers of processors.  Until now ParalleX only defined computations, but modern problems perform as much data I/O as computation.  This project has focused on defining an I/O framework for ParalleX.  Under this project we developed PXFS, an I/O subsystem for a programming environment based on ParalleX called HPX.  HPX is a system based on c++ that manages the creation, execution, and interaction of threads on a large distributed computer system.  PXFS is based on the Parallel Virtual File System (PVFS) which is an open source system for performing I/O on a large distributed computer system.  The primary goal of the project was to develop requirements for PXFS by developing a prototype.  The prototype used PVFS file server processes, and part of the PVFS API but built a new interface on top in order to understand how these two complex software systems might interact.  One of the main issues discovered in this project was that HPX assumes complete control over threads execution on a given computer in the system.  This became problematic, because PVFS regularly interacts with the computerÃ†s operating system in a way that would cause all of the HPX threads to stop while I/O was being done.  To remedy this it was clear that the HPX/PVFS interface had to employ a asynchronous model.  The first approach employed used an asynchronous layer on top of the PVFS file system interface.  This layer was constructed to look much like the common POSIX API, but allowed each call to execute in the background and call a completion function when it had finished.  This was implemented using a distinct thread which solved the conflict discussed, but was not particularly efficient.  Experiments run on the interface showed very little overhead over standard interfaces, and even a performance benefit because I/O could be submitted concurrently.  It was theorized that poor interaction with HPX was the culprit.  Subsequent experimentation involved powering the asynchronous interface with an HPX thread which allowed HPX to manage the I/O.  This produced good results.  Additional experiments involved moving the I/O processing from an HPX compute node to a PVFS server node and using HPX active messaging to send data for I/O.  This produced even better results because the I/O processing was removed from the compute nodes completely.  Thus, the results of this project were to define an appropriate architecture for PXFS as the addition of HPX communication capability to storage nodes rather than the addition of storage processing on compute nodes.  It also demonstrated that this can be done with a very low overhead and that the inherent support for highly asynchronous I/O can be very beneficial.  This has led to a new proposal based on these results where we intend to integrate I/O into the HPX model in such a way that it becomes transparent to the user where storage ends and memory begi...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
