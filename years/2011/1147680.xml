<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SI2-SSE Collaborative Research: SPIKE-An Implementation of a Recursive Divide-and-Conquer Parallel Strategy for Solving  Large  Systems of Linear Equations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2012</AwardEffectiveDate>
<AwardExpirationDate>05/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>117710.00</AwardTotalIntnAmount>
<AwardAmount>117710</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rajiv Ramnath</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Drs. Negrut, Sameh, and Knepley will investigate, produce, and maintain a methodology and its software implementation that leverage emerging heterogeneous hardware architectures to solve billion-unknowns linear systems in a robust, scalable, and efficient fashion. The two classes of problems targeted under this project are banded dense and sparse general linear systems.&lt;br/&gt;&lt;br/&gt;This project is motivated by the observation that the task of solving a linear system is one of the most ubiquitous ingredients in the numerical solution of Applied Mathematics problems. It is relied upon for the implicit integration of Ordinary Differential Equation (ODE) and Differential Algebraic Equation (DAE) problems, in the numerical solution of Partial Differential Equation (PDE) problems, in interior point optimization methods, in least squares approximations, in solving eigenvalue problems, and in data analysis. In fact, the vast majority of nonlinear problems in Scientific Computing are solved iteratively by drawing on local linearizations of nonlinear operators and the solution of linear systems. Recent advances in (a) hardware architecture; i.e., the emergence of General Purpose Graphics Processing Unit (GP-GPU) cards, and (b) scalable solution algorithms, provide an opportunity to develop a new class of parallel algorithms, called SPIKE, which can robustly and efficiently solve very large linear systems of equations.&lt;br/&gt;&lt;br/&gt;Drawing on its divide-and-conquer paradigm, SPIKE builds on several algorithmic primitives: matrix reordering strategies, dense linear algebra operations, sparse direct solvers, and Krylov subspace methods. It provides a scalable solution that can be deployed in a heterogeneous hardware ecosystem and has the potential to solve billion-unknown linear systems in the cloud or on tomorrow?s exascale supercomputers. Its high degree of scalability and improved efficiency stem from (i) optimized memory access pattern owing to an aggressive pre-processing stage that reduces a generic sparse matrix to a banded one through a novel reordering strategy; (ii) good exposure of coarse and fine grain parallelism owing to a recursive, divide-and-conquer solution strategy; (iii) efficient vectorization in evaluating the coupling terms in the divide-and-conquer stage owing to a CPU+GPU heterogeneous computing approach; and (iv) algorithmic polymorphism, given that SPIKE can serve both as a direct solver or an effective preconditioner in an iterative Krylov-type method.&lt;br/&gt;&lt;br/&gt;In Engineering, SPIKE will provide the Computer Aided Engineering (CAE) community with a key component; i.e., fast solution of linear systems, required by the analysis of complex problems through computer simulation. Examples of applications that would benefit from this technology are Structural Mechanics problems (Finite Element Analysis in car crash simulation), Computational Fluid Dynamics problems (solving Navier-Stokes equations in the simulation of turbulent flow around a wing profile), and Computational Multibody Dynamics problems (solving Newton-Euler equations in large granular dynamics problems).&lt;br/&gt;&lt;br/&gt;SPIKE will also be interfaced to the Portable, Extensible Toolkit for Scientific Computation (PETSc), a two decades old flexible and scalable framework for solving Science and Engineering problems on supercomputers. Through PETSc, SPIKE will be made available to a High Performance Computing user community with more than 20,000 members worldwide. PETSc users will be able to run SPIKE without any modifications on vastly different supercomputer architectures such as the IBM BlueGene/P and BlueGene/Q, or the Cray XT5. SPIKE will thus run scalably on the largest machines in the world and will be tuned for very different network and hardware topologies while maintaining a simple code base.&lt;br/&gt;&lt;br/&gt;The experience collected and lessons learned in this project will augment a graduate level class, ?High Performance Computing for Engineering Applications? taught at the University of Wisconsin-Madison. A SPIKE tutorial and research outcomes will be presented each year at the International Conference for High Performance Computing, Networking, Storage and Analysis. A one day High Performance Computing Boot Camp will be organized each year in conjunction with the American Society of Mechanical Engineers (ASME) conference and used to disseminate the software outcomes of this effort. Finally, this project will shape the research agendas of two graduate students working on advanced degrees in Computational Science.</AbstractNarration>
<MinAmdLetterDate>06/20/2012</MinAmdLetterDate>
<MaxAmdLetterDate>06/20/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1147680</AwardID>
<Investigator>
<FirstName>Matthew</FirstName>
<LastName>Knepley</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Matthew Knepley</PI_FULL_NAME>
<EmailAddress>knepley@buffalo.edu</EmailAddress>
<PI_PHON>7166450747</PI_PHON>
<NSF_ID>000503185</NSF_ID>
<StartDate>06/20/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372612</ZipCode>
<PhoneNumber>7737028669</PhoneNumber>
<StreetAddress>6054 South Drexel Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005421136</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CHICAGO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005421136</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The University of Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606371433</ZipCode>
<StreetAddress><![CDATA[5735 South Ellis]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1253</Code>
<Text>OFFICE OF MULTIDISCIPLINARY AC</Text>
</ProgramElement>
<ProgramElement>
<Code>7478</Code>
<Text>DYNAMICAL SYSTEMS</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>8005</Code>
<Text>Scientific Software Elements</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~117710</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>As computers become larger and more integrated, meaning that computational cores are packed more tightly, latency has become a major factor in the overall performance of important algorithms for simulation, such as the simulation of fluid flow, mechanical movement, granular mechanics, or protein binding.</p> <p>Latency means waiting time, and can arise from many different causes. It takes a finite time for electric signals to propagate across a large chip. There are only a few pipes from memory to cores, so that any given core might have to wait for values from memory to be delivered before it can continue computing. Synchronization among tasks on different cores might mean that tasks finishing first will have to wait for slower tasks. The operating system or other system level software can intrude into the operation of a simulation, causing waiting time to increase unpredictably.</p> <p>In this project, we are concerned with the operation of Krylov methods, a very popular solver for systems of linear equations, in an environment with both high and unpredictable latency. A drawback of Krylov methods is that they contain many synchronization points, arising from vctor dot products, which limit the performance and scalability in the presence of high latencies. Krylov method variants, called asynchronous Krylov methods, which are latency tolerant have been previously proposed and shown to perform well in high latency environments. However, no performance analysis has been done, so that we may predict such performance. Moreover, the prior work has not taken into account the random nature of the waiting times.</p> <p>Our group developed a stochastic model of the latency in a Krylov method, meaning we assume that the waiting times are random and cna be described as arising from some theoretical distribution. For a given distribution of waiting times, we could calculate the improvement in performance when using asynchronous Krylov methods as opposed to the normal method. This characterization of the speedup allowed us to run a large number of Krylov solves on a supercomputer, and calculate the probable distributions for the latency, whereas a direct characterization of the latency itself would be either too complex or unobtainable.</p> <p>Using this tool, we were able to demonstrate a misconception about the supercomputing environment and algorithmic performance. It is clear that the latency or waiting times on a large computer are not small, random, uncorrelated events, but instead follow distributions with long tails, meaning long waiting times occur much more often that we suspected. This means that the asynchrous Krylov methods can achieve a greater speedup on avergage than has been reported in the literature. We suspect that this is due to the fact that most people select the fastest runs when reporting performance to a journal, rather than using all runs including those that are anomalously slow. However, it is important to include all runs since this gives the most accurate picture for the average scientist or engineer who will eventually use these methods to carry out their research. Hopefully, our mathematical characterization of performance can be used now to design more efficient and scalable methods for this complex environment, and can be extended to heterogeneous computing platforms, meaning those with different kinds of cores such as regular CPUs combined with GPUs or other co-processors and accelerators.</p><br> <p>            Last Modified: 09/24/2015<br>      Modified by: Matthew&nbsp;Knepley</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ As computers become larger and more integrated, meaning that computational cores are packed more tightly, latency has become a major factor in the overall performance of important algorithms for simulation, such as the simulation of fluid flow, mechanical movement, granular mechanics, or protein binding.  Latency means waiting time, and can arise from many different causes. It takes a finite time for electric signals to propagate across a large chip. There are only a few pipes from memory to cores, so that any given core might have to wait for values from memory to be delivered before it can continue computing. Synchronization among tasks on different cores might mean that tasks finishing first will have to wait for slower tasks. The operating system or other system level software can intrude into the operation of a simulation, causing waiting time to increase unpredictably.  In this project, we are concerned with the operation of Krylov methods, a very popular solver for systems of linear equations, in an environment with both high and unpredictable latency. A drawback of Krylov methods is that they contain many synchronization points, arising from vctor dot products, which limit the performance and scalability in the presence of high latencies. Krylov method variants, called asynchronous Krylov methods, which are latency tolerant have been previously proposed and shown to perform well in high latency environments. However, no performance analysis has been done, so that we may predict such performance. Moreover, the prior work has not taken into account the random nature of the waiting times.  Our group developed a stochastic model of the latency in a Krylov method, meaning we assume that the waiting times are random and cna be described as arising from some theoretical distribution. For a given distribution of waiting times, we could calculate the improvement in performance when using asynchronous Krylov methods as opposed to the normal method. This characterization of the speedup allowed us to run a large number of Krylov solves on a supercomputer, and calculate the probable distributions for the latency, whereas a direct characterization of the latency itself would be either too complex or unobtainable.  Using this tool, we were able to demonstrate a misconception about the supercomputing environment and algorithmic performance. It is clear that the latency or waiting times on a large computer are not small, random, uncorrelated events, but instead follow distributions with long tails, meaning long waiting times occur much more often that we suspected. This means that the asynchrous Krylov methods can achieve a greater speedup on avergage than has been reported in the literature. We suspect that this is due to the fact that most people select the fastest runs when reporting performance to a journal, rather than using all runs including those that are anomalously slow. However, it is important to include all runs since this gives the most accurate picture for the average scientist or engineer who will eventually use these methods to carry out their research. Hopefully, our mathematical characterization of performance can be used now to design more efficient and scalable methods for this complex environment, and can be extended to heterogeneous computing platforms, meaning those with different kinds of cores such as regular CPUs combined with GPUs or other co-processors and accelerators.       Last Modified: 09/24/2015       Submitted by: Matthew Knepley]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
