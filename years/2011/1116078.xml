<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Coordinating Multi-Agent Learning through Emergent Distributed Supervisory Control</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Hector Munoz-Avila</SignBlockName>
<PO_EMAI>hmunoz@nsf.gov</PO_EMAI>
<PO_PHON>7032924481</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The project is focused on developing coordination policies for large-scale multi-agent systems operating in uncertain environments through the use of multi-agent reinforcement learning (MARL). Existing MARL techniques do not scale well. This research addresses the scaling issue by using coordination technology to "coordinate" the individual agent learning so as to speed up convergence and lead to learned policies that better reflect overall system objectives. This novel idea is being implemented using an emergent supervisory organization with low overhead that exploits non-local information to dynamically coordinate and shape the learning processes of individual agents while still allowing agents to react autonomously to local feedback. A key question is how to automate the development of the supervisory control process (including supervisory information generation and organization formation). One approach to automation is using a formal model of interactions among agents that also includes a model of global system objectives and policy space of agents to derive the information necessary for appropriate supervisory control. Another approach is the formulation of the supervision problem as a distributed constraint optimization problem. The results of this work provide a necessary component for the development of a wide variety of next-generation adaptive applications, such as smart power grids, cloud computing, and large-scale sensor networks. The broader impact stems from the wide applicability of the resulting learning technology for distributed control, undergraduate and graduate educational activities at UMass, dissemination efforts that make the experimental domain and algorithms publically available, and the development of international collaborations.</AbstractNarration>
<MinAmdLetterDate>09/06/2011</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1116078</AwardID>
<Investigator>
<FirstName>Victor</FirstName>
<LastName>Lesser</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Victor R Lesser</PI_FULL_NAME>
<EmailAddress>lesser@cs.umass.edu</EmailAddress>
<PI_PHON>4135452744</PI_PHON>
<NSF_ID>000128595</NSF_ID>
<StartDate>09/06/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName>Hadley</CityName>
<StateCode>MA</StateCode>
<ZipCode>010359450</ZipCode>
<StreetAddress><![CDATA[Research Administration Building]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Cooperative multi-agent systems (CMAS) are finding applications in a wide variety of next-generation adaptive applications, such as smart power grids, cloud computing, large-scale sensor networks, autonomic computing, disaster management, etc. A CMAS consists of a group of autonomous software agents that are distributed and interact with one another using limited communication bandwidth in order to optimize global performance. A central challenge in building such systems is to design distributed coordination policies that define what actions local agents should take in the context of the state and actions of other agents. Computing optimal policies offline is infeasible for complex systems with unknown environment characteristics and involving tens to thousands of agents with limited communication bandwidth and partial views of the whole system. Multi-agent reinforcement learning (MARL) potentially provides an attractive approximate approach for agents to incrementally develop effective coordination policies. It allows agents to adapt their behaviors to the dynamics of the uncertain and evolving environment. However, existing MARL techniques do not effectively scale as the number of agents increases because of communication overhead or poor overall performance in terms of likelihood of convergence to stable policies, the time required for this convergence, and the quality of the learned policies as measure against overall system performance characteristics.</p> <p>This novel approach that has been explored in this research project is the use of a supervisory software agent organization, with acceptable levels of computation and communication overhead, that exploits non-local information to dynamically coordinate and shape learning processes of individual learning agents while still allowing agents to react autonomously to local feedbacks. A key idea motivating this work is interaction sparsity (also called nearly-decomposability), where agents interact strongly with a small group of closely related partners (by proximity, similarity, or some task-based measure). This leads to promising opportunities to summarize agent interactions in a compact way. In addition, it may not be necessary to reason about individual interactions but rather some aggregate interaction effect over groups of anonymous agents that is maintained by a regional supervisory agent.</p> <p>The three main research explorations that were conducted focused on 1) What type of information should a supervisory agent provide to assist the learning in a local agent? 2) Can agents share the intermediate results of their learning with other agents to speed up overall learning in the network?, and 3) Can supervisory control be implemented in a way that does not require detail knowledge of the application domain? In all these studies, the result was a significant increase in the speed of learning and in some cases also the quality of the learned policies. We demonstrated the effectiveness of this approach on a large-scale distributed task allocation problem with hundreds of agents operating in an unknown environment.</p> <p>In the first exploration, an adaptive supervisory control was developed that used both action-shaping and reward-shaping to influence local agent learning. Action-shaping biased agent learning to so that all the different action options that agent had a their disposal to react to their specific local state were not uniformly explored in the learning process. Whereas in reward-shaping, a modified version of the reward that agent&rsquo;s received from the environment when taking a specific action was used in the learning process rather than the actual reward. Both of these shaping signals and their importance were generated dynamically based on the supervisor&rsquo;s view of the evolving state of the group of agents under its control.</p> <p>In the second exploration, an approach that adapti...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Cooperative multi-agent systems (CMAS) are finding applications in a wide variety of next-generation adaptive applications, such as smart power grids, cloud computing, large-scale sensor networks, autonomic computing, disaster management, etc. A CMAS consists of a group of autonomous software agents that are distributed and interact with one another using limited communication bandwidth in order to optimize global performance. A central challenge in building such systems is to design distributed coordination policies that define what actions local agents should take in the context of the state and actions of other agents. Computing optimal policies offline is infeasible for complex systems with unknown environment characteristics and involving tens to thousands of agents with limited communication bandwidth and partial views of the whole system. Multi-agent reinforcement learning (MARL) potentially provides an attractive approximate approach for agents to incrementally develop effective coordination policies. It allows agents to adapt their behaviors to the dynamics of the uncertain and evolving environment. However, existing MARL techniques do not effectively scale as the number of agents increases because of communication overhead or poor overall performance in terms of likelihood of convergence to stable policies, the time required for this convergence, and the quality of the learned policies as measure against overall system performance characteristics.  This novel approach that has been explored in this research project is the use of a supervisory software agent organization, with acceptable levels of computation and communication overhead, that exploits non-local information to dynamically coordinate and shape learning processes of individual learning agents while still allowing agents to react autonomously to local feedbacks. A key idea motivating this work is interaction sparsity (also called nearly-decomposability), where agents interact strongly with a small group of closely related partners (by proximity, similarity, or some task-based measure). This leads to promising opportunities to summarize agent interactions in a compact way. In addition, it may not be necessary to reason about individual interactions but rather some aggregate interaction effect over groups of anonymous agents that is maintained by a regional supervisory agent.  The three main research explorations that were conducted focused on 1) What type of information should a supervisory agent provide to assist the learning in a local agent? 2) Can agents share the intermediate results of their learning with other agents to speed up overall learning in the network?, and 3) Can supervisory control be implemented in a way that does not require detail knowledge of the application domain? In all these studies, the result was a significant increase in the speed of learning and in some cases also the quality of the learned policies. We demonstrated the effectiveness of this approach on a large-scale distributed task allocation problem with hundreds of agents operating in an unknown environment.  In the first exploration, an adaptive supervisory control was developed that used both action-shaping and reward-shaping to influence local agent learning. Action-shaping biased agent learning to so that all the different action options that agent had a their disposal to react to their specific local state were not uniformly explored in the learning process. Whereas in reward-shaping, a modified version of the reward that agentÆs received from the environment when taking a specific action was used in the learning process rather than the actual reward. Both of these shaping signals and their importance were generated dynamically based on the supervisorÆs view of the evolving state of the group of agents under its control.  In the second exploration, an approach that adaptively identifies opportunities to periodically transfer experiences among agents in a large network...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
