<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Collaborative Research: Multilingual Gestural Models for Robust Language-Independent Speech Recognition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2017</AwardExpirationDate>
<AwardTotalIntnAmount>52627.00</AwardTotalIntnAmount>
<AwardAmount>52627</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Current state-of-the-art automatic speech recognition (ASR) systems typically model speech as a string of acoustically-defined phones and use contextualized phone units, such as tri-phones or quin-phones to model contextual influences due to coarticulation.  Such acoustic models may suffer from data sparsity and may fail to capture coarticulation appropriately because the span of a tri- or quin-phone's contextual influence is not flexible. In a small vocabulary context, however, research has shown that ASR systems which estimate articulatory gestures from the acoustics and incorporate these gestures in the ASR process can better model coarticulation and are more robust to noise.  The current project investigates the use of estimated articulatory gestures in large vocabulary automatic speech recognition.  Gestural representations of the speech signal are initially created from the acoustic waveform using the Task Dynamic model of speech production.  These data are then used to train automatic models for articulatory gesture recognition where the articulatory gestures serve as subword units in the gesture-based ASR system.  The main goal of the proposed work is to evaluate the performance of a large-vocabulary gesture-based ASR system using American English (AE).  The gesture-based system will be compared to a set of competitive state-of-the-art recognition systems in term of word and phone recognition accuracies, both under clean and noisy acoustic background conditions.&lt;br/&gt;&lt;br/&gt;The broad impact of this research is threefold: (1) the creation of a large vocabulary American English (AE) speech database containing acoustic waveforms and their articulatory representations, (2) the introduction of novel machine learning techniques to model articulatory representations from acoustic waveforms, and (3) the development of a large vocabulary ASR system that uses articulatory representation as subword units.  The robust and accurate ASR system for AE resulting from the proposed project will deal effectively with speech variability, thereby significantly enhancing communication and collaboration between people and machines in AE, and with the promise to generalize the method to multiple languages.  The knowledge gained and the systems developed will contribute to the broad application of articulatory features in speech processing, and will have the potential to transform the fields of ASR, speech-mediated person-machine interaction, and automatic translation among languages.  The interdisciplinary collaboration will facilitate a cross-disciplinary learning environment for the participating faculty, researchers, graduate students and undergraduate students  Thus, this collaboration will result in the broader impact of enhanced training in speech modeling and algorithm development.  Finally, the proposed work will result in a set of databases and tools that will be disseminated to serve the research and education community at large.</AbstractNarration>
<MinAmdLetterDate>09/18/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/18/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1162033</AwardID>
<Investigator>
<FirstName>Elliot</FirstName>
<LastName>Saltzman</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Elliot Saltzman</PI_FULL_NAME>
<EmailAddress>esaltz@bu.edu</EmailAddress>
<PI_PHON>6173537494</PI_PHON>
<NSF_ID>000117701</NSF_ID>
<StartDate>09/18/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Trustees of Boston University</Name>
<CityName>BOSTON</CityName>
<ZipCode>022151300</ZipCode>
<PhoneNumber>6173534365</PhoneNumber>
<StreetAddress>881 COMMONWEALTH AVE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049435266</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF BOSTON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049435266</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Trustees of Boston University]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>022151300</ZipCode>
<StreetAddress><![CDATA[881 Commonwealth Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~52627</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><!--   @font-face  {font-family:Calibri;  panose-1:2 15 5 2 2 2 4 3 2 4;  mso-font-charset:0;  mso-generic-font-family:auto;  mso-font-pitch:variable;  mso-font-signature:-520092929 1073786111 9 0 415 0;}   p.MsoNormal, li.MsoNormal, div.MsoNormal  {mso-style-unhide:no;  mso-style-qformat:yes;  mso-style-parent:";  margin-top:0in;  margin-right:0in;  margin-bottom:10.0pt;  margin-left:0in;  line-height:115%;  mso-pagination:widow-orphan;  font-size:11.0pt;  font-family:Calibri;  mso-ascii-font-family:Calibri;  mso-ascii-theme-font:minor-latin;  mso-fareast-font-family:Calibri;  mso-fareast-theme-font:minor-latin;  mso-hansi-font-family:Calibri;  mso-hansi-theme-font:minor-latin;  mso-bidi-font-family:"Times New Roman";  mso-bidi-theme-font:minor-bidi;} p  {mso-style-priority:99;  mso-margin-top-alt:auto;  margin-right:0in;  mso-margin-bottom-alt:auto;  margin-left:0in;  mso-pagination:widow-orphan;  font-size:12.0pt;  font-family:"Times New Roman";  mso-fareast-font-family:"Times New Roman";} .MsoChpDefault  {mso-style-type:export-only;  mso-default-props:yes;  font-size:11.0pt;  mso-ansi-font-size:11.0pt;  mso-bidi-font-size:11.0pt;  font-family:Calibri;  mso-ascii-font-family:Calibri;  mso-ascii-theme-font:minor-latin;  mso-fareast-font-family:Calibri;  mso-fareast-theme-font:minor-latin;  mso-hansi-font-family:Calibri;  mso-hansi-theme-font:minor-latin;  mso-bidi-font-family:"Times New Roman";  mso-bidi-theme-font:minor-bidi;} .MsoPapDefault  {mso-style-type:export-only;  margin-bottom:10.0pt;  line-height:115%;} @page WordSection1  {size:8.5in 11.0in;  margin:1.0in 1.25in 1.0in 1.25in;  mso-header-margin:.5in;  mso-footer-margin:.5in;  mso-paper-source:0;} div.WordSection1  {page:WordSection1;} --> <p>The project resulted in creation of several speech datasets that contain both speech and articulatory measures. These data were used to train both single speaker and multi-speaker speech inversion models. Several machine learning approaches were explored to train speech inversion models that would take acoustic features as input and produce articulatory information. Results from these explorations were shared with the research community through several publications. The speech inversion models were also tried on speech recognition tasks, where articulatory features were fed as another form of features to train and test speech recognition models and we observed consistent gain in performance when articulatory features were used. In addition to speech recognition, the articulatory features were also tried for mental health applications, where we observed that such features can provide quite impressive performance for detecting mental depression levels from speech.</p> <p>This work supported a graduate student, who received formal training in research and development on signal processing techniques, speech recognition tools and machine learning concepts. The outcomes of the works have been shared with the research community through conference/meeting presentation and posters. The data produced in this work will be shared with the research community through an online website. The work also resulted in novel acoustic modeling architectures, and such architectures have demonstrated reasonable performance gains with respect to strong baseline systems.</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/26/2018<br>      Modified by: Elliot&nbsp;Saltzman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The project resulted in creation of several speech datasets that contain both speech and articulatory measures. These data were used to train both single speaker and multi-speaker speech inversion models. Several machine learning approaches were explored to train speech inversion models that would take acoustic features as input and produce articulatory information. Results from these explorations were shared with the research community through several publications. The speech inversion models were also tried on speech recognition tasks, where articulatory features were fed as another form of features to train and test speech recognition models and we observed consistent gain in performance when articulatory features were used. In addition to speech recognition, the articulatory features were also tried for mental health applications, where we observed that such features can provide quite impressive performance for detecting mental depression levels from speech.  This work supported a graduate student, who received formal training in research and development on signal processing techniques, speech recognition tools and machine learning concepts. The outcomes of the works have been shared with the research community through conference/meeting presentation and posters. The data produced in this work will be shared with the research community through an online website. The work also resulted in novel acoustic modeling architectures, and such architectures have demonstrated reasonable performance gains with respect to strong baseline systems.          Last Modified: 04/26/2018       Submitted by: Elliot Saltzman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
