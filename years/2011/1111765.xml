<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Large: Collaborative Research: 3D Structure and Motion in Dynamic Natural Scenes</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2016</AwardExpirationDate>
<AwardTotalIntnAmount>680000.00</AwardTotalIntnAmount>
<AwardAmount>704999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>How does a vision system recover the 3-dimensional structure of the world -- such as the layout of the environment, surface shape, or object motion -- from the dynamic 2-dimensional images received by the sensors in a camera, or the retinas in our eyes?  This problem is fundamental to both computer and biological vision.  Computer vision has developed a variety of algorithms for estimating specific aspects of a scene such as the 3-dimensional positions of points whose correspondence over time can be established, but obtaining complete and robust scene representations for complex natural scenes and viewing conditions remains a challenge.  Biological vision systems have evolved impressive capabilities that suggest they have detailed and robust representations of the 3-dimensional world, but the neural representations that subserve this are poorly understood and neurophysiological studies thus far have provided little insight into the computational process.  This project will pursue an interdisciplinary approach by attempting the understand the universal principles that lie at the heart of 3-dimensional scene analysis.&lt;br/&gt;&lt;br/&gt;Specifically, the project will  1) develop a novel class of computational models that recover and represent 3-dimensional scene information, 2) collect high quality video and range data of dynamic natural scenes under a variety of controlled motion conditions, and 3) test the perceptual implications of these models in psychophysical experiments.  The computational models will utilize non-linear decomposition - i.e., the ability to explain complex, time-varying images in terms of the non-linear interaction of multiple factors, such as the interaction between observer motion, the 3-dimensional scene layout, and surface patterns.  Importantly, the components of these models will be adapted to the statistics of natural motion patterns that arise from observer motion through natural scenes and movement around points of fixation.&lt;br/&gt;&lt;br/&gt;The project is a collaboration between three laboratories that have played a leading role in developing theoretical models of natural image statistics, visual neural representations, and perceptual processes.  The investigators seek to combine their efforts to develop new models, data sets, and characterizations of 3-dimensional natural scene structure that go beyond previous studies of natural image statistics, and that can be tested in neurophysiological and psychophysical experiments.  This project has the potential to bring about fundamental advances in neuroscience, visual perception, and computer vision by developing new classes of models that robustly infer representations of the 3-dimensional natural environment.  It will create a set of high quality databases that will be made available to help other investigators study these issues.  It will also open up new possibilities for generating realistic stimuli that can guide novel investigations of neural representation and processing.</AbstractNarration>
<MinAmdLetterDate>08/26/2011</MinAmdLetterDate>
<MaxAmdLetterDate>01/24/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1111765</AwardID>
<Investigator>
<FirstName>Bruno</FirstName>
<LastName>Olshausen</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Bruno A Olshausen</PI_FULL_NAME>
<EmailAddress>baolshausen@berkeley.edu</EmailAddress>
<PI_PHON>5106427250</PI_PHON>
<NSF_ID>000444415</NSF_ID>
<StartDate>08/26/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName>BERKELEY</CityName>
<StateCode>CA</StateCode>
<ZipCode>947101749</ZipCode>
<StreetAddress><![CDATA[Sponsored Projects Office]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7298</Code>
<Text>International Research Collab</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>5936</Code>
<Text>GERMANY (F.R.G.)</Text>
</ProgramReference>
<ProgramReference>
<Code>5979</Code>
<Text>Europe and Eurasia</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~680000</FUND_OBLG>
<FUND_OBLG>2012~24999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project developed new computational models for how form (spatial patterns) and motion are separated in the visual system. &nbsp;A major challeng in vision is to infer from the time-varying pixel intensities on the retina the identies of objects and how they are moving in the world. &nbsp;What makes this difficult is that information about form (shape) and motion are entangled in the pixel activities. &nbsp;We have developed here a model for how the cortex could do this from incoming spikes from the retina. &nbsp;The model not only makes predictions about what to look for in neurophysiological experiments, but it also suggests new ways of making cameras that could exploit movement during the acquisition of an image so as to obtain a sharper or improved quality image.</p> <p>A more recent development from this project was to investigate visual representations in an active visual system - i.e., where the eye is moving. &nbsp;We showed that the type of eccentricity-dependent sampling array found in the retina is in fact optimal for an active vision system engaged in a visual search task. &nbsp;We have also developed a model that shows how information obtained from successive eye movements may be combined to form an overal scene reprsesentation. &nbsp;This brings us a step close to understanding how this is done in brains, and it also suggests new ways of engineering visually-guided autonomous systems.</p> <p>&nbsp;</p><br> <p>            Last Modified: 02/09/2017<br>      Modified by: Bruno&nbsp;A&nbsp;Olshausen</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project developed new computational models for how form (spatial patterns) and motion are separated in the visual system.  A major challeng in vision is to infer from the time-varying pixel intensities on the retina the identies of objects and how they are moving in the world.  What makes this difficult is that information about form (shape) and motion are entangled in the pixel activities.  We have developed here a model for how the cortex could do this from incoming spikes from the retina.  The model not only makes predictions about what to look for in neurophysiological experiments, but it also suggests new ways of making cameras that could exploit movement during the acquisition of an image so as to obtain a sharper or improved quality image.  A more recent development from this project was to investigate visual representations in an active visual system - i.e., where the eye is moving.  We showed that the type of eccentricity-dependent sampling array found in the retina is in fact optimal for an active vision system engaged in a visual search task.  We have also developed a model that shows how information obtained from successive eye movements may be combined to form an overal scene reprsesentation.  This brings us a step close to understanding how this is done in brains, and it also suggests new ways of engineering visually-guided autonomous systems.          Last Modified: 02/09/2017       Submitted by: Bruno A Olshausen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
