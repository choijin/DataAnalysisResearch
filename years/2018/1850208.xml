<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: Can Low-Bias Machine Learners Acquire English Grammar? Deep Learning and Linguistic Acceptability</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2019</AwardEffectiveDate>
<AwardExpirationDate>02/28/2021</AwardExpirationDate>
<AwardTotalIntnAmount>174894.00</AwardTotalIntnAmount>
<AwardAmount>174894</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>D.  Langendoen</SignBlockName>
<PO_EMAI>dlangend@nsf.gov</PO_EMAI>
<PO_PHON>7032925088</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Widely-deployed applications of language technology such as translation systems and smart assistants rely heavily on machine learning models for sentence understanding. These models learn to understand language from data, which can often be as simple as a collection of published books or a download of Wikipedia, rather than through any kind of manual engineering or hands-on guidance by linguistic expert. While modern machine learning methods are quite effective, they are not perfect. When they fail understand some text, it can be difficult to discover why, and even more difficult to craft interventions to address those failures. This CISE Research Initiation Initiative (CRII) project develops tools to help use methods and insights from research in linguistic science to analyze and refine machine learning systems for sentence understanding. The project should have a practical impact in making it easier to develop effective language technologies, a scientific impact in helping linguists use machine learning as a proxy to study human language learning, and a training impact in supporting several PhD students---through both research seminars and direct research collaborations---as they develop into experts in the interaction between linguistic science and language technology.&lt;br/&gt;&lt;br/&gt;The methods used in this project relies on the human ability to judge the grammatical acceptability of a sentence; i.e., to decide whether someone could ever use a given sequence of words to say something. The project has three parts: (1) to build a large acceptability-based dataset for English which evaluates machine learning systems on their linguistic knowledge; (2) to use this data to evaluate widely-used standard approaches to machine learning for language, with a focus on promising recent approaches that use artificial neural networks learn from plain text; and (3) to develop methods for using small custom datasets to directly repair any gaps in the knowledge that these machine learning models acquire. Analyzing and improving artificial neural networks is difficult, since their internal representations of language are continuous and at least superficially, their internal representations of language do not at all resemble the kinds of representations that linguists use to analyze language. The investigators' methods are designed to minimize this difficulty, which rely on converging evidence from multiple ways of using the same data in its experiments.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/13/2019</MinAmdLetterDate>
<MaxAmdLetterDate>03/13/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1850208</AwardID>
<Investigator>
<FirstName>Samuel</FirstName>
<LastName>Bowman</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Samuel R Bowman</PI_FULL_NAME>
<EmailAddress>sb6065@nyu.edu</EmailAddress>
<PI_PHON>2129982121</PI_PHON>
<NSF_ID>000736653</NSF_ID>
<StartDate>03/13/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041968306</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEW YORK UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041968306</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[New York University]]></Name>
<CityName>New York</CityName>
<StateCode>NY</StateCode>
<ZipCode>100121019</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~174894</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="gmail-p1">Widely-deployed applications of language technology like translation systems or smart assistants rely heavily on machine learning models for sentence understanding. These models learn to understand language from data, which can often be as simple as a collection of published books or a download of Wikipedia, rather than through any kind of manual engineering or hands-on guidance by expert linguists. While modern machine learning methods are quite effective, they are not perfect. When they fail to understand some text, it can be difficult to discover why, and even more difficult to craft an intervention to address that failure. This project developed tools and datasets to help use methods and insights from research in linguistic science to analyze and refine machine learning systems for sentence understanding.<span class="gmail-">&nbsp;</span></p> <p class="gmail-p1">The methods used in this project focus primarily on the human ability to judge the grammatical acceptability of a sentence, or in other words, to decide whether someone could ever use a given sequence of words to say something. The primary outputs of the projects were data and scientific findings: The project yielded three public dataset releases (CoLA, BLiMP, and MSGS), all of which have been used in the broader language technology research community to facilitate the analysis and debugging of language tools. The scientific results were disseminated through nine papers published in major venues, and collectively point to two surprising conclusions: (i) While the latest language technologies do still struggle with some areas of grammar, this is no longer the primary bottleneck limiting their effectiveness on real language: The competence of current systems is stable, and no longer improves significantly with additional plain or annotated text data. Instead, higher-level conceptual knowledge and reasoning has become the area with the most room for short-term measurable progress. (ii) In addition, these technologies are able to learn the large majority of English grammar using quantities of language data not much greater than humans require, opening up new opportunities to use computational models as proxies to help address scientific questions about language learning in humans.</p> <p class="gmail-p1">Beyond the primary research outputs, this project supported the training of a PhD student at the intersection of formal linguistics and machine learning, a growing area with a talent shortage in both research and industry, and created mentored research and classroom opportunities for fifteen additional advanced students in this area.</p> <p class="p1">&nbsp;</p><br> <p>            Last Modified: 06/10/2021<br>      Modified by: Samuel&nbsp;R&nbsp;Bowman</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Widely-deployed applications of language technology like translation systems or smart assistants rely heavily on machine learning models for sentence understanding. These models learn to understand language from data, which can often be as simple as a collection of published books or a download of Wikipedia, rather than through any kind of manual engineering or hands-on guidance by expert linguists. While modern machine learning methods are quite effective, they are not perfect. When they fail to understand some text, it can be difficult to discover why, and even more difficult to craft an intervention to address that failure. This project developed tools and datasets to help use methods and insights from research in linguistic science to analyze and refine machine learning systems for sentence understanding.  The methods used in this project focus primarily on the human ability to judge the grammatical acceptability of a sentence, or in other words, to decide whether someone could ever use a given sequence of words to say something. The primary outputs of the projects were data and scientific findings: The project yielded three public dataset releases (CoLA, BLiMP, and MSGS), all of which have been used in the broader language technology research community to facilitate the analysis and debugging of language tools. The scientific results were disseminated through nine papers published in major venues, and collectively point to two surprising conclusions: (i) While the latest language technologies do still struggle with some areas of grammar, this is no longer the primary bottleneck limiting their effectiveness on real language: The competence of current systems is stable, and no longer improves significantly with additional plain or annotated text data. Instead, higher-level conceptual knowledge and reasoning has become the area with the most room for short-term measurable progress. (ii) In addition, these technologies are able to learn the large majority of English grammar using quantities of language data not much greater than humans require, opening up new opportunities to use computational models as proxies to help address scientific questions about language learning in humans. Beyond the primary research outputs, this project supported the training of a PhD student at the intersection of formal linguistics and machine learning, a growing area with a talent shortage in both research and industry, and created mentored research and classroom opportunities for fifteen additional advanced students in this area.         Last Modified: 06/10/2021       Submitted by: Samuel R Bowman]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
