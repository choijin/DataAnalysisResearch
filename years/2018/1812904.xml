<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Design of Gradient-Based Methods for Solving General and Huge Convex Optimization Problems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2018</AwardEffectiveDate>
<AwardExpirationDate>07/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>316798.00</AwardTotalIntnAmount>
<AwardAmount>316798</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Pedro Embid</SignBlockName>
<PO_EMAI>pembid@nsf.gov</PO_EMAI>
<PO_PHON>7032924859</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Optimization modeling, and algorithms for solving the models, have been key to gains in efficiency in the US economy since the late 1940's.  The relevance of optimization has increased manyfold alongside advances in computer design, and recently alongside the availability of vast and complex datasets arising from internet applications.  Optimization has become a central part of machine learning.  However, even the most efficient optimization algorithms are unable to succeed for complicated models populated by huge datasets possessing little special structure, the main problem being the limited core memory available on (even large) computers.  While Moore's Law accurately predicted exponentially-increasing computing power, the surprise has been that the sizes of datasets are increasing much faster.  A central focus of the project is the design of algorithms capable of solving a complicated optimization model populated with a huge dataset, by breaking apart the model into a sequence of computational problems, each relying on only a portion of the data, not too much for core memory.  Graduate students participate in the research.&lt;br/&gt;&lt;br/&gt;The general approach makes use of the most elemental of algorithms for convex optimization, namely, subgradient methods, dating to the 1960's.  In tandem, the approach makes use of -- and advances -- a framework promoted by the investigator in recent years, whereby a general convex optimization problem is transformed into an equivalent convex optimization problem whose only constraints are linear equations and for which the objective function is Lipschitz continuous (thereby allowing direct application of subgradient methods).  Exploration is being done initially for linear programming, an ambitious goal being to solve problems even beyond the reach of the simplex method (in cases where the basis-inverse matrix is larger than core memory permits).  Focus also is being given to problems involving an objective function that is itself the sum of many functions, a common setting in machine learning.  Here the goal is to devise algorithms that are able to choose the summand functions in a principled (and efficient) manner, unlike incremental (sub)gradient methods, which choose a summand uniformly at random.  Additionally, attempts are being made to extend the investigator's framework so as to provide, for example, a way to transform a continuously-differentiable objective function, possibly with bounded domain, into an entire function possessing Lipschitz-continuous gradient, thereby allowing accelerated methods to be applied easily.  A particularly important aspect of the project is the design of practical schemes for speeding up first order methods when the optimization problem being solved has some particular kind of geometrical structure (such as "sharpness," where the objective function grows linearly with the distance to optimality).  The goal is to design schemes that require no knowledge of parameters governing the geometrical structure, and yet that are guaranteed to achieve optimal speedup whenever the structure is present (regardless of whether the user knows the structure is present).  Graduate students participate in the research.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/14/2018</MinAmdLetterDate>
<MaxAmdLetterDate>08/14/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1812904</AwardID>
<Investigator>
<FirstName>James</FirstName>
<LastName>Renegar</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME>Jr.</PI_SUFX_NAME>
<PI_FULL_NAME>James M Renegar</PI_FULL_NAME>
<EmailAddress>renegar@orie.cornell.edu</EmailAddress>
<PI_PHON>6072559142</PI_PHON>
<NSF_ID>000362038</NSF_ID>
<StartDate>08/14/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY23</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>872612445</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CORNELL UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002254837</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Cornell University]]></Name>
<CityName>Ithaca</CityName>
<StateCode>NY</StateCode>
<ZipCode>148533801</ZipCode>
<StreetAddress><![CDATA[136 Hoy Rd]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>23</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY23</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1266</Code>
<Text>APPLIED MATHEMATICS</Text>
</ProgramElement>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~316798</FUND_OBLG>
</Award>
</rootTag>
