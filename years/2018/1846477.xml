<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER:  Breakthrough Display Technology as a New Medium for Spatial Thinking in STEM</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2019</AwardEffectiveDate>
<AwardExpirationDate>03/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>312847</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to increase both interest and competence in STEM by developing a new type of 3D display as a tool for teaching spatial reasoning.   Strong spatial reasoning skills have been important to many of humanity's greatest advances in science and engineering.  To help teach these skills, this research will develop a display platform that is entirely different from all preceding 3D display technologies.  The display uses light to trap multiple particles in air, then moves and illuminates these trapped particles to draw 3D images.  We perceive these "spatial images" as physical objects because they are, in fact, physical objects in space. These full-color, high-definition images can be seen from every direction.  The technology makes possible, for the first time, many of the 3D images portrayed in science fiction.  In addition to the broader impacts for STEM education built directly into the primary research objective, a team of undergraduates under the supervision of the principal investigator (PI) will also design a 3D-centric Saturday academy called 'Spatial Forces' for middle and high school students from groups underrepresented in engineering, with a focus on low-income and rural students. These students will create interactive, revolving 3D content for a micro-museum aimed at exposing their communities to STEM topics of local relevance (e.g., preserving Native American artifacts, water conservation, the biology of native game fish, etc.). The programming will include learning from, and creating content for, demonstration installations to be incorporated into micro-museums. As part of the academy experience, students will be evaluated for both affect and spatial ability, and the results of the program will be assessed by an Education Advisory Board and disseminated to both local and out of state partners. Opportunities for program perpetuation and expansion will be provided through a collaboration with the UTAH Stem Action Center.&lt;br/&gt;&lt;br/&gt;The long-term goal of this research is to create glasses-free, interactive, 3D environments as tools to expand human capacity and creativity. The PI's recent research has led to a novel, non-holographic method of screenless 3D display with the potential to change how we interact with our data by making it physically present. To this end, the objective of the current project is to create and evaluate parallel optical trap displays (OTDs) as tools for spatial thinking. Free-floating 3D displays have been the "holy grail" of 3D imaging for over a century. Such a display would be potentially transformative for many information visualization applications; however, the application space with perhaps the greatest potential scientific impact would be that of spatial thinking, a skill that has been foundational to many of the greatest scientific achievements in history and which is also strongly associated with both interest and achievement in STEM. The PI's prototype OTD is a natively spatial 3D technology ideally suited to spatial thinking that is currently capable of drawing full-color, video-rate images of small objects with a single trapped particle. Achieving larger image volumes will require a new "parallel" OTD approach that creates a large volume OTD from multiple trapped particles illuminated independently and simultaneously. The central hypothesis of this work is that parallel OTDs will lead to better mastery of spatial concepts (as measured by standard spatial thinking tests) by making data physical and interactive. To test this hypothesis, the PI will optimize single particle traps, develop at least two parallel display methods (e.g. point trap array and line trap array), and compare parallel OTDs against screen-based tools for spatial reasoning.  Parallel OTDs will be capable of providing all of the 3D visual cues of holography (accommodation, parallax, and potentially even occlusion) without being subject to its limitations (aperture constraints and prohibitive computational complexity).&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/21/2019</MinAmdLetterDate>
<MaxAmdLetterDate>03/13/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1846477</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Smalley</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel Smalley</PI_FULL_NAME>
<EmailAddress>smalley@byu.edu</EmailAddress>
<PI_PHON>8014226177</PI_PHON>
<NSF_ID>000692212</NSF_ID>
<StartDate>03/21/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Brigham Young University</Name>
<CityName>Provo</CityName>
<CountyName>UTAH</CountyName>
<ZipCode>846021231</ZipCode>
<PhoneNumber>8014223360</PhoneNumber>
<StreetAddress>A-285 ASB</StreetAddress>
<StreetAddress2><![CDATA[Campus Drive]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009094012</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BRIGHAM YOUNG UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001940170</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Brigham Young University]]></Name>
<CityName>Provo</CityName>
<CountyName>UTAH</CountyName>
<StateCode>UT</StateCode>
<ZipCode>846021231</ZipCode>
<StreetAddress><![CDATA[A-285 ASB]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~87123</FUND_OBLG>
<FUND_OBLG>2020~115614</FUND_OBLG>
<FUND_OBLG>2021~110110</FUND_OBLG>
</Award>
</rootTag>
