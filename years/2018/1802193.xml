<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research: Doing Evaluation: Knowledge Production in International Development</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2018</AwardEffectiveDate>
<AwardExpirationDate>12/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>11999.00</AwardTotalIntnAmount>
<AwardAmount>11999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Melanie Hughes</SignBlockName>
<PO_EMAI>mehughes@nsf.gov</PO_EMAI>
<PO_PHON>7032927318</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Project evaluation is a profession on the rise within and beyond the field of international development. The field has added more members in the last three years than in the previous thirty, reflecting wider shifts in how organizational decisions are made and justified. Charged with producing impartial accounts of project impact and worth, evaluators must reconcile different ideals of international development, and of evidence and proof.  And yet evaluation as a profession has been little studied to date. This project will study of how evaluators shape development policy and practice.  Results from the project will be useful for government and private funding agencies that require evaluations in their grants, the implementing organizations that commission evaluations of their work, and evaluators themselves as they further define their professional standards and practices. &lt;br/&gt;&lt;br/&gt;The project employs qualitative methods to shed light on the processes that shape evaluators' practice on the ground, and the wider consequences of their work. The project will conduct meeting ethnography and in-depth qualitative interviews with 70 working evaluators at and around the annual meetings of diverse professional evaluation associations, ranging from the regional to the international. This fieldwork will reveal how members of this relatively solitary profession create and contest the norms governing their growing field.  The resulting analysis of evaluators as an emerging professional community gives vital institutional context to the larger study of how evaluators engage with other stakeholders as they carry out project evaluations. This multi-level analysis of professional evaluators will inform policy and practice in the development field, and for evaluation more broadly. The study contributes to a sociology of science that traces how social change happens through everyday processes of measurement, persuasion, and professionalization.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/07/2018</MinAmdLetterDate>
<MaxAmdLetterDate>06/07/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1802193</AwardID>
<Investigator>
<FirstName>Elisa</FirstName>
<LastName>Martinez</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Elisa Martinez</PI_FULL_NAME>
<EmailAddress>elisa@soc.umass.edu</EmailAddress>
<PI_PHON>4135450577</PI_PHON>
<NSF_ID>000757974</NSF_ID>
<StartDate>06/07/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Millicent</FirstName>
<LastName>Thayer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Millicent Thayer</PI_FULL_NAME>
<EmailAddress>thayer@soc.umass.edu</EmailAddress>
<PI_PHON>4135450577</PI_PHON>
<NSF_ID>000757972</NSF_ID>
<StartDate>06/07/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName/>
<StateCode>MA</StateCode>
<ZipCode>010359450</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1331</Code>
<Text>Sociology</Text>
</ProgramElement>
<ProgramReference>
<Code>1331</Code>
<Text>SOCIOLOGY</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~11999</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1"><span class="s1">Over the last half-century, the discipline of evaluation&mdash;the systematic assessment of a program's merit&mdash;has become an increasingly formalized part of social governance. One leading figure in the field of evaluation told me evaluators are "the most important people you'll never know." While targeted intervention to reshape people's lives and communities is a central axis of the modernizing state, the notion that such efforts should be assessed and governed according to disinterested norms of science is a new and noteworthy development. As part of my dissertation study on the knowledge politics of international relief and development, this project explores evaluation from the perspective of the professionals charged with its production. Through participant observation of evaluation conferences, training programs, and seminars, and through document analysis of online forums and association publications, the study traces how diversely positioned evaluators encounter, represent, reproduce and also contest the mechanisms of their discipline. In the dynamics that arise as evaluators convene, the study reveals a knowledge-brokering profession increasingly focused on "speaking truth to power" through thoughtful inquiry, but struggling in doing so with discourses and relations of power that underpin its own legitimacy.&nbsp;</span></p> <p class="p1"><span class="s1">Framed in the idealized language of scientific method in pursuit of objective truth, professional evaluators are hired by stakeholders as nominally neutral observers who apply specific technical and bureaucratic procedures to deliver rigorous, contextualized, and generalizable knowledge about "what works." In packed agendas focused on scientific inquiry, evaluator meetings&nbsp;&nbsp;display methodological innovations, findings of topical interest, and leadership initiatives in the governance of the profession.&nbsp;&nbsp;In theory,&nbsp;this exchange reflects and reinforces the power that guides the future actions of evaluation stakeholders.</span></p> <p class="p1"><span class="s1">In practice, however, for every session that projects the field's confidence in its authority, another reveals its tensions and contradictions. In one room, a Maori evaluator challenges the validity of big data analytics;&nbsp;&nbsp;in another, a young evaluator confesses to compromising on the profession's standards because of her project's stingy budget and demanding client. A session outlining new competencies aimed at professionalizing the field meets calls to decenter markers of expertise in ways that broaden the methods and messengers of authorized truth. Given the internal heterogeneity of a profession where authority is structured by race, ethnicity, gender, and geopolitics, and where persuasion must reflect the interests and epistemologies of those being persuaded, individual evaluators must carefully calibrate their discourses and practices to remain employable while they build their profession's prestige.&nbsp;</span></p> <p class="p1"><span class="s1">What the study reveals is a profession in a rich and productive state of ferment&mdash;engaging more deeply with the politics of knowledge and representation that sustain social inequalities, while also constrained by its reliance on enduring institutions and power structures. When the authority that gets them hired relies on truth-telling that can get them fired, and when unmaking knowledge hierarchies shakes the profession's foundations, evaluators use a range of strategies to get by. Their overt, covert and emergent pathways of change reflect their financial and social embeddedness as much as intellectual or political conviction. An African evaluator describes the expert signaling, tactical silences and emotional labor she must use to retain her multilateral client's trust, while also engaging with a collective of feminist evaluators in advocating for decolonizing the practice.&nbsp;&nbsp;In the "big tent" meetings of a still-young profession, tensions like these&mdash;methodological, organizational, and political&mdash;challenge the discourse of an inevitably standardizing "evaluation machine." A field that seems on the surface to be defined by formal standards, metrics and contracts, finds evaluators "making do" in ways that can yield unstable norms about what knowledge and whose knowledge matters. As such, the study suggests how individual and collective strategies for navigating the profession's challenges can both reinforce and subvert the larger institutional forces that shape knowledge, power and social change.</span></p> <p class="p1"><span class="s1">Evaluators themselves have been active commentators on the evolution of their profession, yielding a rich body of research on evaluation. This project contributes this literature, and to sociologies of science and the professions, with ethnographically-grounded insights on how actors with limited formal power can produce friction and change in hegemonic discourses of science and development. By sharing lessons from the research and fostering dialogue about them among evaluators and their wider field of stakeholders, this project supports a fuller self-understanding and more strategic praxis for program evaluation.</span></p><br> <p>            Last Modified: 04/13/2021<br>      Modified by: Elisa&nbsp;Martinez</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Over the last half-century, the discipline of evaluation&mdash;the systematic assessment of a program's merit&mdash;has become an increasingly formalized part of social governance. One leading figure in the field of evaluation told me evaluators are "the most important people you'll never know." While targeted intervention to reshape people's lives and communities is a central axis of the modernizing state, the notion that such efforts should be assessed and governed according to disinterested norms of science is a new and noteworthy development. As part of my dissertation study on the knowledge politics of international relief and development, this project explores evaluation from the perspective of the professionals charged with its production. Through participant observation of evaluation conferences, training programs, and seminars, and through document analysis of online forums and association publications, the study traces how diversely positioned evaluators encounter, represent, reproduce and also contest the mechanisms of their discipline. In the dynamics that arise as evaluators convene, the study reveals a knowledge-brokering profession increasingly focused on "speaking truth to power" through thoughtful inquiry, but struggling in doing so with discourses and relations of power that underpin its own legitimacy.  Framed in the idealized language of scientific method in pursuit of objective truth, professional evaluators are hired by stakeholders as nominally neutral observers who apply specific technical and bureaucratic procedures to deliver rigorous, contextualized, and generalizable knowledge about "what works." In packed agendas focused on scientific inquiry, evaluator meetings  display methodological innovations, findings of topical interest, and leadership initiatives in the governance of the profession.  In theory, this exchange reflects and reinforces the power that guides the future actions of evaluation stakeholders. In practice, however, for every session that projects the field's confidence in its authority, another reveals its tensions and contradictions. In one room, a Maori evaluator challenges the validity of big data analytics;  in another, a young evaluator confesses to compromising on the profession's standards because of her project's stingy budget and demanding client. A session outlining new competencies aimed at professionalizing the field meets calls to decenter markers of expertise in ways that broaden the methods and messengers of authorized truth. Given the internal heterogeneity of a profession where authority is structured by race, ethnicity, gender, and geopolitics, and where persuasion must reflect the interests and epistemologies of those being persuaded, individual evaluators must carefully calibrate their discourses and practices to remain employable while they build their profession's prestige.  What the study reveals is a profession in a rich and productive state of ferment&mdash;engaging more deeply with the politics of knowledge and representation that sustain social inequalities, while also constrained by its reliance on enduring institutions and power structures. When the authority that gets them hired relies on truth-telling that can get them fired, and when unmaking knowledge hierarchies shakes the profession's foundations, evaluators use a range of strategies to get by. Their overt, covert and emergent pathways of change reflect their financial and social embeddedness as much as intellectual or political conviction. An African evaluator describes the expert signaling, tactical silences and emotional labor she must use to retain her multilateral client's trust, while also engaging with a collective of feminist evaluators in advocating for decolonizing the practice.  In the "big tent" meetings of a still-young profession, tensions like these&mdash;methodological, organizational, and political&mdash;challenge the discourse of an inevitably standardizing "evaluation machine." A field that seems on the surface to be defined by formal standards, metrics and contracts, finds evaluators "making do" in ways that can yield unstable norms about what knowledge and whose knowledge matters. As such, the study suggests how individual and collective strategies for navigating the profession's challenges can both reinforce and subvert the larger institutional forces that shape knowledge, power and social change. Evaluators themselves have been active commentators on the evolution of their profession, yielding a rich body of research on evaluation. This project contributes this literature, and to sociologies of science and the professions, with ethnographically-grounded insights on how actors with limited formal power can produce friction and change in hegemonic discourses of science and development. By sharing lessons from the research and fostering dialogue about them among evaluators and their wider field of stakeholders, this project supports a fuller self-understanding and more strategic praxis for program evaluation.       Last Modified: 04/13/2021       Submitted by: Elisa Martinez]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
