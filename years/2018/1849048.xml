<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: AF: Collaborative Research: Weak Derandomizations in Time and Space Complexity</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2018</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tracy Kimbrel</SignBlockName>
<PO_EMAI>tkimbrel@nsf.gov</PO_EMAI>
<PO_PHON>7032927924</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Computational complexity theory classifies natural computational problems into various complexity classes based on the amount of resources needed to solve them. Typical resources are time, memory, amount of randomness, and circuit size. This project aims to advance the state of the art in understanding the power and limitations of randomness and circuit size, using two new, previously untested, techniques. Intuition gained from this project will enhance our understanding of practical computational problems arising from fields beyond computer science. The project's exploration has the potential to solve central, longstanding, open questions in complexity theory. &lt;br/&gt;&lt;br/&gt;The first part of the project will investigate unconditional de-randomization of probabilistic time via de-randomization of multi-pass, probabilistic space-bounded computations. In particular, this project will explore a new approach to obtain an asymptotically better deterministic simulation of probabilistic linear time than what is currently known.  The second part of this project aims to prove new fixed-polynomial size circuit lower bounds via designing pseudo-deterministic approximation algorithms for certain counting problems.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/11/2018</MinAmdLetterDate>
<MaxAmdLetterDate>09/11/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1849048</AwardID>
<Investigator>
<FirstName>Vinodchandran</FirstName>
<LastName>Variyam</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vinodchandran Variyam</PI_FULL_NAME>
<EmailAddress>vinod@cse.unl.edu</EmailAddress>
<PI_PHON>4024725002</PI_PHON>
<NSF_ID>000190195</NSF_ID>
<StartDate>09/11/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Nebraska-Lincoln</Name>
<CityName>Lincoln</CityName>
<ZipCode>685031435</ZipCode>
<PhoneNumber>4024723171</PhoneNumber>
<StreetAddress>151 Prem S. Paul Research Center</StreetAddress>
<StreetAddress2><![CDATA[2200 Vine St]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<StateCode>NE</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NE01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555456995</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOARD OF REGENTS OF THE UNIVERSITY OF NEBRASKA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068662618</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Nebraska-Lincoln]]></Name>
<CityName/>
<StateCode>NE</StateCode>
<ZipCode>685031435</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Nebraska</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NE01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>7927</Code>
<Text>COMPLEXITY &amp; CRYPTOGRAPHY</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~50000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="p1"><span class="s1">Intellectual Merit: Randomized algorithms, algorithms that use random&nbsp;</span>bits to help with computations, are an indispensable part of the current computational world. Their usefulness stems from the fact that they often are simpler and more efficient compared to their deterministic counterparts and still have provable guarantees on their outputs. However, a major drawback of randomized algorithms is their lack of reproducibility: two different runs of a randomized algorithm may return two different, though correct, values. This is an undesirable feature especially in areas such as distributed computing, trusted computing, and software engineering where multiple invocations of the same algorithm should return the same answer. Randomized algorithms that guarantee deterministic outputs are called pseudodeterministic algorithms: they output a canonical value with very high probability. The project explored pseudodeterministic algorithms and made new significant discoveries. One of the main&nbsp;<span class="s1">contributions of the project is the identification of&nbsp; "</span><span class="s2">complete </span><span class="s2">problems''</span><span class="s1"> in the context of pseudodeterminism. In particular, the&nbsp;</span>project identified three approximation problems: Collision Probability Estimation Problem, Entropy Estimation Problem, and Acceptance Probability Estimation Problem and proved that these problems are complete in the context of pseudodeterminism: if any of these problems admit pseudodeterministic algorithms then all randomized approximation algorithms admit pseudodeterministic algorithms. While completeness is a central notion in complexity theory, this is the first time it has been effectlively employed to explain the nature of pseudodeterministic computations.&nbsp;</p> <p class="p1"><span class="s1">Broader Impacts: Pseudodeterminism is a formalization of reproducible&nbsp;</span>randomized computations which is a demanded feature of enterprise software systems. The grant led to the discovery of certain fundamental properties of such computations which have the potential to significantly impact other areas such as trustworthy cloud computing, software engineering, and distributed computing. Discoveries from this grant were published in peer-reviewed theoretical computer science conferences resulting in broad dissemination. The grant also partly supported Ph.D. students Sutanu Gayen (graduated in Summer 19) and Jason Vander Woude (currently a Ph.D. student).</p> <p class="p1">&nbsp;</p> <p class="p1">&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 04/15/2021<br>      Modified by: Vinodchandran&nbsp;Variyam</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Intellectual Merit: Randomized algorithms, algorithms that use random bits to help with computations, are an indispensable part of the current computational world. Their usefulness stems from the fact that they often are simpler and more efficient compared to their deterministic counterparts and still have provable guarantees on their outputs. However, a major drawback of randomized algorithms is their lack of reproducibility: two different runs of a randomized algorithm may return two different, though correct, values. This is an undesirable feature especially in areas such as distributed computing, trusted computing, and software engineering where multiple invocations of the same algorithm should return the same answer. Randomized algorithms that guarantee deterministic outputs are called pseudodeterministic algorithms: they output a canonical value with very high probability. The project explored pseudodeterministic algorithms and made new significant discoveries. One of the main contributions of the project is the identification of  "complete problems'' in the context of pseudodeterminism. In particular, the project identified three approximation problems: Collision Probability Estimation Problem, Entropy Estimation Problem, and Acceptance Probability Estimation Problem and proved that these problems are complete in the context of pseudodeterminism: if any of these problems admit pseudodeterministic algorithms then all randomized approximation algorithms admit pseudodeterministic algorithms. While completeness is a central notion in complexity theory, this is the first time it has been effectlively employed to explain the nature of pseudodeterministic computations.  Broader Impacts: Pseudodeterminism is a formalization of reproducible randomized computations which is a demanded feature of enterprise software systems. The grant led to the discovery of certain fundamental properties of such computations which have the potential to significantly impact other areas such as trustworthy cloud computing, software engineering, and distributed computing. Discoveries from this grant were published in peer-reviewed theoretical computer science conferences resulting in broad dissemination. The grant also partly supported Ph.D. students Sutanu Gayen (graduated in Summer 19) and Jason Vander Woude (currently a Ph.D. student).              Last Modified: 04/15/2021       Submitted by: Vinodchandran Variyam]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
