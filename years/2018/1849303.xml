<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>S&amp;AS: FND: COLLAB: Planning Coordinated Event Observation for Structured Narratives</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2019</AwardEffectiveDate>
<AwardExpirationDate>02/28/2022</AwardExpirationDate>
<AwardTotalIntnAmount>200000.00</AwardTotalIntnAmount>
<AwardAmount>200000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>People easily recognize the dramatic moments that unfold in human events.  Dramatic turns of events are key to recognizing and communicating effective reports or stories about events.  Autonomous systems will work more effectively with humans in obtaining and conveying such narrative when they too can recognize what is dramatic (or tragic, or comical) about human events.  The challenge is to effectively convey such concepts to a computer in such a way that humans and autonomous systems can effectively work together in this. This research studies how to direct a team of robots to obtain video footage to produce clips that trace a dramatic story arc. It is an examination of how such systems might achieve goals that people consider to be abstract or high-level. Within this project, the programs that command teams of robots must predict likely events, direct the robots to be in position for obtaining the desired footage, and re-plan based on observed events. This challenge encompasses a rich and previously unstudied class of problems for robot systems.  It will constitute a unique demonstration of robots that are capable of achieving high-level goals as they process data in forms which combine both continuous and discrete views of the world in a new and unusual way.  More broadly, the research will advance how computers can fuse and summarize video streams. Both skills are needed for automatically generating synopses and in editing videos. Obvious places where this is useful include helping secure the nation (for surveillance), taming the deluge of online multimedia content (for summarization), and advancing applications in the creative industries (for editing). The research project will also use the ideas underlying these pieces in a new robotics course with students at three institutions going head-to-head in a series of competition-based class projects. This course (taught, among other places, at a Hispanic-Serving Institution) will contribute to the development of the STEM workforce of the future, helping increase American competitiveness.&lt;br/&gt;&lt;br/&gt;The project advances current knowledge by formulating new theory and developing novel algorithms for autonomous and robot systems, with a focus on those systems with minimal or no human operator intervention. The research contributes novel data representations for robots that will inhabit rich environments such as those characterized by uncertain, unanticipated, and dynamically changing circumstances. One of the foundational ideas of the project is a means to specify sophisticated mission objectives via a recursive structure using prior work in compiler theory for computer languages. The project involves a strong connection between this theoretical work and demonstrated systems.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/14/2019</MinAmdLetterDate>
<MaxAmdLetterDate>03/14/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1849303</AwardID>
<Investigator>
<FirstName>Aaron</FirstName>
<LastName>Becker</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Aaron T Becker</PI_FULL_NAME>
<EmailAddress>atbecker@uh.edu</EmailAddress>
<PI_PHON>7137439240</PI_PHON>
<NSF_ID>000668782</NSF_ID>
<StartDate>03/14/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Houston</Name>
<CityName>Houston</CityName>
<CountyName/>
<ZipCode>772042015</ZipCode>
<PhoneNumber>7137435773</PhoneNumber>
<StreetAddress>4800 Calhoun Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>036837920</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF HOUSTON SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042916627</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Houston]]></Name>
<CityName>Houston</CityName>
<CountyName/>
<StateCode>TX</StateCode>
<ZipCode>772042015</ZipCode>
<StreetAddress><![CDATA[4800 Calhoun Boulevard]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>039Y</Code>
<Text>S&amp;AS - Smart &amp; Autonomous Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>046Z</Code>
<Text>S&amp;AS - Smart &amp; Autonomous Systems</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~200000</FUND_OBLG>
</Award>
</rootTag>
