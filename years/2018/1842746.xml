<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Creating the Critical University-to-Workforce Connection: Providing Quantitative Evidence of Collaboration Skills</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2019</AwardEffectiveDate>
<AwardExpirationDate>01/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>224690.00</AwardTotalIntnAmount>
<AwardAmount>224690</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Atherton</SignBlockName>
<PO_EMAI>patherto@nsf.gov</PO_EMAI>
<PO_PHON>7032928772</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This SBIR Phase I project will develop new methods for training and assessing the skill of providing and receiving constructive feedback in collaborative environments. The United States higher education system is failing to train collaboration skills, despite companies listing it as one of the most important skills for new hires to have. Past research has identified specific types of feedback that most effectively encourage peers to improve, but today's higher education instructors have no ready means to effectively assess students' abilities to use these peer feedback strategies. This project will develop simple-to-use software tools to help instructors train peer feedback skills and new methods for automatically assessing student ability to provide and receive constructive feedback. By democratizing the training of the collaborative skills most sought after by today's employers, this project aims to reduce underemployment of college graduates and increase the economic output and tax revenues of the United States by building a better trained and more productive workforce.&lt;br/&gt;&lt;br/&gt;This project will develop an innovative peer-assessment tool coupled with novel natural language processing methods producing accurate measurements of student ability to give and receive constructive feedback based on written feedback. The natural language processing methods will combine word vectors for efficient language processing, a generalizable neural net structure for sentence categorization outlined in existing research literature, and manually defined features based on existing peer feedback research. Research efforts will focus on gathering a large training dataset from partnering higher education classrooms, and iteratively refining a deep-learning natural language processing algorithm to achieve a target 90% accuracy. Automating this assessment process will allow broad and continuous assessment of student collaborative ability without requiring expensive and time-consuming retraining of the United States instructor workforce.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>01/29/2019</MinAmdLetterDate>
<MaxAmdLetterDate>01/29/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1842746</AwardID>
<Investigator>
<FirstName>Turner</FirstName>
<LastName>Bohlen</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Turner K Bohlen</PI_FULL_NAME>
<EmailAddress>turner@beaglelearning.com</EmailAddress>
<PI_PHON>5082022866</PI_PHON>
<NSF_ID>000781129</NSF_ID>
<StartDate>01/29/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>BEAGLE LEARNING LLC</Name>
<CityName>Concord</CityName>
<ZipCode>033016906</ZipCode>
<PhoneNumber>5082022866</PhoneNumber>
<StreetAddress>123 Mountain Rd</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Hampshire</StateName>
<StateCode>NH</StateCode>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NH08</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>080490875</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BEAGLE LEARNING, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[BEAGLE LEARNING LLC]]></Name>
<CityName>Paradise Valley</CityName>
<StateCode>AZ</StateCode>
<ZipCode>852531014</ZipCode>
<StreetAddress><![CDATA[5033 E. Turquoise Ave.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AZ06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8031</Code>
<Text>Education Products</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~224690</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 2"> <div class="section"> <div class="layoutArea"> <div class="column"> <p>During the NSF SBIR Phase 1, our main objective was to develop new natural language processing (NLP) systems to quantitatively score learners? ability to work collaboratively with peers during an online peer-assessment exercise. We were able to demonstrate the viability of this type of automated scoring system by classifying feedback data collected from a series of higher education courses.</p> <p><span>To achieve this objective, we designed, implemented, and launched an online interface for running a peer feedback process in 12 classes at Arizona State University and 1 at Columbia University during Fall 2019. We developed a rubric for assessing peer?s work that was used to set scoring guidelines for an automated scoring system of collaboration. We collected peer feedback data using this rubric from 275 students and built initial natural language processing (NLP) models for the automatic scoring system. </span></p> <p><span>In addition, we also completed 37 interviews from stakeholders in the higher education market to better understand the needs and pain points of instructors and directors around peer feedback and collaboration. We learned that peer feedback is often too onerous for most instructors to implement, that students often do not see the value in providing feedback to their peers, and that measuring collaboration is never a top need for directors of programs. </span></p> <p><span>Our activities produced the following key outcomes: </span></p> <ul> <li> <p><span>Demonstrated the viability of developing an automated scoring system for collaboration&nbsp;</span>by successfully classifying feedback data collected from a series of higher education&nbsp;courses.</p> </li> <li> <p><span>Evaluated the value and consistency of ratings produced by our peer feedback method&nbsp;</span>and rubric and identified areas of improvement for a next iteration.</p> </li> <li> <p><span>Launched a test interface for gathering peer feedback data.</span></p> </li> <li> <p>Identified primary challenges with the use of peer feedback and peer feedback software&nbsp;in higher education which will inform future work.</p> </li> </ul> Beyond these key outcomes, our work has an impact on the field of education. We have contributed new information and strategies for the automatic rating of peer feedback, applying natural language processing strategies used elsewhere in a proof-of-concept test. This research will allow us and others to build far more effective online interfaces for managing these processes. Additionally, a greater understanding of how to train collaboration skills through peer feedback applications in the classroom helps us better design tools that can work in other fields as well, such as corporate, military, and the public sector. <ul> </ul> </div> </div> </div> </div> <div class="page" title="Page 3"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>We were also able to work with at least 14 instructors of higher education classrooms to facilitate peer feedback exercises in their courses - something many of them were not previously familiar with doing. Sharing about methods and techniques for collaborative learning, and co-building new methods, has been an important piece of our work. </span></p> <p><span>Peer feedback and the ability to work collaboratively are important skills for the workforce and are often untrained in higher education classrooms. Easing the burden of this training, and creating automated measures of collaboration could significantly increase students success after graduation and US workforce productivity. Based on what we have learned, we have clear, data-backed next steps on rubric design, interface design, and natural language processing development. </span></p> <p><span>Information about our developed learning methods are written about on Beagle?s Blog: </span><span>https://blog.beaglelearning.com/</span><span>.</span><span>&#8203;</span></p> </div> </div> </div> </div><br> <p>            Last Modified: 03/27/2020<br>      Modified by: Turner&nbsp;K&nbsp;Bohlen</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[     During the NSF SBIR Phase 1, our main objective was to develop new natural language processing (NLP) systems to quantitatively score learners? ability to work collaboratively with peers during an online peer-assessment exercise. We were able to demonstrate the viability of this type of automated scoring system by classifying feedback data collected from a series of higher education courses.  To achieve this objective, we designed, implemented, and launched an online interface for running a peer feedback process in 12 classes at Arizona State University and 1 at Columbia University during Fall 2019. We developed a rubric for assessing peer?s work that was used to set scoring guidelines for an automated scoring system of collaboration. We collected peer feedback data using this rubric from 275 students and built initial natural language processing (NLP) models for the automatic scoring system.   In addition, we also completed 37 interviews from stakeholders in the higher education market to better understand the needs and pain points of instructors and directors around peer feedback and collaboration. We learned that peer feedback is often too onerous for most instructors to implement, that students often do not see the value in providing feedback to their peers, and that measuring collaboration is never a top need for directors of programs.   Our activities produced the following key outcomes:     Demonstrated the viability of developing an automated scoring system for collaboration by successfully classifying feedback data collected from a series of higher education courses.    Evaluated the value and consistency of ratings produced by our peer feedback method and rubric and identified areas of improvement for a next iteration.    Launched a test interface for gathering peer feedback data.    Identified primary challenges with the use of peer feedback and peer feedback software in higher education which will inform future work.   Beyond these key outcomes, our work has an impact on the field of education. We have contributed new information and strategies for the automatic rating of peer feedback, applying natural language processing strategies used elsewhere in a proof-of-concept test. This research will allow us and others to build far more effective online interfaces for managing these processes. Additionally, a greater understanding of how to train collaboration skills through peer feedback applications in the classroom helps us better design tools that can work in other fields as well, such as corporate, military, and the public sector.            We were also able to work with at least 14 instructors of higher education classrooms to facilitate peer feedback exercises in their courses - something many of them were not previously familiar with doing. Sharing about methods and techniques for collaborative learning, and co-building new methods, has been an important piece of our work.   Peer feedback and the ability to work collaboratively are important skills for the workforce and are often untrained in higher education classrooms. Easing the burden of this training, and creating automated measures of collaboration could significantly increase students success after graduation and US workforce productivity. Based on what we have learned, we have clear, data-backed next steps on rubric design, interface design, and natural language processing development.   Information about our developed learning methods are written about on Beagle?s Blog: https://blog.beaglelearning.com/.&#8203;           Last Modified: 03/27/2020       Submitted by: Turner K Bohlen]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
