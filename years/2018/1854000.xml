<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>TWC: Medium: Collaborative: Efficient Repair of Learning Systems via Machine Unlearning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/05/2018</AwardEffectiveDate>
<AwardExpirationDate>08/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>449005.00</AwardTotalIntnAmount>
<AwardAmount>465004</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wei-Shinn Ku</SignBlockName>
<PO_EMAI>weiku@nsf.gov</PO_EMAI>
<PO_PHON>7032928318</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Today individuals and organizations leverage machine learning systems to adjust room temperature, provide recommendations, detect malware, predict earthquakes, forecast weather, maneuver vehicles, and turn Big Data into insights. Unfortunately, these systems are prone to a variety of malicious attacks with potentially disastrous consequences. For example, an attacker might trick an Intrusion Detection System into ignoring the warning signs of a future attack by injecting carefully crafted samples into the training set for the machine learning model (i.e., "polluting" the model). This project is creating an approach to machine unlearning and the necessary algorithms, techniques, and systems to efficiently and effectively repair a learning system after it has been compromised. Machine unlearning provides a last resort against various attacks on learning systems, and is complementary to other existing defenses.  &lt;br/&gt;&lt;br/&gt;The key insight in machine unlearning is that most learning systems can be converted into a form that can be updated incrementally without costly retraining from scratch. For instance, several common learning techniques (e.g., naive Bayesian classifier) can be converted to the non-adaptive statistical query learning form, which depends only on a constant number of summations, each of which is a sum of some efficiently computable transformation of the training data samples. To repair a compromised learning system in this form, operators add or remove the affected training sample and re-compute the trained model by updating a constant number of summations. This approach yields huge speedup -- the asymptotic speedup over retraining is equal to the size of the training set. With unlearning, operators can efficiently correct a polluted learning system by removing the injected sample from the training set, strengthen an evaded learning system by adding evasive samples to the training set, and prevent system inference attacks by forgetting samples stolen by the attacker so that no future attacks can infer anything about the samples.</AbstractNarration>
<MinAmdLetterDate>09/19/2018</MinAmdLetterDate>
<MaxAmdLetterDate>09/19/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1854000</AwardID>
<Investigator>
<FirstName>Yinzhi</FirstName>
<LastName>Cao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yinzhi Cao</PI_FULL_NAME>
<EmailAddress>ycao43@jhu.edu</EmailAddress>
<PI_PHON>4105166718</PI_PHON>
<NSF_ID>000689464</NSF_ID>
<StartDate>09/19/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<StreetAddress2><![CDATA[Suite B001]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001910777</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>JOHNS HOPKINS UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001910777</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Johns Hopkins University]]></Name>
<CityName>Baltimore</CityName>
<StateCode>MD</StateCode>
<ZipCode>212182686</ZipCode>
<StreetAddress><![CDATA[1101 E 33rd St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>025Z</Code>
<Text>SaTC: Secure and Trustworthy Cyberspace</Text>
</ProgramReference>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~449004</FUND_OBLG>
<FUND_OBLG>2017~16000</FUND_OBLG>
</Award>
</rootTag>
