<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Mechanisms supporting feature-based attention</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2019</AwardEffectiveDate>
<AwardExpirationDate>10/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>575310.00</AwardTotalIntnAmount>
<AwardAmount>575310</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kurt Thoroughman</SignBlockName>
<PO_EMAI>kthoroug@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Whether attending to the road while driving or trying to find your keys on a cluttered desk, our ability to select relevant information from other sensory inputs is critical for everyday survival. Many studies have shown that the ability to attend to items is highly limited; yet, it is unclear where these limits stem from and what the underlying neural mechanisms are that determine these limits. In this proposal we aim to test what constrains our ability to select information and to examine the flexibility of the human attention system across different selection demands and sensory environments. We use the human visual system as a model system to test how attention interacts with perceptual representations to guide behavior, and to examine how attention modulates neural activity to enhance perception and memory. Understanding how attention modulates these neural representations in visual cortex will help to uncover how variations in these neural patterns give rise to common perceptual and cognitive disorders like ADHD and schizophrenia.   &lt;br/&gt;&lt;br/&gt;The current research project aims to broadly understand how attention operates in different sensory environments and task contexts. We hypothesize that the capacity limits of feature-based attention are constrained by a combination of the organization of feature representations in the visual system (i.e., perceptual confusability) and by center-surround selection mechanisms. Furthermore, we hypothesize that the selection mechanisms are flexible and adjust to current task demands in terms of magnitude (enhancement and suppression) and shape (e.g., shifts in tuning curves; size of the inhibitory surround). Our approach includes psychophysics, computational modeling, and electrophysiological recordings (frequency-tagged potentials in EEG) that can tease apart perceptual factors from attentional effects, and allow to directly measure feature representations in visual cortex. Together, the proposed experiments will provide important insights into how perceptual organization and attention mechanisms interact to influence limits of attention, and will examine the flexibility of the human attention system across different task contexts. The development of novel experimental designs and neural measures to track perceptual representations in visual cortex and how they are modulated by attention will be important to test perceptual and cognitive models and how they influence visual processing more broadly.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>02/26/2019</MinAmdLetterDate>
<MaxAmdLetterDate>02/26/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1850738</AwardID>
<Investigator>
<FirstName>Viola</FirstName>
<LastName>Stoermer</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Viola S Stoermer</PI_FULL_NAME>
<EmailAddress>viola.s.stoermer@dartmouth.edu</EmailAddress>
<PI_PHON>6178957407</PI_PHON>
<NSF_ID>000728746</NSF_ID>
<StartDate>02/26/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<CountyName>SAN DIEGO</CountyName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The Regents of the Univ. of Calif., U.C. San Diego]]></Name>
<CityName>La Jolla</CityName>
<CountyName>SAN DIEGO</CountyName>
<StateCode>CA</StateCode>
<ZipCode>920930109</ZipCode>
<StreetAddress><![CDATA[UCSD 9500 Gilman Drive, MC 0109]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1699</Code>
<Text>Cognitive Neuroscience</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>1699</Code>
<Text>COGNEURO</Text>
</ProgramReference>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~24915</FUND_OBLG>
</Award>
</rootTag>
