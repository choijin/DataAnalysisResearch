<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRI: CI-P: Collaborative: Towards a Program Analysis Collaboratory</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2018</AwardEffectiveDate>
<AwardExpirationDate>07/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>46546.00</AwardTotalIntnAmount>
<AwardAmount>46546</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
<PO_EMAI>sgreensp@nsf.gov</PO_EMAI>
<PO_PHON>7032927841</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Benchmark programs play an important role in evaluating the advances in program analysis research. Thus, the confidence of the evaluation depends on the quantity and the quality of the benchmark programs. The larger their number and the closer they resemble real-world programs, the higher confidence in the evaluation. Currently, research in the field of program analysis often relies on a few outdated or deficient benchmark programs for evaluation purposes. The difficulty in obtaining sufficient benchmark programs comes from the requirements imposed on the benchmark programs by program analyzers. Usually researchers manually obtain, inspect, and transform potential benchmark programs to create benchmark programs that meet those requirements. Such an unscalable approach often fails to produce adequate benchmark programs. The goal of this research is to automate this process and make it available through an online, public infrastructure. The infrastructure facilitates obtaining, selecting, and transforming open-source software projects for use as benchmark programs. In addition, the infrastructure gives researchers the ability to easily curate benchmark programs for program analysis and share the results of running their analyzers on those benchmarks. &lt;br/&gt;&lt;br/&gt;The objectives of this project are to solicit requirements and feedback from the program analysis community on automating benchmark program generation, and to prototype a Program Analysis Collaboratory (PAClab) research infrastructure that implements that process. Using researchers' specifications for adequate benchmark program generation, PAClab locates and obtains potential benchmark programs in open-source repositories; performs necessary program transformations as specified by researchers; and outputs the adequate benchmark programs. In addition, PAClab enables researchers to share their program analyzers using container technology, the transformed benchmark programs, and the results of running their analyzers on those benchmark programs. The intellectual merit of the project lies in investigating techniques for specifying and automating program transformations for targeted program analyzers. The broader impacts of the project stem from its potential to broaden adaptation of program analysis advances through the increased confidence of their evaluations; to lower the entry barrier for new program analysis researchers; and to accelerate the pace in program analysis research. PAClab's educational benefits include the ability for students to easily locate real-world programs and evaluate their implementations of program analyzers.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/29/2018</MinAmdLetterDate>
<MaxAmdLetterDate>07/29/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1823294</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Dyer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Dyer</PI_FULL_NAME>
<EmailAddress>rdyer@unl.edu</EmailAddress>
<PI_PHON>4024725082</PI_PHON>
<NSF_ID>000679020</NSF_ID>
<StartDate>07/29/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Bowling Green State University</Name>
<CityName>Bowling Green</CityName>
<ZipCode>434030230</ZipCode>
<PhoneNumber>4193722481</PhoneNumber>
<StreetAddress>302 Hayes Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>OH05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>617407325</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BOWLING GREEN STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041349010</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Bowling Green State University]]></Name>
<CityName>Bowling Green</CityName>
<StateCode>OH</StateCode>
<ZipCode>434030230</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>OH05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7359</Code>
<Text>CCRI-CISE Cmnty Rsrch Infrstrc</Text>
</ProgramElement>
<ProgramReference>
<Code>7359</Code>
<Text>COMPUTING RES INFRASTRUCTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~46546</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Static program analyses allow software engineers the ability to analyze computer programs without the need to actually run those programs.&nbsp; The ultimate goal of the Program Analysis Collaboratory (PAClab) project is to improve open-source program benchmark availability for static analysis researchers, which will result in stronger empirical evaluation of static analysis tools and techniques thus giving researchers and practitioners more confidence in the claims made by those tool developers. In particular, this project outlines and implements a new strategy for automatically obtaining program benchmarks, that compile without error, from open-source repositories according to user-defined specifications that describe desirable program benchmark features.</p> <p>The efforts of this planning project resulted in a fully functional web-based prototype tool named PAClab that automatically obtains from the largest repository of open-source programs, GitHub, benchmark programs written in the Java programming language. PAClab provides a web-based and user-friendly interface for entering program selection criteria. The key innovative component of the tool is a sophisticated program transformation toolchain that removes portions of the source code that are not relevant to the analysis at hand and automatically resolves dependencies to make the desired programs compilable outside of their native projects. Using its efficient, scalable backend infrastructure PAClab can process a large number of program benchmarks while support multiple users. PAClab allows researchers to easily share their generated program benchmark sets by providing collaborators with a link to the session. This session page contains selection criteria information as well as the resulting set of benchmark programs that are also available for download.&nbsp; This enables better collaboration within the program analysis research community as well as the ability to more easily replicate prior research results.</p> <p>This planning project disseminated its results through two peer-reviewed publications, an invited talk, two master&rsquo;s theses, and close collaboration with the program analysis community to integrate PAClab in empirical evaluation of program analysis research. &nbsp;In addition, the project provided support and training for two female graduate students in Computer Science.</p><br> <p>            Last Modified: 08/28/2020<br>      Modified by: Robert&nbsp;Dyer</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Static program analyses allow software engineers the ability to analyze computer programs without the need to actually run those programs.  The ultimate goal of the Program Analysis Collaboratory (PAClab) project is to improve open-source program benchmark availability for static analysis researchers, which will result in stronger empirical evaluation of static analysis tools and techniques thus giving researchers and practitioners more confidence in the claims made by those tool developers. In particular, this project outlines and implements a new strategy for automatically obtaining program benchmarks, that compile without error, from open-source repositories according to user-defined specifications that describe desirable program benchmark features.  The efforts of this planning project resulted in a fully functional web-based prototype tool named PAClab that automatically obtains from the largest repository of open-source programs, GitHub, benchmark programs written in the Java programming language. PAClab provides a web-based and user-friendly interface for entering program selection criteria. The key innovative component of the tool is a sophisticated program transformation toolchain that removes portions of the source code that are not relevant to the analysis at hand and automatically resolves dependencies to make the desired programs compilable outside of their native projects. Using its efficient, scalable backend infrastructure PAClab can process a large number of program benchmarks while support multiple users. PAClab allows researchers to easily share their generated program benchmark sets by providing collaborators with a link to the session. This session page contains selection criteria information as well as the resulting set of benchmark programs that are also available for download.  This enables better collaboration within the program analysis research community as well as the ability to more easily replicate prior research results.  This planning project disseminated its results through two peer-reviewed publications, an invited talk, two master’s theses, and close collaboration with the program analysis community to integrate PAClab in empirical evaluation of program analysis research.  In addition, the project provided support and training for two female graduate students in Computer Science.       Last Modified: 08/28/2020       Submitted by: Robert Dyer]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
