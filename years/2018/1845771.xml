<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Combining Learning and Reasoning for Spatial Language Understanding</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2019</AwardEffectiveDate>
<AwardExpirationDate>05/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>513238.00</AwardTotalIntnAmount>
<AwardAmount>256500</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The main goal of this project is to teach machines to understand human language when it contains spatial information. For example, to understand the statement, "Give me the book on AI on the table to your left," a robot needs to understand that the first "on" expresses the topic of the book, while the second "on" and the rest of the sentence convey spatial information. Next, it needs to work out the visual meaning of "on" to identify the correct book. Spatial language understanding is challenging because it requires meaning disambiguation, recognizing the links between referenced objects, and in most cases, background knowledge and common sense. This is particularly true for more complex sentences that include nesting relations or idioms containing spatial terms, like "she is the top expert in her field." The goal of this research is to develop techniques for the integration of machine learning and reasoning to advance spatial language comprehension. Solving this problem will provide tangible benefits to society in a wide variety of applications including healthcare (e.g., extracting biomedical information from patient reports and images), navigation and communication with assistant robots (particularly in risky situations like firefighter robots), situational awareness, information retrieval systems using various types of data, and geographical information systems.&lt;br/&gt;&lt;br/&gt;Providing reasoning capabilities to machine learning models is highly challenging. The proposed research advances this issue in an important and broad sub-problem: spatial language understanding. To this aim, a generic domain-independent symbolic spatial meaning representation will be introduced, which covers the main spatial semantic concepts for a variety of tasks. Deep structured and relational learning techniques will be developed to obtain such representations. The proposed techniques will facilitate indirect supervision by exploiting visual resources, will use ontologies that convey common sense and will integrate the axioms of spatial qualitative reasoning in training structured representations. Such an approach helps to avoid massive and complex data annotation for training structured and complex models. This research will evaluate whether using formal spatial calculi models as an intermediate structured representation will improve spatial reasoning and deeper learning. The interaction between learning and spatial reasoning will be investigated, while exploiting both symbolic and sub-symbolic representations. The impact of integrating explicit spatial reasoning will be tested on textual and visual question-answering tasks with a focus on locative questions. The outcome of this research will advance the generalizability of current state-of-the-art deep learning models at a time when there is a lack of training examples for complex unobserved situations. This work will help bridge the gap between symbolic AI and deep ML to advance spatial language understanding. The project will support research and education for graduate and undergraduate students and increase the contribution of women and minorities in STEM.&lt;br/&gt;&lt;br/&gt;This award is jointly funded by the Division of Information and Intelligent Systems in the Directorate for Computer &amp; Information Science &amp; Engineering and the Established Program to Stimulate Competitive Research in the Office of Integrative Activities.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>02/04/2019</MinAmdLetterDate>
<MaxAmdLetterDate>03/30/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1845771</AwardID>
<Investigator>
<FirstName>Parisa</FirstName>
<LastName>Kordjamshidi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Parisa Kordjamshidi</PI_FULL_NAME>
<EmailAddress>kordjams@msu.edu</EmailAddress>
<PI_PHON>2174187004</PI_PHON>
<NSF_ID>000728296</NSF_ID>
<StartDate>02/04/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Tulane University</Name>
<CityName>NEW ORLEANS</CityName>
<CountyName/>
<ZipCode>701185698</ZipCode>
<PhoneNumber>5048654000</PhoneNumber>
<StreetAddress>6823 ST CHARLES AVENUE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Louisiana</StateName>
<StateCode>LA</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>LA01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>053785812</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ADMINISTRATORS OF THE TULANE EDUCATIONAL FUND, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>053785812</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Tulane University]]></Name>
<CityName>New Orleans</CityName>
<CountyName/>
<StateCode>LA</StateCode>
<ZipCode>701185665</ZipCode>
<StreetAddress><![CDATA[6823 St. Charles Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Louisiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>LA01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>9150</Code>
<Text>EPSCoR Co-Funding</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~38717</FUND_OBLG>
<FUND_OBLG>2020~0</FUND_OBLG>
</Award>
</rootTag>
