<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Multiple Object Awareness</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2019</AwardEffectiveDate>
<AwardExpirationDate>07/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Michael Hout</SignBlockName>
<PO_EMAI>mhout@nsf.gov</PO_EMAI>
<PO_PHON>7032922163</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The world is a dynamic place. Objects move, information changes, and human observers need to keep updating their representations of that world in order to continue to function effectively. It is obvious that humans lack the capacity to monitor everything continuously but what is the limit? In classic experiments where people have to track subsets of items as they move around a screen, they seem to be able track 3-4 if all items in the display are identical, and just 2-3 if the items are distinct and they also have to track the identities of the items. However, these numbers seem artificially low, and our personal experience is that our visual awareness is much larger. The key reason seems to be that traditional experimental designs do not account for partial knowledge of items' identity and location. We developed the Multiple Object Awareness (MOA) task to measure this partial knowledge. When we include partial knowledge, it turns out that you know something about 8-12 items, not just 2-4. Why do we care? Many real-world tasks involve being able to rapidly find and interact with multiple items in a changing world. Think of a commander, trying to keep track of his squad and his enemies while monitoring the rest of the action around him or first responders, handling an evolving accident scene. Understanding MOA capacity can impact the design of workplaces (e.g. aircraft cockpits). Moreover, training may improve MOA capacity. If this proves to be the case, then MOA could provide a path to improvement on tasks from monitoring multiple streams of information in an operating room to keeping any eye on your 3rd grade class.&lt;br/&gt; &lt;br/&gt;The basic MOA tasks, used in this project, are modifications of the Multiple Object Tracking (MOT) and Multiple Identity Tracking (MIT) tasks. Observers monitor a set of moving objects (cartoon animals in our pilot work). The observers are asked to keep track of all of the objects as best they can. Every so often, we cover all items with disks and ask about the location of a specific animal. The crucial innovation is that we ask observers to keep clicking on disks until they find the target animal. As an observer, if you are guessing, you need to click on half the items on average. If you have some idea about the target location, you will need fewer clicks. Based on the number of clicks required, it is possible to derive a MOA capacity estimate of 8-12 items, far greater than MOT or MIT.  Our specific plans are: (1) To screen a large population in order to test the hypothesis that MOA will show a wider natural range of variation than MOT or working memory, (2) to test the hypothesis that MOA is trainable in ways that MOT and working memory are not, and (3) to study MOA in more complex, realistic settings.&lt;br/&gt;&lt;br/&gt;The Behavioral Systems Cluster in the Division of Integrative Organismal Systems participated in co-funding of the award.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/07/2019</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1848783</AwardID>
<Investigator>
<FirstName>Jeremy</FirstName>
<LastName>Wolfe</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeremy M Wolfe</PI_FULL_NAME>
<EmailAddress>jwolfe@bwh.harvard.edu</EmailAddress>
<PI_PHON>6177688818</PI_PHON>
<NSF_ID>000202279</NSF_ID>
<StartDate>08/07/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Brigham &amp; Women's Hospital Inc</Name>
<CityName>Boston</CityName>
<ZipCode>021156110</ZipCode>
<PhoneNumber>8572821670</PhoneNumber>
<StreetAddress>75 Francis Street</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>030811269</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BRIGHAM AND WOMEN'S HOSPITAL, INC., THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>825636988</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Brigham & Women's Hospital Inc]]></Name>
<CityName>Boston</CityName>
<StateCode>MA</StateCode>
<ZipCode>021156110</ZipCode>
<StreetAddress><![CDATA[75 Francis Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramElement>
<Code>7659</Code>
<Text>Animal Behavior</Text>
</ProgramElement>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~450000</FUND_OBLG>
</Award>
</rootTag>
