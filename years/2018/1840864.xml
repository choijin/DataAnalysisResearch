<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CGV: Small: Interactive Sound Rendering for Large-Scale Virtual Environments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2018</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>227696.00</AwardTotalIntnAmount>
<AwardAmount>227696</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Auditory experience is an integral part of our daily life.  Our perception of sound affects how we interpret and respond to various events around us.  Overall, interactive modeling and simulation of sound effects and auditory events can significantly enhance numerous scientific and engineering applications, and also support more intuitive human-computer interaction for desktop and mobile applications.  It also offers an alternative means to visualize datasets with complex characteristics (multi-dimensional, abstract, conceptual, spatial-temporal, etc.).  Yet despite the fact that hearing is one of our dominant senses, sound rendering has not received as much attention as visual rendering to better serve as an effective communication channel for human-computer systems, and interactive audio rendering still poses major computational challenges.  In this project, the PI focuses on rendering of aural effects, with attention to a greater correlation between sound and visual rendering, to communicate information (events, spatial extent, physical setting, emotion, ambience, etc.) to a user in a virtual world and to thereby increase the user's sense of presence and spaciousness while improving his/her ability to locate sound sources.  The PI's goal is to make radical advance in interactive sound rendering and application-specific auditory interaction techniques in order to achieve high-fidelity auditory interfaces for large-scale virtual reality.   In particular, she will address the computational bottlenecks in example-guided, physics-based sound synthesis, develop new hybrid algorithms for creating realistic acoustic effects in complex, dynamic 3D virtual environments, demonstrate the techniques on acoustic walkthrough for a variety of applications, and evaluate the resulting auditory systems and their impact on target applications.  The work will build upon the PI's prior accomplishments to make several major scientific advances that will significantly extend the state of the art in auditory displays and human-centric computing.  Project outcomes will include new hybrid acoustic algorithms for realistic sound effects, novel example-guided physics-based sound synthesis, innovative applications of auditory displays, and better understanding of human auditory perception.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  Applications of interactive sound rendering enabled by this project will span a wide variety of domains, include assistive technology for the visually impaired, multimodal human-centric interfaces, immersive teleconferencing, rapid prototyping of acoustic spaces for urban planning, structural design, and noise control.  Project outcomes, including scientific advances and software systems, will be disseminated through websites, publications, workshops, community outreach, and other professional contacts.  In addition to acoustic simulation, this research will ultimately offer fundamental scientific foundations for solving wave/sound propagation problems in complex domains for seismology, geophysics, meteorology, engineering design, urban planning, etc.</AbstractNarration>
<MinAmdLetterDate>07/20/2018</MinAmdLetterDate>
<MaxAmdLetterDate>07/20/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1840864</AwardID>
<Investigator>
<FirstName>Ming</FirstName>
<LastName>Lin</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ming C Lin</PI_FULL_NAME>
<EmailAddress>lin@cs.umd.edu</EmailAddress>
<PI_PHON>3014052662</PI_PHON>
<NSF_ID>000453947</NSF_ID>
<StartDate>07/20/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7453</Code>
<Text>GRAPHICS &amp; VISUALIZATION</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~227696</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Significant Scientific Results:</strong></p> <p><strong>Glass Half Full: Sound Synthesis for Fluid-Structure Coupling Using Added Mass Operator:</strong>&nbsp;  We present a fast and practical method for simulating the sound of  non-empty objects containing fluids. The method is designed and  demonstrated for use in interactive 3D systems, where live sound  synthesis is important. The key contribution of this work is to enhance  the sound synthesis equation in the rigid-body audio pipeline to account  for the fluid force on an object at the&nbsp;<em>fluid-structure</em>&nbsp;boundary.  Additions include pre-processing steps to identify the mesh nodes of a  tetrahedralized object that are in contact with the liquid and to apply  an "<em>added mass operator</em>" to those structural boundary nodes and  adjacent solid domain nodes by increasing their corresponding elements  in the mass matrix proportional to the liquid's density, which may vary  with temperature and/or type of fluids. Our technique generalizes to any  impermeable tetrahedral mesh representing the rigid objects and  inviscid liquids.</p> <p><strong>Outdoor&nbsp;Sound Propagation with Analytic Ray Curve Tracer and Gaussian Beam:</strong>&nbsp;&nbsp;Outdoor  sound propagation benefits from algorithms that can handle, in a  computationally efficient manner, inhomogeneous media, complex boundary  surfaces, and large spatial expanse. One recent work by Mo, Yeh, Lin,  and Manocha [Appl. Acoust. 104, 142-151 (2016)] proposed a ray tracing  method using analytic ray curves as tracing primitives, which improved  the performance of propagation paths computation over rectilinear ray  tracers. In this paper, an algorithm is developed that extends the  performance improvement to field computation; it combines the analytic  ray curve tracer with fast pressure computation based on the Gaussian  beam model. The algorithm is validated against published results on  benchmarks in atmospheric and ocean acoustics, and its application is  demonstrated on a scene with terrains and buildings of realistic  complexity and under a variety of atmospheric conditions. This algorithm  is able to compute characteristic sound fields for fully general media  profiles and complex three dimensional scenes at close-to-interactive  speed.</p> <p><strong>Psychoacoustic Characterization of Propagation Effects in Virtual Environments: &nbsp;</strong>As  sound propagation algorithms become faster and more accurate, the  question arises as to whether the additional efforts to improve fidelity  actually offer perceptual benefits over existing techniques. Could  environmental sound effects go the way of music, where lower-fidelity  compressed versions are actually favored by listeners? Here we address  this issue with two acoustic phenomena that are known to have perceptual  effects on humans and that, accordingly, might be expected to heighten  their experience with simulated environments. We present two studies  comparing listeners&rsquo; perceptual response to both accurate and  approximate algorithms simulating two key acoustic effects: diffraction  and reverberation. For each effect, we evaluate whether increased  numerical accuracy of a propagation algorithm translates into increased  perceptual differentiation in interactive virtual environments. Our  results suggest that auditory perception does benefit from the increased  accuracy, with subjects showing better perceptual differentiation when  experiencing the more accurate rendering method: the diffraction  experiment shows a more linearly decaying sound field (with respect to  the diffraction angle) for the accurate diffraction method, whereas the  reverberation experiment shows that more accurate reverberation, after  modest user experience, results in near-logarithmic response to  increasing room volume.</p> <p>&nbsp;</p> <p><strong>Other Achievements:</strong></p> <p>- Major publications</p> <p>- Invited keynotes at international conferences</p> <p>- Technology transfer to a successful start-up, Impulsonic.</p><br> <p>            Last Modified: 08/11/2019<br>      Modified by: Ming&nbsp;C&nbsp;Lin</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Significant Scientific Results:  Glass Half Full: Sound Synthesis for Fluid-Structure Coupling Using Added Mass Operator:   We present a fast and practical method for simulating the sound of  non-empty objects containing fluids. The method is designed and  demonstrated for use in interactive 3D systems, where live sound  synthesis is important. The key contribution of this work is to enhance  the sound synthesis equation in the rigid-body audio pipeline to account  for the fluid force on an object at the fluid-structure boundary.  Additions include pre-processing steps to identify the mesh nodes of a  tetrahedralized object that are in contact with the liquid and to apply  an "added mass operator" to those structural boundary nodes and  adjacent solid domain nodes by increasing their corresponding elements  in the mass matrix proportional to the liquid's density, which may vary  with temperature and/or type of fluids. Our technique generalizes to any  impermeable tetrahedral mesh representing the rigid objects and  inviscid liquids.  Outdoor Sound Propagation with Analytic Ray Curve Tracer and Gaussian Beam:  Outdoor  sound propagation benefits from algorithms that can handle, in a  computationally efficient manner, inhomogeneous media, complex boundary  surfaces, and large spatial expanse. One recent work by Mo, Yeh, Lin,  and Manocha [Appl. Acoust. 104, 142-151 (2016)] proposed a ray tracing  method using analytic ray curves as tracing primitives, which improved  the performance of propagation paths computation over rectilinear ray  tracers. In this paper, an algorithm is developed that extends the  performance improvement to field computation; it combines the analytic  ray curve tracer with fast pressure computation based on the Gaussian  beam model. The algorithm is validated against published results on  benchmarks in atmospheric and ocean acoustics, and its application is  demonstrated on a scene with terrains and buildings of realistic  complexity and under a variety of atmospheric conditions. This algorithm  is able to compute characteristic sound fields for fully general media  profiles and complex three dimensional scenes at close-to-interactive  speed.  Psychoacoustic Characterization of Propagation Effects in Virtual Environments:  As  sound propagation algorithms become faster and more accurate, the  question arises as to whether the additional efforts to improve fidelity  actually offer perceptual benefits over existing techniques. Could  environmental sound effects go the way of music, where lower-fidelity  compressed versions are actually favored by listeners? Here we address  this issue with two acoustic phenomena that are known to have perceptual  effects on humans and that, accordingly, might be expected to heighten  their experience with simulated environments. We present two studies  comparing listeners? perceptual response to both accurate and  approximate algorithms simulating two key acoustic effects: diffraction  and reverberation. For each effect, we evaluate whether increased  numerical accuracy of a propagation algorithm translates into increased  perceptual differentiation in interactive virtual environments. Our  results suggest that auditory perception does benefit from the increased  accuracy, with subjects showing better perceptual differentiation when  experiencing the more accurate rendering method: the diffraction  experiment shows a more linearly decaying sound field (with respect to  the diffraction angle) for the accurate diffraction method, whereas the  reverberation experiment shows that more accurate reverberation, after  modest user experience, results in near-logarithmic response to  increasing room volume.     Other Achievements:  - Major publications  - Invited keynotes at international conferences  - Technology transfer to a successful start-up, Impulsonic.       Last Modified: 08/11/2019       Submitted by: Ming C Lin]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
