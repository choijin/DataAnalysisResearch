<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: Self-Attention through the Bayesian Lens</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2019</AwardEffectiveDate>
<AwardExpirationDate>02/28/2021</AwardExpirationDate>
<AwardTotalIntnAmount>174980.00</AwardTotalIntnAmount>
<AwardAmount>174980</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Self-attention, a recently introduced augmentation to neural network architectures, has greatly improved neural network performance in a range of applications, particularly natural language processing and computer vision. Based conceptually on the way the human brain processes complex visual information by learning to selectively focus on the most salient elements, self-attention improves network ability to capture long-range relations in data. However, there is limited study in quantifying uncertainties of the outputs of self-attention networks, though uncertainty quantification is critically important for reliable learning models. The uncertainty of a self-attention network highly depends on where the network pays its attention to. This project will model the uncertainties associated with attention placement, and thereby better quantify the uncertainty in network outputs. The research will also convert part of the architecture design to standard computational procedures by utilizing statistical methods, and thus facilitate the design of new network architectures. This project has a secondary aim of applying the self-attention mechanism to statistical inference for computational efficiency. Ultimately, this project will produce new network architectures that are more reliable and more broadly applicable. This research will also support the development of a deep learning course for both graduate and undergraduate students at Tufts University. &lt;br/&gt;&lt;br/&gt;This project examines self-attention networks using a Bayesian approach and proposes a new modification - Bayesian Self-Attention Networks (BSANs). While self-attention networks use "attention weights" to take information from a special range of the data, BSANs assign probabilities to attention weights. By modeling uncertainties in the attention, BSANs naturally inherit desirable properties of Bayesian methods, such as better estimations of uncertainties and less overfitting of data. BSANs will automate the computation of attention probabilities as statistical inference procedures, simplifying the design of new attention-based neural networks, which will only need to determine where to place the attention structure. The study of BSANs will result in new network architectures, with the potential to improve reliability over a wide span of tasks in both natural language processing and graph data analysis. In addition to BSANs, this project will also use the self-attention mechanism to construct probability distributions that involve large numbers of variables. Being flexible and computationally efficient, the constructed distributions will be suitable for distribution approximation in large-scale statistical inference. This project will produce computationally efficient inference methods for Gaussian processes, a widely used model in machine learning and other related areas.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/14/2019</MinAmdLetterDate>
<MaxAmdLetterDate>03/14/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1850358</AwardID>
<Investigator>
<FirstName>Liping</FirstName>
<LastName>Liu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Liping Liu</PI_FULL_NAME>
<EmailAddress>liping.liu@tufts.edu</EmailAddress>
<PI_PHON/>
<NSF_ID>000754600</NSF_ID>
<StartDate>03/14/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Tufts University</Name>
<CityName>Boston</CityName>
<ZipCode>021111817</ZipCode>
<PhoneNumber>6176273696</PhoneNumber>
<StreetAddress>136 Harrison Ave</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>073134835</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF TUFTS COLLEGE INC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>073134835</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Tufts University School of Engineering]]></Name>
<CityName>Medford</CityName>
<StateCode>MA</StateCode>
<ZipCode>021555530</ZipCode>
<StreetAddress><![CDATA[200 College Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~174980</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><strong>Disclaimer:</strong>&nbsp;<span>This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.&nbsp;</span></p> <p><span>&nbsp;</span>The research goal of this CRII project is to analyze attention models from a probabilistic approach.&nbsp; The research supported by this project has achieved all the objectives of this project and further extend the scope of the proposal. The key outcomes of this project are as follows.&nbsp;&nbsp;</p> <p>The first part of the work focus on the inference of unknown function values given that these function values only have strong correlations with their respective neighbors. We devise a new inference algorithm to leverage this locality. When inferring a function value, the algorithm only pays attention to observations at its neighborhood.&nbsp; Furthermore, the inference of a function value has the same structure at different locations. We use a shared neural network to infer function values at these locations. The neural network is flexible enough to capture complex local correlations. This new inference model works for both Gaussian process and spatial predictions.&nbsp; We also show theoretically that such a neural network generalizes the traditional kriging method in spatial data modeling. Therefore, the new method can model more complex data patterns than the kriging method.&nbsp;&nbsp;</p> <p>The second part of the work treats the attention calculation as activation in a feedforward neural network. By retaining only the multiplication operation from the attention calculation, we construct a neural network that is a polynomial function and provides easy control of the polynomial order. Comparing to previous polynomial models, which are developed in a very different context, this new polynomial network not only generalizes these models but also uses new training techniques from deep learning. As a result, the new polynomial model can fit data better than polynomial models.</p> <p>The third part of the work investigates attention models in the context of graph generation with autoregressive neural networks. We first establish a solid foundation for these generation models from the perspective of statistical distributions of graphs. Specifically, we have investigated the node order in a generative process and give a rigorous definition of the distribution of node orders. We devise an attention-based model to infer node orders. With this inference model, we can accurately compute the likelihood of the data for a generative model.&nbsp; &nbsp;</p> <p>Beyond the technical goals of the project, the research from this project has contributed to the course "Deep Neural Networks" which the PI has taught multiple times at Tufts University. The outcomes of the project have been presented at their respective conference venues as well as three invited talks. Three graduate students have gained research experience through participating in the project.&nbsp; All code repositories associated with the projects have been publicized for reproducing the experiment results and for use by other researchers.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/28/2021<br>      Modified by: Liping&nbsp;Liu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Disclaimer: This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.    The research goal of this CRII project is to analyze attention models from a probabilistic approach.  The research supported by this project has achieved all the objectives of this project and further extend the scope of the proposal. The key outcomes of this project are as follows.    The first part of the work focus on the inference of unknown function values given that these function values only have strong correlations with their respective neighbors. We devise a new inference algorithm to leverage this locality. When inferring a function value, the algorithm only pays attention to observations at its neighborhood.  Furthermore, the inference of a function value has the same structure at different locations. We use a shared neural network to infer function values at these locations. The neural network is flexible enough to capture complex local correlations. This new inference model works for both Gaussian process and spatial predictions.  We also show theoretically that such a neural network generalizes the traditional kriging method in spatial data modeling. Therefore, the new method can model more complex data patterns than the kriging method.    The second part of the work treats the attention calculation as activation in a feedforward neural network. By retaining only the multiplication operation from the attention calculation, we construct a neural network that is a polynomial function and provides easy control of the polynomial order. Comparing to previous polynomial models, which are developed in a very different context, this new polynomial network not only generalizes these models but also uses new training techniques from deep learning. As a result, the new polynomial model can fit data better than polynomial models.  The third part of the work investigates attention models in the context of graph generation with autoregressive neural networks. We first establish a solid foundation for these generation models from the perspective of statistical distributions of graphs. Specifically, we have investigated the node order in a generative process and give a rigorous definition of the distribution of node orders. We devise an attention-based model to infer node orders. With this inference model, we can accurately compute the likelihood of the data for a generative model.     Beyond the technical goals of the project, the research from this project has contributed to the course "Deep Neural Networks" which the PI has taught multiple times at Tufts University. The outcomes of the project have been presented at their respective conference venues as well as three invited talks. Three graduate students have gained research experience through participating in the project.  All code repositories associated with the projects have been publicized for reproducing the experiment results and for use by other researchers.           Last Modified: 06/28/2021       Submitted by: Liping Liu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
