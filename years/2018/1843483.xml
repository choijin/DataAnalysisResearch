<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Energy Efficient Neural Network Accelerator Featuring a Logic Compatible Non-Volatile Synapse Array</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2019</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>224931.00</AwardTotalIntnAmount>
<AwardAmount>224931</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rick Schwerdtfeger</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to accelerate the adoption of AI features in Internet of Things and mobile devices.  The proposed multi-bit non-volatile memory (NVM) based Deep Neural Networks (DNN) IP is based on the standard CMOS logic processes. Existing solution for the DNN hardware typically requires off-chip access to retrieve neural network parameters from external memories, incurring additional communication latency and power consumption. Additionally, when critical neural network parameters are transmitted off-chip, security or privacy concern may arise, which is unacceptable especially for the applications such as personalized AI devices. Alternative approach integrating the DNN engine in a special NVM process requires as much as 10 additional masks beyond the conventional logic CMOS process which is not cost-effective for medium density DNN engine in cost-sensitive edge devices. With this proposed IP, any existing or new system on chip requiring persistent AI functionality can be built quickly and cost effectively.&lt;br/&gt;&lt;br/&gt;This Small Business Innovation Research (SBIR) Phase I project seeks to develop a cost-effective non-volatile neural network accelerator IP for edge devices. To solve the security, latency, power consumption, and cost issues associated with the traditional approaches, a single-poly based low cost, non-volatile, multi-bit eFlash cell is proposed. Multi-bit cell operation however presents significant challenges due to inherent reduction in signal-to-noise ratio. Key technical hurdles include solving disturbances of unselected cells, improving sensing margin, and overcoming reliability issues associated with high voltage operation in readout circuits as well as developing robust neural network cell arrays. To address these challenges, several new ideas related to multi-bit cell, high-voltage circuits, and cell programming methods have been proposed. Once verified successfully in this project, the multi-bit cell IP can then be integrated as logic compatible non-volatile memory to store neural network parameters on-chip, or as logic compatible non-volatile neural network IP to execute entire neural network operation.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>01/30/2019</MinAmdLetterDate>
<MaxAmdLetterDate>01/30/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1843483</AwardID>
<Investigator>
<FirstName>Seung-Hwan</FirstName>
<LastName>Song</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Seung-Hwan Song</PI_FULL_NAME>
<EmailAddress>ssong@anaflash.com</EmailAddress>
<PI_PHON>6122374629</PI_PHON>
<NSF_ID>000765639</NSF_ID>
<StartDate>01/30/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Anaflash Inc.</Name>
<CityName>Sunnyvale</CityName>
<ZipCode>940892233</ZipCode>
<PhoneNumber>6122374629</PhoneNumber>
<StreetAddress>1290B Reamwood Ave Ofc E</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>080955215</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>ANAFLASH INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[http://www.anaflash.com]]></Name>
<CityName>San Jose</CityName>
<StateCode>CA</StateCode>
<ZipCode>951342004</ZipCode>
<StreetAddress><![CDATA[3003 N 1ST ST STE 221]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>123E</Code>
<Text>CENTERS: ADVANCED MATERIALS</Text>
</ProgramReference>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~224931</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This Small Business Innovation Research Phase I project seeks to develop an energy efficient neural networks based on embedded Flash memory in a standard logic process. Existing solution for Deep Neural Networks (DNN) hardware typically requires off-chip access to retrieve neural network parameters, incurring additional communication latency and power consumption. Moreover, when critical neural network parameters are transmitted off-chip, security or privacy concern may arise in applications such as military and personalized AI devices. Alternative approach integrating the DNN engine in a special Non-Volatile Memory (NVM) process requires many additional processing steps beyond the standard CMOS logic process, which is not cost-effective for moderate density neural network engine in IoT edge devices (for example, voice controlled light switch device).</p> <p class="m-5280801260160872390gmail-msonormal">To solve security, latency, power consumption, and cost issues associated with the traditional approaches, a single-poly embedded Flash memory based neural network has been proposed and prototyped in a 180nm standard CMOS logic process. Synaptic cell operation of the logic compatible multi-level embedded Flash memory cell, low power and reliable operation of the proposed high voltage circuits designed in the standard logic process, and high-speed operation of the sensing circuit (neuron) have been verified from the silicon prototype chip. Two-phase multi-level linear programming scheme of the synapse cell has been proposed and tested from the silicon prototype chip to address inherent reduction in signal-to-noise ratio of the multi-level synapse cell operation. Neural network model quantization and compression techniques have been deployed to effectively reduce the required footprint of the synapse array. Necessary software stacks on FPGA prototype board have been developed to communicate it with the proposed embedded Flash based neural network chip.</p> <p class="m-5280801260160872390gmail-msonormal">Since the proposed non-volatile neural network IP is based on standard CMOS logic process, any existing or new SoCs requiring AI functionality can be easily built energy efficiently and cost effectively, accelerating their adoption in virtually every IoT and mobile edge devices. The society will benefit from using the AI edge devices without worrying about security or privacy concern because the proposed solution obviates off-chip memory access for enabling AI inferencing, too.</p><br> <p>            Last Modified: 08/23/2019<br>      Modified by: Seung-Hwan&nbsp;Song</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This Small Business Innovation Research Phase I project seeks to develop an energy efficient neural networks based on embedded Flash memory in a standard logic process. Existing solution for Deep Neural Networks (DNN) hardware typically requires off-chip access to retrieve neural network parameters, incurring additional communication latency and power consumption. Moreover, when critical neural network parameters are transmitted off-chip, security or privacy concern may arise in applications such as military and personalized AI devices. Alternative approach integrating the DNN engine in a special Non-Volatile Memory (NVM) process requires many additional processing steps beyond the standard CMOS logic process, which is not cost-effective for moderate density neural network engine in IoT edge devices (for example, voice controlled light switch device). To solve security, latency, power consumption, and cost issues associated with the traditional approaches, a single-poly embedded Flash memory based neural network has been proposed and prototyped in a 180nm standard CMOS logic process. Synaptic cell operation of the logic compatible multi-level embedded Flash memory cell, low power and reliable operation of the proposed high voltage circuits designed in the standard logic process, and high-speed operation of the sensing circuit (neuron) have been verified from the silicon prototype chip. Two-phase multi-level linear programming scheme of the synapse cell has been proposed and tested from the silicon prototype chip to address inherent reduction in signal-to-noise ratio of the multi-level synapse cell operation. Neural network model quantization and compression techniques have been deployed to effectively reduce the required footprint of the synapse array. Necessary software stacks on FPGA prototype board have been developed to communicate it with the proposed embedded Flash based neural network chip. Since the proposed non-volatile neural network IP is based on standard CMOS logic process, any existing or new SoCs requiring AI functionality can be easily built energy efficiently and cost effectively, accelerating their adoption in virtually every IoT and mobile edge devices. The society will benefit from using the AI edge devices without worrying about security or privacy concern because the proposed solution obviates off-chip memory access for enabling AI inferencing, too.       Last Modified: 08/23/2019       Submitted by: Seung-Hwan Song]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
