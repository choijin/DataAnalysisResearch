<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: AF: Guarantees for Training Neural Networks</AwardTitle>
<AwardEffectiveDate>10/01/2019</AwardEffectiveDate>
<AwardExpirationDate>09/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>174613.00</AwardTotalIntnAmount>
<AwardAmount>174613</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>A. Funda Ergun</SignBlockName>
</ProgramOfficer>
<AbstractNarration>The past decade has seen explosive progress in artificial intelligence, with conspicuous improvements in autonomous vehicles, intelligent virtual assistants, and recommendation systems, to name just a few applications. These advances are all powered by a broad family of techniques known as deep learning. Despite its observed successes, many basic questions about deep learning remain unanswered. To ensure predictable artificial-intelligence outcomes, is it possible to give useful formal guarantees for the performance of deep-learning algorithms? In order to deploy artificial intelligence on less powerful hardware (such as smartphones), is there a practical way to compress a massive deep-learning model into a smaller and more efficient model? Can deep-learning models be made provably robust against maliciously-crafted inputs? This project will search for rigorous answers to such questions in order to expand the theoretical foundations of deep learning. Integral to this project's success is the mentoring of graduate students. Innovations in education will furthermore broaden the community of researchers equipped with tools to solve problems at the intersection of computer science and mathematics.&lt;br/&gt;&lt;br/&gt;This project begins its development of rigorous algorithmic foundations for training neural networks by seeking useful upper bounds on the generalization error and time- and sample-complexity for supervised learning. However, lower bounds can serve as helpful guideposts by suggesting structural assumptions that are necessary for interesting algorithmic guarantees. For example, existing lower bounds rule out efficiently learning many simple concepts over high-dimensional Gaussian inputs. Hence, this project will explore the complexity of learning data labeled by deep neural networks with low-rank weight matrices over low-dimensional inputs. Additionally, this project will seek theoretically rigorous algorithms for compressing trained deep-neural-network models to much shallower approximations. Lower bounds against local or distribution-free approaches to the compression problem will also be developed to guide algorithmic intuition. Furthermore, this project will seek simple hypotheses under which gradient descent provably trains neural-network models that are susceptible to adversarial inputs. Understanding the weaknesses of existing training algorithms will guide the development of algorithms that are robust to adversarial inputs.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/27/2019</MinAmdLetterDate>
<MaxAmdLetterDate>06/27/2019</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1849796</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Wilmes</LastName>
<EmailAddress>wilmes@brandeis.edu</EmailAddress>
<StartDate>06/27/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Brandeis University</Name>
<CityName>WALTHAM</CityName>
<ZipCode>024532728</ZipCode>
<PhoneNumber>7817362121</PhoneNumber>
<StreetAddress>415 SOUTH ST MAILSTOP 116</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
</Institution>
<ProgramElement>
<Code>2878</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>7796</Code>
<Text>ALGORITHMIC FOUNDATIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
</Appropriation>
</Award>
</rootTag>
