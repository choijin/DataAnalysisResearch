<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research: Extending and testing theories of language production by investigating speaker choice in a classifier language</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/01/2019</AwardEffectiveDate>
<AwardExpirationDate>02/28/2021</AwardExpirationDate>
<AwardTotalIntnAmount>18414.00</AwardTotalIntnAmount>
<AwardAmount>18414</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tyler Kendall</SignBlockName>
<PO_EMAI>tkendall@nsf.gov</PO_EMAI>
<PO_PHON>7032922434</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Natural language often gives speakers multiple ways to convey the same meaning. Meanwhile, linguistic communication takes place in the face of environmental and cognitive constraints. When multiple options are available to express more or less the same meaning, what general principles govern speaker choice? Advancing our understanding of this question can potentially enhance a broad array of human language technologies, such as providing more human-like language generation with better understanding of speaker choice, more accurate machine translation, better resources for language learning and teaching, as well as insights to improve treatment for language disorders. &lt;br/&gt;&lt;br/&gt;Within this broader research program, this project focuses on the influence of contextual predictability on the encoding of linguistic content manifested by speaker choice in a classifier language. In English, a numeral modifies a noun directly (e.g., three tables). In classifier languages such as Mandarin Chinese, it is obligatory to use a classifier (CL) with the numeral and the noun (e.g., three CL.flat table, three CL.general table). While different nouns are compatible with different specific classifiers, there is a general classifier 'ge' (CL.general) that can be used with most nouns. This study focuses on the alternating options between using the general classifier versus a specific classifier with the same noun where the options are nearly semantically invariant. The use of a more specific classifier would reduce surprisal at the noun, but the use of that more specific classifier may be dispreferred from a production standpoint if accessing the general classifier requires less effort. This project combines corpus analyses, psycholinguistic behavioral experiments and computational modeling using techniques from statistics, natural language processing, and experimental psychology, examining how language users allocate resources to prepare them to produce and comprehend language, shedding lights on why language is structured in the way it is.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>02/08/2019</MinAmdLetterDate>
<MaxAmdLetterDate>02/08/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1844723</AwardID>
<Investigator>
<FirstName>Roger</FirstName>
<LastName>Levy</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Roger P Levy</PI_FULL_NAME>
<EmailAddress>rplevy@mit.edu</EmailAddress>
<PI_PHON>6172535763</PI_PHON>
<NSF_ID>000508659</NSF_ID>
<StartDate>02/08/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Meilin</FirstName>
<LastName>Zhan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Meilin Zhan</PI_FULL_NAME>
<EmailAddress>meilinz@mit.edu</EmailAddress>
<PI_PHON>6172531000</PI_PHON>
<NSF_ID>000768250</NSF_ID>
<StartDate>02/08/2019</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 Massachusetts Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8374</Code>
<Text>DDRI Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~18414</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Every moment as we speak, we are faced with choices as to how to convert what we want to mean into words, phrases, and sentences in the language we're using to communicate. Most of the time, we do not think about these choices consciously, but there are patterns in the unconscious choices that we make that research in psycholinguistics and cognitive science aims to reveal and understand. Different languages have different vocabularies and grammars, so the kinds of choices we face depend on the language being spoken, but there is good reason to believe that there are general principles applying to speakers of all languages that govern why we tend to make the choices we make. By identifying these general principles, we improve our fundamental understanding of language and the human mind, and lay the groundwork for better clinical diagnosis and treatment of language impairments due to stroke, age-related decline, and other reasons. Identifying these general principles may also help us build robots and other artificial agents that are more effective conversational partners with humans. The goal of this project was to test competing theories of these principles through analysis of usage patterns in a language, Mandarin Chinese, that offered a window into speaker choice that previous research into English and European languages did not.<br /><br />The reason that Mandarin Chinese offers this window has to do with its grammar. Mandarin Chinese is what is known as a "numeral classifier language". To illustrate what this means: in English, a numeral modifies a noun directly, but in numeral classifier languages such as Mandarin Chinese, it is obligatory to use a classifier with the numeral and the noun. Whereas in English one can say "three tables", in Mandarin one would need to say "san zhang zhuozi" which translates word for word as "three [classifier] table".&nbsp; There are many different classifiers, and for most nouns, there is more than one classifier that can be used: a "general" classifier that can be used for almost any noun, and a "specific" classifier that can only be used for a more restricted set of nouns.<br /><br />This configuration allowed us to test two competing theories of the principles governing speaker choice. One theory, Uniform Information Density, predicts that a specific classifier would be *more* likely to be used when the upcoming noun would otherwise be very surprising, because the specific classifier provides a cue as to what the noun might be likely to be and thus reduces the associated surprise. Another theory, Availability Based Production, predicts that a specific classifier would be *less* likely to be used when the upcoming noun would otherwise be very surprising, because when the noun is surprising the speaker might not have the cognitive resources to identify the correct specific classifier and retrieve its representation from memory in order to produce it fast enough to maintain fluent production.<br /><br />In this project we looked at datasets of naturally occuring language and conducted controlled experiments to determine the relationship between a noun's surprisingness (technically called "surprisal") and how likely a specific classifier was to be used. In the naturalistic datasets and controlled experiments alike, specific classifiers were *less* likely to be used when the upcoming noun would otherwise be more surprising, supporting the predictions of Availability-Based Production. We have published these results in top conference proceedings papers in natural language processing and cognitive science, and are currently preparing more comprehensive reports as well.</p><br> <p>            Last Modified: 06/29/2021<br>      Modified by: Roger&nbsp;P&nbsp;Levy</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Every moment as we speak, we are faced with choices as to how to convert what we want to mean into words, phrases, and sentences in the language we're using to communicate. Most of the time, we do not think about these choices consciously, but there are patterns in the unconscious choices that we make that research in psycholinguistics and cognitive science aims to reveal and understand. Different languages have different vocabularies and grammars, so the kinds of choices we face depend on the language being spoken, but there is good reason to believe that there are general principles applying to speakers of all languages that govern why we tend to make the choices we make. By identifying these general principles, we improve our fundamental understanding of language and the human mind, and lay the groundwork for better clinical diagnosis and treatment of language impairments due to stroke, age-related decline, and other reasons. Identifying these general principles may also help us build robots and other artificial agents that are more effective conversational partners with humans. The goal of this project was to test competing theories of these principles through analysis of usage patterns in a language, Mandarin Chinese, that offered a window into speaker choice that previous research into English and European languages did not.  The reason that Mandarin Chinese offers this window has to do with its grammar. Mandarin Chinese is what is known as a "numeral classifier language". To illustrate what this means: in English, a numeral modifies a noun directly, but in numeral classifier languages such as Mandarin Chinese, it is obligatory to use a classifier with the numeral and the noun. Whereas in English one can say "three tables", in Mandarin one would need to say "san zhang zhuozi" which translates word for word as "three [classifier] table".  There are many different classifiers, and for most nouns, there is more than one classifier that can be used: a "general" classifier that can be used for almost any noun, and a "specific" classifier that can only be used for a more restricted set of nouns.  This configuration allowed us to test two competing theories of the principles governing speaker choice. One theory, Uniform Information Density, predicts that a specific classifier would be *more* likely to be used when the upcoming noun would otherwise be very surprising, because the specific classifier provides a cue as to what the noun might be likely to be and thus reduces the associated surprise. Another theory, Availability Based Production, predicts that a specific classifier would be *less* likely to be used when the upcoming noun would otherwise be very surprising, because when the noun is surprising the speaker might not have the cognitive resources to identify the correct specific classifier and retrieve its representation from memory in order to produce it fast enough to maintain fluent production.  In this project we looked at datasets of naturally occuring language and conducted controlled experiments to determine the relationship between a noun's surprisingness (technically called "surprisal") and how likely a specific classifier was to be used. In the naturalistic datasets and controlled experiments alike, specific classifiers were *less* likely to be used when the upcoming noun would otherwise be more surprising, supporting the predictions of Availability-Based Production. We have published these results in top conference proceedings papers in natural language processing and cognitive science, and are currently preparing more comprehensive reports as well.       Last Modified: 06/29/2021       Submitted by: Roger P Levy]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
