<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research: Bringing the Power of Deep Learning to Large-Scale Ordinal Data Classification</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/01/2019</AwardEffectiveDate>
<AwardExpirationDate>05/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>16000.00</AwardTotalIntnAmount>
<AwardAmount>16000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04050000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>SES</Abbreviation>
<LongName>Divn Of Social and Economic Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Cheryl Eavey</SignBlockName>
<PO_EMAI>ceavey@nsf.gov</PO_EMAI>
<PO_PHON>7032927269</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This doctoral dissertation research project will develop a more effective and efficient classification method for ordinal data.  Due to the explosion in the use of digital data, there has been a dramatic increase in the number of ordinal datasets that have hundreds of thousands or even millions of records. Examples include ratings surveys found on sites like Amazon and Yelp, large corporation customer satisfaction/net promoter surveys, and the aggregation of medical history records. Current classification methods are inadequate for analyzing large ordinal datasets. The investigators will develop a classification method that facilitates the analysis of large ordinal datasets across a spectrum of application domains. Open-source software will be developed and made publicly available. Learning material and case studies will be made available for educators to adopt in relevant courses and experiential learning opportunities. As a Doctoral Dissertation Research Improvement award, support is provided to enable a promising student to establish a strong, independent research career.&lt;br/&gt;&lt;br/&gt;This doctoral dissertation research will develop a highly scalable ordinal classification method that can be applied to both structured and unstructured (e.g., images and text) ordinal data.  A core component of the method is a loss function that called Ordinal Hyperplane Loss (OHPL). OHPL is particularly designed for data with ordinal classes and enables deep learning techniques to be applied to the ordinal classification problems. By minimizing OHPL, a deep neural network learns to map data to an optimal space where the distance between points and their class centroid hyper-plane are minimized while a nontrivial ordinal relationship among classes are maintained. Preliminary experimental results indicate that deep neural network with OHPL optimizing significantly outperforms the state-of-the-art alternatives on classification accuracies across multiple datasets. This research will examine strategies that scale the OHPL based learning to big ordinal data. The investigators will apply the OHPL-based learning to real-life critical applications such as determining the severity/stages of a disease. They will develop a ready-to-use open-source package on the OHPL deep learning strategy and make it publicly available.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>05/24/2019</MinAmdLetterDate>
<MaxAmdLetterDate>05/24/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1853191</AwardID>
<Investigator>
<FirstName>Ying</FirstName>
<LastName>Xie</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ying Xie</PI_FULL_NAME>
<EmailAddress>yxie2@kennesaw.edu</EmailAddress>
<PI_PHON>6787972143</PI_PHON>
<NSF_ID>000297170</NSF_ID>
<StartDate>05/24/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Vanderheyden</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert Vanderheyden</PI_FULL_NAME>
<EmailAddress>rvanderh@students.kennesaw.edu</EmailAddress>
<PI_PHON>7703295377</PI_PHON>
<NSF_ID>000786536</NSF_ID>
<StartDate>05/24/2019</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Kennesaw State University Research and Service Foundation</Name>
<CityName>Kennesaw</CityName>
<ZipCode>301445591</ZipCode>
<PhoneNumber>4705786381</PhoneNumber>
<StreetAddress>1000 Chastain Road</StreetAddress>
<StreetAddress2><![CDATA[MD 0111]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>11</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA11</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>832879733</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>KENNESAW STATE UNIVERSITY RESEARCH AND SERVICE FOUNDATION, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Kennesaw State University]]></Name>
<CityName>Marietta</CityName>
<StateCode>GA</StateCode>
<ZipCode>300602855</ZipCode>
<StreetAddress><![CDATA[1100 South Marietta Pkwy]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>11</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA11</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1333</Code>
<Text>Methodology, Measuremt &amp; Stats</Text>
</ProgramElement>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In our preliminary research, we proposed a novel loss function called Ordinal Hyperplane Loss (OHPL) that was particularly designed for data with ordinal classes. The proposal of OHPL enables deep learning techniques to be applied to the ordinal classification problem on both structured and unstructured data. By minimizing OHPL, a deep neural network learns to map data to an optimal space where the distance between points and their class centroids are minimized while a nontrivial ordinal relationship among classes are maintained.&nbsp;</p> <p>The preliminary work on OHPL provided a meaningful improvement over alternative ordinal classifiers on benchmark datasets.&nbsp; However, those benchmark data sets are relatively small in size, so the initial OHPL learning strategy is able to use the entire dataset for calculating the hyperplane centroids for each batch submission.&nbsp; In order to scale OHPL to big data or complex data, new learning strategy needed to be developed. &nbsp;As one of the outcomes of this funded research, we designed, implemented, and experimented the following new learning strategies that enable OHPL to be applied to big data or complex data.&nbsp;</p> <ul> <li> OHPLall Learning Strategy: This strategy is able to effectively assess the loss that is caused by improper ordering of Hyperplane Centroids in the feature space by using mini batch of data that most likely only contains a small number of samples from partial classes. Within each mini batch, instead of relying on the transitive property, this strategy establishes the appropriate hyperplane centroid ordering and distance by comparing each pair of hyperplane centroids while minimizing the point distances from the corresponding hyperplane centroids.</li> <li>OHPL Two-Stage Learning Strategy:&nbsp; Stage-one learning uses mini-batch to learn optimal hyperplane centroid layout; whereas stage-two learning uses mini-batch to minimize point distances from fixed hyperplane centroids.</li> <li>Double Patch Sampling: By this strategy, the data are first sampled into large batches (e.g., 64K records), with each data point being sampled once and only once for each epoch (pass through the dataset). Within each large batch, the standard mini-batch (e.g., 64 records) processing are utilized. The large batch is used to calculate the hyperplane centroids, for the processing of each mini batch that minimizes point distances from hyperplane centroids.</li> <li>Stratified Epoch Sampling: At the beginning of each epoch, a single stratified sample is created. The stratified sample is used to calculate hyperplane centroids with the current model weights at the beginning of each mini batch.&nbsp; Each mini batch is used to minimize point distances from hyperplane centroids that are calculated using the stratified sample.&nbsp;&nbsp;</li> </ul> <p>We tested the above strategies on different data sets and found that OHPL Two-Stage Learning Strategy and OHPLall Learning Strategy are most effective and efficient.&nbsp; Since both these two strategies used mini batch for learning, it should fit any ordinal data of large size or of complex types (such as medical images), even on a single GPU system with reasonable volume of cache memory.</p> <p>OHPL and the corresponding learning strategies have been applied to a couple of real-life applications in a large multi-national corporation and have contributed to solve some challenging issues on processing large volume of unstructured data. Those real-life applications include Net Promoter Sentiment Analyzer and Fact Minor.&nbsp;</p> <p>We have implemented the major OHPL learning strategies as open source python packages. We also provide open access to the source code of different sample applications of OHPL on both text data and image data.&nbsp; All the source code is freely accessible at&nbsp;https://digitalcommons.kennesaw.edu/ohplsoftware.</p> <p>Furthermore, we designed learning materials by utilizing the developed technology (OHPL and corresponding learning strategies) as the primary instrument in order to train both graduate students and undergraduate students on data analytics. At the graduate level, we focused on student's ability to develop comprehensive solutions to challenging problems based on large unstructured data by integrating OHPL, deep learning and advanced embedding techniques. At the undergraduate level, we focused on supervising project groups to apply the OHPL techniques to solve practical problems such as medical image classification, customer survey analysis, and real-time detecting students' concentration levels in online classes based on facial videos captured by webcams.&nbsp;The developed learning materials can be found at https://digitalcommons.kennesaw.edu/ohpllearningdocs.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/15/2021<br>      Modified by: Ying&nbsp;Xie</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In our preliminary research, we proposed a novel loss function called Ordinal Hyperplane Loss (OHPL) that was particularly designed for data with ordinal classes. The proposal of OHPL enables deep learning techniques to be applied to the ordinal classification problem on both structured and unstructured data. By minimizing OHPL, a deep neural network learns to map data to an optimal space where the distance between points and their class centroids are minimized while a nontrivial ordinal relationship among classes are maintained.   The preliminary work on OHPL provided a meaningful improvement over alternative ordinal classifiers on benchmark datasets.  However, those benchmark data sets are relatively small in size, so the initial OHPL learning strategy is able to use the entire dataset for calculating the hyperplane centroids for each batch submission.  In order to scale OHPL to big data or complex data, new learning strategy needed to be developed.  As one of the outcomes of this funded research, we designed, implemented, and experimented the following new learning strategies that enable OHPL to be applied to big data or complex data.    OHPLall Learning Strategy: This strategy is able to effectively assess the loss that is caused by improper ordering of Hyperplane Centroids in the feature space by using mini batch of data that most likely only contains a small number of samples from partial classes. Within each mini batch, instead of relying on the transitive property, this strategy establishes the appropriate hyperplane centroid ordering and distance by comparing each pair of hyperplane centroids while minimizing the point distances from the corresponding hyperplane centroids. OHPL Two-Stage Learning Strategy:  Stage-one learning uses mini-batch to learn optimal hyperplane centroid layout; whereas stage-two learning uses mini-batch to minimize point distances from fixed hyperplane centroids. Double Patch Sampling: By this strategy, the data are first sampled into large batches (e.g., 64K records), with each data point being sampled once and only once for each epoch (pass through the dataset). Within each large batch, the standard mini-batch (e.g., 64 records) processing are utilized. The large batch is used to calculate the hyperplane centroids, for the processing of each mini batch that minimizes point distances from hyperplane centroids. Stratified Epoch Sampling: At the beginning of each epoch, a single stratified sample is created. The stratified sample is used to calculate hyperplane centroids with the current model weights at the beginning of each mini batch.  Each mini batch is used to minimize point distances from hyperplane centroids that are calculated using the stratified sample.     We tested the above strategies on different data sets and found that OHPL Two-Stage Learning Strategy and OHPLall Learning Strategy are most effective and efficient.  Since both these two strategies used mini batch for learning, it should fit any ordinal data of large size or of complex types (such as medical images), even on a single GPU system with reasonable volume of cache memory.  OHPL and the corresponding learning strategies have been applied to a couple of real-life applications in a large multi-national corporation and have contributed to solve some challenging issues on processing large volume of unstructured data. Those real-life applications include Net Promoter Sentiment Analyzer and Fact Minor.   We have implemented the major OHPL learning strategies as open source python packages. We also provide open access to the source code of different sample applications of OHPL on both text data and image data.  All the source code is freely accessible at https://digitalcommons.kennesaw.edu/ohplsoftware.  Furthermore, we designed learning materials by utilizing the developed technology (OHPL and corresponding learning strategies) as the primary instrument in order to train both graduate students and undergraduate students on data analytics. At the graduate level, we focused on student's ability to develop comprehensive solutions to challenging problems based on large unstructured data by integrating OHPL, deep learning and advanced embedding techniques. At the undergraduate level, we focused on supervising project groups to apply the OHPL techniques to solve practical problems such as medical image classification, customer survey analysis, and real-time detecting students' concentration levels in online classes based on facial videos captured by webcams. The developed learning materials can be found at https://digitalcommons.kennesaw.edu/ohpllearningdocs.           Last Modified: 06/15/2021       Submitted by: Ying Xie]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
