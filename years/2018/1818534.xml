<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: An Audio-Based Spatiotemporal System for Automated Monitoring of Construction Operations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/15/2017</AwardEffectiveDate>
<AwardExpirationDate>02/28/2021</AwardExpirationDate>
<AwardTotalIntnAmount>109081.00</AwardTotalIntnAmount>
<AwardAmount>232291</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yueyue Fan</SignBlockName>
<PO_EMAI>yfan@nsf.gov</PO_EMAI>
<PO_PHON>7032924453</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The construction industry has been suffering from the lack of real-time performance monitoring, holistic project management, labor efficiency, and waste-preventive tools. This has led to cost overruns in almost 90 percent of construction projects with an average of 28 percent higher than forecast costs. This research project is expected to facilitate a transformative change in the ways that construction activities and operations are tracked and monitored. It will play a significant role in future rapid, nonintrusive and cost effective data acquisition capacity due to the use of audio signals instead of active sensors or digital cameras. Eventually the incurred efficiencies, sustainability benefits, and reduced costs of an automated project monitoring system will significantly benefit the U.S. Architecture, Engineering, Construction and Facilities Management industry. Success in this project also promises significant impacts to engineering education. The project's educational activities are highly integrated and inter-related with the research activities. The research results will be used to create educational material and will be made publicly available to educators at other institutions. The research results will also be integrated into the outreach and engagement activities and will result in engaging minorities and underrepresented groups through various programs.&lt;br/&gt;&lt;br/&gt;Despite recent advances in developing and implementing audio signal processing techniques for analyzing and modeling complex systems and processes, the real added value and potential applications of audio signals are still unknown to the civil engineering research community. This project is the first attempt to introduce audio signals as an alternative source of information for recognizing, tracking and monitoring construction operations at jobsites. Current approaches for recognizing and monitoring construction operations are either location-based or machine-vision-based, and implying audio signals is a significant leap to overcome their limitations. The framework provides the missing link between generic signal processing and construction performance monitoring. This project will expand the research horizons for academics in civil infrastructure systems as well as in digital signal processing domains. Particularly, this scientific breakthrough will set the stage for future research in automatically identifying and life-logging construction operations and equipment actions, estimating project performance indices, and creating corrective measures to keep the project performance as planned. This, in turn, will enable the future development of novel, automated applications for construction sequence analysis, productivity measuring, project monitoring and control systems, and maintenance decision making.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>02/26/2018</MinAmdLetterDate>
<MaxAmdLetterDate>10/26/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1818534</AwardID>
<Investigator>
<FirstName>Abbas</FirstName>
<LastName>Rashidi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Abbas Rashidi</PI_FULL_NAME>
<EmailAddress>abbas.rashidi@utah.edu</EmailAddress>
<PI_PHON>8015813155</PI_PHON>
<NSF_ID>000692366</NSF_ID>
<StartDate>02/26/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Utah</Name>
<CityName>SALT LAKE CITY</CityName>
<ZipCode>841128930</ZipCode>
<PhoneNumber>8015816903</PhoneNumber>
<StreetAddress>75 S 2000 E</StreetAddress>
<StreetAddress2><![CDATA[Second Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009095365</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF UTAH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009095365</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName>Salt Lake City</CityName>
<StateCode>UT</StateCode>
<ZipCode>841120610</ZipCode>
<StreetAddress><![CDATA[110 Central Campus Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1504</Code>
<Text>GOALI-Grnt Opp Acad Lia wIndus</Text>
</ProgramElement>
<ProgramElement>
<Code>1631</Code>
<Text>CIS-Civil Infrastructure Syst</Text>
</ProgramElement>
<ProgramReference>
<Code>019Z</Code>
<Text>Grad Prep APG:Enhan. Experience</Text>
</ProgramReference>
<ProgramReference>
<Code>029E</Code>
<Text>INFRASTRUCTURE SYSTEMS MGT</Text>
</ProgramReference>
<ProgramReference>
<Code>036E</Code>
<Text>CIVIL INFRASTRUCTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>039E</Code>
<Text>STRUCTURAL SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>1057</Code>
<Text>CIS BASE RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>116E</Code>
<Text>RESEARCH EXP FOR UNDERGRADS</Text>
</ProgramReference>
<ProgramReference>
<Code>129E</Code>
<Text>CENTERS: MANUFACTURING &amp; PROC</Text>
</ProgramReference>
<ProgramReference>
<Code>1504</Code>
<Text>GRANT OPP FOR ACAD LIA W/INDUS</Text>
</ProgramReference>
<ProgramReference>
<Code>8021</Code>
<Text>Materials Engineering</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9231</Code>
<Text>SUPPL FOR UNDERGRAD RES ASSIST</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>CVIS</Code>
<Text>CIVIL INFRASTRUCTURE</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~109081</FUND_OBLG>
<FUND_OBLG>2016~10000</FUND_OBLG>
<FUND_OBLG>2018~16000</FUND_OBLG>
<FUND_OBLG>2019~48483</FUND_OBLG>
<FUND_OBLG>2020~48727</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The main objective of this project is to automatically recognize heavy equipment activities using sound data and estimate the productivity rates and the amount of completed work. We have categorized this project into several phases, and we describe the general and specific outcomes of each phase as follows:</p> <p><strong>General Outcomes:</strong></p> <ol> <li><strong>A comprehensive literature review on automated activity recognition of construction workers and equipment:</strong> We studied the feasibility of implementing a wide range of technologies and computational techniques for automated activity detection and tracking construction equipment and workers. This activity provided a concise review of these methods and techniques and the advantages and limitations of each category. Additionally, we presented a comprehensive comparison between these methods and discussed potential knowledge gaps and future research directions.</li> <li><strong>Data collection sessions:</strong> During this project, we collected the sound and vibration data of more than 20 types of equipment (e.g., loaders, graders, excavators, trucks, jackhammer, lifts, etc.) on different construction job sites and under various weather conditions.</li> <li><strong>Proposed models:</strong> We have trained different machine learning and deep learning models such as Support Vector Machines (SVM) and Convolutional Neural Network (CNN). These models are trained with several features extracted from both the sound and kinematic signals.</li> </ol> <p><strong>Specific Outcomes:</strong></p> <ol> <li>In the first phase, we used a data fusion approach called feature fusion to integrate sound and kinematic data to improve the model performance. Results showed that the proposed hybrid system could achieve higher accuracy (e.g., some cases more than 20% higher accuracy) than when we use one type of data. We used this method to recognize single-equipment scenarios (i.e., when only one piece of equipment is active).</li> <li>In the second phase, we moved one step forward and evaluated different methods for a multiple-equipment scenario (i.e., when multiple machines perform simultaneously). This scenario is more challenging than the single-equipment scenario because equipment sound signals are mixed in the environment, and there is a need for advanced methods. We proposed using a hardware-based method by utilizing the microphone array and beamforming technique, a sound source separation method. We evaluated and compared six beamforming techniques. Results show that Frost beamformer and time-delay Linear Constraint Minimum Variance (LCMV) generate outputs with array gains of more than 4.0 more reliable than the other beamformers on the construction job sites.</li> <li>Although the hardware-based results were promising, we found that microphone arrays are not appropriate on construction job sites. They need specific setups such as connecting them to laptops and running them through particular applications such as Audacity. Furthermore, single-channel microphones are more common and affordable than microphone arrays. However, the use of a single-channel microphone requires advanced signal processing or deep learning techniques.</li> <li>In the first step of the third phase, we proposed a software-based method using Deep Neural Networks (DNNs) and time-frequency masks (TFMs) for activity recognition of multiple-equipment scenarios. This method can recognize the activity of two machines using a single-channel off-the-shelf microphone. We tested and validated the presented approach under simulated job site conditions where two machines operated simultaneously. Results show that the average accuracy for soft TFM is 38% higher than binary TFM.</li> <li>In the second step of the third phase, we extended and improved our previous algorithms and models when more than two pieces of equipment perform activities. We proposed a novel method to identify equipment activities by extracting activity-related sound patterns from the equipment sound data. This method utilizes Short Time Fourier Transform (STFT) to convert equipment activity sound signals to images, which are then used in a CNN to identify the specific features of an equipment activity. Also, we introduced a novel data augmentation method to simulate the real-world sound mixtures when multiple types of machines operate simultaneously. This method utilizes real single-machine sound signals and mixes them with specific considerations to mimic the real sound mixtures. Finally, we trained the CNN model to extract hidden features from the sound-based images and identify multiple equipment activities.</li> </ol> <p>&nbsp;</p><br> <p>            Last Modified: 04/18/2021<br>      Modified by: Abbas&nbsp;Rashidi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The main objective of this project is to automatically recognize heavy equipment activities using sound data and estimate the productivity rates and the amount of completed work. We have categorized this project into several phases, and we describe the general and specific outcomes of each phase as follows:  General Outcomes:  A comprehensive literature review on automated activity recognition of construction workers and equipment: We studied the feasibility of implementing a wide range of technologies and computational techniques for automated activity detection and tracking construction equipment and workers. This activity provided a concise review of these methods and techniques and the advantages and limitations of each category. Additionally, we presented a comprehensive comparison between these methods and discussed potential knowledge gaps and future research directions. Data collection sessions: During this project, we collected the sound and vibration data of more than 20 types of equipment (e.g., loaders, graders, excavators, trucks, jackhammer, lifts, etc.) on different construction job sites and under various weather conditions. Proposed models: We have trained different machine learning and deep learning models such as Support Vector Machines (SVM) and Convolutional Neural Network (CNN). These models are trained with several features extracted from both the sound and kinematic signals.   Specific Outcomes:  In the first phase, we used a data fusion approach called feature fusion to integrate sound and kinematic data to improve the model performance. Results showed that the proposed hybrid system could achieve higher accuracy (e.g., some cases more than 20% higher accuracy) than when we use one type of data. We used this method to recognize single-equipment scenarios (i.e., when only one piece of equipment is active). In the second phase, we moved one step forward and evaluated different methods for a multiple-equipment scenario (i.e., when multiple machines perform simultaneously). This scenario is more challenging than the single-equipment scenario because equipment sound signals are mixed in the environment, and there is a need for advanced methods. We proposed using a hardware-based method by utilizing the microphone array and beamforming technique, a sound source separation method. We evaluated and compared six beamforming techniques. Results show that Frost beamformer and time-delay Linear Constraint Minimum Variance (LCMV) generate outputs with array gains of more than 4.0 more reliable than the other beamformers on the construction job sites. Although the hardware-based results were promising, we found that microphone arrays are not appropriate on construction job sites. They need specific setups such as connecting them to laptops and running them through particular applications such as Audacity. Furthermore, single-channel microphones are more common and affordable than microphone arrays. However, the use of a single-channel microphone requires advanced signal processing or deep learning techniques. In the first step of the third phase, we proposed a software-based method using Deep Neural Networks (DNNs) and time-frequency masks (TFMs) for activity recognition of multiple-equipment scenarios. This method can recognize the activity of two machines using a single-channel off-the-shelf microphone. We tested and validated the presented approach under simulated job site conditions where two machines operated simultaneously. Results show that the average accuracy for soft TFM is 38% higher than binary TFM. In the second step of the third phase, we extended and improved our previous algorithms and models when more than two pieces of equipment perform activities. We proposed a novel method to identify equipment activities by extracting activity-related sound patterns from the equipment sound data. This method utilizes Short Time Fourier Transform (STFT) to convert equipment activity sound signals to images, which are then used in a CNN to identify the specific features of an equipment activity. Also, we introduced a novel data augmentation method to simulate the real-world sound mixtures when multiple types of machines operate simultaneously. This method utilizes real single-machine sound signals and mixes them with specific considerations to mimic the real sound mixtures. Finally, we trained the CNN model to extract hidden features from the sound-based images and identify multiple equipment activities.           Last Modified: 04/18/2021       Submitted by: Abbas Rashidi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
