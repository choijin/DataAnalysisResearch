<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Leveraging Heterogeneous Data Across International Borders in a Privacy Preserving Manner for Clinical Deep Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2018</AwardEffectiveDate>
<AwardExpirationDate>02/28/2021</AwardExpirationDate>
<AwardTotalIntnAmount>300000.00</AwardTotalIntnAmount>
<AwardAmount>300000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Alejandro Suarez</SignBlockName>
<PO_EMAI>alsuarez@nsf.gov</PO_EMAI>
<PO_PHON>7032927092</PO_PHON>
</ProgramOfficer>
<AbstractNarration>There is a growing awareness of the need for multi-center clinical databases and multi-institutional analyses of healthcare data to ensure reproducibility and generalizability of research findings. Single-instance database algorithms are prone to three distinct problems. First, in the context of Big Data science, the size of the data compared to the number of variables makes it difficult to develop complex predictors without overfitting, and more traditional learning algorithms may lead to over-simplified models that do not capture important related influences or interactions between different types of healthcare information. Second, training and testing predictive models on a single database can lead to learning noise or other irrelevant local practices or differences in definitions that are correlated with, but not causally related to, the outcome in question. This leads to models that do not work in other institutions or in the future when practices or the environment changes. Third, sharing data between institutions, and in particular, across borders, is extremely problematic because of trust, legal issues, privacy issues and national policies. The significance of solving these issues is threefold: 1) it would allow the creating of strong generalizable data science models, which leverage enormous pools of data from around the world; 2) it would also allow the identification of rare diseases or patient types, which, as we compile databases, become less rare; and 3) perhaps most importantly, it would allow the free exchange of data science models and generalized approaches to solving medical problems in the cloud.&lt;br/&gt;&lt;br/&gt;This project aims to develop a set of distributed deep learning and cloud computation techniques for cross-institution and cross-border machine learning on health and medical data without the need for protected health information to leave the generating institution. The goals are to create demonstration programs which illustrate feasibility and open source the architecture. The scope of this project encompasses the broad set of machine learning-based tasks multiple institutions may want to apply to their healthcare data in the cloud, as well as the technical issues surrounding transfer learning of knowledge across domains (e.g., institutions/demographics) and tasks (e.g., types of classification and prediction problems). The project has three specific aims: 1) develop a cloud-based infrastructure which preserves regional autonomy of data, but allows the sharing of parameters of the partially trained deep neural network (including weights and hyperparameters) between regions, to allow transfer learning across domains and tasks; 2) develop a standardized coded model for deep learning approaches in medical applications; and 3) evaluate the effect of training and testing the model across multiple centers and national boundaries, by comparing improvement in performance with cross-institutional training without loss of privacy protection, using metrics of sensitivity, specificity, positive predictive value, area under the receiver operating characteristic (ROC) curve and model calibration. Aims 1-3 will be achieved by taking four databases (including, a database of intensive care unit patients with sepsis, a free text corpus of nursing progress notes, voice recordings taken from a public corpus classically used for speaker identification, and a public database of full-face images used for classification of facial expressions) and placing them in the cloud (Google, AWS and Azure) at different geopolitical locations (namely US and Europe) and developing a distributed deep learning architecture that learns to improve its performance by sharing weights across borders, but not sensitive patient data. This project has the potential to make several contributions to the field. First, it will demonstrate that medical data across geopolitical boundaries can be made available in an interoperable manner (using the FHIR standard) and can be used for training of deep learning algorithms in a privacy-preserving manner, thus addressing both the concerns of Health Insurance, Portability and Privacy Act (HIPPA) and interoperability. Secondly, it will provide open-source deep learning algorithms for several medical datasets and data types that can be used across institutions to solve similar problems with some fine-tuning (e.g., via transfer learning). Third, it will provide a set of open-source meta algorithms for transfer learning (across domains and tasks) implemented on the cloud in containers (dockers) that can be downloaded for local use or transferred across the different cloud vendors.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/15/2018</MinAmdLetterDate>
<MaxAmdLetterDate>03/15/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1822378</AwardID>
<Investigator>
<FirstName>Shamim</FirstName>
<LastName>Nemati</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shamim Nemati</PI_FULL_NAME>
<EmailAddress>snemati@health.ucsd.edu</EmailAddress>
<PI_PHON>4058504751</PI_PHON>
<NSF_ID>000713215</NSF_ID>
<StartDate>03/15/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Gari</FirstName>
<LastName>Clifford</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gari D Clifford</PI_FULL_NAME>
<EmailAddress>gari.clifford@emory.edu</EmailAddress>
<PI_PHON>4047272503</PI_PHON>
<NSF_ID>000688076</NSF_ID>
<StartDate>03/15/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Emory University</Name>
<CityName>Atlanta</CityName>
<ZipCode>303224250</ZipCode>
<PhoneNumber>4047272503</PhoneNumber>
<StreetAddress>1599 Clifton Rd NE, 4th Floor</StreetAddress>
<StreetAddress2><![CDATA[1599-001-1BA]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>066469933</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>EMORY UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>066469933</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Emory University]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303220001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>024Y</Code>
<Text>BD Spokes -Big Data Regional I</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~300000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-9a22270e-7fff-0b83-0bf5-fc2a24a3487b" style="font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;">For legal and moral reasons, those who collect medical data are required to protect the identity of the individuals from whom the data is captured. De-identification of the data is cumbersome, and imperfect. For this reason, medical information is often siloed at the institution at which is was collected, reducing the benefit to medical research, or the utility to the medical teams and patients. Moreover, moving personal data across international or domestic borders is often prohibited. One strong approach to addressing this issue is to build models and use &lsquo;transfer learning&rsquo; to move the models, rather than the data, so that a model can be updated on a series of databases in a distributed manner. However, modern artificial intelligence (AI), or more specifically, machine learning, can allow the creation of powerful and complex models to analyze health data. However, because there are sometimes millions of parameters in these models, there&rsquo;s the chance that they could encode some information that allows users of the technology to uncover the identity of the individuals used either to train the models, or on whom the models are used. This research aimed at determining the likelihood that such models do not encode individual information, and therefore can be move across borders. To test this idea, we took four classic problems in medical diagnostics involving data which clearly involved personally identifiable elements. These were: 1) prediction of sepsis; 2) de-identification of personal information in medical notes collected during routine care; 3) recognition of voices in audio recordings; and 4) &nbsp; recognition of faces in video recordings (for identifying depression). We trained state-of-the-art &lsquo;deep&rsquo; AI models for each of these problems, then &lsquo;attacked&rsquo; the models by trying to train new models to identify individuals from the millions of parameters in the models. Results show that there is no evidence that we are able to determine whether any particular individual was used in the training of a given AI model, regardless of the data type. This doesn&rsquo;t mean it isn&rsquo;t possible that someone could eventually develop some method to identify an individual, but it does not seem likely. This is important, because it means we can feel relatively safe that those who use our medical data to develop powerful models that help push medicine forward are not significantly risking the leakage of our identities, especially when compared to other entities that hold our data for commercial reasons. This is an exciting result that may help push the field of medical research forward.&nbsp; However, the results do not indicate there is little to no risk for all AI models, and in particular, poorly trained models that are overfit on the training data may pose a significant risk of revealing the identities of individuals. It is therefore vital we continue to test models to ensure we are preserving the identity of those whose medical data is critical to the future development of important clinical models.</span></p><br> <p>            Last Modified: 07/14/2021<br>      Modified by: Gari&nbsp;D&nbsp;Clifford</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ For legal and moral reasons, those who collect medical data are required to protect the identity of the individuals from whom the data is captured. De-identification of the data is cumbersome, and imperfect. For this reason, medical information is often siloed at the institution at which is was collected, reducing the benefit to medical research, or the utility to the medical teams and patients. Moreover, moving personal data across international or domestic borders is often prohibited. One strong approach to addressing this issue is to build models and use ‘transfer learning’ to move the models, rather than the data, so that a model can be updated on a series of databases in a distributed manner. However, modern artificial intelligence (AI), or more specifically, machine learning, can allow the creation of powerful and complex models to analyze health data. However, because there are sometimes millions of parameters in these models, there’s the chance that they could encode some information that allows users of the technology to uncover the identity of the individuals used either to train the models, or on whom the models are used. This research aimed at determining the likelihood that such models do not encode individual information, and therefore can be move across borders. To test this idea, we took four classic problems in medical diagnostics involving data which clearly involved personally identifiable elements. These were: 1) prediction of sepsis; 2) de-identification of personal information in medical notes collected during routine care; 3) recognition of voices in audio recordings; and 4)   recognition of faces in video recordings (for identifying depression). We trained state-of-the-art ‘deep’ AI models for each of these problems, then ‘attacked’ the models by trying to train new models to identify individuals from the millions of parameters in the models. Results show that there is no evidence that we are able to determine whether any particular individual was used in the training of a given AI model, regardless of the data type. This doesn’t mean it isn’t possible that someone could eventually develop some method to identify an individual, but it does not seem likely. This is important, because it means we can feel relatively safe that those who use our medical data to develop powerful models that help push medicine forward are not significantly risking the leakage of our identities, especially when compared to other entities that hold our data for commercial reasons. This is an exciting result that may help push the field of medical research forward.  However, the results do not indicate there is little to no risk for all AI models, and in particular, poorly trained models that are overfit on the training data may pose a significant risk of revealing the identities of individuals. It is therefore vital we continue to test models to ensure we are preserving the identity of those whose medical data is critical to the future development of important clinical models.       Last Modified: 07/14/2021       Submitted by: Gari D Clifford]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
