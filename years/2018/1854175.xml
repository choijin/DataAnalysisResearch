<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Convergence HTF: Collaborative: Workshop on Convergence Research about Multimodal Human Learning Data during Human Machine Interactions</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2018</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>12515.00</AwardTotalIntnAmount>
<AwardAmount>12515</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07030000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>CMMI</Abbreviation>
<LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jordan Berg</SignBlockName>
<PO_EMAI>jberg@nsf.gov</PO_EMAI>
<PO_PHON>7032925365</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Intelligent, interactive, and highly networked machines -- with which people increasingly share their autonomy and agency -- are a growing part of the landscape, particularly in regard to work.  As automation today moves from the factory floor to knowledge and service occupations, insight and action are needed to reap the benefits in increased productivity and increased job opportunities, and to mitigate social costs. Such innovations also have significant implications and potential value for lifelong learning, skills assessments, and job training/retraining in an environment in which workforce demands are changing rapidly. The workshop supported by this award will promote the convergence of cognitive psychology, learning sciences, data science, computer science, and engineering disciplines to define key challenges and research imperatives of the nexus of humans, technology, and work with focus on human affect, motivation, metacognition, and cognition during learning and problem solving. Convergence is the deep integration of knowledge, theories, methods, and data from multiple fields to form new and expanded frameworks for addressing scientific and societal challenges and opportunities. This convergence workshop addresses the future of work at the human-technology frontier.&lt;br/&gt;&lt;br/&gt;The specific focus of this multi-phased workshop approach is to advance fundamental understanding of how to collect and analyze multimodal, multichannel sensor on human affect, motivation, metacognition, and cognition during learning and problem solving, and effectively integrate this data into actionable educational interventions in advanced learning technology environments (e.g., intelligent tutoring systems). The impacts of this research extend to a diverse range of learning environments, and job training and retraining opportunities. A multi-phased workshop approach will be used to explore the implications in multiple job sectors, and the outcomes will be broadly disseminated across geographic and disciplinary boundaries.</AbstractNarration>
<MinAmdLetterDate>09/21/2018</MinAmdLetterDate>
<MaxAmdLetterDate>09/21/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1854175</AwardID>
<Investigator>
<FirstName>Roger</FirstName>
<LastName>Azevedo</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Roger Azevedo</PI_FULL_NAME>
<EmailAddress>roger.azevedo@ucf.edu</EmailAddress>
<PI_PHON>9193021867</PI_PHON>
<NSF_ID>000206946</NSF_ID>
<StartDate>09/21/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>The University of Central Florida Board of Trustees</Name>
<CityName>Orlando</CityName>
<ZipCode>328168005</ZipCode>
<PhoneNumber>4078230387</PhoneNumber>
<StreetAddress>4000 CNTRL FLORIDA BLVD</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>150805653</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CENTRAL FLORIDA BOARD OF TRUSTEES, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Central Florida]]></Name>
<CityName/>
<StateCode>FL</StateCode>
<ZipCode>328168005</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1340</Code>
<Text>EngEd-Engineering Education</Text>
</ProgramElement>
<ProgramReference>
<Code>060Z</Code>
<Text>Convergent Research</Text>
</ProgramReference>
<ProgramReference>
<Code>063Z</Code>
<Text>FW-HTF Futr Wrk Hum-Tech Frntr</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~12515</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our NSF&nbsp;Convergence HDR project&nbsp;focused on interdisciplinary research about complex human learning data during human-machine interactions. Studying methods for effective and accurate collection and analysis of multimodal human data during learning and problem-solving in advanced learning technology (ALT) environments (e.g., intelligent tutoring systems, serious games, immersive virtual learning environments) was the focus of our project. Our two workshops made substantial advances in the fundamental understanding of how to integrate multimodal sensor data that contains information about human affect, motivation, metacognition, and cognition across multiple data channels (e.g., log files, eye tracking, facial expressions of emotions, physiological sensors, screen recordings, concurrent verbalizations) into actionable educational interventions such as providign real-time intelligent scaffolding and feedback that support self-regulated human learning in ALT environments.</p> <p>Our interdisciplinary team has disseminated the results of this project to several communities. First, we have communicated our results to fellow interdisciplinary researchers, academics, industry partners, and educational leaders at several national and international conferences in the last two years. Second, we have shared our results with young scholars such as our postdoctoral fellows and graduate students in our respective research laboratories. Third, we have discussed our results with our and other NSF program officers in terms of securing additional funding to pursue this type of research. Lastly, we have strategically disseminated our results to educational and industry partners to emphasize the advantages of using multimodal multichannel data to design, develop, and implement intelligent learning technologies to support STEM learning in K-12 and the workforce.</p> <p>In terms of impact on society, our workshop is important in terms of understanding the nature of humans&rsquo; self-regulated learning and making these processes visible to them, parents, teachers, administrators, and other stakeholders (e.g., trainers), and leaning technologies (to make them intelligent). Self-regulated learning is fundamental to all learning and our findings and future work in the area stands to contribute, advance, and transform how people learn.</p><br> <p>            Last Modified: 03/10/2020<br>      Modified by: Roger&nbsp;Azevedo</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our NSF Convergence HDR project focused on interdisciplinary research about complex human learning data during human-machine interactions. Studying methods for effective and accurate collection and analysis of multimodal human data during learning and problem-solving in advanced learning technology (ALT) environments (e.g., intelligent tutoring systems, serious games, immersive virtual learning environments) was the focus of our project. Our two workshops made substantial advances in the fundamental understanding of how to integrate multimodal sensor data that contains information about human affect, motivation, metacognition, and cognition across multiple data channels (e.g., log files, eye tracking, facial expressions of emotions, physiological sensors, screen recordings, concurrent verbalizations) into actionable educational interventions such as providign real-time intelligent scaffolding and feedback that support self-regulated human learning in ALT environments.  Our interdisciplinary team has disseminated the results of this project to several communities. First, we have communicated our results to fellow interdisciplinary researchers, academics, industry partners, and educational leaders at several national and international conferences in the last two years. Second, we have shared our results with young scholars such as our postdoctoral fellows and graduate students in our respective research laboratories. Third, we have discussed our results with our and other NSF program officers in terms of securing additional funding to pursue this type of research. Lastly, we have strategically disseminated our results to educational and industry partners to emphasize the advantages of using multimodal multichannel data to design, develop, and implement intelligent learning technologies to support STEM learning in K-12 and the workforce.  In terms of impact on society, our workshop is important in terms of understanding the nature of humansâ€™ self-regulated learning and making these processes visible to them, parents, teachers, administrators, and other stakeholders (e.g., trainers), and leaning technologies (to make them intelligent). Self-regulated learning is fundamental to all learning and our findings and future work in the area stands to contribute, advance, and transform how people learn.       Last Modified: 03/10/2020       Submitted by: Roger Azevedo]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
