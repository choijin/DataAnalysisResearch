<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: Principled Methods for Learning and Understanding of Neural Networks</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2019</AwardEffectiveDate>
<AwardExpirationDate>04/30/2022</AwardExpirationDate>
<AwardTotalIntnAmount>175000.00</AwardTotalIntnAmount>
<AwardAmount>175000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rebecca Hwa</SignBlockName>
<PO_EMAI>rhwa@nsf.gov</PO_EMAI>
<PO_PHON>7032927148</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Deep neural networks have elicited breakthrough successes in machine learning by achieving impressive accuracies on diverse tasks such as facial recognition, object identification, anomaly detection and monitoring assistance on a large scale. However, deep neural networks are not theoretically guaranteed to always perform well, and they could, although rarely, fail in the presence of previously unseen data or small/imperceptible (adversarial) changes to the data. For instance, a home security system using a deep neural network facial recognition algorithm could mistake a stranger wearing pixelated sunglasses for the homeowner; a slight change of the environment, such as a rainy day, could cause a computer vision based autonomous driving vehicle to wrongly recognize a "STOP" sign as an outdoor commercial sign. The existence of such failure cases in widely used machine learning systems today could put our daily lives and even national security at risk. One way to make machine learning systems robust against these failure cases is to design algorithms that are guaranteed to provide an optimal solution, generalize to unseen scenarios and be robust to adversarial changes even if the attacker is given full knowledge of the algorithm. The methods developed via this research will provide theoretical bases that explain "black-box" deep neural networks and provide guarantees over their performance when applied to high-stakes problems. The project will be integrated with graduate and undergraduate education, fostering collaboration between researchers from Computer Science, Applied Math, Physics and Business. Software programs developed via this project will be released as an open-source toolkit, allowing widespread dissemination to researchers and practitioners in a range of fields.&lt;br/&gt;&lt;br/&gt;This project will advocate theoretically guaranteed training and understanding of neural networks via techniques from learning theory, nonconvex optimization and consistent latent variable model learning using spectral methods. The investigator's goal is to design compressed neural networks that are theoretically guaranteed to generalize well, fit into Internet of Things devices with memory constraints, and are robust to adversarial examples. Concretely, the technical aims of the project are divided into three thrusts.  (1) Guaranteed training of deep nets. The investigator proposes to develop a theoretical justification of why deep residual networks are easier to optimize than non-residual ones when each layer provides a better-than-a-weak-baseline oracle in predicting labels. The investigator plans to use two approaches to guarantee existence and implement the better-than-a-weak-baseline oracle: (a) exploiting theoretically guaranteed training of shallow convolutional neural networks, a.k.a. convolutional dictionary learning, using spectral methods and (b) ensuring escaping from local optima using Homotopy transformations to "sharpen" local optima of network's objective landscape as recent advances in escaping from local optima showed that SGD will not get stuck at sharp local optima with small diameters. (2) Analyzing generalization ability of compressed deep neural networks. The investigator will introduce deep neural network compression using tensorized tensor decomposition, and develop tighter bounds for generalization error, which takes the input distribution and the compressibility of the network into account.  (3) Reliable deep neural networks robust to the worst attackers. To provide a universal defense mechanism against the worst possible adversarial examples using a minimax formulation, the investigator proposes to analyze the robustness of nonlinear single-layer neural nets using tensor decomposition method and ultimately design universal defense mechanisms for deep neural nets.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/29/2019</MinAmdLetterDate>
<MaxAmdLetterDate>03/29/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1850220</AwardID>
<Investigator>
<FirstName>Furong</FirstName>
<LastName>Huang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Furong Huang</PI_FULL_NAME>
<EmailAddress>furongh@cs.umd.edu</EmailAddress>
<PI_PHON>3014058010</PI_PHON>
<NSF_ID>000762350</NSF_ID>
<StartDate>03/29/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~175000</FUND_OBLG>
</Award>
</rootTag>
