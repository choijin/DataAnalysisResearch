<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Tensor Network Computation: Representations, Algebra, and Applications</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2018</AwardEffectiveDate>
<AwardExpirationDate>06/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>180000.00</AwardTotalIntnAmount>
<AwardAmount>180000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Christopher Stark</SignBlockName>
<PO_EMAI>cstark@nsf.gov</PO_EMAI>
<PO_PHON>7032924869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Modern data collection and computational simulations produce many high-dimensional data sets. Such high-dimensional data sets represent one of the key challenges in computational mathematics. As is well-known, the main difficulty lies in the curse of the dimensionality, that is, the number of degrees of freedom required to represent and analyze high-dimensional functions in the traditional way grows exponentially with the number of dimensions. Recently, tensor networks have emerged, mostly from the theoretical and computational physics community, as a promising tool for representing and manipulating high-dimensional functions and probabilities.  This project undertakes a systematic computational study of the tensor network approach. The investigation serves as an initial step in providing a general framework for work with high-dimensional data sets. &lt;br/&gt;&lt;br/&gt;The project focuses on four aspects of tensor networks. (1) The investigator aims to improve on recent work in tensor network skeletonization, where the main task is to develop efficient tensor contraction algorithms for inhomogeneous systems and general tensor networks, with an emphasis on parallel implementation. (2) The investigator plans to develop novel algorithms for constructing a tensor network either via sampling or from an existing but redundant tensor representation. The approach will draw tools and ideas from randomized numerical algebra, nonlinear optimization, and multiscale methods. (3) The project will develop algorithms for basic algebraic operations for manipulating high-dimensional functions in the tensor network representation. These basic operations include addition, subtraction, entry-wise multiplication, entry-wise inversion, entry-wise function application, and application of linear operators in tensor network operator form. (4) The last objective is to investigate new applications of tensor networks outside the traditional realm of statistical and quantum mechanics, for example in numerical homogenization, uncertainty quantification in very high dimension, and high-dimensional partial differential equations in control and molecular dynamics.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/14/2018</MinAmdLetterDate>
<MaxAmdLetterDate>06/19/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1818449</AwardID>
<Investigator>
<FirstName>Lexing</FirstName>
<LastName>Ying</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lexing Ying</PI_FULL_NAME>
<EmailAddress>lexing@math.stanford.edu</EmailAddress>
<PI_PHON>6507232221</PI_PHON>
<NSF_ID>000148894</NSF_ID>
<StartDate>06/14/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Stanford University</Name>
<CityName>Stanford</CityName>
<ZipCode>943052004</ZipCode>
<PhoneNumber>6507232300</PhoneNumber>
<StreetAddress>450 Jane Stanford Way</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009214214</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>LELAND STANFORD JUNIOR UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009214214</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Stanford University]]></Name>
<CityName>Stanford</CityName>
<StateCode>CA</StateCode>
<ZipCode>943052125</ZipCode>
<StreetAddress><![CDATA[450 Serra Mall, Bldg 380]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramReference>
<Code>062Z</Code>
<Text>Harnessing the Data Revolution</Text>
</ProgramReference>
<ProgramReference>
<Code>8083</Code>
<Text>Big Data Science &amp;Engineering</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~58173</FUND_OBLG>
<FUND_OBLG>2019~59856</FUND_OBLG>
<FUND_OBLG>2020~61971</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>In the past decade, tensor networks (TNs) have emerged, mostly from the theoretical andcomputational physics community, as a promising tool for representing and manipulating highdimensional functions and probabilities. The tensor network approach is designed for thehigh-dimensional functions such that the DOFs of the function have a geometric structure and thatthe dependence among geometrically nearby DOFs dominates. Motivated by these assumptions, the tensornetwork approach represents a high-dimensional function as the product of a set of low-ordertensors, one for each vertex of the graph, contracted along the edges.</p> <p><br />Intellectual merits. In this project, we have developed and improved tensor network skeletonization,where the main task is to develop efficient tensor contraction algorithms for rather tensornetworks. We have also designed and implemented efficient algorithms for constructing tensor ringrepresentations from samples. A closely related approach for representing high-dimensional tensorsis through projection to multiple marginals. In this direction, we have also developed an efficientconvex relaxation approach can be solved by modern semidefinite programming (SDP) tools. Using thisapproach, we have solved strictly correlated electron problems both in density function theory andin the second quantization.</p> <p><br />Broader impacts. This project have overlapped with several challenging computational problems instatistical mechanics and quantum many-body systems. One postdoc and several graduate students weresupported via this grant for their research work.</p> <p>&nbsp;</p><br> <p>            Last Modified: 07/16/2021<br>      Modified by: Lexing&nbsp;Ying</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ In the past decade, tensor networks (TNs) have emerged, mostly from the theoretical andcomputational physics community, as a promising tool for representing and manipulating highdimensional functions and probabilities. The tensor network approach is designed for thehigh-dimensional functions such that the DOFs of the function have a geometric structure and thatthe dependence among geometrically nearby DOFs dominates. Motivated by these assumptions, the tensornetwork approach represents a high-dimensional function as the product of a set of low-ordertensors, one for each vertex of the graph, contracted along the edges.   Intellectual merits. In this project, we have developed and improved tensor network skeletonization,where the main task is to develop efficient tensor contraction algorithms for rather tensornetworks. We have also designed and implemented efficient algorithms for constructing tensor ringrepresentations from samples. A closely related approach for representing high-dimensional tensorsis through projection to multiple marginals. In this direction, we have also developed an efficientconvex relaxation approach can be solved by modern semidefinite programming (SDP) tools. Using thisapproach, we have solved strictly correlated electron problems both in density function theory andin the second quantization.   Broader impacts. This project have overlapped with several challenging computational problems instatistical mechanics and quantum many-body systems. One postdoc and several graduate students weresupported via this grant for their research work.          Last Modified: 07/16/2021       Submitted by: Lexing Ying]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
