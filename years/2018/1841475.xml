<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Data Infrastructure for Open Science in Support of LIGO and IceCube</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2018</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>146937.00</AwardTotalIntnAmount>
<AwardAmount>146937</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>William Miller</SignBlockName>
<PO_EMAI>wlmiller@nsf.gov</PO_EMAI>
<PO_PHON>7032927886</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In 2015, the NSF-funded LIGO Observatory made the first-ever detection of gravitational waves, from the collision of two black holes, a discovery that was recognized by the 2017 Nobel Prize in Physics. In 2017, LIGO and its sister observatory Virgo in Italy made the first detection of gravitational waves from another extreme event in the Universe - the collision of two neutron stars. Gamma rays from the same neutron star collision were also simultaneously detected by NASA's Fermi space telescope. Meanwhile, the NSF-funded IceCube facility, located at the U.S. South Pole Station, has made the first detection of high-energy neutrinos from beyond our galaxy, giving us unobstructed views of other extreme objects in Universe such as supermassive black holes and supernova remnants. The revolutionary ability to observe gravitational waves, neutrinos, and optical and radio waves from the same celestial events has launched the era of "Multi-Messenger Astrophysics," an exciting new field supported by one of NSF's ten Big Ideas, "Windows on the Universe".&lt;br/&gt;&lt;br/&gt;The success of Multi-Messenger Astrophysics depends on building new data infrastructure to seamlessly share, integrate, and analyze data from many large observing instruments. The investigators propose a cohesive, federated, national-scale research data infrastructure for large instruments, focused initially on LIGO and IceCube, to address the need to access, share, and combine science data, and make the entire data processing life cycle more robust. The novel working model of the project is a multi-institutional collaboration comprising the LIGO and IceCube observatories, Internet2, and platform integration experts. The investigators will conduct a fast-track two-year effort that draws heavily on prior and concurrent NSF investments in software, computing and data infrastructure, and international software developments including at CERN.  Internet2 will establish data caches inside the national network backbone to optimize the LIGO data analysis. The goal is to achieve a data infrastructure platform that addresses the production needs of LIGO and IceCube while serving as an exemplar for the entire scope of Multi-messenger Astrophysics and beyond. In the process, the investigators are prototyping a redefinition of the role the academic internet plays in supporting science.&lt;br/&gt;&lt;br/&gt;This project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer and Information Science and Engineering.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/07/2018</MinAmdLetterDate>
<MaxAmdLetterDate>09/07/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1841475</AwardID>
<Investigator>
<FirstName>Laura</FirstName>
<LastName>Cadonati</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Laura Cadonati</PI_FULL_NAME>
<EmailAddress>cadonati@gatech.edu</EmailAddress>
<PI_PHON>4048945201</PI_PHON>
<NSF_ID>000069965</NSF_ID>
<StartDate>09/07/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Ave NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7684</Code>
<Text>CESER-Cyberinfrastructure for</Text>
</ProgramElement>
<ProgramReference>
<Code>020Z</Code>
<Text>OAC Facility Cyberinfrastructure</Text>
</ProgramReference>
<ProgramReference>
<Code>062Z</Code>
<Text>Harnessing the Data Revolution</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~146937</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-c68073c4-7fff-7274-de80-23ab95269c11"> <span id="docs-internal-guid-2a8bf0da-7fff-3758-a787-d3cc86df83e8"> </span></span></p> <p dir="ltr"><span>This collaborative grant has supported the gravitational wave community (LIGO, Virgo and users of their public available data) and the IceCube collaboration in the adoption of shared tools for common data infrastructure problems, and integration of data and computing resources from global partners and collaborators. The grant also explored the extent to which tools developed in the particle physics LHC community are good fits for the data infrastructure needs of LIGO/Virgo and IceCube, and more broadly for sharing of data in Multi-Messenger Astrophysics and beyond.&nbsp; Our team at GeorgiaTech focused on the deployment of a global content delivery network and expansion of the GPU capacity for LIGO, for a better global resource and data integration which allows for more simulations and analysis and ultimately enables better delivery of science.&nbsp;</span></p> <p dir="ltr"><span>We deployed new LIGO/Virgo workflows onto the Open Science Grid (OSG), which are now routinely used in the analysis of gravitational wave data from LIGO/Virgo: "BayesWave", a flagship algorithm for the robust detection and characterization of unmodeled gravitational wave transients, "RIFT", a GPU-enabled algorithm for rapid characterization of compact binary coalescence signals and "Powerflux", a multicore algorithm for the all-sky search for continuous gravitational wave signals from previously unknown neutron stars.&nbsp; The opportunity to leverage and test GPU resources by the RIFT analysis establishes a model for the deployment of future GPU analyses in OSG. The deployment of these workflows to the OSG was timely: as LIGO and Virgo were accumulating data from their third observing run, much larger-scale background and simulation runs were needed to better determine the statistical significance and properties of near-weekly gravitational wave detections.&nbsp;&nbsp;&nbsp;</span></p> <p dir="ltr"><span>We introduced Rucio as a data management system for archival LIGO data, and transitioned knowledge of how to operate them to the LIGO-Virgo-KAGRA collaboration. We architected a solution for the application of Rucio to LIGO data management and developed software infrastructure for its implementation. Having established the satisfactory utility and performance of Rucio for LIGO's needs, this system is now configured to operate at a production level and was used for data replication between the US (LIGO), Italy (Virgo) and Japan (KAGRA). This also serves as a demonstration of the deployment of critical service infrastructure to cloud resources.</span></p> <p dir="ltr"><span>We deployed a total of thirteen data caches for the OSG. In addition to the three caches mentioned in the proposal (Kansas City, Chicago, NYC), caches in Amsterdam, Cardiff, KISTI (South Korea), UCSD, and Georgia Tech have been deployed in support of LIGO/Virgo operations. These are in addition to caches in Madison, Nebraska, Syracuse, Chicago, PIC (Barcelona), CNAF (Bologna) and an older, smaller cache at UCSD that preceded this project.&nbsp; We have devised a test to continuously check the health and availability of the LIGO compute sites and automated it for online reporting. We created a Grafana dashboard to aggregate the cache choices for pilots at sites and understand how far the pilots are from the closest cache. By the end of this award, 6.8 PB of data were read from the caches, 13.4 TB of data were written, benefitting 9 LIGO/Virgo detection papers from their third observational run.</span></p> <p dir="ltr"><span><br /></span></p> <p dir="ltr">&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/10/2021<br>      Modified by: Laura&nbsp;Cadonati</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    This collaborative grant has supported the gravitational wave community (LIGO, Virgo and users of their public available data) and the IceCube collaboration in the adoption of shared tools for common data infrastructure problems, and integration of data and computing resources from global partners and collaborators. The grant also explored the extent to which tools developed in the particle physics LHC community are good fits for the data infrastructure needs of LIGO/Virgo and IceCube, and more broadly for sharing of data in Multi-Messenger Astrophysics and beyond.  Our team at GeorgiaTech focused on the deployment of a global content delivery network and expansion of the GPU capacity for LIGO, for a better global resource and data integration which allows for more simulations and analysis and ultimately enables better delivery of science.  We deployed new LIGO/Virgo workflows onto the Open Science Grid (OSG), which are now routinely used in the analysis of gravitational wave data from LIGO/Virgo: "BayesWave", a flagship algorithm for the robust detection and characterization of unmodeled gravitational wave transients, "RIFT", a GPU-enabled algorithm for rapid characterization of compact binary coalescence signals and "Powerflux", a multicore algorithm for the all-sky search for continuous gravitational wave signals from previously unknown neutron stars.  The opportunity to leverage and test GPU resources by the RIFT analysis establishes a model for the deployment of future GPU analyses in OSG. The deployment of these workflows to the OSG was timely: as LIGO and Virgo were accumulating data from their third observing run, much larger-scale background and simulation runs were needed to better determine the statistical significance and properties of near-weekly gravitational wave detections.    We introduced Rucio as a data management system for archival LIGO data, and transitioned knowledge of how to operate them to the LIGO-Virgo-KAGRA collaboration. We architected a solution for the application of Rucio to LIGO data management and developed software infrastructure for its implementation. Having established the satisfactory utility and performance of Rucio for LIGO's needs, this system is now configured to operate at a production level and was used for data replication between the US (LIGO), Italy (Virgo) and Japan (KAGRA). This also serves as a demonstration of the deployment of critical service infrastructure to cloud resources. We deployed a total of thirteen data caches for the OSG. In addition to the three caches mentioned in the proposal (Kansas City, Chicago, NYC), caches in Amsterdam, Cardiff, KISTI (South Korea), UCSD, and Georgia Tech have been deployed in support of LIGO/Virgo operations. These are in addition to caches in Madison, Nebraska, Syracuse, Chicago, PIC (Barcelona), CNAF (Bologna) and an older, smaller cache at UCSD that preceded this project.  We have devised a test to continuously check the health and availability of the LIGO compute sites and automated it for online reporting. We created a Grafana dashboard to aggregate the cache choices for pilots at sites and understand how far the pilots are from the closest cache. By the end of this award, 6.8 PB of data were read from the caches, 13.4 TB of data were written, benefitting 9 LIGO/Virgo detection papers from their third observational run.              Last Modified: 05/10/2021       Submitted by: Laura Cadonati]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
