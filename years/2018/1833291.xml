<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CSR: Medium: Collaborative Research: Portable Performance for Parallel Managed Languages Across the Many-Core Spectrum</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/13/2017</AwardEffectiveDate>
<AwardExpirationDate>05/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>9100.00</AwardTotalIntnAmount>
<AwardAmount>9100</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Computers with many tens to hundreds of ?cores? are on their way, but programming languages and tools&lt;br/&gt;that exploit them well have lagged. At the same time, there are emerging programming languages intended&lt;br/&gt;for writing programs to run on these computers. These languages, such as X10 and Fortress, add support for&lt;br/&gt;new concepts that make it easier to write many-core programs, but there does not yet exist good compiler and&lt;br/&gt;run-time support for these languages. Systems that run Java, namely Java virtual machines such as those that&lt;br/&gt;run on virtually every laptop, desktop, and server today, supply much of what the new languages need, but&lt;br/&gt;fall short in some important ways. In particular they do not provide for saying in which part of memory to&lt;br/&gt;place particular objects, on which core to run which computations, easy ways to get all cores busy working&lt;br/&gt;on different parts of a big piece of data, or for synchronizing and getting right all the data manipulations&lt;br/&gt;happening at the same time. This project is extending an existing research Java virtual machine (Jikes&lt;br/&gt;RVM) with support for many ways of doing the things that the new languages need in order to run well&lt;br/&gt;on many-core computers. The primary goal is to devise extensions to standard Java virtual machines for&lt;br/&gt;this new world, and to make it possible for many others to experiment with different ways of implementing&lt;br/&gt;these extensions, thus leveraging the creativity of the whole community of language and virtual machine&lt;br/&gt;researchers. Secondary goals include offering reasonably good initial implementations of virtual machine&lt;br/&gt;extensions as a starting point for future research and development, and proposing specific extensions to the&lt;br/&gt;Java virtual machine specification standard.</AbstractNarration>
<MinAmdLetterDate>04/16/2018</MinAmdLetterDate>
<MaxAmdLetterDate>05/08/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1833291</AwardID>
<Investigator>
<FirstName>J. Eliot</FirstName>
<LastName>Moss</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>J. Eliot B Moss</PI_FULL_NAME>
<EmailAddress>moss@cs.umass.edu</EmailAddress>
<PI_PHON>4135454206</PI_PHON>
<NSF_ID>000261930</NSF_ID>
<StartDate>05/08/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Antony</FirstName>
<LastName>Hosking</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Antony L Hosking</PI_FULL_NAME>
<EmailAddress>hosking@cs.umass.edu</EmailAddress>
<PI_PHON>7654163068</PI_PHON>
<NSF_ID>000107257</NSF_ID>
<StartDate>04/16/2018</StartDate>
<EndDate>05/08/2018</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Massachusetts Amherst</Name>
<CityName>Hadley</CityName>
<ZipCode>010359450</ZipCode>
<PhoneNumber>4135450698</PhoneNumber>
<StreetAddress>Research Administration Building</StreetAddress>
<StreetAddress2><![CDATA[100 Venture Way, Suite 201]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>153926712</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MASSACHUSETTS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>079520631</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Massachusetts Amherst]]></Name>
<CityName>Hadley</CityName>
<StateCode>MA</StateCode>
<ZipCode>010359450</ZipCode>
<StreetAddress><![CDATA[100 Venture Way]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7354</Code>
<Text>COMPUTER SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~9100</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 1"> <div class="layoutArea"> <div class="column"> <p>This project was a supplement to offer Research Experiences for Undergraduates.&nbsp; Two undergraduates helped run benchmark programs and collect useful detailed information about actions the programs perform as they run, which informs performance analysis and prediction of future performance.&nbsp; The information cllected to date is only for a subset of the available benchmarks and inputs in the suite used, so it would be helpful to continue this project to complete the set of information.</p> <p>The set of programs: An independent industry consortium, the Systems Performance Evaluation Corporation (SPEC), publishes a range of benchmark suites.&nbsp; We developed analyses some years ago of the SPEC CPU 2006 suite, and the present project repeats and extends that work with the more modern SPEC CPU 2017 suite.</p> <p>Program inputs: Each SPEC benchmark offers three "sizes" of inputs: test, train, and reference (ref).&nbsp; These are in rough increase in length/time.&nbsp; So far we have collected test and train from every benchmark and are in the middle of collecting the ref runs, which are significantly larger (weeks for each run in our measurement framework).&nbsp; In addition, Prof. Nelson Amaral of the Univ of Alberta has assembled a collection of additional inputs, called the Alberta Workloads.&nbsp; We wish also to measure these - the larger a coherent collection one has, the better analysis and prediction one is likely to achieve.</p> </div> </div> </div><br> <p>            Last Modified: 09/30/2019<br>      Modified by: J. Eliot&nbsp;B&nbsp;Moss</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    This project was a supplement to offer Research Experiences for Undergraduates.  Two undergraduates helped run benchmark programs and collect useful detailed information about actions the programs perform as they run, which informs performance analysis and prediction of future performance.  The information cllected to date is only for a subset of the available benchmarks and inputs in the suite used, so it would be helpful to continue this project to complete the set of information.  The set of programs: An independent industry consortium, the Systems Performance Evaluation Corporation (SPEC), publishes a range of benchmark suites.  We developed analyses some years ago of the SPEC CPU 2006 suite, and the present project repeats and extends that work with the more modern SPEC CPU 2017 suite.  Program inputs: Each SPEC benchmark offers three "sizes" of inputs: test, train, and reference (ref).  These are in rough increase in length/time.  So far we have collected test and train from every benchmark and are in the middle of collecting the ref runs, which are significantly larger (weeks for each run in our measurement framework).  In addition, Prof. Nelson Amaral of the Univ of Alberta has assembled a collection of additional inputs, called the Alberta Workloads.  We wish also to measure these - the larger a coherent collection one has, the better analysis and prediction one is likely to achieve.          Last Modified: 09/30/2019       Submitted by: J. Eliot B Moss]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
