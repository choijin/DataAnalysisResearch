<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Pilot Study on Bias and Trust in AI Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2018</AwardEffectiveDate>
<AwardExpirationDate>02/29/2020</AwardExpirationDate>
<AwardTotalIntnAmount>74529.00</AwardTotalIntnAmount>
<AwardAmount>74529</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Robots and other autonomous systems are proliferating rapidly, and are being used in the public domain for such purposes as companionship, facilitating interactive learning, and making recommendations to users. However, little is understood about how trust, including mistrust and over-trust, and bias develop as humans interact with these systems. The investigators of this project will use basic real-life scenarios to understand human-human interaction in the context of trust and bias, and then will translate these data into quantized attributes.  This data will be used to develop general algorithms that will be useful in programming robots in ways that ensure human safety and well-being when interacting with robots. This project will conduct a series of pilot tests to explore fundamental research questions to find ways to minimize potential negative impacts and consequences of human-robot interaction. Additionally, the investigators will integrate the research with teaching and training of graduate and undergraduate students.&lt;br/&gt;&lt;br/&gt;The aim of this project is to gain knowledge that informs the design of future robot systems by understanding how trust is established and how bias impacts human perception and algorithmic performance. As part of this work, the team also aims to establish baseline algorithms that mitigate the impacts of bias in autonomous systems. The specific research objectives of this project are to quantify the impact of trust and bias in human-robot interaction scenarios where the algorithms are designed based on learned data from human experts; and to develop methods for objectively mitigating bias, while still optimizing for robot performance.  The pilot tests are designed to contribute to understanding of social cognition in human-robot interaction. Direct societal befits will be gained as robots and other autonomous systems proliferate in the public domain.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/15/2018</MinAmdLetterDate>
<MaxAmdLetterDate>09/15/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1849101</AwardID>
<Investigator>
<FirstName>Jason</FirstName>
<LastName>Borenstein</LastName>
<PI_MID_INIT>D</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jason D Borenstein</PI_FULL_NAME>
<EmailAddress>jason.borenstein@pubpolicy.gatech.edu</EmailAddress>
<PI_PHON>4043852801</PI_PHON>
<NSF_ID>000265890</NSF_ID>
<StartDate>09/15/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ayanna</FirstName>
<LastName>Howard</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ayanna M Howard</PI_FULL_NAME>
<EmailAddress>ah260@gatech.edu</EmailAddress>
<PI_PHON>4043854824</PI_PHON>
<NSF_ID>000108981</NSF_ID>
<StartDate>09/15/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>097394084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>GEORGIA TECH RESEARCH CORPORATION</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>097394084</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303320002</ZipCode>
<StreetAddress><![CDATA[225 North Avenue, NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~74529</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>There is a strong and ongoing push for robots to be used in more and more sectors in society. Key ethical problems related to the current use of robots and Artificial Intelligence (AI) are trust and bias; these problems are interconnected and must be addressed.&nbsp; Thus, the primary aim of this project was to obtain knowledge that could inform the design of future robotic and AI systems. &nbsp;Key objectives were to 1) quantify the impact of trust and bias in scenarios where AI algorithms have been trained based on historical data obtained from human beings and 2) develop algorithms that could lessen biases found in the resulting algorithms. In order to accomplish these objectives, the research team conducted four main studies.&nbsp; Results from these studies were published in a series of publications.&nbsp;</p> <p>The first study sought to quantify biases found in facial recognition algorithms, specifically those that might be used to scan the faces of children.&nbsp; The team found that emotion recognition systems display subpar performance on datasets of children's expressions. Based on these findings, the team developed a preliminary matrix that can be utilized for assessing whether there is sufficient diversity of people represented in a set of images used as a basis for emotion recognition.</p> <p>The second study involved quantifying the level of trust clinicians and parents have in pediatric robotic exoskeletons, specifically a robotic device attached to the child?s legs that can assist with their movements. &nbsp;Included in this assessment were clinicians that treated children who had any form of disability that affected movement, muscle control, and/or balance. Through study results, the team discovered that most clinicians were somewhat to very concerned that a child might not safely use a robotic exoskeleton outside of the clinical setting. Parents, on the other hand, reported higher trust (in other words, lower concern) about their child using an exoskeleton outside of the clinical setting.</p> <p>The third study sought to develop methods to mitigate bias by changing how robots communicate with people.&nbsp; This study examined the best way for a robot to communicate information to people while they performed a memory-based task and the impact on their performance if the robot provided incorrect information during the task. &nbsp;&nbsp;Findings indicated that the best form of communication from a robot, after it makes a mistake, occurs when a person directly asks the robot for help.</p> <p>The fourth study examined the impact of robot gender on people?s perception of a robot?s ability to perform a task. The research team discovered that perceived job competency is a better predictor for human trust than robot gender or the gender of the participant in the study. As such, developers should critically reconsider how they design robots in order to reduce the potential for the technology to perpetuate gender stereotypes.</p> <p>Results from this collection of research are designed to make important contributions by providing methods for understanding and quantifying the potential harmful effects that robots could have on society and providing a foundation for increasing the technology?s positive benefits. Project results could have significant value especially considering that placing too much trust in robots may place vulnerable populations at risk, including children and those with disabilities.</p><br> <p>            Last Modified: 05/01/2020<br>      Modified by: Ayanna&nbsp;M&nbsp;Howard</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1849101/1849101_10583171_1588375736153_GenderStudy--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1849101/1849101_10583171_1588375736153_GenderStudy--rgov-800width.jpg" title="Gender Study for Robot"><img src="/por/images/Reports/POR/2020/1849101/1849101_10583171_1588375736153_GenderStudy--rgov-66x44.jpg" alt="Gender Study for Robot"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Robot used for two of the studies: Gender and Trust Study</div> <div class="imageCredit">Georgia Institute of Technology</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Ayanna&nbsp;M&nbsp;Howard</div> <div class="imageTitle">Gender Study for Robot</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ There is a strong and ongoing push for robots to be used in more and more sectors in society. Key ethical problems related to the current use of robots and Artificial Intelligence (AI) are trust and bias; these problems are interconnected and must be addressed.  Thus, the primary aim of this project was to obtain knowledge that could inform the design of future robotic and AI systems.  Key objectives were to 1) quantify the impact of trust and bias in scenarios where AI algorithms have been trained based on historical data obtained from human beings and 2) develop algorithms that could lessen biases found in the resulting algorithms. In order to accomplish these objectives, the research team conducted four main studies.  Results from these studies were published in a series of publications.   The first study sought to quantify biases found in facial recognition algorithms, specifically those that might be used to scan the faces of children.  The team found that emotion recognition systems display subpar performance on datasets of children's expressions. Based on these findings, the team developed a preliminary matrix that can be utilized for assessing whether there is sufficient diversity of people represented in a set of images used as a basis for emotion recognition.  The second study involved quantifying the level of trust clinicians and parents have in pediatric robotic exoskeletons, specifically a robotic device attached to the child?s legs that can assist with their movements.  Included in this assessment were clinicians that treated children who had any form of disability that affected movement, muscle control, and/or balance. Through study results, the team discovered that most clinicians were somewhat to very concerned that a child might not safely use a robotic exoskeleton outside of the clinical setting. Parents, on the other hand, reported higher trust (in other words, lower concern) about their child using an exoskeleton outside of the clinical setting.  The third study sought to develop methods to mitigate bias by changing how robots communicate with people.  This study examined the best way for a robot to communicate information to people while they performed a memory-based task and the impact on their performance if the robot provided incorrect information during the task.   Findings indicated that the best form of communication from a robot, after it makes a mistake, occurs when a person directly asks the robot for help.  The fourth study examined the impact of robot gender on people?s perception of a robot?s ability to perform a task. The research team discovered that perceived job competency is a better predictor for human trust than robot gender or the gender of the participant in the study. As such, developers should critically reconsider how they design robots in order to reduce the potential for the technology to perpetuate gender stereotypes.  Results from this collection of research are designed to make important contributions by providing methods for understanding and quantifying the potential harmful effects that robots could have on society and providing a foundation for increasing the technology?s positive benefits. Project results could have significant value especially considering that placing too much trust in robots may place vulnerable populations at risk, including children and those with disabilities.       Last Modified: 05/01/2020       Submitted by: Ayanna M Howard]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
