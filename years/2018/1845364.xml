<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Towards Robust and Natural Underwater Human-Robot Interaction</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/15/2019</AwardEffectiveDate>
<AwardExpirationDate>04/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>101540.00</AwardTotalIntnAmount>
<AwardAmount>141540</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The underwater domain takes up almost four-fifths of the planet and is inherently hostile towards human exploration. However, in numerous applications in the marine environment (e.g., in surveillance, environmental monitoring, security, and search-and-rescue), human assessment is necessary for efficient and effective task completion. Current technology for underwater exploration sees limited applications of autonomous underwater vehicles (AUVs) but relies heavily on remotely operated vehicles, which unfortunately does not take advantage of robot autonomy. This project will develop novel algorithms and protocols to enable humans to communicate safely with AUVs while preserving and leveraging their autonomy. Specifically, the intent is to create novel methods for gesture- and motion-based bidirectional human-robot communication methods and enable autonomous underwater robots to detect, identify and interact with specific individuals. The research objectives will be evaluated individually and as an integrated, coherent system onboard underwater vehicles. The proposed research has the potential to create a fundamentally new direction in human-in-the-loop field robotics, with underwater robot companions being able to assist divers in a range of tasks and even learning to carry out these tasks in an autonomous manner, greatly reducing risk to humans. This research will also impact a broad range of disciplines, including human-machine dialog, machine vision, activity recognition, and robot control.&lt;br/&gt;&lt;br/&gt;This research will develop novel algorithms and protocols to enable humans to communicate safely with AUVs while preserving and leveraging their autonomy. Specific goals include: development of a gesture-based human-to-robot language with multiple communication granularities; creation of algorithms for visual identification of humans by learning from spatial and periodic cues; and development of a non-verbal, motion-based underwater robot-to-human communication scheme.  The research objectives will be evaluated individually and as an integrated, coherent system onboard underwater vehicles.  The investigation into gesture-based visual languages will quantify the detectability, usability, and efficacy of such methods in realistic settings. Statistical, convolutional and generative learning-based approaches will be applied to understand both identifiable human features and gestural communication. Additionally, robot body language and motion will be used as cues for robot-to-human non-verbal communication. These methods will be quantitatively and qualitatively validated via user studies and robot field trials to characterize their advantages and limitations.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>05/09/2019</MinAmdLetterDate>
<MaxAmdLetterDate>06/09/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1845364</AwardID>
<Investigator>
<FirstName>Junaed</FirstName>
<LastName>Sattar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Junaed Sattar</PI_FULL_NAME>
<EmailAddress>junaed@umn.edu</EmailAddress>
<PI_PHON>6126267235</PI_PHON>
<NSF_ID>000678398</NSF_ID>
<StartDate>05/09/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<StateCode>MN</StateCode>
<ZipCode>554552070</ZipCode>
<StreetAddress><![CDATA[200 UNION ST SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~117540</FUND_OBLG>
<FUND_OBLG>2020~24000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>This project, titled "Towards Robust and Natural Underwater Human-Robot Interaction", investigated the viability of computational methods, namely machine learning and visual perception, to establish reliable interaction between autonomous underwater vehicles (AUVs) and their human dive partners. During the two-year duration on this project, of which one year was entirely within the Coronavirus pandemic, the PI and his students have published a number of novel findings. These include</p> <ol> <li>The use of gesture-based communication is a viable method for underwater human-to-robot communication.</li> <li>Underwater robot motion can be used to communicate information to their dive partners. </li> <li>Visual perception and machine learning can reliably detect and understand human gestures, as it can understand different classes of objects underwater and their semantic meanings. </li> <li>In the context of underwater human-robot interaction (HRI), semantic&nbsp; knowledge is effective in safe robot navigation. </li> </ol> <p>Additionally, the project enabled development of novel materials for college-level classes in robotics and human-centric computing. Resarch funded by this grant has produced one Ph.D. thesis, 3 undergraduate honors theses, and one open-source, low-cost, freely available autonomous underwater robot. A number of outreach events were also conducted, particularly for K-12 students belonging to underpresented groups in STEM and industrial entities. As part of the number of field trials conducted through this grant, the broader public in general was able to observe underwater robots in lakes, rivers, and open-ocean environments, and understand their numerous benefits towards the preservation of the aquatic environment as a whole.</p><br> <p>            Last Modified: 07/27/2021<br>      Modified by: Junaed&nbsp;Sattar</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1845364/1845364_10604506_1627357925012_junaed_gesture_for_RCVM_success--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1845364/1845364_10604506_1627357925012_junaed_gesture_for_RCVM_success--rgov-800width.jpg" title="Diver controlling AUV"><img src="/por/images/Reports/POR/2021/1845364/1845364_10604506_1627357925012_junaed_gesture_for_RCVM_success--rgov-66x44.jpg" alt="Diver controlling AUV"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Gesture-based control of an Autonomous Underwater Robot in the ocean.</div> <div class="imageCredit">Junaed Sattar</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Junaed&nbsp;Sattar</div> <div class="imageTitle">Diver controlling AUV</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1845364/1845364_10604506_1627358068459_Screenshotfromloco_on_reef_01_bbd2020.mp4-2--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1845364/1845364_10604506_1627358068459_Screenshotfromloco_on_reef_01_bbd2020.mp4-2--rgov-800width.jpg" title="LoCO in the Caribbean Sea"><img src="/por/images/Reports/POR/2021/1845364/1845364_10604506_1627358068459_Screenshotfromloco_on_reef_01_bbd2020.mp4-2--rgov-66x44.jpg" alt="LoCO in the Caribbean Sea"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The LoCO Autonomous Robot working with a team of divers in trials at the Caribbean Sea.</div> <div class="imageCredit">Junaed Sattar</div> <div class="imagePermisssions">Royalty-free (unrestricted use)</div> <div class="imageSubmitted">Junaed&nbsp;Sattar</div> <div class="imageTitle">LoCO in the Caribbean Sea</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/1845364/1845364_10604506_1627358389299_ScreenshotfromRoboGestDemo--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1845364/1845364_10604506_1627358389299_ScreenshotfromRoboGestDemo--rgov-800width.jpg" title="Hand gesture detection"><img src="/por/images/Reports/POR/2021/1845364/1845364_10604506_1627358389299_ScreenshotfromRoboGestDemo--rgov-66x44.jpg" alt="Hand gesture detection"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Robot's eye view of hand gestures being deciphered</div> <div class="imageCredit">Junaed Sattar</div> <div class="imagePermisssions">Royalty-free (restricted use - cannot be shared)</div> <div class="imageSubmitted">Junaed&nbsp;Sattar</div> <div class="imageTitle">Hand gesture detection</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ This project, titled "Towards Robust and Natural Underwater Human-Robot Interaction", investigated the viability of computational methods, namely machine learning and visual perception, to establish reliable interaction between autonomous underwater vehicles (AUVs) and their human dive partners. During the two-year duration on this project, of which one year was entirely within the Coronavirus pandemic, the PI and his students have published a number of novel findings. These include  The use of gesture-based communication is a viable method for underwater human-to-robot communication. Underwater robot motion can be used to communicate information to their dive partners.  Visual perception and machine learning can reliably detect and understand human gestures, as it can understand different classes of objects underwater and their semantic meanings.  In the context of underwater human-robot interaction (HRI), semantic  knowledge is effective in safe robot navigation.    Additionally, the project enabled development of novel materials for college-level classes in robotics and human-centric computing. Resarch funded by this grant has produced one Ph.D. thesis, 3 undergraduate honors theses, and one open-source, low-cost, freely available autonomous underwater robot. A number of outreach events were also conducted, particularly for K-12 students belonging to underpresented groups in STEM and industrial entities. As part of the number of field trials conducted through this grant, the broader public in general was able to observe underwater robots in lakes, rivers, and open-ocean environments, and understand their numerous benefits towards the preservation of the aquatic environment as a whole.       Last Modified: 07/27/2021       Submitted by: Junaed Sattar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
