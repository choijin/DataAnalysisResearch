<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Inverse Rendering by Co-Evolutionary Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2018</AwardEffectiveDate>
<AwardExpirationDate>05/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>230860.00</AwardTotalIntnAmount>
<AwardAmount>230860</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project addresses the problem of inverse rendering: recovering 3D shape, material, and lighting from a single image. Inverse rendering is a fundamental problem in computer vision; it recovers the basic properties of a visual scene, and serves as a foundation for higher-level scene understanding such as recognizing objects, actions, and functionalities. Despite its fundamental importance, inverse rendering remains difficult. Solving inverse rendering can significantly advance computer vision and benefit a wide variety of applications from autonomous driving to assisting the visually impaired. This project develops new machine learning algorithms to advance the state of the art of inverse rendering. In addition, the project contributes to education and diversity by integrating research results into courses at various levels and by recruiting underrepresented groups to participate in this research. &lt;br/&gt;&lt;br/&gt;This research advances inverse rendering technologies using computer graphics and machine learning. In particular, the research team develops two machine learning systems that co-evolve as adversaries: a rendering system that learns to compose 3D scenes and renders images using a graphics engine, and an inverse rendering system that learns to recover shape, material, and lighting from the rendered images. To develop the rendering system, the research team investigates new learning algorithms for adaptive, automatic scene composition. To develop the inverse rendering system, the research team investigates new learning algorithms that integrate neural networks and physics-based vision.</AbstractNarration>
<MinAmdLetterDate>09/19/2018</MinAmdLetterDate>
<MaxAmdLetterDate>09/19/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1854435</AwardID>
<Investigator>
<FirstName>Jia</FirstName>
<LastName>Deng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jia Deng</PI_FULL_NAME>
<EmailAddress>jiadeng@princeton.edu</EmailAddress>
<PI_PHON>6092581203</PI_PHON>
<NSF_ID>000662553</NSF_ID>
<StartDate>09/19/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085405233</ZipCode>
<StreetAddress><![CDATA[35 Olden Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~230860</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-1276371e-7fff-2b23-9eed-8fd3f8bb74cd"> <p dir="ltr"><span>The goal of this project is to advance the ability of the computer to perform the task of inverse rendering: extracting physical information of visual scenes from images, including 3D shape, material, and illumination. The main focus of this project is on recovering 3D geometry. A main thrust of this project is to explore the use of the synthetic data generated from computer graphics. Such synthetic data is used to train a computer to get better at the task of inverse rendering.&nbsp;</span></p> <p dir="ltr"><span>The research team has developed multiple new algorithms and datasets. In particular, the research team developed new algorithms to enable automatic and effective generation of synthetic training data. To use computer graphics to generate synthetic data, there are many design choices that need to be made. The new algorithms developed in this project allowed such design choices to be made automatically in a way that seeks to maximize the usefulness of the generated data.&nbsp;</span></p> <p dir="ltr"><span>The research team has published multiple peer-reviewed papers at top-tier conferences, including NeurIPS, ICCV, CVPR, ECCV, ICLR. One of the papers was selected from over 1,000 papers to receive the Best Paper Award at the European Conference on Computer Vision (ECCV) 2020, a top computer vision conference. In addition to peer-reviewed publications, the research team has also disseminated research results in the form of open source code and publicly available datasets.&nbsp;</span></p> <div><span><br /></span></div> </span></p> <p>&nbsp;</p><br> <p>            Last Modified: 10/15/2020<br>      Modified by: Jia&nbsp;Deng</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  The goal of this project is to advance the ability of the computer to perform the task of inverse rendering: extracting physical information of visual scenes from images, including 3D shape, material, and illumination. The main focus of this project is on recovering 3D geometry. A main thrust of this project is to explore the use of the synthetic data generated from computer graphics. Such synthetic data is used to train a computer to get better at the task of inverse rendering.  The research team has developed multiple new algorithms and datasets. In particular, the research team developed new algorithms to enable automatic and effective generation of synthetic training data. To use computer graphics to generate synthetic data, there are many design choices that need to be made. The new algorithms developed in this project allowed such design choices to be made automatically in a way that seeks to maximize the usefulness of the generated data.  The research team has published multiple peer-reviewed papers at top-tier conferences, including NeurIPS, ICCV, CVPR, ECCV, ICLR. One of the papers was selected from over 1,000 papers to receive the Best Paper Award at the European Conference on Computer Vision (ECCV) 2020, a top computer vision conference. In addition to peer-reviewed publications, the research team has also disseminated research results in the form of open source code and publicly available datasets.              Last Modified: 10/15/2020       Submitted by: Jia Deng]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
