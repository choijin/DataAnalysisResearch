<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Information-Based Subdata Selection Inspired by Optimal Design of Experiments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2018</AwardEffectiveDate>
<AwardExpirationDate>06/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>60000.00</AwardTotalIntnAmount>
<AwardAmount>60000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Gabor Szekely</SignBlockName>
<PO_EMAI>gszekely@nsf.gov</PO_EMAI>
<PO_PHON>7032928869</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Extraordinary amounts of data are collected in many branches of science, in industry, and in government. The massive amounts of data provide incredible opportunities for making knowledge-based decisions and for advancing complicated research problems through data-driven discoveries. To capitalize on these opportunities, it is critical to develop methodology that facilitates the extraction of useful information from massive data in a computationally efficient way. Even the simplest analyses of the data can be computationally intensive or may no longer be feasible for big data. It is however often the case that valid conclusions can be drawn by considering only some of the data, referred to as subdata. This project develops optimal strategies for selecting subdata that retain, as much as possible, relevant information that was available in the massive data set. The methodology helps to identify the most informative data points, after which an analysis can proceed based on the selected subdata only. This facilitates data-driven decisions, scientific discoveries, and technological breakthroughs with computing resources that are readily available.&lt;br/&gt; &lt;br/&gt;Existing investigations for extracting information from big data with common computing power have focused on random subsampling-based approaches, which have as limitation that the amount of information extracted is only scalable to the subdata size, not the full data size. This project develops and expands the Information-Based Optimal Subdata Selection (IBOSS) method proposed by the PIs in the following directions: 1) It combines IBOSS with sparse variable selection methods in linear regression; 2) it develops subdata selection methods for generalized linear models; 3) it constructs computationally efficient algorithms for selecting the most informative subdata; and 4) it develops user-friendly software that supports the methodology.  The research is a significant addition to the field of big data science. It advances a new method for dealing with big data and has the potential to create novel research opportunities in statistical science and other quantitative fields. The results are valuable even when supercomputers are available, because cutting edge high performance computing facilities will always trail the exponential growth of data volume.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/11/2018</MinAmdLetterDate>
<MaxAmdLetterDate>07/11/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1812013</AwardID>
<Investigator>
<FirstName>HaiYing</FirstName>
<LastName>Wang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>HaiYing Wang</PI_FULL_NAME>
<EmailAddress>haiying.wang@uconn.edu</EmailAddress>
<PI_PHON>8604863414</PI_PHON>
<NSF_ID>000695717</NSF_ID>
<StartDate>07/11/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Connecticut</Name>
<CityName>Storrs</CityName>
<ZipCode>062691133</ZipCode>
<PhoneNumber>8604863622</PhoneNumber>
<StreetAddress>438 Whitney Road Ext.</StreetAddress>
<StreetAddress2><![CDATA[Unit 1133]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<StateCode>CT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>614209054</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CONNECTICUT</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>004534830</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Connecticut]]></Name>
<CityName>Storrs</CityName>
<StateCode>CT</StateCode>
<ZipCode>062694120</ZipCode>
<StreetAddress><![CDATA[215 Glenbrook Road]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~60000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The project lays the foundation for an entirely new research area, namely Information-Based Optimal Subdata Selection (IBOSS). With the support of this award, I have twenty-one papers published or accepted in journals and conferences, including Biometrika, International Conference on Machine Learning, Journal of Machine Learning Research, and Journal of the American Statistical Association [1-21]. There are seven additional papers under review: [22-28]. I have also developed and freely shared two R packages, IBOSS [29] and OSMAC [30], and one Julia package [31]. I have also given thirty-seven invited presentations to disseminate my findings.<br /><strong>References</strong><br />[1] Yishu Xue, Wang, HaiYing, Jun Yan, and Elizabeth D Schifano. An online updating approach for testing the proportional hazards assumption with streams of survival data. Biometrics, 76(1):171?182, 2020.<br />[2] Yaqiong Yao and Wang, HaiYing. Optimal subsampling for softmax regression. Statistical Papers, 60(2):235?249, 2019.<br />[3] Wang, HaiYing. Divide-and-conquer information-based optimalsubdata selection algorithm. Journal of Statistical Theory and Practice, 13(3):46, Jul 2019.<br />[4] Mingyao Ai, Jun Yu, Huiming Zhang, and Wang, HaiYing. Optimal subsampling algorithms for big data regressions. Statistica Sinica, 31(2):749?772, 2021.<br />[5] Wang, HaiYing. More efficient estimation for logistic regression with optimal subsamples. Journal of Machine Learning Research, 20(132):1?59, 2019.<br />[6] Wang, HaiYing and Yanyuan Ma. Optimal subsampling for quantile regression in big data. Biometrika, 108(1):99?112, 2021.&nbsp;<br />[7] Lulu Zuo, Haixiang Zhang, Wang, HaiYing, and Lei Liu. Sampling-based estimation for massive survival data with additive hazards model. Statistics in medicine, 40(2):441?450, 2021.<br />[8] Haixiang Zhang and Wang, HaiYing. Distributed subdata selection for big data via sampling-based approach. Computational Statistics &amp; Data Analysis, 153:107072, 2021.<br />[9] Luc Pronzato and Wang, HaiYing. Sequential online subsampling for thinning experimental designs. Journal of Statistical Planning and Inference, 212:169 ? 193, 2021.<br />[10] Yaqiong Yao and Wang, HaiYing. A selective review on statistical techniques for big data. In Modern Statistical Methods for Health Research, page accepted. Springer, 2020.<br />[11] Wang, HaiYing. Logistic regression for massive data with rare events. In Proceedings of Machine Learning and Systems 2020 (ICML-2020), pages 8264?8271. ., 2020.<br />[12] Jun Yu, Wang, HaiYing, Mingyao Ai, and Huiming Zhang. Optimal distributed subsampling for maximum quasi-likelihood estimators with massive data. Journal of the American Statistical Association, 0(0):1?12, 2020.<br />[13] Qianshun Cheng, Wang, HaiYing, and Min Yang. Information-based optimal subdata selection for big data logistic regression. Journal of Statistical Planning and Inference, 209:112 ? 122, 2020.<br />[14] JooChul Lee, Wang, HaiYing, and Elizabeth D Schifano. Online updating method to correct for measurement error in big data streams. Computational Statistics &amp; Data Analysis, 149:106976, 2020.<br />[15] Guanyu Hu and Wang, HaiYing. Most likely optimal subsampled markov chain monte carlo. Journal of Systems Science and Complexity, 34(3):1121?1134, 2021.<br />[16] Yaqiong Yao and Wang, HaiYing. A review on optimal subsampling methods for massive datasets. Journal of Data Science, 19(1):151?172, 2021.<br />[17] Haim Bar and Wang, HaiYing. Reproducible science with LATEX. Journal of Data Science., 19(1):111?125, 2021.<br />[18] Lulu Zuo, Haixiang Zhang, Wang, HaiYing, and Liuquan Sun. Optimal subsamples selection for massive logistic regression with inherently distributed data. Computational Statistics, DOI:10.1007/s00180?021?01089?0, 2021.<br />[19] Wang, HaiYing, Dixin Zhang, Hua Liang, and David Ruppert. Iterative likelihood: A unified inference tool. Journal of Computational and Graphical Statistics, DOI:10.1080/10618600.2021.1904961, 2021.<br />[20] Wang, HaiYing and Jiahui Zou. A comparative study on sampling with replacement vs Poisson sampling in optimal subsampling. Proceedings of Machine Learning Research, pages 289?297. PMLR, 13?15 Apr 2021.<br />[21] Joochul Lee, Elizabeth Schifano, and Wang, HaiYing. Fast optimal subsampling probability approximation for generalized linear models. Econometrics and Statistics, DOI:10.1016/j.ecosta.2021.02.007, 2021.<br />[22] Rong Zhu, Xinyu Zhang, Wang, HaiYing, and Hua Liang. A scalable frequentist model averaging method. 2020+.<br />[23] Yaqiong Yao, Jiaui Zou, and Wang, HaiYing. Optimal Poisson subsampling for softmax regression. 2020+.<br />[24] Jing Wang, Wang, HaiYing, and Shifeng Xiong. Unweighted estimation based on optimal sample under measurement constraints. 2020+.<br />[25] Wang, HaiYing and Jae Kwang Kim. Maximum sampled conditional likelihood for informative subsampling. 2020+,<br />[26] Fei Wang, Wang, HaiYing, and Jun Yan. Can we lose weight? a revisit on diagnostic tests for informative weight in regression with survey data. 2021+.<br />[27] Joochul Lee, Elizabeth Schifano, and Wang, HaiYing. Sampling-based gaussian mixture regression for big data. 2021+.<br />[28] Jun Yu and Wang, HaiYing. Subdata selection algorithm for linear model selection. 2021+.<br />[29] Wang, HaiYing. IBOSS: Information-Based Optimal Subdata Selection for Linear Regression. https://github.com/Ossifragus/IBOSS.<br />[30] Wang, HaiYing. OSMAC: Optimal Subsampling for Logistic Regression. https://github.com/Ossifragus/OSMAC.<br />[31] Wang, HaiYing. Julia program for Divide-and-Conquer Information-Based Optimal Subdata Selection for Linear Regression. https://filedn.com/l3ajGDP3gyLyPFvbUFtvg48/code/IBOSS-DC-Linear/.</p><br> <p>            Last Modified: 07/09/2021<br>      Modified by: Haiying&nbsp;Wang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The project lays the foundation for an entirely new research area, namely Information-Based Optimal Subdata Selection (IBOSS). With the support of this award, I have twenty-one papers published or accepted in journals and conferences, including Biometrika, International Conference on Machine Learning, Journal of Machine Learning Research, and Journal of the American Statistical Association [1-21]. There are seven additional papers under review: [22-28]. I have also developed and freely shared two R packages, IBOSS [29] and OSMAC [30], and one Julia package [31]. I have also given thirty-seven invited presentations to disseminate my findings. References [1] Yishu Xue, Wang, HaiYing, Jun Yan, and Elizabeth D Schifano. An online updating approach for testing the proportional hazards assumption with streams of survival data. Biometrics, 76(1):171?182, 2020. [2] Yaqiong Yao and Wang, HaiYing. Optimal subsampling for softmax regression. Statistical Papers, 60(2):235?249, 2019. [3] Wang, HaiYing. Divide-and-conquer information-based optimalsubdata selection algorithm. Journal of Statistical Theory and Practice, 13(3):46, Jul 2019. [4] Mingyao Ai, Jun Yu, Huiming Zhang, and Wang, HaiYing. Optimal subsampling algorithms for big data regressions. Statistica Sinica, 31(2):749?772, 2021. [5] Wang, HaiYing. More efficient estimation for logistic regression with optimal subsamples. Journal of Machine Learning Research, 20(132):1?59, 2019. [6] Wang, HaiYing and Yanyuan Ma. Optimal subsampling for quantile regression in big data. Biometrika, 108(1):99?112, 2021.  [7] Lulu Zuo, Haixiang Zhang, Wang, HaiYing, and Lei Liu. Sampling-based estimation for massive survival data with additive hazards model. Statistics in medicine, 40(2):441?450, 2021. [8] Haixiang Zhang and Wang, HaiYing. Distributed subdata selection for big data via sampling-based approach. Computational Statistics &amp; Data Analysis, 153:107072, 2021. [9] Luc Pronzato and Wang, HaiYing. Sequential online subsampling for thinning experimental designs. Journal of Statistical Planning and Inference, 212:169 ? 193, 2021. [10] Yaqiong Yao and Wang, HaiYing. A selective review on statistical techniques for big data. In Modern Statistical Methods for Health Research, page accepted. Springer, 2020. [11] Wang, HaiYing. Logistic regression for massive data with rare events. In Proceedings of Machine Learning and Systems 2020 (ICML-2020), pages 8264?8271. ., 2020. [12] Jun Yu, Wang, HaiYing, Mingyao Ai, and Huiming Zhang. Optimal distributed subsampling for maximum quasi-likelihood estimators with massive data. Journal of the American Statistical Association, 0(0):1?12, 2020. [13] Qianshun Cheng, Wang, HaiYing, and Min Yang. Information-based optimal subdata selection for big data logistic regression. Journal of Statistical Planning and Inference, 209:112 ? 122, 2020. [14] JooChul Lee, Wang, HaiYing, and Elizabeth D Schifano. Online updating method to correct for measurement error in big data streams. Computational Statistics &amp; Data Analysis, 149:106976, 2020. [15] Guanyu Hu and Wang, HaiYing. Most likely optimal subsampled markov chain monte carlo. Journal of Systems Science and Complexity, 34(3):1121?1134, 2021. [16] Yaqiong Yao and Wang, HaiYing. A review on optimal subsampling methods for massive datasets. Journal of Data Science, 19(1):151?172, 2021. [17] Haim Bar and Wang, HaiYing. Reproducible science with LATEX. Journal of Data Science., 19(1):111?125, 2021. [18] Lulu Zuo, Haixiang Zhang, Wang, HaiYing, and Liuquan Sun. Optimal subsamples selection for massive logistic regression with inherently distributed data. Computational Statistics, DOI:10.1007/s00180?021?01089?0, 2021. [19] Wang, HaiYing, Dixin Zhang, Hua Liang, and David Ruppert. Iterative likelihood: A unified inference tool. Journal of Computational and Graphical Statistics, DOI:10.1080/10618600.2021.1904961, 2021. [20] Wang, HaiYing and Jiahui Zou. A comparative study on sampling with replacement vs Poisson sampling in optimal subsampling. Proceedings of Machine Learning Research, pages 289?297. PMLR, 13?15 Apr 2021. [21] Joochul Lee, Elizabeth Schifano, and Wang, HaiYing. Fast optimal subsampling probability approximation for generalized linear models. Econometrics and Statistics, DOI:10.1016/j.ecosta.2021.02.007, 2021. [22] Rong Zhu, Xinyu Zhang, Wang, HaiYing, and Hua Liang. A scalable frequentist model averaging method. 2020+. [23] Yaqiong Yao, Jiaui Zou, and Wang, HaiYing. Optimal Poisson subsampling for softmax regression. 2020+. [24] Jing Wang, Wang, HaiYing, and Shifeng Xiong. Unweighted estimation based on optimal sample under measurement constraints. 2020+. [25] Wang, HaiYing and Jae Kwang Kim. Maximum sampled conditional likelihood for informative subsampling. 2020+, [26] Fei Wang, Wang, HaiYing, and Jun Yan. Can we lose weight? a revisit on diagnostic tests for informative weight in regression with survey data. 2021+. [27] Joochul Lee, Elizabeth Schifano, and Wang, HaiYing. Sampling-based gaussian mixture regression for big data. 2021+. [28] Jun Yu and Wang, HaiYing. Subdata selection algorithm for linear model selection. 2021+. [29] Wang, HaiYing. IBOSS: Information-Based Optimal Subdata Selection for Linear Regression. https://github.com/Ossifragus/IBOSS. [30] Wang, HaiYing. OSMAC: Optimal Subsampling for Logistic Regression. https://github.com/Ossifragus/OSMAC. [31] Wang, HaiYing. Julia program for Divide-and-Conquer Information-Based Optimal Subdata Selection for Linear Regression. https://filedn.com/l3ajGDP3gyLyPFvbUFtvg48/code/IBOSS-DC-Linear/.       Last Modified: 07/09/2021       Submitted by: Haiying Wang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
