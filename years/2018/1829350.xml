<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: CompCog: Broad-coverage probabilistic models of communication in context</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>11/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>115338.00</AwardTotalIntnAmount>
<AwardAmount>115338</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>People often mean more than they say. To take an example, imagine Adam says "I could use a cup of coffee" and Bob responds by saying "There's a place called Joe's around the corner." We understand this as a coherent exchange even though Adam's utterance wasn't phrased overtly as a question and Bob didn't explicitly say that Joe's sells coffee. Extracting this rich additional meaning requires us to consider sentences in light of both the context they are used in and the cooperative motivations of Adam and Bob in using language (what are called "pragmatic inferences"). This project is devoted to constructing formal models of these pragmatic inferences. Modeling pragmatic inference is a major scientific challenge in the study of language and the human mind and a key to the future development of autonomous intelligent systems that can communicate with humans using natural language. Machines that can do robust language understanding in context will pave the way for societally beneficial technological applications such as adaptive intelligent tutoring and assistive technologies. &lt;br/&gt;&lt;br/&gt;The technical core of the project involves developing and extending models of pragmatic reasoning, drawing on ideas and insights from decision theory, probabilistic models of cognition, bounded rationality, and linguistics.  In particular, the work extends the recently developed family of "rational speech act" (RSA) models, which provides a set of formal tools that can be used to address basic challenges in psycholinguistics concerning how major principles of pragmatic inference fall out of simple assumptions about cooperativity and shared context among conversation participants. This enterprise has the potential to fill a major open theoretical gap in our scientific understanding of human language and social cognition. Project work includes developing computational Bayesian models of semantic composition and pragmatic inference and testing those models using controlled psycholinguistic experiments.  The work will also yield new models and publicly available datasets and will contribute to interdisciplinary connections by creating and reinforcing links between linguistics, psychology, and computer science.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/28/2018</MinAmdLetterDate>
<MaxAmdLetterDate>03/28/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1829350</AwardID>
<Investigator>
<FirstName>Roger</FirstName>
<LastName>Levy</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Roger P Levy</PI_FULL_NAME>
<EmailAddress>rplevy@mit.edu</EmailAddress>
<PI_PHON>6172535763</PI_PHON>
<NSF_ID>000508659</NSF_ID>
<StartDate>03/28/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress><![CDATA[77 MASSACHUSETTS AVE NE18-901]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~115336</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>People often mean more than they say. To take an example, imagine Adam says "I could use a cup of coffee" and Bob responds by saying "There's a place called Joe's around the corner." We understand this as a coherent exchange even though Adam's utterance wasn't phrased overtly as a question and Bob didn't explicitly say that Joe's sells coffee. Extracting this rich additional meaning requires us to consider sentences in light of both the context they are used in and the cooperative motivations of Adam and Bob in using language (what are called "pragmatic inferences"). This project is devoted to constructing formal models of these pragmatic inferences. Modeling pragmatic inference is a major scientific challenge in the study of language and the human mind and a key to the future development of autonomous intelligent systems that can communicate with humans using natural language. Machines that can do robust language understanding in context will pave the way for societally beneficial technological applications such as adaptive intelligent tutoring and assistive technologies.&nbsp;<br />The technical core of the project involved developing and extending models of pragmatic reasoning, drawing on ideas and insights from decision theory, probabilistic models of cognition, bounded rationality, and linguistics. We extended the recently developed family of "rational speech act" (RSA) models, which provides a set of formal tools that can be used to address basic challenges in psycholinguistics concerning how major principles of pragmatic inference fall out of simple assumptions about cooperativity and shared context among conversation participants. Our extensions allowed computationally implemented models to account for an increasingly wide variety of contexts and potential communicative goals, and how those contexts and goals influence linguistic communication. &nbsp;These include uncertainty about what information can be taken to be mutually known by both speaker and listener; simultaneously communicating facts about the world and about the language being spoken; non-literal language use like metaphor and sarcasm; politeness; and more. &nbsp;We also improved the scalability of pragmatically informed natural language processing models so that they can handle new domains like communicating abot colors, and automatic picture captioning. &nbsp;During the project, we developed and released new software, and collected and released new datasets, which contribute t o the field and foster interdisciplinary connections between psychology, computer science, and linguistics.</p><br> <p>            Last Modified: 11/21/2019<br>      Modified by: Roger&nbsp;P&nbsp;Levy</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ People often mean more than they say. To take an example, imagine Adam says "I could use a cup of coffee" and Bob responds by saying "There's a place called Joe's around the corner." We understand this as a coherent exchange even though Adam's utterance wasn't phrased overtly as a question and Bob didn't explicitly say that Joe's sells coffee. Extracting this rich additional meaning requires us to consider sentences in light of both the context they are used in and the cooperative motivations of Adam and Bob in using language (what are called "pragmatic inferences"). This project is devoted to constructing formal models of these pragmatic inferences. Modeling pragmatic inference is a major scientific challenge in the study of language and the human mind and a key to the future development of autonomous intelligent systems that can communicate with humans using natural language. Machines that can do robust language understanding in context will pave the way for societally beneficial technological applications such as adaptive intelligent tutoring and assistive technologies.  The technical core of the project involved developing and extending models of pragmatic reasoning, drawing on ideas and insights from decision theory, probabilistic models of cognition, bounded rationality, and linguistics. We extended the recently developed family of "rational speech act" (RSA) models, which provides a set of formal tools that can be used to address basic challenges in psycholinguistics concerning how major principles of pragmatic inference fall out of simple assumptions about cooperativity and shared context among conversation participants. Our extensions allowed computationally implemented models to account for an increasingly wide variety of contexts and potential communicative goals, and how those contexts and goals influence linguistic communication.  These include uncertainty about what information can be taken to be mutually known by both speaker and listener; simultaneously communicating facts about the world and about the language being spoken; non-literal language use like metaphor and sarcasm; politeness; and more.  We also improved the scalability of pragmatically informed natural language processing models so that they can handle new domains like communicating abot colors, and automatic picture captioning.  During the project, we developed and released new software, and collected and released new datasets, which contribute t o the field and foster interdisciplinary connections between psychology, computer science, and linguistics.       Last Modified: 11/21/2019       Submitted by: Roger P Levy]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
