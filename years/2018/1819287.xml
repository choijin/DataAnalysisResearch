<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Natural Language Voice Controlled Science Equipment</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2018</AwardEffectiveDate>
<AwardExpirationDate>04/30/2019</AwardExpirationDate>
<AwardTotalIntnAmount>225000.00</AwardTotalIntnAmount>
<AwardAmount>225000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rajesh Mehta</SignBlockName>
<PO_EMAI>rmehta@nsf.gov</PO_EMAI>
<PO_PHON>7032922174</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This SBIR Phase I project will develop Natural Language Voice-Controlled Lab Assistant technology that enables students to control and interact with hands-on science lab equipment using natural language dialog. The lab assistant technology will connect with science equipment hardware, a mobile or computer app, and cloud-based software for speech recognition and data analysis. A student can perform hands-on experiments, ask questions like "How high did my rocket go?" or "What was the force of the cart collision?", and receive audible responses based on their own experimental measurements. The lab assistant is designed for students that encompass (1) students who are blind or low vision, (2) students with disabilities affecting physical skills, (3) students that would benefit from multi-sensory learning methods as required in Individualized Education Plans (IEPs), and (4) generally students that would benefit from increased engagement in Science, Technology, Engineering, and Mathematics (STEM). The broader impacts of the lab assistant technology will be to promote teaching and learning through professional development of K-12 educators in STEM, and enable broad participation of under-represented groups of people in authentic science inquiry.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The proposed Lab Assistant will be the first application of state-of-the-art voice recognition technology for educational science experiments. The Lab Assistant will integrate with sensor hardware and mobile apps to enable hands-on experiments in physics, earth science, chemistry, and engineering. The intellectual merits of the Lab Assistant are (1) the development of software that can respond to a wide spectrum of natural language questions and (2) the systems integration of many cutting-edge technologies (wireless sensors, mobile apps, voice recognition software, cloud-based data analysis algorithms) into a simple user interface for science education. The voice interactions will help students overcome disabilities with traditional touch and visual technology, work more independently due to the presence of auditory help, work more effectively in groups, and gain confidence in their STEM abilities. For teachers, the Lab Assistant will provide an "expert in the room" to help guide the hands-on activities that they already do, and provide technical support. The Lab Assistant will be especially useful to teachers without formal science training, that nevertheless need to lead hands-on STEM activities.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/14/2018</MinAmdLetterDate>
<MaxAmdLetterDate>06/14/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1819287</AwardID>
<Investigator>
<FirstName>Clifton</FirstName>
<LastName>Roozeboom</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Clifton L Roozeboom</PI_FULL_NAME>
<EmailAddress>clifton.roozeboom@gmail.com</EmailAddress>
<PI_PHON>7196513764</PI_PHON>
<NSF_ID>000696713</NSF_ID>
<StartDate>06/14/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Myriad Sensors</Name>
<CityName>Mountain View</CityName>
<ZipCode>940434849</ZipCode>
<PhoneNumber>7196513764</PhoneNumber>
<StreetAddress>505 Cypress Point Dr.</StreetAddress>
<StreetAddress2><![CDATA[Unit 218]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>079825163</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MYRIAD SENSORS, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Myriad Sensors]]></Name>
<CityName>San Jose</CityName>
<StateCode>CA</StateCode>
<ZipCode>951294951</ZipCode>
<StreetAddress><![CDATA[1475 Saratoga Ave, Suite 120]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8031</Code>
<Text>Education Products</Text>
</ProgramReference>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~225000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The Natural Language Voice-Controlled Science Equipment Project (nicknamed Jarvis) is a lab assistant technology that enables students to control and interact with hands-on science lab equipment using natural language dialog. Jarvis will connect with science equipment hardware (called PocketLab), a mobile or computer app, and cloud-based software for speech recognition and data analysis. A student can perform hands-on experiments, ask questions like &ldquo;How high did my rocket go?&rdquo; or &ldquo;What was the force of the cart collision?&rdquo;, and receive audible responses based on their own experimental measurements.</p> <p>Jarvis is designed for:&nbsp;</p> <p>1.<span> </span>Students who are blind or low vision</p> <p>2.<span> </span>Students with disabilities affecting physical skills</p> <p>3.<span> </span>Students that would benefit from multi-sensory learning methods as required in Individualized Education Plans (IEPs)&nbsp;</p> <p>4.<span> </span>General student population that would benefit from increased engagement in STEM&nbsp;</p> <p>The typical educational user would be a teacher of students with visual impairments, special education teacher, or broadly, a STEM teacher in middle school or high school.</p> <p>The goal of Jarvis is to provide new state-of-the-art natural language recognition and response software that integrates with the existing PocketLab sensor hardware and mobile apps for specific use in hands-on science learning. The SBIR Phase I funding will support the engineering development and user feedback testing of an end-to-end voice-controlled prototype. The grant funding will decrease the technical risk involved with transforming the latest voice recognition services into a product that is simple, intuitive, and robust for use in challenging educational applications.</p><br> <p>            Last Modified: 06/10/2019<br>      Modified by: Clifton&nbsp;L&nbsp;Roozeboom</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The Natural Language Voice-Controlled Science Equipment Project (nicknamed Jarvis) is a lab assistant technology that enables students to control and interact with hands-on science lab equipment using natural language dialog. Jarvis will connect with science equipment hardware (called PocketLab), a mobile or computer app, and cloud-based software for speech recognition and data analysis. A student can perform hands-on experiments, ask questions like "How high did my rocket go?" or "What was the force of the cart collision?", and receive audible responses based on their own experimental measurements.  Jarvis is designed for:   1. Students who are blind or low vision  2. Students with disabilities affecting physical skills  3. Students that would benefit from multi-sensory learning methods as required in Individualized Education Plans (IEPs)   4. General student population that would benefit from increased engagement in STEM   The typical educational user would be a teacher of students with visual impairments, special education teacher, or broadly, a STEM teacher in middle school or high school.  The goal of Jarvis is to provide new state-of-the-art natural language recognition and response software that integrates with the existing PocketLab sensor hardware and mobile apps for specific use in hands-on science learning. The SBIR Phase I funding will support the engineering development and user feedback testing of an end-to-end voice-controlled prototype. The grant funding will decrease the technical risk involved with transforming the latest voice recognition services into a product that is simple, intuitive, and robust for use in challenging educational applications.       Last Modified: 06/10/2019       Submitted by: Clifton L Roozeboom]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
