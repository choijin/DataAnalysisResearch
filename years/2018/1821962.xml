<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>US Ignite: Focus Area 1: Predictable Wireless Networking and Collaborative 3D Reconstruction for Real-Time Augmented Vision</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/31/2017</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>550371.00</AwardTotalIntnAmount>
<AwardAmount>550371</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Deepankar Medhi</SignBlockName>
<PO_EMAI>dmedhi@nsf.gov</PO_EMAI>
<PO_PHON>7032922935</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Eliminating the line-of-sight constraint of human vision and machine vision, the developed network systems foundations of predictable wireless networked and 3D reconstruction will enable 'see-through vision' which will transform the ways humans and engineered systems interact with environments and thus have far-reaching impact on domains such as road transportation, public safety, and disaster response. This project develops the network systems foundation for a vehicle equipped with sensors and an augmented reality display to indicate the presence of other nearby vehicles hidden by obstacles.  In collaboration with Wayne State University (WSU) police and Ford Research and leveraging the WSU living lab and the OpenXC open-source platform for connected vehicles, the project will take an integrated approach to the research, deployment, and dissemination of the wireless network systems for see-through vision. This project proposes a cross-layer framework for addressing physical-domain uncertainties and the interdependencies between wireless networking and 3D reconstruction, and it develops novel algorithms for predictable wireless networking and real-time wireless networked 3D reconstruction. Using the developed network system, this project will develop a see-through vision application for human-driving. The wireless networked see-through vision system will be deployed in the WSU police patrol vehicles, and the project team will outreach to the Detroit and State of Michigan police as well as open-source communities for broad adoption and deployment of the see-through vision system.&lt;br/&gt;&lt;br/&gt;With the bold objective of eliminating the line-of-sight constraint of human vision and machine vision, this project addresses wireless networking and 3D reconstruction challenges in a holistic cross-layer framework. By integrating research investigation with systems development and deployment, this project will make the following significant contributions: 1) Effectively leveraging multi-scale physical structures of traffic flows, the multi-scale approach to resource management in vehicular wireless networks not only ensures predictable vehicular wireless networking, it also transforms fundamental challenges of vehicular networks to ones similar to those of mostly-immobile networks, thus enabling the exploration of fundamental, generically-applicable principles and mechanisms for predictable wireless networking; 2) the multi-scale approach to joint scheduling, channel assignment, power control, and rate control enables predictable control of per-packet transmission reliability in the presence of fast-varying network and environmental conditions such as wireless channel attenuation, internal and external interference, data traffic dynamics, and vehicle mobility; 3) the real-time scheduling algorithm enables controllable exploration of real-time capacity regions for system-level optimization; 4) the collaborative 3D reconstruction model integrates visual sensors in a divide-and-conquer fashion, and it enhances the capability of networked vision as well as its robustness to physical uncertainties; 5) the co-design of collaborative 3D reconstruction and wireless networking permits adaptive communication capacity allocation to optimize the quality of 3D reconstruction; 6) the attention-aware see-through vision application creates a new research field of vision augmentation by uniquely integrating computer vision and computer graphics research and by proposing a practical solution for displaying augmented 3D vision.</AbstractNarration>
<MinAmdLetterDate>02/22/2018</MinAmdLetterDate>
<MaxAmdLetterDate>02/22/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1821962</AwardID>
<Investigator>
<FirstName>Hongwei</FirstName>
<LastName>Zhang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hongwei Zhang</PI_FULL_NAME>
<EmailAddress>hongwei@iastate.edu</EmailAddress>
<PI_PHON>5152942143</PI_PHON>
<NSF_ID>000075819</NSF_ID>
<StartDate>02/22/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Iowa State University</Name>
<CityName>AMES</CityName>
<ZipCode>500112207</ZipCode>
<PhoneNumber>5152945225</PhoneNumber>
<StreetAddress>1138 Pearson</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<StateCode>IA</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IA04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005309844</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>IOWA STATE UNIVERSITY OF SCIENCE AND TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005309844</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Iowa State University]]></Name>
<CityName>Ames</CityName>
<StateCode>IA</StateCode>
<ZipCode>500112207</ZipCode>
<StreetAddress><![CDATA[1138 Pearson]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Iowa</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IA04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>2890</Code>
<Text>CISE Research Resources</Text>
</ProgramElement>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~550371</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Accounting for over 80% of human-perceived information about the physical world and as a foundational mechanism for autonomous vehicles to ``observe'' driving conditions, vision is a critical window into the physical environment for both humans and engineered systems. Natural human and machine vision, however, is subject to inherent physical constraints such as being limited to the line of sight (LOS). In road transportation, such vision constraints lead to stress, inefficiency, and accidents in driving, and making turns with obstructed views is a major cause for about 2.5 million intersection accidents in U.S. every year. To transform human vision and machine vision beyond the line-of-sight constraint, this project proposes to leverage multiple visual sensors to enable humans and engineered systems to see-through obstacles, thus transforming the ways humans and engineered systems interact with environments. To this end, this project develops the wireless networking and 3D vision foundations for real-time wireless-networked augmented vision which holds the potential to enable drivers and vehicles to see through obstacles.</p> <p>&nbsp;</p> <p>In wireless networking, this project has developed field-deployable approaches to predictable control of wireless interference in mobile data transmission scheduling, a problem that has been open for more than 45 years and is fundamental to V2X communication in 5G and beyond systems. Innovating the Physical-Ratio-K (PRK) wireless interference model and model instantiation method and leveraging the cyber-physical structures of road transportation as well as the multi-scale structures of wireless networking itself, the approaches transform the non-local interference control problem into a local control problem, and they integrate control theory, real-time scheduling theory, and wireless networking practice to enable predictable guarantee of the reliability and timeliness of every packet transmission while maximizing network throughput. In collaborative augmented 3D vision, visual processing of dynamic moving objects as well as a large environment becomes essential. &nbsp;A new method of defining and computing convolutions directly on 3D point clouds effectively and efficiently conducts cross-camera modeling of dynamic objects and environment. It also infers 3D depth information and full 3D shapes directly from multiple partial inputs, which enables personalized augmented 3D vision by rendering reconstructed objects and scenes from the user&rsquo;s perspective.</p> <p>&nbsp;</p> <p>Eliminating the LOS constraint of natural human and machine vision and enabling non-LOS surrounding sensing, the enabled augmented vision system will not only transform the experience, safety, and comfort of human driving, it will also serve as an important building block for human-in-the-loop autonomous driving and fully-autonomous driving. The developed technologies are also broadly applicable to domains such as public safety and disaster response, thus having positive societal impact. This project has also integrated wireless-networked augmented vision research with the cyber-physical-systems graduate programs at Iowa State University and Wayne State University, and it has used augmented vision research to enrich undergraduate education and research as well as community and K-12 outreach.</p> <p>&nbsp;</p><br> <p>            Last Modified: 01/05/2021<br>      Modified by: Hongwei&nbsp;Zhang</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/1821962/1821962_10457966_1609873471951_Wireless_Networked_Augmented_Vision--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/1821962/1821962_10457966_1609873471951_Wireless_Networked_Augmented_Vision--rgov-800width.jpg" title="Wireless-Networked Augmented 3D Vision"><img src="/por/images/Reports/POR/2021/1821962/1821962_10457966_1609873471951_Wireless_Networked_Augmented_Vision--rgov-66x44.jpg" alt="Wireless-Networked Augmented 3D Vision"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Wireless-Networked Augmented 3D Vision</div> <div class="imageCredit">Jing Hua, Hongwei Zhang</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Hongwei&nbsp;Zhang</div> <div class="imageTitle">Wireless-Networked Augmented 3D Vision</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Accounting for over 80% of human-perceived information about the physical world and as a foundational mechanism for autonomous vehicles to ``observe'' driving conditions, vision is a critical window into the physical environment for both humans and engineered systems. Natural human and machine vision, however, is subject to inherent physical constraints such as being limited to the line of sight (LOS). In road transportation, such vision constraints lead to stress, inefficiency, and accidents in driving, and making turns with obstructed views is a major cause for about 2.5 million intersection accidents in U.S. every year. To transform human vision and machine vision beyond the line-of-sight constraint, this project proposes to leverage multiple visual sensors to enable humans and engineered systems to see-through obstacles, thus transforming the ways humans and engineered systems interact with environments. To this end, this project develops the wireless networking and 3D vision foundations for real-time wireless-networked augmented vision which holds the potential to enable drivers and vehicles to see through obstacles.     In wireless networking, this project has developed field-deployable approaches to predictable control of wireless interference in mobile data transmission scheduling, a problem that has been open for more than 45 years and is fundamental to V2X communication in 5G and beyond systems. Innovating the Physical-Ratio-K (PRK) wireless interference model and model instantiation method and leveraging the cyber-physical structures of road transportation as well as the multi-scale structures of wireless networking itself, the approaches transform the non-local interference control problem into a local control problem, and they integrate control theory, real-time scheduling theory, and wireless networking practice to enable predictable guarantee of the reliability and timeliness of every packet transmission while maximizing network throughput. In collaborative augmented 3D vision, visual processing of dynamic moving objects as well as a large environment becomes essential.  A new method of defining and computing convolutions directly on 3D point clouds effectively and efficiently conducts cross-camera modeling of dynamic objects and environment. It also infers 3D depth information and full 3D shapes directly from multiple partial inputs, which enables personalized augmented 3D vision by rendering reconstructed objects and scenes from the user’s perspective.     Eliminating the LOS constraint of natural human and machine vision and enabling non-LOS surrounding sensing, the enabled augmented vision system will not only transform the experience, safety, and comfort of human driving, it will also serve as an important building block for human-in-the-loop autonomous driving and fully-autonomous driving. The developed technologies are also broadly applicable to domains such as public safety and disaster response, thus having positive societal impact. This project has also integrated wireless-networked augmented vision research with the cyber-physical-systems graduate programs at Iowa State University and Wayne State University, and it has used augmented vision research to enrich undergraduate education and research as well as community and K-12 outreach.          Last Modified: 01/05/2021       Submitted by: Hongwei Zhang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
