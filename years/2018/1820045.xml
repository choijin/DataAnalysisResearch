<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  VideoPoints: A Companion for Classroom Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2018</AwardEffectiveDate>
<AwardExpirationDate>03/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>225000.00</AwardTotalIntnAmount>
<AwardAmount>225000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Peter Atherton</SignBlockName>
<PO_EMAI>patherto@nsf.gov</PO_EMAI>
<PO_PHON>7032928772</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This SBIR Phase I project will develop text, speech and image analysis technologies to transform a set of educational lecture videos into an interactive learning companion. A lecture video will be presented as a series of topically cohesive sections, each represented by a textual summary, a visual summary, and a video segment. Students will be able to query a series of lecture videos with the answers presented as a combination of text, images, speech with transcript, and video, along with links to additional relevant resources. These technological enhancements will drive the development of the lecture video management system with capabilities well beyond commercial state of the art. The students will gain the ability to instantly access any information in a semester long course, with little overhead for the instructor. The business plan focuses on a freemium model designed for wide adoption with no cost to instructors and a very low cost to students. The ultimate potential societal benefit is a significantly better learning experience and learning outcomes for higher education students.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The key innovations in this project are summarization of video segments and a Questions &amp; Answers (QA) system customized for a series of lecture videos. These are formidable challenges despite existing substantial related research in text mining and image analysis. The information content of a lecture video spans multiple modalities, specifically, screen text, images, and speech. Further, each modality has unique characteristics. Screen text is typically unstructured and includes Optical Character Recognition errors. Transcripts from classroom lecture videos contain informal classroom interactions and suffer from speech recognition errors. The images in a lecture video can represent a variety of concepts including illustrative examples, graphs and charts, and camera images. Innovative approaches to topic modeling will be developed to handle the unique nature of the input compared to most documents. The project will employ representation learning approaches to map from each modality to a common semantic space that will drive the matching between student questions and the content of lecture videos. Since the best answer to a student question may not be fully contained in a lecture video, external resources including textbooks, message boards, and actual quizzes and exams will also be analyzed to drive the QA module.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/19/2018</MinAmdLetterDate>
<MaxAmdLetterDate>01/23/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1820045</AwardID>
<Investigator>
<FirstName>Ioannis</FirstName>
<LastName>Konstantinidis</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ioannis Konstantinidis</PI_FULL_NAME>
<EmailAddress>ioannis.konstantinidis@videopoints.org</EmailAddress>
<PI_PHON>8325120348</PI_PHON>
<NSF_ID>000233312</NSF_ID>
<StartDate>12/10/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Tayfun</FirstName>
<LastName>Tuna</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tayfun Tuna</PI_FULL_NAME>
<EmailAddress>taytun@gmail.com</EmailAddress>
<PI_PHON>8326466637</PI_PHON>
<NSF_ID>000726063</NSF_ID>
<StartDate>06/19/2018</StartDate>
<EndDate>12/10/2018</EndDate>
<RoleCode>Former Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Videopoints LLC</Name>
<CityName>Houston</CityName>
<ZipCode>770064225</ZipCode>
<PhoneNumber>8328787217</PhoneNumber>
<StreetAddress>3615 Graustark St</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>080290318</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>VIDEOPOINTS, LLC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Videopoints LLC]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>770064225</ZipCode>
<StreetAddress><![CDATA[3615 Graustark St]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8031</Code>
<Text>Education Products</Text>
</ProgramReference>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~225000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The key outcome of this Phase I project is Videopoints, a lecture video portal designed to&nbsp;transform an instructional video into an interactive learning companion. A major focus of the project is tools and technologies that allow student users to quickly navigate to content of interest in a long lecture video, or in a collection of lecture videos covering an academic course.&nbsp;Videopoints is designed to improve a student's learning experience and an instructor's teaching effectiveness.&nbsp;</p> <p>Videopoints automatically divides a lecture video into topical segments and presents the segments with automatically generated text and visual summaries. The technical underpinnings of Videopoints span multiple areas of AI including text analysis, natural language processing, image analysis, data mining, and machine learning, as well as web and user interface technologies.</p> <p>The specific accomplishments of this Phase I project are the following:</p> <p>1) Content-based lecture video indexing algorithms and software to automatically split lecture videos into short-form topical segment based on an analysis of screen text.</p> <p>2) Text summarization algorithms and software that extract and analyze text on a lecture video screen to identify and display the most important keywords and key phrases. Selection of keywords is based on an analysis of several factors, including frequency, time on screen, font size, how common a word is, and presence of word in field and language dictionaries.</p> <p>3) Visual summarization algorithms and software that extract the images on frames in a lecture video segment to generate and display a summary frame. Selection of images for the summary frame is based on analysis of the similarity between images in the video segment, and an analysis of the importance of images that is based on multiple factors including image size, time on screen, and visual complexity.</p> <p>4) Rigorous evaluation of content-based indexing and summarization features based on the "ground truth" collected from experts in the relevant areas of the lecture video content.</p> <p>5) Implementation of Videopoints, an advanced cloud based lecture video portal that is deployed for several STEM courses with hundreds of users. In addition to the novel lecture video indexing and summarization features, Videopoints supports multi-modal content discovery via on-screen test and audio transcript, and tracks viewing statistics via an analytics dashboard.</p> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/30/2020<br>      Modified by: Ioannis&nbsp;Konstantinidis</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The key outcome of this Phase I project is Videopoints, a lecture video portal designed to transform an instructional video into an interactive learning companion. A major focus of the project is tools and technologies that allow student users to quickly navigate to content of interest in a long lecture video, or in a collection of lecture videos covering an academic course. Videopoints is designed to improve a student's learning experience and an instructor's teaching effectiveness.   Videopoints automatically divides a lecture video into topical segments and presents the segments with automatically generated text and visual summaries. The technical underpinnings of Videopoints span multiple areas of AI including text analysis, natural language processing, image analysis, data mining, and machine learning, as well as web and user interface technologies.  The specific accomplishments of this Phase I project are the following:  1) Content-based lecture video indexing algorithms and software to automatically split lecture videos into short-form topical segment based on an analysis of screen text.  2) Text summarization algorithms and software that extract and analyze text on a lecture video screen to identify and display the most important keywords and key phrases. Selection of keywords is based on an analysis of several factors, including frequency, time on screen, font size, how common a word is, and presence of word in field and language dictionaries.  3) Visual summarization algorithms and software that extract the images on frames in a lecture video segment to generate and display a summary frame. Selection of images for the summary frame is based on analysis of the similarity between images in the video segment, and an analysis of the importance of images that is based on multiple factors including image size, time on screen, and visual complexity.  4) Rigorous evaluation of content-based indexing and summarization features based on the "ground truth" collected from experts in the relevant areas of the lecture video content.  5) Implementation of Videopoints, an advanced cloud based lecture video portal that is deployed for several STEM courses with hundreds of users. In addition to the novel lecture video indexing and summarization features, Videopoints supports multi-modal content discovery via on-screen test and audio transcript, and tracks viewing statistics via an analytics dashboard.             Last Modified: 11/30/2020       Submitted by: Ioannis Konstantinidis]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
