<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Improving Multi-Fingered Manipulation by Unifying Learning and Planning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2019</AwardEffectiveDate>
<AwardExpirationDate>02/29/2024</AwardExpirationDate>
<AwardTotalIntnAmount>532664.00</AwardTotalIntnAmount>
<AwardAmount>564664</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>For robots to act autonomously as assistants in one's daily life, as surrogates for humans in dangerous environments, or as workers in busy factories and processing centers, they must be able to fluently grasp and manipulate objects that they have never previously encountered. This project investigates new approaches to perform grasping and in-hand manipulation using multi-fingered robot hands. The primary novelty in this work comes from examining new ways of combining knowledge gained automatically by the robot from its own sensors with models given to the robot by its programmer. The research team will perform extensive empirical evaluation of the algorithms developed in order to quantify the benefit of these new techniques over previously proposed analytic or data-driven approaches. This will allow for fast translation of the research results into robots used in industrial and service settings. This project will conduct educational outreach activities using robot manipulation to increase and reinforce middle and high schoolers' interest in computing.&lt;br/&gt;&lt;br/&gt;For autonomous robots to operate seamlessly as assistants in human environments, they must be endowed with the ability to aptly perform dexterous manipulation. Contemporary robot hardware provides the necessary dexterity and sensitivity to perform dexterous manipulation, but algorithmic shortcomings currently cripple deployment of robust multi-fingered grasping, regrasping, and in-hand manipulation of unknown and partially modeled objects. The research goal of this project is to unify concepts from model-based planning and data-driven learning for manipulation to improve dexterous manipulation of unknown objects. The first research thrust examines adding model-based constraints to perform grasp synthesis with a learned deep network. The second research thrust evaluates the hypothesis that learning feedback policies, approximate models, and graph structures from tactile and visual sensing will improve execution of pre-planned in-hand manipulation trajectories.  The investigator proposes viewing these problems as an instance of probabilistic inference. This enables the robot to directly reason over uncertain sensory observations and partial object-pose and contact-state information.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/15/2019</MinAmdLetterDate>
<MaxAmdLetterDate>03/19/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1846341</AwardID>
<Investigator>
<FirstName>Tucker</FirstName>
<LastName>Hermans</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Tucker Hermans</PI_FULL_NAME>
<EmailAddress>thermans@cs.utah.edu</EmailAddress>
<PI_PHON>8015818122</PI_PHON>
<NSF_ID>000701391</NSF_ID>
<StartDate>03/15/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Utah</Name>
<CityName>SALT LAKE CITY</CityName>
<ZipCode>841128930</ZipCode>
<PhoneNumber>8015816903</PhoneNumber>
<StreetAddress>75 S 2000 E</StreetAddress>
<StreetAddress2><![CDATA[Second Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009095365</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF UTAH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009095365</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName/>
<StateCode>UT</StateCode>
<ZipCode>841128930</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~134064</FUND_OBLG>
<FUND_OBLG>2020~136360</FUND_OBLG>
<FUND_OBLG>2021~294240</FUND_OBLG>
</Award>
</rootTag>
