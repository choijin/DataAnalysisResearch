<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: A Biologically-Inspired Algorithm to Detect, Segment, and Track Moving Objects with Observer Motion</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/31/2017</AwardEffectiveDate>
<AwardExpirationDate>04/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>124588.00</AwardTotalIntnAmount>
<AwardAmount>124588</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kenneth Whang</SignBlockName>
<PO_EMAI>kwhang@nsf.gov</PO_EMAI>
<PO_PHON>7032925149</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project aims to develop a real-time computational algorithm to detect, segment and track moving objects in the presence of observer motion. This algorithm will be based on a computational model of experimentally measured properties of Object Motion Sensitive (OMS) cells in the vertebrate retina, which solve a similar problem. From the known computational properties of the retina, it is expected that the algorithm will be robust under difficult tasks in motion tracking, including object occlusion, multiple moving objects, varying scene statistics, substantial background motion, and optic flow. From a biological perspective, the developed computational model will give insight to how the retina encodes moving objects. From an engineering perspective, the resultant computational algorithm will be applicable to machine vision from a moving platform, including autonomous vehicles, surveillance and reconnaissance applications, as well as, smart sensor design and neuromorphic systems.  &lt;br/&gt;&lt;br/&gt;Despite many advances in motion analysis methods, techniques based on moving observations are still in a preliminary stage when compared to static observations, as far as reliability, efficiency, robustness, and runtime are concerned in real world scenarios. At other hand, our biological visual system is capable of performing similar motion computations reliably in the presence of constant eye movements. Recently, it was discovered that segmentation of moving objects, and rejection of background motion, begins in the retina. A subset of retinal ganglion cells responds to differential motion between the receptive field center and surround, as produced by an object moving over the background, but are strongly suppressed by global image motion, as produced by the observer's head or eye movements. This selectivity for differential motion is independent of direction and the spatial pattern of the object, enabling our visual system to find the boundaries of the moving objects, segregate multiple moving objects, and also anticipate the direction of motion. The retina performs these tasks simultaneously, in real-time, and with high accuracy using a network of only five basic cell types. These properties, along with the experimental accessibility of the retina, makes this neural circuit an ideal working biological system to serve as the design for an object-tracking algorithm. This project will develop a unique motion analysis algorithm for mobile observers based on recent findings of retina's motion computations and circuitry, and demonstrate its performance in a variety of realistic scenarios, which includes lateral observer motion, optic flow, dynamic scenes, and other objects that occlude the moving object.</AbstractNarration>
<MinAmdLetterDate>11/09/2017</MinAmdLetterDate>
<MaxAmdLetterDate>11/09/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1811543</AwardID>
<Investigator>
<FirstName>Neda</FirstName>
<LastName>Nategh</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Neda Nategh</PI_FULL_NAME>
<EmailAddress>neda.nategh@utah.edu</EmailAddress>
<PI_PHON>8012133675</PI_PHON>
<NSF_ID>000677089</NSF_ID>
<StartDate>11/09/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Utah</Name>
<CityName>SALT LAKE CITY</CityName>
<ZipCode>841128930</ZipCode>
<PhoneNumber>8015816903</PhoneNumber>
<StreetAddress>75 S 2000 E</StreetAddress>
<StreetAddress2><![CDATA[Second Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>UT02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>009095365</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF UTAH, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>009095365</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Utah]]></Name>
<CityName/>
<StateCode>UT</StateCode>
<ZipCode>841128930</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>UT02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramElement>
<Code>9150</Code>
<Text>EPSCoR Co-Funding</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~124588</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>To detect moving objects, the brain must distinguish local motion within the scene from the global retinal image shift due to eye movements. Recently, it was discovered that this process begins in the retina: a subset of retinal ganglion cells, termed Object Motion Sensitive (OMS) cells, responds to motion in the receptive field center, but only if the wider surround moves with a different trajectory?called ?differential motion?, but are silent during ?global motion? like when eye or head moves. The retina performs this differential motion selectivity using a network of only 5 basic cell types, but more than 50 subtypes. It has been shown that in the OMS circuit, the output of each bipolar cell is rectified before being transmitted to OMS ganglion cells. A specific type of inhibitory amacrine cells, called polyaxonal amacrine cells, delivers inhibition from the background to OMS cells. Polyaxonal amacrine cells also receive excitatory input from rectified bipolar cells.&nbsp;The interplay of excitatory and inhibitory processes among these cell types involves nonlinear computations, which makes it challenging to understand the circuitry underlying the retinal functions from the experimental data.&nbsp;</p> <p>This project develops a computational framework that can account for the nonlinear properties of neuronal networks, which is not possible using existing computational frameworks. The developed model offers a general framework applicable to a variety of neural systems, including the OMS circuit, as&nbsp;one of the great exemplars of such nonlinear computations,&nbsp;where inhibitory signals from interneurons such as amacrine cells could alter the processing along the parallel excitatory pathways from bipolar cells to ganglion cells through nonlinear operations. Moreover, this computational framework will enable advancing the capabilities of existing computational vision systems processing visual information including motion signals in the presence of moving observers.&nbsp;</p> <p>Numerous studies investigating changes in the visual processing during eye movements have found that retinal responses during large eye movements are&nbsp;suppressed to reduce the effect of&nbsp;the retinal motion smear induced by movement of the eyes. Several studies have also shown multiple changes in the perceived location, motion, and duration of visual stimuli around the time of eye movements.&nbsp;This project investigates the visual information representation during eye movements in higher brain areas that underlies the perception of the visual scene, including the motion signals, in the presence of eye movements. Our combined physiological and computational approach allows us to develop the first model capable of capturing changes in neuronal responses across eye movements with high temporal precision using sparse, high-dimensional spiking data from visual cortical neurons. Using the decoding aspect of this model for reading out the relevant information to visual perception on the actual timescale of the eye movement, is critically needed for developing visual prostheses and brain-machine interfaces which aim to restore our natural visual behavior.</p> <p>In addition, the project has provided professional development and training opportunities for multiple graduate and undergraduate students, research experience for underrepresented community, outreach programs for high school students and minorities, and a new course integrating the outcome of this research with the class materials.</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/17/2020<br>      Modified by: Neda&nbsp;Nategh</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ To detect moving objects, the brain must distinguish local motion within the scene from the global retinal image shift due to eye movements. Recently, it was discovered that this process begins in the retina: a subset of retinal ganglion cells, termed Object Motion Sensitive (OMS) cells, responds to motion in the receptive field center, but only if the wider surround moves with a different trajectory?called ?differential motion?, but are silent during ?global motion? like when eye or head moves. The retina performs this differential motion selectivity using a network of only 5 basic cell types, but more than 50 subtypes. It has been shown that in the OMS circuit, the output of each bipolar cell is rectified before being transmitted to OMS ganglion cells. A specific type of inhibitory amacrine cells, called polyaxonal amacrine cells, delivers inhibition from the background to OMS cells. Polyaxonal amacrine cells also receive excitatory input from rectified bipolar cells. The interplay of excitatory and inhibitory processes among these cell types involves nonlinear computations, which makes it challenging to understand the circuitry underlying the retinal functions from the experimental data.   This project develops a computational framework that can account for the nonlinear properties of neuronal networks, which is not possible using existing computational frameworks. The developed model offers a general framework applicable to a variety of neural systems, including the OMS circuit, as one of the great exemplars of such nonlinear computations, where inhibitory signals from interneurons such as amacrine cells could alter the processing along the parallel excitatory pathways from bipolar cells to ganglion cells through nonlinear operations. Moreover, this computational framework will enable advancing the capabilities of existing computational vision systems processing visual information including motion signals in the presence of moving observers.   Numerous studies investigating changes in the visual processing during eye movements have found that retinal responses during large eye movements are suppressed to reduce the effect of the retinal motion smear induced by movement of the eyes. Several studies have also shown multiple changes in the perceived location, motion, and duration of visual stimuli around the time of eye movements. This project investigates the visual information representation during eye movements in higher brain areas that underlies the perception of the visual scene, including the motion signals, in the presence of eye movements. Our combined physiological and computational approach allows us to develop the first model capable of capturing changes in neuronal responses across eye movements with high temporal precision using sparse, high-dimensional spiking data from visual cortical neurons. Using the decoding aspect of this model for reading out the relevant information to visual perception on the actual timescale of the eye movement, is critically needed for developing visual prostheses and brain-machine interfaces which aim to restore our natural visual behavior.  In addition, the project has provided professional development and training opportunities for multiple graduate and undergraduate students, research experience for underrepresented community, outreach programs for high school students and minorities, and a new course integrating the outcome of this research with the class materials.          Last Modified: 06/17/2020       Submitted by: Neda Nategh]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
