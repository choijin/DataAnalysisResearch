<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Automated Perception for Robotic Chopsticks Manipulating Small and Large Objects in Constrained Spaces</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2021</AwardEffectiveDate>
<AwardExpirationDate>12/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>256000.00</AwardTotalIntnAmount>
<AwardAmount>256000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Muralidharan Nair</SignBlockName>
<PO_EMAI>mnair@nsf.gov</PO_EMAI>
<PO_PHON>7032927059</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will use robots to mechanize one of the last remaining fully manual tasks in logistics chains: unloading packages. Even before COVID-19, shipping growth was severely straining the human workforce that moves packages between shipping containers and distribution centers. The pool of unskilled laborers willing and able to monotonously move heavy packages a few feet at a time is much too small. COVID-19 made the importance of these supply chains evident. Mechanization will both speed the supply chain and help safeguard it from infectious diseases and other natural disasters. While addressing a $32 billion year problem in the logistics industry, the project will simultaneously enhance understanding of geometric modeling for more accurate sensing and robotic manipulation of objects that exhibit significant geometric variation.&lt;br/&gt;&lt;br/&gt;This Small Business Innovation Research (SBIR) Phase I project seeks to establish machine vision algorithms targeted to robotic manipulation applications in order to recognize objects from RGB-D images (a combination of red, green blue images and its corresponding depth image) via curated geometric models parameterizable with tuples of numbers.  The technology will also estimate the state and parameters of recognized objects. The state-of-the-art in existing machine vision approaches to model-based object recognition and state estimation use fixed models, i.e., models of objects with constant size, shape, color, and texture. Those approaches have demonstrated the ability to identify scores of different objects and localize them in space. But they work on only some specific images. This project will pursue comparable accuracy, sufficient for robotic manipulation of packages in supply chains, on the same identification and localization tasks for multiple object categories. Research objectives specify the minimum performance numbers for the precision and recall required to detect objects in a scene, to estimate accuracy, and to calculate runtime performance (in frames processed per second).  The anticipated technical result is a semi-autonomous system that is able to work with a human to speed object recognition and parameter identification, and to provide estimations beyond a fully manual system.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/30/2021</MinAmdLetterDate>
<MaxAmdLetterDate>06/30/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2051644</AwardID>
<Investigator>
<FirstName>Evan</FirstName>
<LastName>Drumwright</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Evan Drumwright</PI_FULL_NAME>
<EmailAddress>drum@dextrousrobotics.com</EmailAddress>
<PI_PHON>9015980441</PI_PHON>
<NSF_ID>000802056</NSF_ID>
<StartDate>06/30/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>DEXTROUS ROBOTICS, INC.</Name>
<CityName>Memphis</CityName>
<ZipCode>381042027</ZipCode>
<PhoneNumber>9015980441</PhoneNumber>
<StreetAddress>1350 Concourse Ave Ste 465</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>117048112</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>DEXTROUS ROBOTICS, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Dextrous Robotics]]></Name>
<CityName>Memphis</CityName>
<StateCode>TN</StateCode>
<ZipCode>381042010</ZipCode>
<StreetAddress><![CDATA[1350 Concourse Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>7632</Code>
<Text>HUMAN-ROBOT INTERACTION</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~256000</FUND_OBLG>
</Award>
</rootTag>
