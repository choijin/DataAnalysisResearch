{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis for NSF Grant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import nltk and download stopwords if first time.\n",
    "# import nltk \n",
    "# nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, webtext, PlaintextCorpusReader\n",
    "import inflect\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove null values for abstract and division_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null(df):\n",
    "    df1 = df.copy()\n",
    "    df1 = df[pd.notnull(df['abstract'])]\n",
    "    df1 = df[pd.notnull(df['division_code'])]   \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering irrelevant grants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_grants(df):\n",
    "    df1 = df.copy() \n",
    "    # Filter irrelevant grants\n",
    "    df1 = df1[~df1['ProgramReference'].apply(lambda x: 'REU SUPP-Res Exp for Ugrd Supp' in x)]\n",
    "    df1 = df1[~df1['ProgramReference'].apply(lambda x: 'RET SITE-Res Exp for Tchr Site' in x)]\n",
    "    df1 = df1[~df1['ProgramReference'].apply(lambda x: 'Improv Undergrad STEM Ed(IUSE)' in x)]\n",
    "    df1 = df1[~df1['ProgramReference'].apply(lambda x: 'UNDERGRADUATE EDUCATION' in x)]\n",
    "    df1 = df1[~df1['ProgramReference'].apply(lambda x: 'CONFERENCE AND WORKSHOPS' in x)]\n",
    "    df1 = df1[~df1['ProgramReference'].apply(lambda x: 'HBCU-Strengthening Research Capacities' in x)]\n",
    "    df1 = df1[~df1['title'].str.contains(\"REU\", case = False)]\n",
    "    df1 = df1[~df1['title'].str.contains(\"SBIR\", case = False)]\n",
    "    df1 = df1[~df1['title'].str.contains(\"STTR\", case = False)]\n",
    "    df1 = df1[~df1['title'].str.contains(\"conference\", case = False)]\n",
    "    # Alternative: df = df[df['title'].str.contains(\"conference\", case = False) == False]\n",
    "    \n",
    "    if df1['abstract'].str.contains(\"REU\", case = False).any():\n",
    "        df1 = df1[df1['abstract'].str.contains(\"REU\", case = False) == False]\n",
    "    df1 = df1[~df1['abstract'].str.contains(\"SBIR\", case = False)]\n",
    "    df1 = df1[~df1['abstract'].str.contains(\"STTR\", case = False)]\n",
    "    df1 = df1[~df1['abstract'].str.contains(\"conference\", case = False)]\n",
    "    # Removes duplicate titles. These are same research under different PIs\n",
    "    df1.drop_duplicates(subset=['title'], inplace=True)\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean title to be the last two elements of a \":\" separated list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function removes the program categories that are meaningless to the titles\n",
    "def clean_title(title):\n",
    "    if(\":\" not in title):\n",
    "        return title\n",
    "    split_title = title.split(\":\")\n",
    "    title_with_last_two = split_title[-2:]\n",
    "    first_element = title_with_last_two[0]\n",
    "    last_element = title_with_last_two[1]\n",
    "    split_first_element = first_element.split()\n",
    "\n",
    "    if(len(split_first_element) > 1):\n",
    "        if ((\"Excellence in Research\" in first_element) or\n",
    "            (\"ECR EIE DCL\" in first_element) or\n",
    "            (\"Research Initiation Award\" in first_element) or\n",
    "            (\"RET Site\" in first_element) or\n",
    "            (\"Collaborative Research\" in first_element) or\n",
    "            (\"ECR DBER DCL\" in first_element) or\n",
    "            (\"Catalyst Project\" in first_element) or\n",
    "            (\"COLLABORATIVE RESEARCH\" in first_element) or\n",
    "            (\"REU Site\" in first_element)    \n",
    "            ):\n",
    "            return last_element\n",
    "        return (\":\").join(title_with_last_two)\n",
    "    return last_element\n",
    "\n",
    "def clean_title_df(df):\n",
    "    df1 = df.copy()\n",
    "    df1['title'] = df1['title'].apply(clean_title)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove meaningless words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is different from filter_grants because filter_grants method removes the entire row,\n",
    "# whereas this function removes specific words in the title so that it will not count its\n",
    "# frequency.\n",
    "\n",
    "def remove_irrelevent(df, keywords):\n",
    "    df1 = df.copy()\n",
    "    df1.title = df1.title.apply(lambda x: [word for word in x if word not in keywords])\n",
    "    df1.abstract = df1.abstract.apply(lambda x: [word for word in x if word not in keywords])\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter stopwords\n",
    "\n",
    "Filtering function retrieved from stackoverflow: https://stackoverflow.com/questions/50444346/fast-punctuation-removal-with-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(df):\n",
    "    stop = stopwords.words('english')\n",
    "    df1 = df.copy()\n",
    "    # This function removes all characters except dash symbol   \n",
    "    punct = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{}~'   # `|` is not present here\n",
    "    transtab = str.maketrans(dict.fromkeys(punct, ''))\n",
    "    df1['title'] = '|'.join(df1['title'].tolist()).translate(transtab).split('|')\n",
    "    df1['abstract'] = '|'.join(df1['abstract'].tolist()).translate(transtab).split('|')\n",
    "    \n",
    "    # Convert to lower case\n",
    "    df1['title'] = df1['title'].apply(lambda x: x.lower())\n",
    "    df1['abstract'] = df1['abstract'].apply(lambda x: x.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    df1['title'] = df1['title'].apply(lambda x: [item for item in x.split() if item not in stop])\n",
    "    df1['abstract'] = df1['abstract'].apply(lambda x: [item for item in x.split() if item not in stop])\n",
    "    df1.reset_index(drop=True, inplace=True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load csv and convert to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021 = pd.read_csv('years_csv/df_2021.csv', index_col=0)\n",
    "df_2020 = pd.read_csv('years_csv/df_2020.csv', index_col=0)\n",
    "df_2019 = pd.read_csv('years_csv/df_2019.csv', index_col=0)\n",
    "df_2018 = pd.read_csv('years_csv/df_2018.csv', index_col=0)\n",
    "df_2017 = pd.read_csv('years_csv/df_2017.csv', index_col=0)\n",
    "df_2016 = pd.read_csv('years_csv/df_2016.csv', index_col=0)\n",
    "df_2015 = pd.read_csv('years_csv/df_2015.csv', index_col=0)\n",
    "df_2014 = pd.read_csv('years_csv/df_2014.csv', index_col=0)\n",
    "df_2013 = pd.read_csv('years_csv/df_2013.csv', index_col=0)\n",
    "df_2012 = pd.read_csv('years_csv/df_2012.csv', index_col=0)\n",
    "df_2011 = pd.read_csv('years_csv/df_2011.csv', index_col=0)\n",
    "df_2010 = pd.read_csv('years_csv/df_2010.csv', index_col=0)\n",
    "df_2009 = pd.read_csv('years_csv/df_2009.csv', index_col=0)\n",
    "df_2008 = pd.read_csv('years_csv/df_2008.csv', index_col=0)\n",
    "df_2007 = pd.read_csv('years_csv/df_2007.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all filtering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(df):\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    # Drop abstracts/division code that are null\n",
    "    df1 = remove_null(df1)\n",
    "    \n",
    "    # Working with Division of Computer and Network System\n",
    "    df1 = df1[df1['division_code'].str.contains(\"CNS\") == True] \n",
    "\n",
    "    # Filters irrelevant grants\n",
    "    df1 = filter_grants(df1)\n",
    "    \n",
    "    # Filters title with categories of programs\n",
    "    df1 = clean_title_df(df1)   \n",
    "    \n",
    "    # Filters stopwords\n",
    "    df1 = filter_stopwords(df1)\n",
    "\n",
    "    # Remove keywords\n",
    "    keywords = ['track','iucrc','research','scc-cvic-pg','scc-civic-pg', 'using', 'toward', 'student','broader','impacts',\n",
    "                'intellectual','merit','support','evaluation','award','reflects','nsfs','statutory','mission','deemed',\n",
    "                'worthy','foundation','review','criteria','project','aims','high','school','also','undergraduate','graduate',\n",
    "                'new','b','nsf','next','department','state','university','united','states','grant','planning','collaboration',\n",
    "                'challenge','innovation','professional','wide','range','publicly','available','made','available','large',\n",
    "                'number','novel','communities','curriculum','centers','broadening','participation','k-12', 'brings', 'together',\n",
    "               'whole', 'part', 'american', 'plan', 'part', 'best', 'i-corps', 'i-corp']\n",
    "    \n",
    "    df1 = remove_irrelevent(df1, keywords)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_2021_filtered = filter(df_2021)\n",
    "df_2020_filtered = filter(df_2020)\n",
    "df_2019_filtered = filter(df_2019)\n",
    "df_2018_filtered = filter(df_2018)\n",
    "df_2017_filtered = filter(df_2017)\n",
    "df_2016_filtered = filter(df_2016)\n",
    "df_2015_filtered = filter(df_2015)\n",
    "df_2014_filtered = filter(df_2014)\n",
    "df_2013_filtered = filter(df_2013)\n",
    "df_2012_filtered = filter(df_2012)\n",
    "df_2011_filtered = filter(df_2011)\n",
    "\n",
    "# Working just with years 2021 - 2011\n",
    "# df_2010_filtered = filter(df_2010)\n",
    "# df_2009_filtered = filter(df_2009)\n",
    "# df_2008_filtered = filter(df_2008)\n",
    "# df_2007_filtered = filter(df_2007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020_filtered.to_csv(\"df_2020_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_year(year):    \n",
    "    dispatcher = {'2021': df_2021_filtered,\n",
    "                  '2020': df_2020_filtered,\n",
    "                  '2019': df_2019_filtered,\n",
    "                  '2018': df_2018_filtered,\n",
    "                  '2017': df_2017_filtered,\n",
    "                  '2016': df_2016_filtered,\n",
    "                  '2015': df_2015_filtered,\n",
    "                  '2014': df_2014_filtered,\n",
    "                  '2013': df_2013_filtered,\n",
    "                  '2012': df_2012_filtered,\n",
    "                  '2011': df_2011_filtered,\n",
    "                  '2010': df_2010_filtered,\n",
    "                  '2009': df_2009_filtered,\n",
    "                  '2008': df_2008_filtered,\n",
    "                  '2007': df_2007_filtered}\n",
    "    \n",
    "    return dispatcher[year]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter plural words. Count method calls upon this function\n",
    "def get_singular(plural_noun):\n",
    "    p = inflect.engine()\n",
    "    plural = p.singular_noun(plural_noun)\n",
    "    if (plural):\n",
    "        return plural\n",
    "    else:\n",
    "        return plural_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find frequency of each word in a string in Python using dictionary.   \n",
    "# If there exists a word then simply\n",
    "# increase the word's frequency.\n",
    "def count(elements, dictionary):\n",
    "    \n",
    "    # Gets the word in singular form. If the word ends with two ss, \n",
    "    # it will skil the get_singular function (i.e. wireless)\n",
    "    if (elements[-2:] == \"ss\") or (elements[-2:] == \"is\"):\n",
    "        singular_form = elements\n",
    "    else:\n",
    "        singular_form = get_singular(elements)\n",
    "        \n",
    "    if singular_form in dictionary:\n",
    "        dictionary[singular_form] += 1\n",
    "   \n",
    "    # If the dictionary does not have the key as \"elements\" \n",
    "    # then create a key \"elements\" and assign its value to 1.\n",
    "    else:\n",
    "        dictionary.update({singular_form: 1}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a dictionary of entire frequencies\n",
    "\n",
    "def dict_frequency(df, column):\n",
    "    dictionary = {} \n",
    "    for rows in df[column]:\n",
    "        # Take each word from lst and pass it to the method count.\n",
    "        for elements in rows:\n",
    "            count(elements, dictionary)\n",
    "            \n",
    "    sorted_dict = sorted(dictionary.items(), key=lambda x: x[1], reverse=True) \n",
    "    \n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a dictionary of frequencies for a single row\n",
    "\n",
    "def dict_frequency_row(series, column):\n",
    "    dictionary = {} \n",
    "    # Take each word from lst and pass it to thh count method.\n",
    "    for elements in series[column]:\n",
    "        count(elements, dictionary)\n",
    "            \n",
    "    sorted_dict = sorted(dictionary.items(), key=lambda x: x[1], reverse=True) \n",
    "    \n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataframe of frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a dataframe of frequency of each word in the title\n",
    "def title_freq_single(df):\n",
    "    df1 = df.copy()\n",
    "    # Find frequency of each word in the title\n",
    "    freq_title = dict_frequency(df1, 'title')\n",
    "    df_frq_title = pd.DataFrame(freq_title)\n",
    "    df_frq_title.set_axis(['Word','Frequency'], axis=1, inplace=True)\n",
    "    df_frq_title.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_frq_title\n",
    "\n",
    "# Return a dataframe of frequency of each word in the abstract\n",
    "def abstract_freq_single(df):\n",
    "    df1 = df.copy()\n",
    "    # Find frequency of each word in the title\n",
    "    freq_abstract = dict_frequency(df1, 'title')\n",
    "    df_frq_abstract = pd.DataFrame(freq_abstract)\n",
    "    df_frq_abstract.set_axis(['Word','Frequency'], axis=1, inplace=True)\n",
    "    df_frq_abstract.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_frq_abstract\n",
    "\n",
    "title_2021_one_word = title_freq_single(get_df_year('2021'))\n",
    "title_2020_one_word = title_freq_single(get_df_year('2020'))\n",
    "title_2019_one_word = title_freq_single(get_df_year('2019'))\n",
    "\n",
    "abstract_2021_one_word = abstract_freq_single(get_df_year('2021'))\n",
    "abstract_2020_one_word = abstract_freq_single(get_df_year('2020'))\n",
    "abstract_2019_one_word = abstract_freq_single(get_df_year('2019'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot single word frequency counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = title_2021_one_word.iloc[:10]\n",
    "df2 = title_2020_one_word.iloc[:10]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(df1['Word'], df1['Frequency'], width = 0.8, label='2021')\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Most frequent words\")\n",
    "plt.xticks(rotation=90, ha=\"right\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(df2['Word'], df2['Frequency'], width = 0.8, label='2020', color='orange')\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Most frequent words\")\n",
    "plt.xticks(rotation=90, ha=\"right\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets frequent two-three word phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to find bigrams, which are pairs of words\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_phrases() takes arguments: \n",
    "1) required positional argument: a data frame,\n",
    "2) a variable number of column titles,\n",
    "3) optional keyword argument: the length of a phrase (1-3 words), default is 2 \n",
    "\"\"\"\n",
    "def get_phrases(df, *args, num_words=2):\n",
    "    columns = [*args]\n",
    "    # Creates a list of words for context\n",
    "    word_list = []\n",
    "    freq_list = []\n",
    "    for column in columns:\n",
    "        for rows in df[column]:\n",
    "            for words in rows:\n",
    "                word_list.append(words.lower())\n",
    "    print(word_list)\n",
    "\n",
    "    # Runs the library that counts most frequent phrases\n",
    "    if(num_words == 1):\n",
    "        return dict_frequency(df, column)\n",
    "    if(num_words == 2):\n",
    "        bigram_collocation = BigramCollocationFinder.from_words(word_list)\n",
    "        # only bigrams that appear 3+ times\n",
    "        bigram_collocation.apply_freq_filter(3)\n",
    "        #Gets the top bigrams with PMI Scores\n",
    "        for k,v in bigram_collocation.ngram_fd.items():\n",
    "              freq_list.append((k,v))\n",
    "        freq_list = [(' '.join(values[0]), values[1]) for values in freq_list]\n",
    "\n",
    "    if(num_words == 3):\n",
    "        freq_list = []\n",
    "        trigram_collocation = TrigramCollocationFinder.from_words(word_list)\n",
    "        # only trigrams that appear 3+ times\n",
    "        trigram_collocation.apply_freq_filter(3)\n",
    "        freq_list = []\n",
    "        for k,v in trigram_collocation.ngram_fd.items():\n",
    "              freq_list.append((k,v))\n",
    "        freq_list = [(' '.join(values[0]), values[1]) for values in freq_list]       \n",
    "        \n",
    "    freq_list.sort(key=itemgetter(1), reverse=True)\n",
    "    return freq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list top TWO word phrases with frequencies for a given df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get top two-word phrases for title\n",
    "top_two_phrase_title_2021 = get_phrases(get_df_year('2021'), 'title', num_words=2)\n",
    "top_two_phrase_title_2020 = get_phrases(get_df_year('2020'), 'title', num_words=2)\n",
    "top_two_phrase_title_2019 = get_phrases(get_df_year('2019'), 'title', num_words=2)\n",
    "top_two_phrase_title_2018 = get_phrases(get_df_year('2018'), 'title', num_words=2)\n",
    "top_two_phrase_title_2017 = get_phrases(get_df_year('2017'), 'title', num_words=2)\n",
    "top_two_phrase_title_2016 = get_phrases(get_df_year('2016'), 'title', num_words=2)\n",
    "top_two_phrase_title_2015 = get_phrases(get_df_year('2015'), 'title', num_words=2)\n",
    "top_two_phrase_title_2014 = get_phrases(get_df_year('2014'), 'title', num_words=2)\n",
    "top_two_phrase_title_2013 = get_phrases(get_df_year('2013'), 'title', num_words=2)\n",
    "top_two_phrase_title_2012 = get_phrases(get_df_year('2012'), 'title', num_words=2)\n",
    " \n",
    "# Get top two-word phrases for abstract\n",
    "top_two_phrase_abstract_2021 = get_phrases(get_df_year('2021'), 'abstract', num_words=2)\n",
    "top_two_phrase_abstract_2020 = get_phrases(get_df_year('2020'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2019 = get_phrases(get_df_year('2019'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2018 = get_phrases(get_df_year('2018'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2017 = get_phrases(get_df_year('2017'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2016 = get_phrases(get_df_year('2016'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2015 = get_phrases(get_df_year('2015'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2014 = get_phrases(get_df_year('2014'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2013 = get_phrases(get_df_year('2013'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2012 = get_phrases(get_df_year('2012'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2011 = get_phrases(get_df_year('2011'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2010 = get_phrases(get_df_year('2010'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2009 = get_phrases(get_df_year('2009'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2008 = get_phrases(get_df_year('2008'), 'abstract', num_words=2) \n",
    "top_two_phrase_abstract_2007 = get_phrases(get_df_year('2007'), 'abstract', num_words=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020_list = pd.DataFrame(top_two_phrase_abstract_2020, columns=['phrase', 'frequency'])\n",
    "df_2020_list.to_csv(\"df_2020_topics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list of top THREE word phrases with frequencies for a given df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top two-word phrases for title\n",
    "top_three_phrase_title_2021 = get_phrases(df_2021_filtered, 'title', num_words=3)\n",
    "top_three_phrase_title_2020 = get_phrases(df_2020_filtered, 'title', num_words=3)\n",
    "top_three_phrase_title_2019 = get_phrases(df_2019_filtered, 'title', num_words=3)\n",
    "top_three_phrase_title_2018 = get_phrases(df_2018_filtered, 'title', num_words=3)\n",
    "top_three_phrase_title_2017 = get_phrases(df_2017_filtered, 'title', num_words=3)\n",
    "top_three_phrase_title_2016 = get_phrases(df_2016_filtered, 'title', num_words=3)\n",
    "top_three_phrase_title_2015 = get_phrases(df_2015_filtered, 'title', num_words=3)\n",
    "top_three_phrase_title_2014 = get_phrases(df_2014_filtered, 'title', num_words=3)\n",
    "top_three_phrase_title_2013 = get_phrases(df_2013_filtered, 'title', num_words=3)\n",
    "top_three_phrase_title_2012 = get_phrases(df_2012_filtered, 'title', num_words=3)\n",
    "\n",
    "# Get top two-word phrases for abstract\n",
    "top_three_phrase_abstract_2021 = get_phrases(df_2021_filtered, 'abstract', num_words=3)\n",
    "top_three_phrase_abstract_2020 = get_phrases(df_2020_filtered, 'abstract', num_words=3)\n",
    "top_three_phrase_abstract_2019 = get_phrases(df_2019_filtered, 'abstract', num_words=3)\n",
    "top_three_phrase_abstract_2018 = get_phrases(df_2018_filtered, 'abstract', num_words=3)\n",
    "top_three_phrase_abstract_2017 = get_phrases(df_2017_filtered, 'abstract', num_words=3)\n",
    "top_three_phrase_abstract_2016 = get_phrases(df_2016_filtered, 'abstract', num_words=3)\n",
    "top_three_phrase_abstract_2015 = get_phrases(df_2015_filtered, 'abstract', num_words=3)\n",
    "top_three_phrase_abstract_2014 = get_phrases(df_2014_filtered, 'abstract', num_words=3)\n",
    "top_three_phrase_abstract_2013 = get_phrases(df_2013_filtered, 'abstract', num_words=3)\n",
    "top_three_phrase_abstract_2012 = get_phrases(df_2012_filtered, 'abstract', num_words=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Frequency of Two and Three Word Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plots two bar graphs, one for two-worded phrases \n",
    "and another for three-worded phrases.\n",
    "argument: list of two-worded phrases, three-worded phrases\n",
    "\"\"\"\n",
    "def plot_phrases(two_word_phrases, three_word_phrases):\n",
    "    two_word_x_values = [values[0] for values in two_word_phrases[:5]]\n",
    "    two_word_y_values = [values[1] for values in two_word_phrases[:5]]\n",
    "    plt.figure(figsize=(15,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.bar(two_word_x_values, two_word_y_values,\n",
    "            width = 0.8, label='Two Words')\n",
    "    plt.xlabel(\"Two Word Phrases\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Top 5 Two Word Phrases\")\n",
    "    plt.xticks(rotation=90, ha=\"right\")\n",
    "    plt.legend()\n",
    "    # plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "\n",
    "    three_word_x_values = [' '.join(values[0]) for values in three_word_phrases[:5]]\n",
    "    three_word_y_values = [values[1] for values in three_word_phrases[:5]]\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.bar(three_word_x_values, three_word_y_values,\n",
    "            width = 0.8, label='Three Words', color='purple')\n",
    "    plt.xlabel(\"Three Word Phrases\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Top 5 Three Word Phrases\")\n",
    "    plt.xticks(rotation=90, ha=\"right\")\n",
    "    plt.legend()\n",
    "    # plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_phrases(top_two_phrase_title_2021,top_three_phrase_title_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_phrases(top_three_phrase_abstract_2021, top_three_phrase_abstract_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a dataframe of word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_word(word_list):\n",
    "    freq_list = []\n",
    "    \n",
    "    bigram_collocation = BigramCollocationFinder.from_words(word_list)\n",
    "    \n",
    "    for k,v in bigram_collocation.ngram_fd.items():\n",
    "          freq_list.append((k,v))\n",
    "    freq_list = [(' '.join(values[0]), values[1]) for values in freq_list]\n",
    "    return freq_list\n",
    "\n",
    "\"\"\"\n",
    "Creates a data frame with keyword list as columns \n",
    "and add the frequency of the keyword per each row.\n",
    "Args:\n",
    "df - dataframe that we are working with\n",
    "keywords_list - the keywords generated from get_phrases\n",
    "num_keyword - number of keyword to find frequency for\n",
    "\"\"\"\n",
    "def df_of_keyword(df, keywords_list, num_keyword):  \n",
    "   \n",
    "    # Create a copy of df\n",
    "    df1 = df.copy()\n",
    "\n",
    "    # Iterate through dataframe\n",
    "    for index, row in df1.iterrows():\n",
    "\n",
    "        # Get the bigram of abstract information of a row\n",
    "        rows_bigram = get_two_word(row['abstract'] + row['title'])\n",
    "        # Iterate through the top frequent words that we have generated previously\n",
    "        for phrase in keywords_list[:num_keyword]:\n",
    "            \n",
    "            # Get column name of top keywords by combining first element of the tuple\n",
    "            column = phrase[0]\n",
    "\n",
    "            # Iterate through the row abstracts bigram\n",
    "            for row_phrase_abstract in rows_bigram:\n",
    "                \n",
    "                # Compare if the keyword is in the row abstracts bigram\n",
    "                if (phrase[0] == row_phrase_abstract[0]):\n",
    "    \n",
    "                    # If keyword is in the row abstract, set the frequency\n",
    "                    df1.loc[index, column] = row_phrase_abstract[1]\n",
    "    \n",
    "    # Fill NaN with zeros and get columns of keyword\n",
    "#     df1 = df1.iloc[:,-num_keyword:].fillna(0)\n",
    "    df1 = df1.fillna(0)\n",
    "    \n",
    "#     # Add effective_date column\n",
    "#     df1.insert(loc=0, column='effective_date', value=df.effective_date)\n",
    "    \n",
    "    # Set effective_date as datetime\n",
    "    df1['effective_date'] = pd.to_datetime(df.effective_date)\n",
    "    \n",
    "    # Sort and reset index\n",
    "    df1.sort_values(by=['effective_date'], inplace=True, ascending=False)\n",
    "    df1.reset_index(drop=True, inplace=True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering additional keywords per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method removes phrases that are irrelevant to the research study.\n",
    "Args:\n",
    "year_list: list of keywords and frequency as a tuple in a list\n",
    "Return: year_list\n",
    "\"\"\"\n",
    "def remove_phrases(year_list):\n",
    "\n",
    "    phrases = ['learning algorithms', 'funded rescue', '2021 public', 'act 2021', 'public law', 'science engineering', \n",
    "               'foundations funded', 'things iot', 'securitybrbrthis foundations', 'goal develop', 'outreach activities',\n",
    "               'science technology', 'learning techniques', 'underrepresented groups', 'national network', \n",
    "               'networking opportunities', 'technology engineering', 'chest center', 'proposed work', 'five years',\n",
    "               'thrust first', 'second thrust', 'recent years', 'five years, smart connected', 'underrepresented students',\n",
    "               'address challenges', 'quality life', 'rescue act', 'community members', 'decision making', \n",
    "               'science engineering', 'future internet', 'technology concepts', 'underrepresented groups', 'transition work',\n",
    "               'opportunities training', 'software tools', 'foundations impactcommercial', 'contribute national', \n",
    "               'significant impact', 'work marketplace', 'impactcommercial potential', 'researchers entrepreneurs', \n",
    "               'enable groups', 'hardware software', 'spectrum access', 'teams transition', 'industry partners',\n",
    "               'formal verification', 'distributed systems', 'real time', 'data collected', 'design implementation',\n",
    "               'wireless networking', 'data sets', ]\n",
    "            \n",
    "    for phrase in phrases:\n",
    "        year_list = [x for x in year_list if x[0] != phrase]\n",
    "        \n",
    "    return year_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function generates a DataFrame of given years \n",
    "Args: years - number of years, single or multiple, to generate\n",
    "Return: a DataFrame, single or combined\n",
    "\"\"\"\n",
    "def combine_df(*years):     \n",
    "    df = pd.DataFrame() \n",
    "    for year in years:\n",
    "        df_year = get_df_year(year)\n",
    "        df_year.effective_date = year\n",
    "        df = pd.concat([df, df_year], sort= False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate CSV for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method creates a csv file for given years as the parameter\n",
    "Args:\n",
    "keywordNum - number of most of occured keywords we want to use for constructing the dataframe\n",
    "years - number of years we want to generate a dataframe\n",
    "Return:\n",
    "a dataframe and a csv file in the same directory\n",
    "\"\"\"\n",
    "def df_to_csv(keywordNum, *years):\n",
    "    dataframe = combine_df(*years)\n",
    "    keywords = get_phrases(dataframe, 'abstract', num_words=2)\n",
    "    keywords = remove_phrases(keywords)\n",
    "    df = df_of_keyword(dataframe,keywords, keywordNum)\n",
    "    df.drop(columns=['award_id', 'institution', 'expiration_date', 'division_name', 'investigator', \n",
    "                     'state_name', 'ProgramReference', 'program_element_name'], inplace=True)\n",
    "#     df1 = df.iloc[:,-keywordNum:]\n",
    "#     for column in df1:\n",
    "#         df1.loc[df1[column] > 0, column] = 1\n",
    "#     df.insert(loc=0, column='effective_date', value=df.effective_date)\n",
    "#     df.insert(loc=1, column='amount', value=df.amount)\n",
    "#     df1.insert(loc=2, column='total keywords', value=df1.iloc[:,-keywordNum:].sum(axis=1))\n",
    "    df.to_csv('csv_data/df_awards.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create a csv file for ML \n",
    "# df_analysis = df_to_csv(50, '2020','2019','2018','2017','2016','2015','2014','2013','2012','2011')\n",
    "# df_analysis.iloc[:,-50:].transpose()[0].to_csv('csv_data/phraseList.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a phrase list and save it as a csv\n",
    "df_combined_2021_2011 = combine_df('2021', '2020', '2019', '2018', '2017', '2016', '2015', '2014', '2013', '2012', '2011')\n",
    "phrases = get_phrases(df_combined_2021_2011, 'abstract', num_words=2)\n",
    "phrases = remove_phrases(phrases)\n",
    "df_phrases = pd.DataFrame(phrases, columns=['phrase', 'frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV with only topics and its frequency in the award\n",
    "df_combined_2021_2011.to_csv(\"csv_data/combined_2021_2011.csv\", index=False)\n",
    "df_phrases.to_csv('csv_data/PhraseFrequency.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequency list of key phrases in abstract\n",
    "phrases_test = get_phrases(get_df_year('2021'), 'abstract', num_words=2)\n",
    "# Remove irrelevant phrases\n",
    "phrases_test = remove_phrases(phrases)\n",
    "# Return a dataframe with keyword as columns\n",
    "df_test = df_of_keyword(get_df_year('2021'), phrases_test, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function generates heatmap with given number of keyword and year.\n",
    "The heatmap can be generated with multiple years.\n",
    "Args: num_keyword - number of keyword phrases to analyze correlation\n",
    "      years - number of years, single or multiple, to generate a heatmap\n",
    "Return: heatmap graph\n",
    "\"\"\"\n",
    "def heatmap_generate(num_keyword, *years): \n",
    "    \n",
    "    df = combine_df(*years)\n",
    "    \n",
    "    # Get frequency list of key phrases in abstract\n",
    "    phrases = get_phrases(df, 'abstract', num_words=2)\n",
    "    \n",
    "    # Remove irrelevant phrases\n",
    "    phrases = remove_phrases(phrases)\n",
    "    # Return a dataframe with keyword as columns\n",
    "    df = df_of_keyword(df, phrases, num_keyword)\n",
    "    # Display heatmap\n",
    "    plt.rcParams['figure.figsize'] = [15, 12]\n",
    "    sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heatmap_generate(50, '2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heatmap_generate(50, '2021', '2020', '2019', '2018', '2013')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the frequency topics inside abstract section of the award"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_keyword_row(df):  \n",
    "    # Create a copy of df\n",
    "    df1 = df.copy()\n",
    "    # Iterate through dataframe\n",
    "    for index, row in df1.iterrows():      \n",
    "        # Get single most common word of a row\n",
    "        row_single = dict_frequency_row(row, 'abstract')\n",
    "        \n",
    "        # Add columns for top 1 single keyword\n",
    "        df1.at[index, 'top1_single_keyword'] = row_single[0][0]\n",
    "        # Add columns for top 1 single keyword frequency\n",
    "        df1.at[index, 'top1_single_frequency'] = row_single[0][1]\n",
    "        # Add columns for top 2 single keyword\n",
    "        df1.at[index, 'top2_single_keyword'] = row_single[1][0]\n",
    "        # Add columns for top 2 single keyword frequency\n",
    "        df1.at[index, 'top2_single_frequency'] = row_single[1][1]\n",
    "        # Add columns for top 3 single keyword\n",
    "        df1.at[index, 'top3_single_keyword'] = row_single[2][0]\n",
    "        # Add columns for top 3 single keyword frequency\n",
    "        df1.at[index, 'top3_single_frequency'] = row_single[2][1]\n",
    "                \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a trend graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a combined top words for multiple years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency_phrase(year):   \n",
    "    dispatcher = {'2021': top_two_phrase_abstract_2021,\n",
    "                  '2020': top_two_phrase_abstract_2020,\n",
    "                  '2019': top_two_phrase_abstract_2019,\n",
    "                  '2018': top_two_phrase_abstract_2018,\n",
    "                  '2017': top_two_phrase_abstract_2017,\n",
    "                  '2016': top_two_phrase_abstract_2016,\n",
    "                  '2015': top_two_phrase_abstract_2015,\n",
    "                  '2014': top_two_phrase_abstract_2014,\n",
    "                  '2013': top_two_phrase_abstract_2013,\n",
    "                  '2012': top_two_phrase_abstract_2012,\n",
    "                  '2011': top_two_phrase_abstract_2011,\n",
    "                  '2010': top_two_phrase_abstract_2010,\n",
    "                  '2009': top_two_phrase_abstract_2009,\n",
    "                  '2008': top_two_phrase_abstract_2008,\n",
    "                  '2007': top_two_phrase_abstract_2007}\n",
    "    \n",
    "    return dispatcher[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined each year's top phrases into one single list.\n",
    "# It will eliminate the union phrases.\n",
    "def combine_phrases(num, *list_years):\n",
    "    mergedlist = []\n",
    "    for year in list_years:\n",
    "        list_frequency = get_frequency_phrase(year)\n",
    "        list_frequency = remove_phrases(list_frequency)\n",
    "        phraseList = [values[0] for values in list_frequency[:num]]\n",
    "        mergedlist = list(set(mergedlist + phraseList))\n",
    "    return mergedlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a DataFrame that counts the number of frequency\n",
    "of each mergedlist key phrases.\n",
    "\n",
    "Args: list_years - list of tuples that has key phrases and its frequency\n",
    "Return: df - a DataFrame with yearly data of key phrases and its frequency\n",
    "\"\"\"\n",
    "def get_df_years(mergedlist, *years):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for year in years:      \n",
    "        list_frequency = get_frequency_phrase(year)\n",
    "        list_in_merged = [i for i in list_frequency if i[0] in mergedlist]\n",
    "        dict_from_list = dict(list_in_merged)\n",
    "        for title in mergedlist:\n",
    "            if title not in dict_from_list.keys():\n",
    "                dict_from_list[title] = 0\n",
    "\n",
    "        df1 = pd.DataFrame.from_dict(dict_from_list.items())    \n",
    "        df1['year'] = year\n",
    "        df = pd.concat([df, df1], sort= False)\n",
    "    df.set_axis(['phrase', 'frequency', 'year'], axis=1, inplace=True)\n",
    "    df = df.pivot('year','phrase','frequency')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mergedlist = combine_phrases(5, '2021', '2020', '2019', '2018', '2017', '2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_years_merged = get_df_years(mergedlist, '2020', '2019', '2018', '2017', '2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,3), dpi=300)\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "df_years_merged.iloc[:,len(df_years_merged.columns)//2:].plot()\n",
    "plt.xlabel(\"Years\")\n",
    "plt.ylabel(\"Occurance of keyword in a year\")\n",
    "plt.title(\"Keyword Trend\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend(bbox_to_anchor=(1.1, .7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,3), dpi=300)\n",
    "plt.rcParams['figure.figsize'] = [8,6]\n",
    "df_years_merged.iloc[:,0:len(df_years_merged.columns)//2].plot()\n",
    "plt.xlabel(\"Years\")\n",
    "plt.ylabel(\"Occurance of keyword in a year\")\n",
    "plt.title(\"Keyword Trend\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend(bbox_to_anchor=(1.1, .5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a DataFrame with Award Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_award_amount(year, phrase):\n",
    "    df = get_df_year(year)\n",
    "    total_amount_for_phrase = 0\n",
    "    for index, row in df.iterrows():\n",
    "        abstract_and_title_string = (' ').join(row['abstract']) + (' ').join(row['title'])\n",
    "        if(phrase in abstract_and_title_string):\n",
    "            total_amount_for_phrase += row['amount']\n",
    "    return total_amount_for_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_coded_phrases = df_years_merged.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amount_df(year, phrases):\n",
    "    new_df = pd.DataFrame()\n",
    "    for phrase in phrases:\n",
    "        amount_to_add = get_award_amount(year, phrase)\n",
    "        new_row = pd.Series(data={'amount':amount_to_add}, name=phrase)\n",
    "        new_df = new_df.append(new_row)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_df = get_amount_df('2020', hard_coded_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_amounts(df):\n",
    "    x_values = list(amount_df.index.values.tolist())\n",
    "    y_values = [row for row in amount_df['amount']]\n",
    "    plt.figure(figsize=(15,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.bar(x_values, y_values,\n",
    "            width = 0.8)\n",
    "    plt.xlabel(\"Phrase\")\n",
    "    plt.ylabel(\"Amount (millions)\")\n",
    "    plt.title(\"Total Amount vs Phrase (2021)\")\n",
    "    plt.xticks(rotation=90, ha=\"right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_amounts(df_2020_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a Dataframe for Machine Learning purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method prepares the dataframe for our ML purpose. Takes in two parameters, num and *years, and generates a\n",
    "dataframe with frequency, award amount, and year columns per each key phrases.\n",
    "Args:\n",
    "num - how much key phrases we want to work with. The higher the number, more key phrases will be added to the dataframe\n",
    "years - the years of research we want to analyze\n",
    "Return:\n",
    "dataframe with frequency, award amount, and year columns.\n",
    "\"\"\"\n",
    "\n",
    "def df_for_ML(num, *years):\n",
    "    \n",
    "    mergedPhrase = []    \n",
    "    # Create a list of phrases for all the years in the parameter\n",
    "    for year in years:\n",
    "        list_frequency = get_frequency_phrase(year)\n",
    "        list_frequency = remove_phrases(list_frequency)\n",
    "        year_phrase = [values[0] for values in list_frequency[:num]]\n",
    "        \n",
    "        mergedPhrase = list(set(mergedPhrase + year_phrase))\n",
    "    \n",
    "    # Create a dataframe with years, frequency, award amount\n",
    "    df = pd.DataFrame()\n",
    "    for year in years:                   \n",
    "        # Create a list of phrases and frequency if it matches the mergedPhrase\n",
    "        list_frequency = get_frequency_phrase(year)\n",
    "        list_merged = [i for i in list_frequency if i[0] in mergedPhrase]\n",
    "        # Convert to dictionary\n",
    "        dict_from_list = dict(list_merged)\n",
    "        # Iterate through mergedPhrases and if the title is not in the phrases, set to 1.\n",
    "        df_amount = pd.DataFrame()\n",
    "        for title in mergedPhrase:\n",
    "            if title not in dict_from_list.keys():\n",
    "                dict_from_list[title] = 0\n",
    "            \n",
    "            # Create a dataframe with award amount as a column\n",
    "            amount_to_add = get_award_amount(year, title)\n",
    "            new_row = pd.Series(data={'amount':amount_to_add}, name=title)\n",
    "            df_amount = df_amount.append(new_row, ignore_index=True)        \n",
    "        # Convert dictionary of phrases and frequencies to a dataframe\n",
    "        df1 = pd.DataFrame.from_dict(dict_from_list.items())    \n",
    "        # Combine frequency df and amount df\n",
    "        df1 = pd.concat([df1, df_amount], axis=1)       \n",
    "        df1['year'] = year\n",
    "        df = pd.concat([df, df1], sort= False)\n",
    "    df.set_axis(['phrase', 'frequency', 'amount', 'year'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_ML = df_for_ML(10, '2020', '2019', '2018','2017','2016','2015', '2014')\n",
    "df_for_ML = df_for_ML[~(df_for_ML == 0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_ML.sort_values(by=['phrase'], ascending=False).to_csv('csv_data/df_for_ml.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_for_ML.groupby(['phrase']).sum().sort_values(by=['frequency'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_for_cluster = df_for_ML(20, '2020', '2019', '2018','2017','2016','2015','2014','2013','2012')\n",
    "# df_for_cluster.groupby(['phrase']).sum().sort_values(by=['frequency'], ascending=False).to_csv('DataFrame/df_for_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
